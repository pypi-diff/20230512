# Comparing `tmp/timm-0.8.6.dev0.tar.gz` & `tmp/timm-0.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "timm-0.8.6.dev0.tar", last modified: Thu Jan 12 05:34:02 2023, max compression
+gzip compressed data, was "timm-0.9.0.tar", last modified: Thu May 11 22:53:15 2023, max compression
```

## Comparing `timm-0.8.6.dev0.tar` & `timm-0.9.0.tar`

### file list

```diff
@@ -1,237 +1,272 @@
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.058302 timm-0.8.6.dev0/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11343 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/LICENSE
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       34 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/MANIFEST.in
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    48089 2023-01-12 05:34:02.058302 timm-0.8.6.dev0/PKG-INFO
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    46956 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/README.md
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      126 2023-01-12 05:34:02.058302 timm-0.8.6.dev0/setup.cfg
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1930 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/setup.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.014302 timm-0.8.6.dev0/timm/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      292 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/__init__.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.014302 timm-0.8.6.dev0/timm/data/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      672 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    35635 2023-01-05 22:31:53.000000 timm-0.8.6.dev0/timm/data/auto_augment.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3350 2022-12-05 18:23:23.000000 timm-0.8.6.dev0/timm/data/config.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      442 2022-12-05 18:23:23.000000 timm-0.8.6.dev0/timm/data/constants.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5768 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/dataset.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6923 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/data/dataset_factory.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5540 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/data/distributed_sampler.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11683 2022-12-05 18:23:23.000000 timm-0.8.6.dev0/timm/data/loader.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14722 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/data/mixup.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4964 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/random_erasing.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.018302 timm-0.8.6.dev0/timm/data/readers/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       72 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      895 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/data/readers/class_map.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1482 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/img_extensions.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      487 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/reader.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1364 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/data/readers/reader_factory.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2431 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/data/readers/reader_hfds.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3315 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/reader_image_folder.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9182 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/reader_image_in_tar.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2644 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/reader_image_tar.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17529 2022-12-05 18:23:23.000000 timm-0.8.6.dev0/timm/data/readers/reader_tfds.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16328 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/reader_wds.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      303 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/data/readers/shared_count.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1590 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/data/real_labels.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9169 2022-12-05 18:23:23.000000 timm-0.8.6.dev0/timm/data/tf_preprocessing.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11992 2022-12-05 18:23:23.000000 timm-0.8.6.dev0/timm/data/transforms.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9840 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/data/transforms_factory.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.030302 timm-0.8.6.dev0/timm/layers/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3081 2023-01-05 22:31:53.000000 timm-0.8.6.dev0/timm/layers/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4468 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/activations.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2529 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/activations_jit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5886 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/activations_me.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3890 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/adaptive_avgmax_pool.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4934 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/attention_pool2d.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1594 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/blur_pool.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6895 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/bottleneck_attn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4426 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/cbam.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2320 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/classifier.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5199 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/cond_conv2d.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3069 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/config.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1490 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/conv2d_same.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3188 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/conv_bn_act.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5294 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/create_act.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3514 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/create_attn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1622 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/create_conv2d.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1814 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/create_norm.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3748 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/create_norm_act.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6872 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/drop.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6386 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/eca.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13862 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/evo_norm.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2426 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/fast_norm.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2540 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/filter_response_norm.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3824 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/gather_excite.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2445 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/global_context.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1319 2023-01-05 22:31:53.000000 timm-0.8.6.dev0/timm/layers/grn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10662 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/halo_attn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1053 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/helpers.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3374 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/inplace_abn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5941 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/lambda_layer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      743 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/linear.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1737 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/median_pool.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1843 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/mixed_conv2d.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7008 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/ml_decoder.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6277 2023-01-05 22:31:53.000000 timm-0.8.6.dev0/timm/layers/mlp.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6218 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/non_local_attn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4519 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/norm.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10397 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/norm_act.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2167 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/padding.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6371 2022-12-24 22:38:03.000000 timm-0.8.6.dev0/timm/layers/patch_embed.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3045 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/pool2d_same.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1644 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/pos_embed.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11619 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/pos_embed_rel.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7316 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/pos_embed_sincos.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5387 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/selective_kernel.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2620 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/separable_conv.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1750 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/space_to_depth.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3076 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/split_attn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3441 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/split_batchnorm.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4327 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/squeeze_excite.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5887 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/std_conv.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1996 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/test_time_pool.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      335 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/trace_utils.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4765 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/layers/weight_init.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.030302 timm-0.8.6.dev0/timm/loss/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      245 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/loss/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3343 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/loss/asymmetric_loss.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2030 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/loss/binary_cross_entropy.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1145 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/loss/cross_entropy.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1595 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/loss/jsd.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.050302 timm-0.8.6.dev0/timm/models/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2823 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17170 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_builder.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12148 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_efficientnet_blocks.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    19879 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_efficientnet_builder.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4467 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_factory.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12234 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_features.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4345 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_features_fx.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4938 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_helpers.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10107 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/_hub.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10086 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_manipulate.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4365 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_pretrained.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4178 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_prune.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9007 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/_registry.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    23325 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/beit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18309 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/byoanet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    66406 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/byobnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16229 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/cait.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    27989 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/coat.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14667 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/convit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4433 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/convmixer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    38533 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/convnext.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    23354 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/crossvit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    39245 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/cspnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    19845 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/deit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16375 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/densenet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18951 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/dla.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13911 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/dpn.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21517 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/edgenext.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18546 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/efficientformer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    96066 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/efficientnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      150 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/factory.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      151 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/features.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      154 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/fx_features.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21273 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/gcvit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10098 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/ghostnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11322 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/gluon_resnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9255 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/gluon_xception.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8115 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/hardcorenas.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      223 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/helpers.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    30236 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/hrnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      145 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/hub.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13508 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/inception_resnet_v2.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18301 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/inception_v3.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11220 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/inception_v4.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.050302 timm-0.8.6.dev0/timm/models/layers/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3432 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/layers/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    22497 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/levit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    79967 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/maxxvit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    26864 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/mlp_mixer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    29987 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/mobilenetv3.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    27501 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/mobilevit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    34836 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/mvitv2.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    26534 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/nasnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    20207 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/nest.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    39482 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/nfnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13963 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/pit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15346 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/pnasnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11257 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/poolformer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16851 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/pvt_v2.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      151 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/registry.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    33921 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/regnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8034 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/res2net.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10221 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/resnest.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    75381 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/resnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    33117 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/resnetv2.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9973 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/rexnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13561 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/selecsls.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17993 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/senet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14970 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/sequencer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8914 2023-01-05 22:31:53.000000 timm-0.8.6.dev0/timm/models/sknet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    29383 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/swin_transformer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    31576 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/swin_transformer_v2.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    41530 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/swin_transformer_v2_cr.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12352 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/tnt.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12782 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/tresnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18273 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/twins.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10988 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/vgg.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16634 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/visformer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    72952 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/vision_transformer.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14496 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/vision_transformer_hybrid.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21513 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/vision_transformer_relpos.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    28244 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/volo.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15639 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/models/vovnet.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7901 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/xception.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13727 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/xception_aligned.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    36901 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/models/xcit.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.054302 timm-0.8.6.dev0/timm/optim/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      507 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/optim/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9827 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/adabelief.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7459 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/adafactor.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6535 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/adahessian.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3574 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/adamp.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5147 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/adamw.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5071 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/optim/adan.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9184 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/lamb.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5256 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/lars.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2463 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/lookahead.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6893 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/madgrad.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3871 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/nadam.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4856 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/nvnovograd.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12682 2022-12-24 00:11:50.000000 timm-0.8.6.dev0/timm/optim/optim_factory.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3468 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/radam.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6143 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/rmsprop_tf.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2296 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/optim/sgdp.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.054302 timm-0.8.6.dev0/timm/scheduler/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      330 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3843 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/cosine_lr.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1930 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/multistep_lr.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3573 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/plateau_lr.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3673 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/poly_lr.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5408 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/scheduler.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6622 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/scheduler_factory.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1732 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/step_lr.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3607 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/scheduler/tanh_lr.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.058302 timm-0.8.6.dev0/timm/utils/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      764 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/utils/__init__.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1624 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/agc.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6133 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/checkpoint_saver.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      796 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/clip_grad.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1703 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/cuda.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1762 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/decay_batch.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4256 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/utils/distributed.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2203 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/jit.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1015 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/log.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      901 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/metrics.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1105 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/utils/misc.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12085 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/model.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5670 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/model_ema.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      178 2022-10-03 21:35:12.000000 timm-0.8.6.dev0/timm/utils/random.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1300 2022-11-23 22:16:11.000000 timm-0.8.6.dev0/timm/utils/summary.py
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       26 2023-01-12 05:33:28.000000 timm-0.8.6.dev0/timm/version.py
-drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-01-12 05:34:02.014302 timm-0.8.6.dev0/timm.egg-info/
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    48089 2023-01-12 05:34:01.000000 timm-0.8.6.dev0/timm.egg-info/PKG-INFO
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5667 2023-01-12 05:34:01.000000 timm-0.8.6.dev0/timm.egg-info/SOURCES.txt
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)        1 2023-01-12 05:34:01.000000 timm-0.8.6.dev0/timm.egg-info/dependency_links.txt
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       46 2023-01-12 05:34:01.000000 timm-0.8.6.dev0/timm.egg-info/requires.txt
--rw-rw-r--   0 wiggs     (1000) wiggs     (1000)        5 2023-01-12 05:34:01.000000 timm-0.8.6.dev0/timm.egg-info/top_level.txt
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.214532 timm-0.9.0/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11343 2023-02-24 22:35:42.000000 timm-0.9.0/LICENSE
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       95 2023-03-31 00:49:24.000000 timm-0.9.0/MANIFEST.in
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    68314 2023-05-11 22:53:15.214532 timm-0.9.0/PKG-INFO
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    67196 2023-05-11 22:52:53.000000 timm-0.9.0/README.md
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      551 2023-03-31 00:49:24.000000 timm-0.9.0/pyproject.toml
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      126 2023-05-11 22:53:15.214532 timm-0.9.0/setup.cfg
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1935 2023-03-31 00:49:24.000000 timm-0.9.0/setup.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.154531 timm-0.9.0/timm/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      292 2023-03-31 00:49:24.000000 timm-0.9.0/timm/__init__.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.158532 timm-0.9.0/timm/data/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      819 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/__init__.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.174532 timm-0.9.0/timm/data/_info/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   118210 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet12k_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   218430 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet21k_goog_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    64070 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet21k_goog_to_12k_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   119937 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet21k_goog_to_22k_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   112210 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet21k_miil_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   104500 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet21k_miil_w21_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   218410 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet22k_ms_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    63625 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet22k_ms_to_12k_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   119938 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet22k_ms_to_22k_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   218410 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet22k_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    64070 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet22k_to_12k_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      774 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_a_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2000 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_a_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      769 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_r_indices.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2000 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_r_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   388478 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_real_labels.json
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)  1748917 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_synset_to_definition.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   741457 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_synset_to_lemma.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10000 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/_info/imagenet_synsets.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    35550 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/auto_augment.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4531 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/config.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      442 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/constants.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5833 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/dataset.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6989 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/dataset_factory.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2391 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/dataset_info.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5540 2023-02-24 22:35:42.000000 timm-0.9.0/timm/data/distributed_sampler.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4167 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/imagenet_info.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11849 2023-04-28 20:49:14.000000 timm-0.9.0/timm/data/loader.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14640 2023-05-10 22:09:19.000000 timm-0.9.0/timm/data/mixup.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4964 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/random_erasing.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.174532 timm-0.9.0/timm/data/readers/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       72 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      895 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/class_map.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1482 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/img_extensions.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      487 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1364 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_factory.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2431 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_hfds.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3315 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_image_folder.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9182 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_image_in_tar.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2644 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_image_tar.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17858 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_tfds.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16724 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/reader_wds.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      303 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/readers/shared_count.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1800 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/real_labels.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9169 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/tf_preprocessing.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11992 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/transforms.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9840 2023-03-31 00:49:24.000000 timm-0.9.0/timm/data/transforms_factory.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.186532 timm-0.9.0/timm/layers/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3611 2023-05-10 14:38:22.000000 timm-0.9.0/timm/layers/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4468 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/activations.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2529 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/activations_jit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5886 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/activations_me.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6310 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/adaptive_avgmax_pool.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4934 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/attention_pool2d.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1594 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/blur_pool.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6895 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/bottleneck_attn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4426 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/cbam.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7486 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/classifier.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5199 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/cond_conv2d.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4175 2023-04-12 15:57:13.000000 timm-0.9.0/timm/layers/config.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3216 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/conv2d_same.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3836 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/conv_bn_act.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5320 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/create_act.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3514 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/create_attn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1622 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/create_conv2d.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1740 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/create_norm.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3748 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/create_norm_act.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6872 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/drop.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6386 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/eca.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13862 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/evo_norm.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4008 2023-05-10 14:38:22.000000 timm-0.9.0/timm/layers/fast_norm.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2540 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/filter_response_norm.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1109 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/format.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3824 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/gather_excite.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2445 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/global_context.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1319 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/grn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10662 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/halo_attn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1053 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/helpers.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3374 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/inplace_abn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5941 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/lambda_layer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      743 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/linear.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1737 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/median_pool.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1843 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/mixed_conv2d.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7008 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/ml_decoder.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8470 2023-05-10 14:38:22.000000 timm-0.9.0/timm/layers/mlp.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6218 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/non_local_attn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6040 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/norm.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17418 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/norm_act.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2877 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/padding.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1741 2023-04-12 15:57:13.000000 timm-0.9.0/timm/layers/patch_dropout.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8516 2023-05-10 14:38:22.000000 timm-0.9.0/timm/layers/patch_embed.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3045 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/pool2d_same.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1678 2023-05-10 14:38:22.000000 timm-0.9.0/timm/layers/pos_embed.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11764 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/pos_embed_rel.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14314 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/pos_embed_sincos.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5387 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/selective_kernel.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2620 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/separable_conv.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1775 2023-04-28 20:49:14.000000 timm-0.9.0/timm/layers/space_to_depth.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3076 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/split_attn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3441 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/split_batchnorm.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4327 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/squeeze_excite.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5887 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/std_conv.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1996 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/test_time_pool.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      335 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/trace_utils.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4765 2023-03-31 00:49:24.000000 timm-0.9.0/timm/layers/weight_init.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.186532 timm-0.9.0/timm/loss/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      245 2023-02-24 22:35:42.000000 timm-0.9.0/timm/loss/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3343 2023-03-31 00:49:24.000000 timm-0.9.0/timm/loss/asymmetric_loss.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2030 2023-02-24 22:35:42.000000 timm-0.9.0/timm/loss/binary_cross_entropy.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1145 2023-02-24 22:35:42.000000 timm-0.9.0/timm/loss/cross_entropy.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1595 2023-02-24 22:35:42.000000 timm-0.9.0/timm/loss/jsd.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.206532 timm-0.9.0/timm/models/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3020 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18037 2023-05-10 22:09:19.000000 timm-0.9.0/timm/models/_builder.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12148 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_efficientnet_blocks.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    19879 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_efficientnet_builder.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5069 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/_factory.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15488 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_features.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4743 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_features_fx.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5940 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_helpers.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15370 2023-04-28 20:49:14.000000 timm-0.9.0/timm/models/_hub.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10503 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_manipulate.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3525 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/_pretrained.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4171 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_prune.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.206532 timm-0.9.0/timm/models/_pruned/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8734 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_pruned/ecaresnet101d_pruned.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4520 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_pruned/ecaresnet50d_pruned.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18596 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_pruned/efficientnet_b1_pruned.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18676 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_pruned/efficientnet_b2_pruned.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21133 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/_pruned/efficientnet_b3_pruned.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13506 2023-04-06 15:39:02.000000 timm-0.9.0/timm/models/_registry.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    23963 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/beit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18985 2023-05-11 22:52:53.000000 timm-0.9.0/timm/models/byoanet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    68705 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/byobnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18106 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/cait.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    29712 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/coat.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15301 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/convit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4473 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/convmixer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    49685 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/convnext.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    24271 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/crossvit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    40028 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/cspnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    23064 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/davit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18657 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/deit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15634 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/densenet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18599 2023-05-10 22:09:19.000000 timm-0.9.0/timm/models/dla.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13621 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/dpn.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21277 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/edgenext.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18911 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/efficientformer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    24873 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/efficientformer_v2.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)   101929 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/efficientnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    39672 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/eva.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      150 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/factory.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      151 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/features.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    24033 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/focalnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      154 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/fx_features.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21393 2023-05-10 22:09:19.000000 timm-0.9.0/timm/models/gcvit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    10473 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/ghostnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7697 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/hardcorenas.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      223 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/helpers.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    32902 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/hrnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      145 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/hub.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12077 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/inception_resnet_v2.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17104 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/inception_v3.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11076 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/inception_v4.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.206532 timm-0.9.0/timm/models/layers/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3432 2023-05-11 22:52:53.000000 timm-0.9.0/timm/models/layers/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    32443 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/levit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    84448 2023-05-11 22:52:53.000000 timm-0.9.0/timm/models/maxxvit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    35028 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/metaformer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    24231 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/mlp_mixer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    30795 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/mobilenetv3.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    25734 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/mobilevit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    35530 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/mvitv2.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    26627 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/nasnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    21489 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/nest.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    41085 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/nfnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14946 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/pit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15389 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/pnasnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17261 2023-05-10 22:09:19.000000 timm-0.9.0/timm/models/pvt_v2.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      151 2023-03-31 00:49:24.000000 timm-0.9.0/timm/models/registry.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    44020 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/regnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7691 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/res2net.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9635 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/resnest.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    91012 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/resnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    30450 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/resnetv2.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11918 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/rexnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13254 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/selecsls.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18165 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/senet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    17254 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/sequencer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8777 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/sknet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    31900 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/swin_transformer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    33196 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/swin_transformer_v2.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    41811 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/swin_transformer_v2_cr.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    13362 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/tnt.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    12893 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/tresnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18988 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/twins.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    11004 2023-04-28 20:49:14.000000 timm-0.9.0/timm/models/vgg.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    18957 2023-05-11 22:52:53.000000 timm-0.9.0/timm/models/visformer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    91238 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/vision_transformer.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    16265 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/vision_transformer_hybrid.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    23853 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/vision_transformer_relpos.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    30354 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/volo.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15477 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/vovnet.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     8098 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/xception.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    14086 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/xception_aligned.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    37617 2023-05-10 14:38:22.000000 timm-0.9.0/timm/models/xcit.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.210532 timm-0.9.0/timm/optim/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      507 2023-03-31 00:49:24.000000 timm-0.9.0/timm/optim/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9827 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/adabelief.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7459 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/adafactor.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6535 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/adahessian.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3574 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/adamp.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5147 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/adamw.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5071 2023-03-31 00:49:24.000000 timm-0.9.0/timm/optim/adan.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9184 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/lamb.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5256 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/lars.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     7099 2023-03-31 00:49:24.000000 timm-0.9.0/timm/optim/lion.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2463 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/lookahead.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6893 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/madgrad.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3871 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/nadam.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4856 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/nvnovograd.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    15280 2023-05-10 14:38:22.000000 timm-0.9.0/timm/optim/optim_factory.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3468 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/radam.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6143 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/rmsprop_tf.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2296 2023-02-24 22:35:42.000000 timm-0.9.0/timm/optim/sgdp.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.210532 timm-0.9.0/timm/scheduler/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      330 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3843 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/cosine_lr.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1930 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/multistep_lr.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3573 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/plateau_lr.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3673 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/poly_lr.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5408 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/scheduler.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6622 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/scheduler_factory.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1732 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/step_lr.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3607 2023-03-31 00:49:24.000000 timm-0.9.0/timm/scheduler/tanh_lr.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.214532 timm-0.9.0/timm/utils/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      764 2023-03-31 00:49:24.000000 timm-0.9.0/timm/utils/__init__.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1624 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/agc.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6133 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/checkpoint_saver.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      796 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/clip_grad.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2029 2023-04-28 20:49:14.000000 timm-0.9.0/timm/utils/cuda.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1762 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/decay_batch.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     4256 2023-03-31 00:49:24.000000 timm-0.9.0/timm/utils/distributed.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     2203 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/jit.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1015 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/log.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      901 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/metrics.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1105 2023-03-31 00:49:24.000000 timm-0.9.0/timm/utils/misc.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     9848 2023-03-31 00:49:24.000000 timm-0.9.0/timm/utils/model.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     5670 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/model_ema.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     3342 2023-04-28 20:49:14.000000 timm-0.9.0/timm/utils/onnx.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)      178 2023-02-24 22:35:42.000000 timm-0.9.0/timm/utils/random.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     1300 2023-03-31 00:49:24.000000 timm-0.9.0/timm/utils/summary.py
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       22 2023-05-11 22:52:53.000000 timm-0.9.0/timm/version.py
+drwxrwxr-x   0 wiggs     (1000) wiggs     (1000)        0 2023-05-11 22:53:15.154531 timm-0.9.0/timm.egg-info/
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)    68314 2023-05-11 22:53:15.000000 timm-0.9.0/timm.egg-info/PKG-INFO
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)     6937 2023-05-11 22:53:15.000000 timm-0.9.0/timm.egg-info/SOURCES.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)        1 2023-05-11 22:53:15.000000 timm-0.9.0/timm.egg-info/dependency_links.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)       58 2023-05-11 22:53:15.000000 timm-0.9.0/timm.egg-info/requires.txt
+-rw-rw-r--   0 wiggs     (1000) wiggs     (1000)        5 2023-05-11 22:53:15.000000 timm-0.9.0/timm.egg-info/top_level.txt
```

### Comparing `timm-0.8.6.dev0/LICENSE` & `timm-0.9.0/LICENSE`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/setup.py` & `timm-0.9.0/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,42 +10,42 @@
 with open(path.join(here, 'README.md'), encoding='utf-8') as f:
     long_description = f.read()
 
 exec(open('timm/version.py').read())
 setup(
     name='timm',
     version=__version__,
-    description='(Unofficial) PyTorch Image Models',
+    description='PyTorch Image Models',
     long_description=long_description,
     long_description_content_type='text/markdown',
-    url='https://github.com/rwightman/pytorch-image-models',
+    url='https://github.com/huggingface/pytorch-image-models',
     author='Ross Wightman',
-    author_email='hello@rwightman.com',
+    author_email='ross@huggingface.co',
     classifiers=[
         # How mature is this project? Common values are
         #   3 - Alpha
         #   4 - Beta
         #   5 - Production/Stable
         'Development Status :: 4 - Beta',
         'Intended Audience :: Education',
         'Intended Audience :: Science/Research',
         'License :: OSI Approved :: Apache Software License',
-        'Programming Language :: Python :: 3.6',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
         'Programming Language :: Python :: 3.9',
         'Programming Language :: Python :: 3.10',
+        'Programming Language :: Python :: 3.11',
         'Topic :: Scientific/Engineering',
         'Topic :: Scientific/Engineering :: Artificial Intelligence',
         'Topic :: Software Development',
         'Topic :: Software Development :: Libraries',
         'Topic :: Software Development :: Libraries :: Python Modules',
     ],
 
     # Note that this is a string of words separated by whitespace, not a list.
     keywords='pytorch pretrained models efficientnet mobilenetv3 mnasnet resnet vision transformer vit',
     packages=find_packages(exclude=['convert', 'tests', 'results']),
     include_package_data=True,
-    install_requires=['torch >= 1.7', 'torchvision', 'pyyaml', 'huggingface_hub'],
-    python_requires='>=3.6',
+    install_requires=['torch >= 1.7', 'torchvision', 'pyyaml', 'huggingface_hub', 'safetensors'],
+    python_requires='>=3.7',
 )
```

### Comparing `timm-0.8.6.dev0/timm/data/__init__.py` & `timm-0.9.0/timm/data/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 from .auto_augment import RandAugment, AutoAugment, rand_augment_ops, auto_augment_policy,\
     rand_augment_transform, auto_augment_transform
-from .config import resolve_data_config
+from .config import resolve_data_config, resolve_model_data_config
 from .constants import *
 from .dataset import ImageDataset, IterableImageDataset, AugMixDataset
 from .dataset_factory import create_dataset
+from .dataset_info import DatasetInfo, CustomDatasetInfo
+from .imagenet_info import ImageNetInfo, infer_imagenet_subset
 from .loader import create_loader
 from .mixup import Mixup, FastCollateMixup
 from .readers import create_reader
 from .readers import get_img_extensions, is_img_extension, set_img_extensions, add_img_extensions, del_img_extensions
 from .real_labels import RealLabelsImagenet
 from .transforms import *
 from .transforms_factory import create_transform
```

### Comparing `timm-0.8.6.dev0/timm/data/auto_augment.py` & `timm-0.9.0/timm/data/auto_augment.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,16 +50,15 @@
     _DEFAULT_INTERPOLATION = Image.BICUBIC
 
 
 def _interpolation(kwargs):
     interpolation = kwargs.pop('resample', _DEFAULT_INTERPOLATION)
     if isinstance(interpolation, (list, tuple)):
         return random.choice(interpolation)
-    else:
-        return interpolation
+    return interpolation
 
 
 def _check_args_tf(kwargs):
     if 'fillcolor' in kwargs and _PIL_VER < (5, 0):
         kwargs.pop('fillcolor')
     kwargs['resample'] = _interpolation(kwargs)
 
@@ -96,15 +95,15 @@
     return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)
 
 
 def rotate(img, degrees, **kwargs):
     _check_args_tf(kwargs)
     if _PIL_VER >= (5, 2):
         return img.rotate(degrees, **kwargs)
-    elif _PIL_VER >= (5, 0):
+    if _PIL_VER >= (5, 0):
         w, h = img.size
         post_trans = (0, 0)
         rotn_center = (w / 2.0, h / 2.0)
         angle = -math.radians(degrees)
         matrix = [
             round(math.cos(angle), 15),
             round(math.sin(angle), 15),
@@ -120,16 +119,15 @@
 
         matrix[2], matrix[5] = transform(
             -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix
         )
         matrix[2] += rotn_center[0]
         matrix[5] += rotn_center[1]
         return img.transform(img.size, Image.AFFINE, matrix, **kwargs)
-    else:
-        return img.rotate(degrees, resample=kwargs['resample'])
+    return img.rotate(degrees, resample=kwargs['resample'])
 
 
 def auto_contrast(img, **__):
     return ImageOps.autocontrast(img)
 
 
 def invert(img, **__):
@@ -147,20 +145,21 @@
 def solarize_add(img, add, thresh=128, **__):
     lut = []
     for i in range(256):
         if i < thresh:
             lut.append(min(255, i + add))
         else:
             lut.append(i)
+
     if img.mode in ("L", "RGB"):
         if img.mode == "RGB" and len(lut) == 256:
             lut = lut + lut + lut
         return img.point(lut)
-    else:
-        return img
+
+    return img
 
 
 def posterize(img, bits_to_keep, **__):
     if bits_to_keep >= 8:
         return img
     return ImageOps.posterize(img, bits_to_keep)
 
@@ -222,15 +221,15 @@
     level = (level / _LEVEL_DENOM) * .9
     level = max(0.1, 1.0 + _randomly_negate(level))  # keep it >= 0.1
     return level,
 
 
 def _minmax_level_to_arg(level, _hparams, min_val=0., max_val=1.0, clamp=True):
     level = (level / _LEVEL_DENOM)
-    min_val + (max_val - min_val) * level
+    level = min_val + (max_val - min_val) * level
     if clamp:
         level = max(min_val, min(max_val, level))
     return level,
 
 
 def _shear_level_to_arg(level, _hparams):
     # range [-0.3, 0.3]
@@ -548,39 +547,38 @@
     return pc
 
 
 def auto_augment_policy(name='v0', hparams=None):
     hparams = hparams or _HPARAMS_DEFAULT
     if name == 'original':
         return auto_augment_policy_original(hparams)
-    elif name == 'originalr':
+    if name == 'originalr':
         return auto_augment_policy_originalr(hparams)
-    elif name == 'v0':
+    if name == 'v0':
         return auto_augment_policy_v0(hparams)
-    elif name == 'v0r':
+    if name == 'v0r':
         return auto_augment_policy_v0r(hparams)
-    elif name == '3a':
+    if name == '3a':
         return auto_augment_policy_3a(hparams)
-    else:
-        assert False, 'Unknown AA policy (%s)' % name
+    assert False, f'Unknown AA policy {name}'
 
 
 class AutoAugment:
 
     def __init__(self, policy):
         self.policy = policy
 
     def __call__(self, img):
         sub_policy = random.choice(self.policy)
         for op in sub_policy:
             img = op(img)
         return img
 
     def __repr__(self):
-        fs = self.__class__.__name__ + f'(policy='
+        fs = self.__class__.__name__ + '(policy='
         for p in self.policy:
             fs += '\n\t['
             fs += ', '.join([str(op) for op in p])
             fs += ']'
         fs += ')'
         return fs
 
@@ -632,15 +630,15 @@
     'Contrast',
     'Brightness',
     'Sharpness',
     'ShearX',
     'ShearY',
     'TranslateXRel',
     'TranslateYRel',
-    #'Cutout'  # NOTE I've implement this as random erasing separately
+    # 'Cutout'  # NOTE I've implement this as random erasing separately
 ]
 
 
 _RAND_INCREASING_TRANSFORMS = [
     'AutoContrast',
     'Equalize',
     'Invert',
@@ -652,26 +650,26 @@
     'ContrastIncreasing',
     'BrightnessIncreasing',
     'SharpnessIncreasing',
     'ShearX',
     'ShearY',
     'TranslateXRel',
     'TranslateYRel',
-    #'Cutout'  # NOTE I've implement this as random erasing separately
+    # 'Cutout'  # NOTE I've implement this as random erasing separately
 ]
 
 
 _RAND_3A = [
     'SolarizeIncreasing',
     'Desaturate',
     'GaussianBlur',
 ]
 
 
-_RAND_CHOICE_3A = {
+_RAND_WEIGHTED_3A = {
     'SolarizeIncreasing': 6,
     'Desaturate': 6,
     'GaussianBlur': 6,
     'Rotate': 3,
     'ShearX': 2,
     'ShearY': 2,
     'PosterizeIncreasing': 1,
@@ -683,15 +681,15 @@
     'Equalize': 1,
     'Invert': 1,
 }
 
 
 # These experimental weights are based loosely on the relative improvements mentioned in paper.
 # They may not result in increased performance, but could likely be tuned to so.
-_RAND_CHOICE_WEIGHTS_0 = {
+_RAND_WEIGHTED_0 = {
     'Rotate': 3,
     'ShearX': 2,
     'ShearY': 2,
     'TranslateXRel': 1,
     'TranslateYRel': 1,
     'ColorIncreasing': .25,
     'SharpnessIncreasing': 0.25,
@@ -711,21 +709,20 @@
     probs = np.array(probs)
     probs = probs / np.sum(probs)
     return transforms, probs
 
 
 def rand_augment_choices(name: str, increasing=True):
     if name == 'weights':
-        return _RAND_CHOICE_WEIGHTS_0
-    elif name == '3aw':
-        return _RAND_CHOICE_3A
-    elif name == '3a':
+        return _RAND_WEIGHTED_0
+    if name == '3aw':
+        return _RAND_WEIGHTED_3A
+    if name == '3a':
         return _RAND_3A
-    else:
-        return _RAND_INCREASING_TRANSFORMS if increasing else _RAND_TRANSFORMS
+    return _RAND_INCREASING_TRANSFORMS if increasing else _RAND_TRANSFORMS
 
 
 def rand_augment_ops(
         magnitude: Union[int, float] = 10,
         prob: float = 0.5,
         hparams: Optional[Dict] = None,
         transforms: Optional[Union[Dict, List]] = None,
```

### Comparing `timm-0.8.6.dev0/timm/data/dataset.py` & `timm-0.9.0/timm/data/dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -84,28 +84,30 @@
 class IterableImageDataset(data.IterableDataset):
 
     def __init__(
             self,
             root,
             reader=None,
             split='train',
+            class_map=None,
             is_training=False,
             batch_size=None,
             seed=42,
             repeats=0,
             download=False,
             transform=None,
             target_transform=None,
     ):
         assert reader is not None
         if isinstance(reader, str):
             self.reader = create_reader(
                 reader,
                 root=root,
                 split=split,
+                class_map=class_map,
                 is_training=is_training,
                 batch_size=batch_size,
                 seed=seed,
                 repeats=repeats,
                 download=download,
             )
         else:
```

### Comparing `timm-0.8.6.dev0/timm/data/dataset_factory.py` & `timm-0.9.0/timm/data/dataset_factory.py`

 * *Files 3% similar despite different names*

```diff
@@ -153,26 +153,28 @@
         # There will be a IterableDataset variant too, TBD
         ds = ImageDataset(root, reader=name, split=split, class_map=class_map, **kwargs)
     elif name.startswith('tfds/'):
         ds = IterableImageDataset(
             root,
             reader=name,
             split=split,
+            class_map=class_map,
             is_training=is_training,
             download=download,
             batch_size=batch_size,
             repeats=repeats,
             seed=seed,
             **kwargs
         )
     elif name.startswith('wds/'):
         ds = IterableImageDataset(
             root,
             reader=name,
             split=split,
+            class_map=class_map,
             is_training=is_training,
             batch_size=batch_size,
             repeats=repeats,
             seed=seed,
             **kwargs
         )
     else:
```

### Comparing `timm-0.8.6.dev0/timm/data/distributed_sampler.py` & `timm-0.9.0/timm/data/distributed_sampler.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/loader.py` & `timm-0.9.0/timm/data/loader.py`

 * *Files 3% similar despite different names*

```diff
@@ -312,20 +312,23 @@
 
 
 class MultiEpochsDataLoader(torch.utils.data.DataLoader):
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._DataLoader__initialized = False
-        self.batch_sampler = _RepeatSampler(self.batch_sampler)
+        if self.batch_sampler is None:
+            self.sampler = _RepeatSampler(self.sampler)
+        else:
+            self.batch_sampler = _RepeatSampler(self.batch_sampler)
         self._DataLoader__initialized = True
         self.iterator = super().__iter__()
 
     def __len__(self):
-        return len(self.batch_sampler.sampler)
+        return len(self.sampler) if self.batch_sampler is None else len(self.batch_sampler.sampler)
 
     def __iter__(self):
         for i in range(len(self)):
             yield next(self.iterator)
 
 
 class _RepeatSampler(object):
```

### Comparing `timm-0.8.6.dev0/timm/data/mixup.py` & `timm-0.9.0/timm/data/mixup.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,24 +10,24 @@
 
 Hacked together by / Copyright 2019, Ross Wightman
 """
 import numpy as np
 import torch
 
 
-def one_hot(x, num_classes, on_value=1., off_value=0., device='cuda'):
+def one_hot(x, num_classes, on_value=1., off_value=0.):
     x = x.long().view(-1, 1)
-    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(1, x, on_value)
+    return torch.full((x.size()[0], num_classes), off_value, device=x.device).scatter_(1, x, on_value)
 
 
-def mixup_target(target, num_classes, lam=1., smoothing=0.0, device='cuda'):
+def mixup_target(target, num_classes, lam=1., smoothing=0.0):
     off_value = smoothing / num_classes
     on_value = 1. - smoothing + off_value
-    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value, device=device)
-    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value, device=device)
+    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value)
+    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value)
     return y1 * lam + y2 * (1. - lam)
 
 
 def rand_bbox(img_shape, lam, margin=0., count=None):
     """ Standard CutMix bounding-box
     Generates a random square bbox based on lambda value. This impl includes
     support for enforcing a border margin as percent of bbox dimensions.
@@ -210,15 +210,15 @@
         assert len(x) % 2 == 0, 'Batch size should be even when using this'
         if self.mode == 'elem':
             lam = self._mix_elem(x)
         elif self.mode == 'pair':
             lam = self._mix_pair(x)
         else:
             lam = self._mix_batch(x)
-        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, x.device)
+        target = mixup_target(target, self.num_classes, lam, self.label_smoothing)
         return x, target
 
 
 class FastCollateMixup(Mixup):
     """ Fast Collate w/ Mixup/Cutmix that applies different params to each element or whole batch
 
     A Mixup impl that's performed while collating the batches.
@@ -306,11 +306,11 @@
         if self.mode == 'elem' or self.mode == 'half':
             lam = self._mix_elem_collate(output, batch, half=half)
         elif self.mode == 'pair':
             lam = self._mix_pair_collate(output, batch)
         else:
             lam = self._mix_batch_collate(output, batch)
         target = torch.tensor([b[1] for b in batch], dtype=torch.int64)
-        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, device='cpu')
+        target = mixup_target(target, self.num_classes, lam, self.label_smoothing)
         target = target[:batch_size]
         return output, target
```

### Comparing `timm-0.8.6.dev0/timm/data/random_erasing.py` & `timm-0.9.0/timm/data/random_erasing.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/class_map.py` & `timm-0.9.0/timm/data/readers/class_map.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/img_extensions.py` & `timm-0.9.0/timm/data/readers/img_extensions.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_factory.py` & `timm-0.9.0/timm/data/readers/reader_factory.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_hfds.py` & `timm-0.9.0/timm/data/readers/reader_hfds.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_image_folder.py` & `timm-0.9.0/timm/data/readers/reader_image_folder.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_image_in_tar.py` & `timm-0.9.0/timm/data/readers/reader_image_in_tar.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_image_tar.py` & `timm-0.9.0/timm/data/readers/reader_image_tar.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_tfds.py` & `timm-0.9.0/timm/data/readers/reader_tfds.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,14 +30,15 @@
     # low, high = resource.getrlimit(resource.RLIMIT_NOFILE)
     # resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))
 except ImportError as e:
     print(e)
     print("Please install tensorflow_datasets package `pip install tensorflow-datasets`.")
     exit(1)
 
+from .class_map import load_class_map
 from .reader import Reader
 from .shared_count import SharedCount
 
 
 MAX_TP_SIZE = int(os.environ.get('TFDS_TP_SIZE', 8))  # maximum TF threadpool size, for jpeg decodes and queuing activities
 SHUFFLE_SIZE = int(os.environ.get('TFDS_SHUFFLE_SIZE', 8192))  # samples to shuffle in DS queue
 PREFETCH_SIZE = int(os.environ.get('TFDS_PREFETCH_SIZE', 2048))  # samples to prefetch
@@ -90,14 +91,15 @@
     """
 
     def __init__(
             self,
             root,
             name,
             split='train',
+            class_map=None,
             is_training=False,
             batch_size=None,
             download=False,
             repeats=0,
             seed=42,
             input_name='image',
             input_img_mode='RGB',
@@ -147,15 +149,20 @@
         self.input_img_mode = input_img_mode
         self.target_name = target_name
         self.target_img_mode = target_img_mode
         self.builder = tfds.builder(name, data_dir=root)
         # NOTE: the tfds command line app can be used download & prepare datasets if you don't enable download flag
         if download:
             self.builder.download_and_prepare()
-        self.class_to_idx = get_class_labels(self.builder.info) if self.target_name == 'label' else {}
+        self.remap_class = False
+        if class_map:
+            self.class_to_idx = load_class_map(class_map)
+            self.remap_class = True
+        else:
+            self.class_to_idx = get_class_labels(self.builder.info) if self.target_name == 'label' else {}
         self.split_info = self.builder.info.splits[split]
         self.num_samples = self.split_info.num_examples
 
         # Distributed world state
         self.dist_rank = 0
         self.dist_num_replicas = 1
         if dist.is_available() and dist.is_initialized() and dist.get_world_size() > 1:
@@ -295,14 +302,16 @@
         for sample in self.ds:
             input_data = sample[self.input_name]
             if self.input_img_mode:
                 input_data = Image.fromarray(input_data, mode=self.input_img_mode)
             target_data = sample[self.target_name]
             if self.target_img_mode:
                 target_data = Image.fromarray(target_data, mode=self.target_img_mode)
+            elif self.remap_class:
+                target_data = self.class_to_idx[target_data]
             yield input_data, target_data
             sample_count += 1
             if self.is_training and sample_count >= target_sample_count:
                 # Need to break out of loop when repeat() is enabled for training w/ oversampling
                 # this results in extra samples per epoch but seems more desirable than dropping
                 # up to N*J batches per epoch (where N = num distributed processes, and J = num worker processes)
                 break
```

### Comparing `timm-0.8.6.dev0/timm/data/readers/reader_wds.py` & `timm-0.9.0/timm/data/readers/reader_wds.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,34 +25,35 @@
     from webdataset.filters import _shuffle
     from webdataset.shardlists import expand_urls
     from webdataset.tariterators import base_plus_ext, url_opener, tar_file_expander, valid_sample
 except ImportError:
     wds = None
     expand_urls = None
 
+from .class_map import load_class_map
 from .reader import Reader
 from .shared_count import SharedCount
 
 _logger = logging.getLogger(__name__)
 
 SHUFFLE_SIZE = int(os.environ.get('WDS_SHUFFLE_SIZE', 8192))
 
 
 def _load_info(root, basename='info'):
     info_json = os.path.join(root, basename + '.json')
     info_yaml = os.path.join(root, basename + '.yaml')
     err_str = ''
     try:
-        with wds.gopen.gopen(info_json) as f:
+        with wds.gopen(info_json) as f:
             info_dict = json.load(f)
         return info_dict
     except Exception as e:
         err_str = str(e)
     try:
-        with wds.gopen.gopen(info_yaml) as f:
+        with wds.gopen(info_yaml) as f:
             info_dict = yaml.safe_load(f)
         return info_dict
     except Exception:
         pass
     _logger.warning(
         f'Dataset info file not found at {info_json} or {info_yaml}. Error: {err_str}. '
         'Falling back to provided split and size arg.')
@@ -106,16 +107,16 @@
         else:
             split_info = SplitInfo(
                 name=split_name,
                 num_samples=num_samples,
                 filenames=split_filenames,
             )
     else:
-        if split not in info['splits']:
-            raise RuntimeError(f"split {split} not found in info ({info['splits'].keys()})")
+        if 'splits' not in info or split not in info['splits']:
+            raise RuntimeError(f"split {split} not found in info ({info.get('splits', {}).keys()})")
         split = split
         split_info = info['splits'][split]
         split_info = _info_convert(split_info)
 
     return split_info
 
 
@@ -286,14 +287,15 @@
             root,
             name,
             split,
             is_training=False,
             batch_size=None,
             repeats=0,
             seed=42,
+            class_map=None,
             input_name='jpg',
             input_image='RGB',
             target_name='cls',
             target_image='',
             prefetch_size=None,
             shuffle_size=None,
     ):
@@ -316,14 +318,20 @@
         self.key_ext = '.JPEG'  # extension to add to key for original filenames (DS specific, default ImageNet)
 
         self.info = _load_info(self.root)
         self.split_info = _parse_split_info(split, self.info)
         self.num_samples = self.split_info.num_samples
         if not self.num_samples:
             raise RuntimeError(f'Invalid split definition, no samples found.')
+        self.remap_class = False
+        if class_map:
+            self.class_to_idx = load_class_map(class_map)
+            self.remap_class = True
+        else:
+            self.class_to_idx = {}
 
         # Distributed world state
         self.dist_rank = 0
         self.dist_num_replicas = 1
         if dist.is_available() and dist.is_initialized() and dist.get_world_size() > 1:
             self.dist_rank = dist.get_rank()
             self.dist_num_replicas = dist.get_world_size()
@@ -427,15 +435,18 @@
             ds = self.ds.with_epoch(num_worker_samples)
         else:
             ds = self.ds
 
         i = 0
         # _logger.info(f'start {i}, {self.worker_id}')  # FIXME temporary debug
         for sample in ds:
-            yield sample[self.image_key], sample[self.target_key]
+            target = sample[self.target_key]
+            if self.remap_class:
+                target = self.class_to_idx[target]
+            yield sample[self.image_key], target
             i += 1
         # _logger.info(f'end {i}, {self.worker_id}')  # FIXME temporary debug
 
     def __len__(self):
         num_samples = self._num_samples_per_worker() * self.num_workers
         return num_samples
```

### Comparing `timm-0.8.6.dev0/timm/data/real_labels.py` & `timm-0.9.0/timm/data/real_labels.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,22 +3,27 @@
 Based on Numpy example at https://github.com/google-research/reassessed-imagenet
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import os
 import json
 import numpy as np
+import pkgutil
 
 
 class RealLabelsImagenet:
 
-    def __init__(self, filenames, real_json='real.json', topk=(1, 5)):
-        with open(real_json) as real_labels:
-            real_labels = json.load(real_labels)
-            real_labels = {f'ILSVRC2012_val_{i + 1:08d}.JPEG': labels for i, labels in enumerate(real_labels)}
+    def __init__(self, filenames, real_json=None, topk=(1, 5)):
+        if real_json is not None:
+            with open(real_json) as real_labels:
+                real_labels = json.load(real_labels)
+        else:
+            real_labels = json.loads(
+                pkgutil.get_data(__name__, os.path.join('_info', 'imagenet_real_labels.json')).decode('utf-8'))
+        real_labels = {f'ILSVRC2012_val_{i + 1:08d}.JPEG': labels for i, labels in enumerate(real_labels)}
         self.real_labels = real_labels
         self.filenames = filenames
         assert len(self.filenames) == len(self.real_labels)
         self.topk = topk
         self.is_correct = {k: [] for k in topk}
         self.sample_idx = 0
```

### Comparing `timm-0.8.6.dev0/timm/data/tf_preprocessing.py` & `timm-0.9.0/timm/data/tf_preprocessing.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/transforms.py` & `timm-0.9.0/timm/data/transforms.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/data/transforms_factory.py` & `timm-0.9.0/timm/data/transforms_factory.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/__init__.py` & `timm-0.9.0/timm/layers/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,49 +1,53 @@
 from .activations import *
 from .adaptive_avgmax_pool import \
     adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d
 from .attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding
 from .blur_pool import BlurPool2d
-from .classifier import ClassifierHead, create_classifier
+from .classifier import ClassifierHead, create_classifier, NormMlpClassifierHead
 from .cond_conv2d import CondConv2d, get_condconv_initializer
-from .config import is_exportable, is_scriptable, is_no_jit, set_exportable, set_scriptable, set_no_jit,\
-    set_layer_config
+from .config import is_exportable, is_scriptable, is_no_jit, use_fused_attn, \
+    set_exportable, set_scriptable, set_no_jit, set_layer_config, set_fused_attn
 from .conv2d_same import Conv2dSame, conv2d_same
 from .conv_bn_act import ConvNormAct, ConvNormActAa, ConvBnAct
 from .create_act import create_act_layer, get_act_layer, get_act_fn
 from .create_attn import get_attn, create_attn
 from .create_conv2d import create_conv2d
 from .create_norm import get_norm_layer, create_norm_layer
 from .create_norm_act import get_norm_act_layer, create_norm_act_layer, get_norm_act_layer
 from .drop import DropBlock2d, DropPath, drop_block_2d, drop_path
 from .eca import EcaModule, CecaModule, EfficientChannelAttn, CircularEfficientChannelAttn
 from .evo_norm import EvoNorm2dB0, EvoNorm2dB1, EvoNorm2dB2,\
     EvoNorm2dS0, EvoNorm2dS0a, EvoNorm2dS1, EvoNorm2dS1a, EvoNorm2dS2, EvoNorm2dS2a
 from .fast_norm import is_fast_norm, set_fast_norm, fast_group_norm, fast_layer_norm
 from .filter_response_norm import FilterResponseNormTlu2d, FilterResponseNormAct2d
+from .format import Format, get_channel_dim, get_spatial_dim, nchw_to, nhwc_to
 from .gather_excite import GatherExcite
 from .global_context import GlobalContext
 from .helpers import to_ntuple, to_2tuple, to_3tuple, to_4tuple, make_divisible, extend_tuple
 from .inplace_abn import InplaceAbn
 from .linear import Linear
 from .mixed_conv2d import MixedConv2d
-from .mlp import Mlp, GluMlp, GatedMlp, ConvMlp, GlobalResponseNormMlp
+from .mlp import Mlp, GluMlp, GatedMlp, SwiGLU, SwiGLUPacked, ConvMlp, GlobalResponseNormMlp
 from .non_local_attn import NonLocalAttn, BatNonLocalAttn
-from .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d
-from .norm_act import BatchNormAct2d, GroupNormAct, convert_sync_batchnorm
+from .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d, RmsNorm
+from .norm_act import BatchNormAct2d, GroupNormAct, GroupNorm1Act, LayerNormAct, LayerNormAct2d,\
+    SyncBatchNormAct, convert_sync_batchnorm, FrozenBatchNormAct2d, freeze_batch_norm_2d, unfreeze_batch_norm_2d
 from .padding import get_padding, get_same_padding, pad_same
-from .patch_embed import PatchEmbed, resample_patch_embed
+from .patch_dropout import PatchDropout
+from .patch_embed import PatchEmbed, PatchEmbedWithSize, resample_patch_embed
 from .pool2d_same import AvgPool2dSame, create_pool2d
 from .pos_embed import resample_abs_pos_embed
 from .pos_embed_rel import RelPosMlp, RelPosBias, RelPosBiasTf, gen_relative_position_index, gen_relative_log_coords
-from .pos_embed_sincos import build_sincos2d_pos_embed, build_fourier_pos_embed, build_rotary_pos_embed, \
-    FourierEmbed, RotaryEmbedding
+from .pos_embed_sincos import pixel_freq_bands, freq_bands, build_sincos2d_pos_embed, build_fourier_pos_embed, \
+    build_rotary_pos_embed, apply_rot_embed, apply_rot_embed_cat, apply_rot_embed_list, apply_keep_indices_nlc, \
+    FourierEmbed, RotaryEmbedding, RotaryEmbeddingCat
 from .squeeze_excite import SEModule, SqueezeExcite, EffectiveSEModule, EffectiveSqueezeExcite
 from .selective_kernel import SelectiveKernel
 from .separable_conv import SeparableConv2d, SeparableConvNormAct
-from .space_to_depth import SpaceToDepthModule
+from .space_to_depth import SpaceToDepthModule, SpaceToDepth, DepthToSpace
 from .split_attn import SplitAttn
 from .split_batchnorm import SplitBatchNorm2d, convert_splitbn_model
 from .std_conv import StdConv2d, StdConv2dSame, ScaledStdConv2d, ScaledStdConv2dSame
 from .test_time_pool import TestTimePoolHead, apply_test_time_pool
 from .trace_utils import _assert, _float_to_int
 from .weight_init import trunc_normal_, trunc_normal_tf_, variance_scaling_, lecun_normal_
```

### Comparing `timm-0.8.6.dev0/timm/layers/activations.py` & `timm-0.9.0/timm/layers/activations.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/activations_jit.py` & `timm-0.9.0/timm/layers/activations_jit.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/activations_me.py` & `timm-0.9.0/timm/layers/activations_me.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/attention_pool2d.py` & `timm-0.9.0/timm/layers/attention_pool2d.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/blur_pool.py` & `timm-0.9.0/timm/layers/blur_pool.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/bottleneck_attn.py` & `timm-0.9.0/timm/layers/bottleneck_attn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/cbam.py` & `timm-0.9.0/timm/layers/cbam.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/cond_conv2d.py` & `timm-0.9.0/timm/layers/cond_conv2d.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/conv_bn_act.py` & `timm-0.9.0/timm/layers/conv_bn_act.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,26 +7,49 @@
 
 from .create_conv2d import create_conv2d
 from .create_norm_act import get_norm_act_layer
 
 
 class ConvNormAct(nn.Module):
     def __init__(
-            self, in_channels, out_channels, kernel_size=1, stride=1, padding='', dilation=1, groups=1,
-            bias=False, apply_act=True, norm_layer=nn.BatchNorm2d, act_layer=nn.ReLU, drop_layer=None):
+            self,
+            in_channels,
+            out_channels,
+            kernel_size=1,
+            stride=1,
+            padding='',
+            dilation=1,
+            groups=1,
+            bias=False,
+            apply_act=True,
+            norm_layer=nn.BatchNorm2d,
+            norm_kwargs=None,
+            act_layer=nn.ReLU,
+            act_kwargs=None,
+            drop_layer=None,
+    ):
         super(ConvNormAct, self).__init__()
+        norm_kwargs = norm_kwargs or {}
+        act_kwargs = act_kwargs or {}
+
         self.conv = create_conv2d(
             in_channels, out_channels, kernel_size, stride=stride,
             padding=padding, dilation=dilation, groups=groups, bias=bias)
 
         # NOTE for backwards compatibility with models that use separate norm and act layer definitions
         norm_act_layer = get_norm_act_layer(norm_layer, act_layer)
         # NOTE for backwards (weight) compatibility, norm layer name remains `.bn`
-        norm_kwargs = dict(drop_layer=drop_layer) if drop_layer is not None else {}
-        self.bn = norm_act_layer(out_channels, apply_act=apply_act, **norm_kwargs)
+        if drop_layer:
+            norm_kwargs['drop_layer'] = drop_layer
+        self.bn = norm_act_layer(
+            out_channels,
+            apply_act=apply_act,
+            act_kwargs=act_kwargs,
+            **norm_kwargs,
+        )
 
     @property
     def in_channels(self):
         return self.conv.in_channels
 
     @property
     def out_channels(self):
@@ -53,28 +76,46 @@
         return aa_layer(stride)
     else:
         return aa_layer(channels=channels, stride=stride)
 
 
 class ConvNormActAa(nn.Module):
     def __init__(
-            self, in_channels, out_channels, kernel_size=1, stride=1, padding='', dilation=1, groups=1,
-            bias=False, apply_act=True, norm_layer=nn.BatchNorm2d, act_layer=nn.ReLU, aa_layer=None, drop_layer=None):
+            self,
+            in_channels,
+            out_channels,
+            kernel_size=1,
+            stride=1,
+            padding='',
+            dilation=1,
+            groups=1,
+            bias=False,
+            apply_act=True,
+            norm_layer=nn.BatchNorm2d,
+            norm_kwargs=None,
+            act_layer=nn.ReLU,
+            act_kwargs=None,
+            aa_layer=None,
+            drop_layer=None,
+    ):
         super(ConvNormActAa, self).__init__()
         use_aa = aa_layer is not None and stride == 2
+        norm_kwargs = norm_kwargs or {}
+        act_kwargs = act_kwargs or {}
 
         self.conv = create_conv2d(
             in_channels, out_channels, kernel_size, stride=1 if use_aa else stride,
             padding=padding, dilation=dilation, groups=groups, bias=bias)
 
         # NOTE for backwards compatibility with models that use separate norm and act layer definitions
         norm_act_layer = get_norm_act_layer(norm_layer, act_layer)
         # NOTE for backwards (weight) compatibility, norm layer name remains `.bn`
-        norm_kwargs = dict(drop_layer=drop_layer) if drop_layer is not None else {}
-        self.bn = norm_act_layer(out_channels, apply_act=apply_act, **norm_kwargs)
+        if drop_layer:
+            norm_kwargs['drop_layer'] = drop_layer
+        self.bn = norm_act_layer(out_channels, apply_act=apply_act, act_kwargs=act_kwargs, **norm_kwargs)
         self.aa = create_aa(aa_layer, out_channels, stride=stride, enable=use_aa)
 
     @property
     def in_channels(self):
         return self.conv.in_channels
 
     @property
```

### Comparing `timm-0.8.6.dev0/timm/layers/create_act.py` & `timm-0.9.0/timm/layers/create_act.py`

 * *Files 2% similar despite different names*

```diff
@@ -74,14 +74,15 @@
     gelu=GELU,
     gelu_tanh=GELUTanh,
     sigmoid=Sigmoid,
     tanh=Tanh,
     hard_sigmoid=nn.Hardsigmoid if _has_hardsigmoid else HardSigmoid,
     hard_swish=nn.Hardswish if _has_hardswish else HardSwish,
     hard_mish=HardMish,
+    identity=nn.Identity,
 )
 
 _ACT_LAYER_JIT = dict(
     silu=nn.SiLU if _has_silu else SwishJit,
     swish=nn.SiLU if _has_silu else SwishJit,
     mish=nn.Mish if _has_mish else MishJit,
     hard_sigmoid=nn.Hardsigmoid if _has_hardsigmoid else HardSigmoidJit,
```

### Comparing `timm-0.8.6.dev0/timm/layers/create_attn.py` & `timm-0.9.0/timm/layers/create_attn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/create_conv2d.py` & `timm-0.9.0/timm/layers/create_conv2d.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/create_norm.py` & `timm-0.9.0/timm/layers/create_norm.py`

 * *Files 7% similar despite different names*

```diff
@@ -19,17 +19,17 @@
     groupnorm1=GroupNorm1,
     layernorm=LayerNorm,
     layernorm2d=LayerNorm2d,
 )
 _NORM_TYPES = {m for n, m in _NORM_MAP.items()}
 
 
-def create_norm_layer(layer_name, num_features, act_layer=None, apply_act=True, **kwargs):
-    layer = get_norm_layer(layer_name, act_layer=act_layer)
-    layer_instance = layer(num_features, apply_act=apply_act, **kwargs)
+def create_norm_layer(layer_name, num_features, **kwargs):
+    layer = get_norm_layer(layer_name)
+    layer_instance = layer(num_features, **kwargs)
     return layer_instance
 
 
 def get_norm_layer(norm_layer):
     assert isinstance(norm_layer, (type, str,  types.FunctionType, functools.partial))
     norm_kwargs = {}
```

### Comparing `timm-0.8.6.dev0/timm/layers/create_norm_act.py` & `timm-0.9.0/timm/layers/create_norm_act.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/drop.py` & `timm-0.9.0/timm/layers/drop.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/eca.py` & `timm-0.9.0/timm/layers/eca.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/evo_norm.py` & `timm-0.9.0/timm/layers/evo_norm.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/fast_norm.py` & `timm-0.9.0/timm/layers/fast_norm.py`

 * *Files 23% similar despite different names*

```diff
@@ -13,14 +13,20 @@
 
 try:
     from apex.normalization.fused_layer_norm import fused_layer_norm_affine
     has_apex = True
 except ImportError:
     has_apex = False
 
+try:
+    from apex.normalization.fused_layer_norm import fused_rms_norm_affine, fused_rms_norm
+    has_apex_rmsnorm = True
+except ImportError:
+    has_apex_rmsnorm = False
+
 
 # fast (ie lower precision LN) can be disabled with this flag if issues crop up
 _USE_FAST_NORM = False  # defaulting to False for now
 
 
 def is_fast_norm():
     return _USE_FAST_NORM
@@ -43,15 +49,15 @@
         return F.group_norm(x, num_groups, weight, bias, eps)
 
     if torch.is_autocast_enabled():
         # normally native AMP casts GN inputs to float32
         # here we use the low precision autocast dtype
         # FIXME what to do re CPU autocast?
         dt = torch.get_autocast_gpu_dtype()
-        x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt)
+        x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt) if bias is not None else None
 
     with torch.cuda.amp.autocast(enabled=False):
         return F.group_norm(x, num_groups, weight, bias, eps)
 
 
 def fast_layer_norm(
     x: torch.Tensor,
@@ -68,11 +74,53 @@
         return fused_layer_norm_affine(x, weight, bias, normalized_shape, eps)
 
     if torch.is_autocast_enabled():
         # normally native AMP casts LN inputs to float32
         # apex LN does not, this is behaving like Apex
         dt = torch.get_autocast_gpu_dtype()
         # FIXME what to do re CPU autocast?
-        x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt)
+        x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt) if bias is not None else None
 
     with torch.cuda.amp.autocast(enabled=False):
         return F.layer_norm(x, normalized_shape, weight, bias, eps)
+
+
+def rms_norm(
+    x: torch.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[torch.Tensor] = None,
+    eps: float = 1e-5,
+):
+    norm_ndim = len(normalized_shape)
+    if torch.jit.is_scripting():
+        # ndim = len(x.shape)
+        # dims = list(range(ndim - norm_ndim, ndim))  # this doesn't work on pytorch <= 1.13.x
+        # NOTE -ve dims cause torchscript to crash in some cases, out of options to work around
+        assert norm_ndim == 1
+        v = torch.var(x, dim=-1).unsqueeze(-1)  # ts crashes with -ve dim + keepdim=True
+    else:
+        dims = tuple(range(-1, -norm_ndim - 1, -1))
+        v = torch.var(x, dim=dims, keepdim=True)
+    x = x * torch.rsqrt(v + eps)
+    if weight is not None:
+        x = x * weight
+    return x
+
+
+def fast_rms_norm(
+    x: torch.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[torch.Tensor] = None,
+    eps: float = 1e-5,
+) -> torch.Tensor:
+    if torch.jit.is_scripting():
+        # this must be by itself, cannot merge with has_apex_rmsnorm
+        return rms_norm(x, normalized_shape, weight, eps)
+
+    if has_apex_rmsnorm:
+        if weight is None:
+            return fused_rms_norm(x, normalized_shape, eps)
+        else:
+            return fused_rms_norm_affine(x, weight, normalized_shape, eps)
+
+    # fallback
+    return rms_norm(x, normalized_shape, weight, eps)
```

### Comparing `timm-0.8.6.dev0/timm/layers/filter_response_norm.py` & `timm-0.9.0/timm/layers/filter_response_norm.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/gather_excite.py` & `timm-0.9.0/timm/layers/gather_excite.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/global_context.py` & `timm-0.9.0/timm/layers/global_context.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/grn.py` & `timm-0.9.0/timm/layers/grn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/halo_attn.py` & `timm-0.9.0/timm/layers/halo_attn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/helpers.py` & `timm-0.9.0/timm/layers/helpers.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/inplace_abn.py` & `timm-0.9.0/timm/layers/inplace_abn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/lambda_layer.py` & `timm-0.9.0/timm/layers/lambda_layer.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/linear.py` & `timm-0.9.0/timm/layers/linear.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/median_pool.py` & `timm-0.9.0/timm/layers/median_pool.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/mixed_conv2d.py` & `timm-0.9.0/timm/layers/mixed_conv2d.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/ml_decoder.py` & `timm-0.9.0/timm/layers/ml_decoder.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/mlp.py` & `timm-0.9.0/timm/layers/mlp.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,28 +15,30 @@
     """
     def __init__(
             self,
             in_features,
             hidden_features=None,
             out_features=None,
             act_layer=nn.GELU,
+            norm_layer=None,
             bias=True,
             drop=0.,
             use_conv=False,
     ):
         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         bias = to_2tuple(bias)
         drop_probs = to_2tuple(drop)
         linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear
 
         self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
+        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
         self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])
         self.drop2 = nn.Dropout(drop_probs[1])
 
     def forward(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.drop1(x)
@@ -51,58 +53,114 @@
     """
     def __init__(
             self,
             in_features,
             hidden_features=None,
             out_features=None,
             act_layer=nn.Sigmoid,
+            norm_layer=None,
             bias=True,
             drop=0.,
             use_conv=False,
+            gate_last=True,
     ):
         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         assert hidden_features % 2 == 0
         bias = to_2tuple(bias)
         drop_probs = to_2tuple(drop)
         linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear
         self.chunk_dim = 1 if use_conv else -1
+        self.gate_last = gate_last  # use second half of width for gate
 
         self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
+        self.norm = norm_layer(hidden_features // 2) if norm_layer is not None else nn.Identity()
         self.fc2 = linear_layer(hidden_features // 2, out_features, bias=bias[1])
         self.drop2 = nn.Dropout(drop_probs[1])
 
     def init_weights(self):
         # override init of fc1 w/ gate portion set to weight near zero, bias=1
         fc1_mid = self.fc1.bias.shape[0] // 2
         nn.init.ones_(self.fc1.bias[fc1_mid:])
         nn.init.normal_(self.fc1.weight[fc1_mid:], std=1e-6)
 
     def forward(self, x):
         x = self.fc1(x)
-        x, gates = x.chunk(2, dim=self.chunk_dim)
-        x = x * self.act(gates)
+        x1, x2 = x.chunk(2, dim=self.chunk_dim)
+        x = x1 * self.act(x2) if self.gate_last else self.act(x1) * x2
         x = self.drop1(x)
+        x = self.norm(x)
+        x = self.fc2(x)
+        x = self.drop2(x)
+        return x
+
+
+SwiGLUPacked = partial(GluMlp, act_layer=nn.SiLU, gate_last=False)
+
+
+class SwiGLU(nn.Module):
+    """ SwiGLU
+    NOTE: GluMLP above can implement SwiGLU, but this impl has split fc1 and
+    better matches some other common impl which makes mapping checkpoints simpler.
+    """
+    def __init__(
+            self,
+            in_features,
+            hidden_features=None,
+            out_features=None,
+            act_layer=nn.SiLU,
+            norm_layer=None,
+            bias=True,
+            drop=0.,
+    ):
+        super().__init__()
+        out_features = out_features or in_features
+        hidden_features = hidden_features or in_features
+        bias = to_2tuple(bias)
+        drop_probs = to_2tuple(drop)
+
+        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])
+        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])
+        self.act = act_layer()
+        self.drop1 = nn.Dropout(drop_probs[0])
+        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
+        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])
+        self.drop2 = nn.Dropout(drop_probs[1])
+
+        self.drop = nn.Dropout(drop)
+
+    def init_weights(self):
+        # override init of fc1 w/ gate portion set to weight near zero, bias=1
+        nn.init.ones_(self.fc1_g.bias)
+        nn.init.normal_(self.fc1_g.weight, std=1e-6)
+
+    def forward(self, x):
+        x_gate = self.fc1_g(x)
+        x = self.fc1_x(x)
+        x = self.act(x_gate) * x
+        x = self.drop1(x)
+        x = self.norm(x)
         x = self.fc2(x)
         x = self.drop2(x)
         return x
 
 
 class GatedMlp(nn.Module):
     """ MLP as used in gMLP
     """
     def __init__(
             self,
             in_features,
             hidden_features=None,
             out_features=None,
             act_layer=nn.GELU,
+            norm_layer=None,
             gate_layer=None,
             bias=True,
             drop=0.,
     ):
         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
@@ -114,22 +172,24 @@
         self.drop1 = nn.Dropout(drop_probs[0])
         if gate_layer is not None:
             assert hidden_features % 2 == 0
             self.gate = gate_layer(hidden_features)
             hidden_features = hidden_features // 2  # FIXME base reduction on gate property?
         else:
             self.gate = nn.Identity()
+        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
         self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])
         self.drop2 = nn.Dropout(drop_probs[1])
 
     def forward(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.drop1(x)
         x = self.gate(x)
+        x = self.norm(x)
         x = self.fc2(x)
         x = self.drop2(x)
         return x
 
 
 class ConvMlp(nn.Module):
     """ MLP using 1x1 convs that keeps spatial dims
```

### Comparing `timm-0.8.6.dev0/timm/layers/non_local_attn.py` & `timm-0.9.0/timm/layers/non_local_attn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/padding.py` & `timm-0.9.0/timm/layers/padding.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,39 +1,62 @@
 """ Padding Helpers
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import math
 from typing import List, Tuple
 
+import torch
 import torch.nn.functional as F
 
 
 # Calculate symmetric padding for a convolution
 def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:
     padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2
     return padding
 
 
 # Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution
-def get_same_padding(x: int, k: int, s: int, d: int):
-    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)
+def get_same_padding(x: int, kernel_size: int, stride: int, dilation: int):
+    if isinstance(x, torch.Tensor):
+        return torch.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min=0)
+    else:
+        return max((math.ceil(x / stride) - 1) * stride + (kernel_size - 1) * dilation + 1 - x, 0)
 
 
 # Can SAME padding for given args be done statically?
 def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):
     return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0
 
 
+def pad_same_arg(
+        input_size: List[int],
+        kernel_size: List[int],
+        stride: List[int],
+        dilation: List[int] = (1, 1),
+) -> List[int]:
+    ih, iw = input_size
+    kh, kw = kernel_size
+    pad_h = get_same_padding(ih, kh, stride[0], dilation[0])
+    pad_w = get_same_padding(iw, kw, stride[1], dilation[1])
+    return [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]
+
+
 # Dynamically pad input x with 'SAME' padding for conv with specified args
-def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1), value: float = 0):
+def pad_same(
+        x,
+        kernel_size: List[int],
+        stride: List[int],
+        dilation: List[int] = (1, 1),
+        value: float = 0,
+):
     ih, iw = x.size()[-2:]
-    pad_h, pad_w = get_same_padding(ih, k[0], s[0], d[0]), get_same_padding(iw, k[1], s[1], d[1])
-    if pad_h > 0 or pad_w > 0:
-        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)
+    pad_h = get_same_padding(ih, kernel_size[0], stride[0], dilation[0])
+    pad_w = get_same_padding(iw, kernel_size[1], stride[1], dilation[1])
+    x = F.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2), value=value)
     return x
 
 
 def get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:
     dynamic = False
     if isinstance(padding, str):
         # for any string padding, the padding will be calculated for you, one of three ways
```

### Comparing `timm-0.8.6.dev0/timm/layers/patch_embed.py` & `timm-0.9.0/timm/layers/patch_embed.py`

 * *Files 20% similar despite different names*

```diff
@@ -5,62 +5,123 @@
 Based on code in:
   * https://github.com/google-research/vision_transformer
   * https://github.com/google-research/big_vision/tree/main/big_vision
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import logging
-from typing import List
+from typing import Callable, List, Optional, Tuple, Union
 
 import torch
 from torch import nn as nn
 import torch.nn.functional as F
 
+from .format import Format, nchw_to
 from .helpers import to_2tuple
 from .trace_utils import _assert
 
 _logger = logging.getLogger(__name__)
 
 
 class PatchEmbed(nn.Module):
     """ 2D Image to Patch Embedding
     """
+    output_fmt: Format
+
     def __init__(
             self,
-            img_size=224,
-            patch_size=16,
-            in_chans=3,
-            embed_dim=768,
-            norm_layer=None,
-            flatten=True,
-            bias=True,
+            img_size: Optional[int] = 224,
+            patch_size: int = 16,
+            in_chans: int = 3,
+            embed_dim: int = 768,
+            norm_layer: Optional[Callable] = None,
+            flatten: bool = True,
+            output_fmt: Optional[str] = None,
+            bias: bool = True,
     ):
         super().__init__()
-        img_size = to_2tuple(img_size)
-        patch_size = to_2tuple(patch_size)
-        self.img_size = img_size
-        self.patch_size = patch_size
-        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
-        self.num_patches = self.grid_size[0] * self.grid_size[1]
-        self.flatten = flatten
+        self.patch_size = to_2tuple(patch_size)
+        if img_size is not None:
+            self.img_size = to_2tuple(img_size)
+            self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])
+            self.num_patches = self.grid_size[0] * self.grid_size[1]
+        else:
+            self.img_size = None
+            self.grid_size = None
+            self.num_patches = None
+
+        if output_fmt is not None:
+            self.flatten = False
+            self.output_fmt = Format(output_fmt)
+        else:
+            # flatten spatial dim and transpose to channels last, kept for bwd compat
+            self.flatten = flatten
+            self.output_fmt = Format.NCHW
 
         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)
         self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()
 
     def forward(self, x):
         B, C, H, W = x.shape
-        _assert(H == self.img_size[0], f"Input image height ({H}) doesn't match model ({self.img_size[0]}).")
-        _assert(W == self.img_size[1], f"Input image width ({W}) doesn't match model ({self.img_size[1]}).")
+        if self.img_size is not None:
+            _assert(H == self.img_size[0], f"Input image height ({H}) doesn't match model ({self.img_size[0]}).")
+            _assert(W == self.img_size[1], f"Input image width ({W}) doesn't match model ({self.img_size[1]}).")
+
         x = self.proj(x)
         if self.flatten:
-            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
+            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
+        elif self.output_fmt != Format.NCHW:
+            x = nchw_to(x, self.output_fmt)
         x = self.norm(x)
         return x
 
 
+class PatchEmbedWithSize(PatchEmbed):
+    """ 2D Image to Patch Embedding
+    """
+    output_fmt: Format
+
+    def __init__(
+            self,
+            img_size: Optional[int] = 224,
+            patch_size: int = 16,
+            in_chans: int = 3,
+            embed_dim: int = 768,
+            norm_layer: Optional[Callable] = None,
+            flatten: bool = True,
+            output_fmt: Optional[str] = None,
+            bias: bool = True,
+    ):
+        super().__init__(
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+            norm_layer=norm_layer,
+            flatten=flatten,
+            output_fmt=output_fmt,
+            bias=bias,
+        )
+
+    def forward(self, x) -> Tuple[torch.Tensor, List[int]]:
+        B, C, H, W = x.shape
+        if self.img_size is not None:
+            _assert(H % self.patch_size[0] == 0, f"Input image height ({H}) must be divisible by patch size ({self.patch_size[0]}).")
+            _assert(W % self.patch_size[1] == 0, f"Input image width ({W}) must be divisible by patch size ({self.patch_size[1]}).")
+
+        x = self.proj(x)
+        grid_size = x.shape[-2:]
+        if self.flatten:
+            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
+        elif self.output_fmt != Format.NCHW:
+            x = nchw_to(x, self.output_fmt)
+        x = self.norm(x)
+        return x, grid_size
+
+
 def resample_patch_embed(
         patch_embed,
         new_size: List[int],
         interpolation: str = 'bicubic',
         antialias: bool = True,
         verbose: bool = False,
 ):
```

### Comparing `timm-0.8.6.dev0/timm/layers/pool2d_same.py` & `timm-0.9.0/timm/layers/pool2d_same.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/pos_embed_rel.py` & `timm-0.9.0/timm/layers/pos_embed_rel.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,34 +11,56 @@
 
 from .mlp import Mlp
 from .weight_init import trunc_normal_
 
 
 def gen_relative_position_index(
         q_size: Tuple[int, int],
-        k_size: Tuple[int, int] = None,
-        class_token: bool = False) -> torch.Tensor:
+        k_size: Optional[Tuple[int, int]] = None,
+        class_token: bool = False,
+) -> torch.Tensor:
     # Adapted with significant modifications from Swin / BeiT codebases
     # get pair-wise relative position index for each token inside the window
-    q_coords = torch.stack(torch.meshgrid([torch.arange(q_size[0]), torch.arange(q_size[1])])).flatten(1)  # 2, Wh, Ww
     if k_size is None:
-        k_coords = q_coords
-        k_size = q_size
+        coords = torch.stack(
+            torch.meshgrid([
+                torch.arange(q_size[0]),
+                torch.arange(q_size[1])
+            ])
+        ).flatten(1)  # 2, Wh, Ww
+        relative_coords = coords[:, :, None] - coords[:, None, :]  # 2, Wh*Ww, Wh*Ww
+        relative_coords = relative_coords.permute(1, 2, 0)  # Qh*Qw, Kh*Kw, 2
+        num_relative_distance = (2 * q_size[0] - 1) * (2 * q_size[1] - 1) + 3
     else:
-        # different q vs k sizes is a WIP
-        k_coords = torch.stack(torch.meshgrid([torch.arange(k_size[0]), torch.arange(k_size[1])])).flatten(1)
-    relative_coords = q_coords[:, :, None] - k_coords[:, None, :]  # 2, Wh*Ww, Wh*Ww
-    relative_coords = relative_coords.permute(1, 2, 0)  # Wh*Ww, Wh*Ww, 2
+        # FIXME different q vs k sizes is a WIP, need to better offset the two grids?
+        q_coords = torch.stack(
+            torch.meshgrid([
+                torch.arange(q_size[0]),
+                torch.arange(q_size[1])
+            ])
+        ).flatten(1)  # 2, Wh, Ww
+        k_coords = torch.stack(
+            torch.meshgrid([
+                torch.arange(k_size[0]),
+                torch.arange(k_size[1])
+            ])
+        ).flatten(1)
+        relative_coords = q_coords[:, :, None] - k_coords[:, None, :]  # 2, Wh*Ww, Wh*Ww
+        relative_coords = relative_coords.permute(1, 2, 0)  # Qh*Qw, Kh*Kw, 2
+        # relative_coords[:, :, 0] += max(q_size[0], k_size[0]) - 1  # shift to start from 0
+        # relative_coords[:, :, 1] += max(q_size[1], k_size[1]) - 1
+        # relative_coords[:, :, 0] *= k_size[1] + q_size[1] - 1
+        # relative_position_index = relative_coords.sum(-1)  # Qh*Qw, Kh*Kw
+        num_relative_distance = (q_size[0] + k_size[0] - 1) * (q_size[1] + q_size[1] - 1) + 3
+
     _, relative_position_index = torch.unique(relative_coords.view(-1, 2), return_inverse=True, dim=0)
 
     if class_token:
         # handle cls to token & token 2 cls & cls to cls as per beit for rel pos bias
         # NOTE not intended or tested with MLP log-coords
-        max_size = (max(q_size[0], k_size[0]), max(q_size[1], k_size[1]))
-        num_relative_distance = (2 * max_size[0] - 1) * (2 * max_size[1] - 1) + 3
         relative_position_index = F.pad(relative_position_index, [1, 0, 1, 0])
         relative_position_index[0, 0:] = num_relative_distance - 3
         relative_position_index[0:, 0] = num_relative_distance - 2
         relative_position_index[0, 0] = num_relative_distance - 1
 
     return relative_position_index.contiguous()
 
@@ -55,40 +77,40 @@
         self.window_area = window_size[0] * window_size[1]
         self.bias_shape = (self.window_area + prefix_tokens,) * 2 + (num_heads,)
 
         num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3 * prefix_tokens
         self.relative_position_bias_table = nn.Parameter(torch.zeros(num_relative_distance, num_heads))
         self.register_buffer(
             "relative_position_index",
-            gen_relative_position_index(self.window_size, class_token=prefix_tokens > 0),
+            gen_relative_position_index(self.window_size, class_token=prefix_tokens > 0).view(-1),
             persistent=False,
         )
 
         self.init_weights()
 
     def init_weights(self):
         trunc_normal_(self.relative_position_bias_table, std=.02)
 
     def get_bias(self) -> torch.Tensor:
-        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]
+        relative_position_bias = self.relative_position_bias_table[self.relative_position_index]
         # win_h * win_w, win_h * win_w, num_heads
         relative_position_bias = relative_position_bias.view(self.bias_shape).permute(2, 0, 1)
         return relative_position_bias.unsqueeze(0).contiguous()
 
     def forward(self, attn, shared_rel_pos: Optional[torch.Tensor] = None):
         return attn + self.get_bias()
 
 
 def gen_relative_log_coords(
         win_size: Tuple[int, int],
         pretrained_win_size: Tuple[int, int] = (0, 0),
         mode='swin',
 ):
-    assert mode in ('swin', 'cr', 'rw')
-    # as per official swin-v2 impl, supporting timm specific 'cr' and 'rw' log coords as well
+    assert mode in ('swin', 'cr')
+    # as per official swin-v2 impl, supporting timm specific 'cr' log coords as well
     relative_coords_h = torch.arange(-(win_size[0] - 1), win_size[0], dtype=torch.float32)
     relative_coords_w = torch.arange(-(win_size[1] - 1), win_size[1], dtype=torch.float32)
     relative_coords_table = torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w]))
     relative_coords_table = relative_coords_table.permute(1, 2, 0).contiguous()  # 2*Wh-1, 2*Ww-1, 2
     if mode == 'swin':
         if pretrained_win_size[0] > 0:
             relative_coords_table[:, :, 0] /= (pretrained_win_size[0] - 1)
@@ -96,26 +118,17 @@
         else:
             relative_coords_table[:, :, 0] /= (win_size[0] - 1)
             relative_coords_table[:, :, 1] /= (win_size[1] - 1)
         relative_coords_table *= 8  # normalize to -8, 8
         relative_coords_table = torch.sign(relative_coords_table) * torch.log2(
             1.0 + relative_coords_table.abs()) / math.log2(8)
     else:
-        if mode == 'rw':
-            # cr w/ window size normalization -> [-1,1] log coords
-            relative_coords_table[:, :, 0] /= (win_size[0] - 1)
-            relative_coords_table[:, :, 1] /= (win_size[1] - 1)
-            relative_coords_table *= 8  # scale to -8, 8
-            relative_coords_table = torch.sign(relative_coords_table) * torch.log2(
-                1.0 + relative_coords_table.abs())
-            relative_coords_table /= math.log2(9)   # -> [-1, 1]
-        else:
-            # mode == 'cr'
-            relative_coords_table = torch.sign(relative_coords_table) * torch.log(
-                1.0 + relative_coords_table.abs())
+        # mode == 'cr'
+        relative_coords_table = torch.sign(relative_coords_table) * torch.log(
+            1.0 + relative_coords_table.abs())
 
     return relative_coords_table
 
 
 class RelPosMlp(nn.Module):
     """ Log-Coordinate Relative Position MLP
     Based on ideas presented in Swin-V2 paper (https://arxiv.org/abs/2111.09883)
@@ -137,18 +150,14 @@
         self.prefix_tokens = prefix_tokens
         self.num_heads = num_heads
         self.bias_shape = (self.window_area,) * 2 + (num_heads,)
         if mode == 'swin':
             self.bias_act = nn.Sigmoid()
             self.bias_gain = 16
             mlp_bias = (True, False)
-        elif mode == 'rw':
-            self.bias_act = nn.Tanh()
-            self.bias_gain = 4
-            mlp_bias = True
         else:
             self.bias_act = nn.Identity()
             self.bias_gain = None
             mlp_bias = True
 
         self.mlp = Mlp(
             2,  # x, y
@@ -157,28 +166,27 @@
             act_layer=nn.ReLU,
             bias=mlp_bias,
             drop=(0.125, 0.)
         )
 
         self.register_buffer(
             "relative_position_index",
-            gen_relative_position_index(window_size),
+            gen_relative_position_index(window_size).view(-1),
             persistent=False)
 
         # get relative_coords_table
         self.register_buffer(
             "rel_coords_log",
             gen_relative_log_coords(window_size, pretrained_window_size, mode=mode),
             persistent=False)
 
     def get_bias(self) -> torch.Tensor:
         relative_position_bias = self.mlp(self.rel_coords_log)
         if self.relative_position_index is not None:
-            relative_position_bias = relative_position_bias.view(-1, self.num_heads)[
-                self.relative_position_index.view(-1)]  # Wh*Ww,Wh*Ww,nH
+            relative_position_bias = relative_position_bias.view(-1, self.num_heads)[self.relative_position_index]
             relative_position_bias = relative_position_bias.view(self.bias_shape)
         relative_position_bias = relative_position_bias.permute(2, 0, 1)
         relative_position_bias = self.bias_act(relative_position_bias)
         if self.bias_gain is not None:
             relative_position_bias = self.bias_gain * relative_position_bias
         if self.prefix_tokens:
             relative_position_bias = F.pad(relative_position_bias, [self.prefix_tokens, 0, self.prefix_tokens, 0])
```

### Comparing `timm-0.8.6.dev0/timm/layers/selective_kernel.py` & `timm-0.9.0/timm/layers/selective_kernel.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/separable_conv.py` & `timm-0.9.0/timm/layers/separable_conv.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/space_to_depth.py` & `timm-0.9.0/timm/layers/space_to_depth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 import torch
 import torch.nn as nn
 
 
 class SpaceToDepth(nn.Module):
+    bs: torch.jit.Final[int]
+
     def __init__(self, block_size=4):
         super().__init__()
         assert block_size == 4
         self.bs = block_size
 
     def forward(self, x):
         N, C, H, W = x.size()
         x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  # (N, C, H//bs, bs, W//bs, bs)
         x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)
-        x = x.view(N, C * (self.bs ** 2), H // self.bs, W // self.bs)  # (N, C*bs^2, H//bs, W//bs)
+        x = x.view(N, C * self.bs * self.bs, H // self.bs, W // self.bs)  # (N, C*bs^2, H//bs, W//bs)
         return x
 
 
 @torch.jit.script
-class SpaceToDepthJit(object):
+class SpaceToDepthJit:
     def __call__(self, x: torch.Tensor):
         # assuming hard-coded that block_size==4 for acceleration
         N, C, H, W = x.size()
         x = x.view(N, C, H // 4, 4, W // 4, 4)  # (N, C, H//bs, bs, W//bs, bs)
         x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)
         x = x.view(N, C * 16, H // 4, W // 4)  # (N, C*bs^2, H//bs, W//bs)
         return x
```

### Comparing `timm-0.8.6.dev0/timm/layers/split_attn.py` & `timm-0.9.0/timm/layers/split_attn.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/split_batchnorm.py` & `timm-0.9.0/timm/layers/split_batchnorm.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/squeeze_excite.py` & `timm-0.9.0/timm/layers/squeeze_excite.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/std_conv.py` & `timm-0.9.0/timm/layers/std_conv.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/test_time_pool.py` & `timm-0.9.0/timm/layers/test_time_pool.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/layers/weight_init.py` & `timm-0.9.0/timm/layers/weight_init.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/loss/asymmetric_loss.py` & `timm-0.9.0/timm/loss/asymmetric_loss.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/loss/binary_cross_entropy.py` & `timm-0.9.0/timm/loss/binary_cross_entropy.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/loss/cross_entropy.py` & `timm-0.9.0/timm/loss/cross_entropy.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/loss/jsd.py` & `timm-0.9.0/timm/loss/jsd.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/models/__init__.py` & `timm-0.9.0/timm/models/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -4,42 +4,44 @@
 from .cait import *
 from .coat import *
 from .convit import *
 from .convmixer import *
 from .convnext import *
 from .crossvit import *
 from .cspnet import *
+from .davit import *
 from .deit import *
 from .densenet import *
 from .dla import *
 from .dpn import *
 from .edgenext import *
 from .efficientformer import *
+from .efficientformer_v2 import *
 from .efficientnet import *
+from .eva import *
+from .focalnet import *
 from .gcvit import *
 from .ghostnet import *
-from .gluon_resnet import *
-from .gluon_xception import *
 from .hardcorenas import *
 from .hrnet import *
 from .inception_resnet_v2 import *
 from .inception_v3 import *
 from .inception_v4 import *
 from .levit import *
 from .maxxvit import *
+from .metaformer import *
 from .mlp_mixer import *
 from .mobilenetv3 import *
 from .mobilevit import *
 from .mvitv2 import *
 from .nasnet import *
 from .nest import *
 from .nfnet import *
 from .pit import *
 from .pnasnet import *
-from .poolformer import *
 from .pvt_v2 import *
 from .regnet import *
 from .res2net import *
 from .resnest import *
 from .resnet import *
 from .resnetv2 import *
 from .rexnet import *
@@ -65,17 +67,18 @@
 from .xcit import *
 
 from ._builder import build_model_with_cfg, load_pretrained, load_custom_pretrained, resolve_pretrained_cfg, \
     set_pretrained_download_progress, set_pretrained_check_hash
 from ._factory import create_model, parse_model_name, safe_model_name
 from ._features import FeatureInfo, FeatureHooks, FeatureHookNet, FeatureListNet, FeatureDictNet
 from ._features_fx import FeatureGraphNet, GraphExtractNet, create_feature_extractor, \
-    register_notrace_module, register_notrace_function
-from ._helpers import clean_state_dict, load_state_dict, load_checkpoint, remap_checkpoint, resume_checkpoint
+    register_notrace_module, is_notrace_module, get_notrace_modules, \
+    register_notrace_function, is_notrace_function, get_notrace_functions
+from ._helpers import clean_state_dict, load_state_dict, load_checkpoint, remap_state_dict, resume_checkpoint
 from ._hub import load_model_config_from_hf, load_state_dict_from_hf, push_to_hf_hub
 from ._manipulate import model_parameters, named_apply, named_modules, named_modules_with_params, \
     group_modules, group_parameters, checkpoint_seq, adapt_input_conv
-from ._pretrained import PretrainedCfg, DefaultCfg, \
-    filter_pretrained_cfg, generate_default_cfgs, split_model_name_tag
+from ._pretrained import PretrainedCfg, DefaultCfg, filter_pretrained_cfg
 from ._prune import adapt_model_from_string
-from ._registry import register_model, model_entrypoint, list_models, list_pretrained, is_model, list_modules, \
-    is_model_in_modules, is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value
+from ._registry import split_model_name_tag, get_arch_name, generate_default_cfgs, register_model, \
+    register_model_deprecations, model_entrypoint, list_models, list_pretrained, get_deprecated_models, \
+    is_model, list_modules, is_model_in_modules, is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value
```

### Comparing `timm-0.8.6.dev0/timm/models/_builder.py` & `timm-0.9.0/timm/models/_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,44 +18,52 @@
 
 _logger = logging.getLogger(__name__)
 
 # Global variables for rarely used pretrained checkpoint download progress and hash check.
 # Use set_pretrained_download_progress / set_pretrained_check_hash functions to toggle.
 _DOWNLOAD_PROGRESS = False
 _CHECK_HASH = False
-
+_USE_OLD_CACHE = int(os.environ.get('TIMM_USE_OLD_CACHE', 0)) > 0
 
 __all__ = ['set_pretrained_download_progress', 'set_pretrained_check_hash', 'load_custom_pretrained', 'load_pretrained',
            'pretrained_cfg_for_features', 'resolve_pretrained_cfg', 'build_model_with_cfg']
 
 
 def _resolve_pretrained_source(pretrained_cfg):
     cfg_source = pretrained_cfg.get('source', '')
     pretrained_url = pretrained_cfg.get('url', None)
     pretrained_file = pretrained_cfg.get('file', None)
+    pretrained_sd = pretrained_cfg.get('state_dict', None)
     hf_hub_id = pretrained_cfg.get('hf_hub_id', None)
 
     # resolve where to load pretrained weights from
     load_from = ''
     pretrained_loc = ''
     if cfg_source == 'hf-hub' and has_hf_hub(necessary=True):
         # hf-hub specified as source via model identifier
         load_from = 'hf-hub'
         assert hf_hub_id
         pretrained_loc = hf_hub_id
     else:
         # default source == timm or unspecified
-        if pretrained_file:
-            # file load override is the highest priority if set
+        if pretrained_sd:
+            # direct state_dict pass through is the highest priority
+            load_from = 'state_dict'
+            pretrained_loc = pretrained_sd
+            assert isinstance(pretrained_loc, dict)
+        elif pretrained_file:
+            # file load override is the second-highest priority if set
             load_from = 'file'
             pretrained_loc = pretrained_file
         else:
-            # next, HF hub is prioritized unless a valid cached version of weights exists already
-            cached_url_valid = check_cached_file(pretrained_url) if pretrained_url else False
-            if hf_hub_id and has_hf_hub(necessary=True) and not cached_url_valid:
+            old_cache_valid = False
+            if _USE_OLD_CACHE:
+                # prioritized old cached weights if exists and env var enabled
+                old_cache_valid = check_cached_file(pretrained_url) if pretrained_url else False
+            if not old_cache_valid and hf_hub_id and has_hf_hub(necessary=True):
                 # hf-hub available as alternate weight source in default_cfg
                 load_from = 'hf-hub'
                 pretrained_loc = hf_hub_id
             elif pretrained_url:
                 load_from = 'url'
                 pretrained_loc = pretrained_url
 
@@ -102,15 +110,15 @@
         _logger.warning("Invalid pretrained config, cannot load weights.")
         return
 
     load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)
     if not load_from:
         _logger.warning("No pretrained weights exist for this model. Using random initialization.")
         return
-    if load_from == 'hf-hub':  # FIXME
+    if load_from == 'hf-hub':
         _logger.warning("Hugging Face hub not currently supported for custom load pretrained models.")
     elif load_from == 'url':
         pretrained_loc = download_cached_file(
             pretrained_loc,
             check_hash=_CHECK_HASH,
             progress=_DOWNLOAD_PROGRESS,
         )
@@ -140,19 +148,21 @@
         in_chans (int): in_chans for target model
         filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)
         strict (bool): strict load of checkpoint
 
     """
     pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)
     if not pretrained_cfg:
-        _logger.warning("Invalid pretrained config, cannot load weights.")
-        return
+        raise RuntimeError("Invalid pretrained config, cannot load weights. Use `pretrained=False` for random init.")
 
     load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)
-    if load_from == 'file':
+    if load_from == 'state_dict':
+        _logger.info(f'Loading pretrained weights from state dict')
+        state_dict = pretrained_loc  # pretrained_loc is the actual state dict for this override
+    elif load_from == 'file':
         _logger.info(f'Loading pretrained weights from file ({pretrained_loc})')
         state_dict = load_state_dict(pretrained_loc)
     elif load_from == 'url':
         _logger.info(f'Loading pretrained weights from url ({pretrained_loc})')
         if pretrained_cfg.get('custom_load', False):
             pretrained_loc = download_cached_file(
                 pretrained_loc,
@@ -171,23 +181,23 @@
     elif load_from == 'hf-hub':
         _logger.info(f'Loading pretrained weights from Hugging Face hub ({pretrained_loc})')
         if isinstance(pretrained_loc, (list, tuple)):
             state_dict = load_state_dict_from_hf(*pretrained_loc)
         else:
             state_dict = load_state_dict_from_hf(pretrained_loc)
     else:
-        _logger.warning("No pretrained weights exist or were found for this model. Using random initialization.")
-        return
+        model_name = pretrained_cfg.get('architecture', 'this model')
+        raise RuntimeError(f"No pretrained weights exist for {model_name}. Use `pretrained=False` for random init.")
 
     if filter_fn is not None:
-        # for backwards compat with filter fn that take one arg, try one first, the two
         try:
-            state_dict = filter_fn(state_dict)
-        except TypeError:
             state_dict = filter_fn(state_dict, model)
+        except TypeError as e:
+            # for backwards compat with filter fn that take one arg
+            state_dict = filter_fn(state_dict)
 
     input_convs = pretrained_cfg.get('first_conv', None)
     if input_convs is not None and in_chans != 3:
         if isinstance(input_convs, str):
             input_convs = (input_convs,)
         for input_conv_name in input_convs:
             weight_name = input_conv_name + '.weight'
@@ -388,22 +398,25 @@
             filter_fn=pretrained_filter_fn,
             strict=pretrained_strict,
         )
 
     # Wrap the model in a feature extraction module if enabled
     if features:
         feature_cls = FeatureListNet
+        output_fmt = getattr(model, 'output_fmt', None)
+        if output_fmt is not None:
+            feature_cfg.setdefault('output_fmt', output_fmt)
         if 'feature_cls' in feature_cfg:
             feature_cls = feature_cfg.pop('feature_cls')
             if isinstance(feature_cls, str):
                 feature_cls = feature_cls.lower()
                 if 'hook' in feature_cls:
                     feature_cls = FeatureHookNet
                 elif feature_cls == 'fx':
                     feature_cls = FeatureGraphNet
                 else:
                     assert False, f'Unknown feature class {feature_cls}'
         model = feature_cls(model, **feature_cfg)
-        model.pretrained_cfg = pretrained_cfg_for_features(pretrained_cfg)  # add back default_cfg
-        model.default_cfg = model.pretrained_cfg  # alias for backwards compat
+        model.pretrained_cfg = pretrained_cfg_for_features(pretrained_cfg)  # add back pretrained cfg
+        model.default_cfg = model.pretrained_cfg  # alias for rename backwards compat (default_cfg -> pretrained_cfg)
 
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/_efficientnet_blocks.py` & `timm-0.9.0/timm/models/_efficientnet_blocks.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/models/_efficientnet_builder.py` & `timm-0.9.0/timm/models/_efficientnet_builder.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/models/_factory.py` & `timm-0.9.0/timm/models/_factory.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,36 +1,36 @@
 import os
 from typing import Any, Dict, Optional, Union
 from urllib.parse import urlsplit
 
 from timm.layers import set_layer_config
-from ._pretrained import PretrainedCfg, split_model_name_tag
 from ._helpers import load_checkpoint
 from ._hub import load_model_config_from_hf
-from ._registry import is_model, model_entrypoint
+from ._pretrained import PretrainedCfg
+from ._registry import is_model, model_entrypoint, split_model_name_tag
 
 
 __all__ = ['parse_model_name', 'safe_model_name', 'create_model']
 
 
-def parse_model_name(model_name):
+def parse_model_name(model_name: str):
     if model_name.startswith('hf_hub'):
         # NOTE for backwards compat, deprecate hf_hub use
         model_name = model_name.replace('hf_hub', 'hf-hub')
     parsed = urlsplit(model_name)
     assert parsed.scheme in ('', 'timm', 'hf-hub')
     if parsed.scheme == 'hf-hub':
         # FIXME may use fragment as revision, currently `@` in URI path
         return parsed.scheme, parsed.path
     else:
         model_name = os.path.split(parsed.path)[-1]
         return 'timm', model_name
 
 
-def safe_model_name(model_name, remove_source=True):
+def safe_model_name(model_name: str, remove_source: bool = True):
     # return a filename / path safe model name
     def make_safe(name):
         return ''.join(c if c.isalnum() else '_' for c in name).rstrip('_')
     if remove_source:
         model_name = parse_model_name(model_name)[-1]
     return make_safe(model_name)
 
@@ -42,50 +42,71 @@
         pretrained_cfg_overlay:  Optional[Dict[str, Any]] = None,
         checkpoint_path: str = '',
         scriptable: Optional[bool] = None,
         exportable: Optional[bool] = None,
         no_jit: Optional[bool] = None,
         **kwargs,
 ):
-    """Create a model
+    """Create a model.
 
     Lookup model's entrypoint function and pass relevant args to create a new model.
 
-    **kwargs will be passed through entrypoint fn to timm.models.build_model_with_cfg()
-    and then the model class __init__(). kwargs values set to None are pruned before passing.
+    <Tip>
+        **kwargs will be passed through entrypoint fn to ``timm.models.build_model_with_cfg()``
+        and then the model class __init__(). kwargs values set to None are pruned before passing.
+    </Tip>
 
     Args:
-        model_name (str): name of model to instantiate
-        pretrained (bool): load pretrained ImageNet-1k weights if true
-        pretrained_cfg (Union[str, dict, PretrainedCfg]): pass in external pretrained_cfg for model
-        pretrained_cfg_overlay (dict): replace key-values in base pretrained_cfg with these
-        checkpoint_path (str): path of checkpoint to load _after_ the model is initialized
-        scriptable (bool): set layer config so that model is jit scriptable (not working for all models yet)
-        exportable (bool): set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet)
-        no_jit (bool): set layer config so that model doesn't utilize jit scripted layers (so far activations only)
+        model_name: Name of model to instantiate.
+        pretrained: If set to `True`, load pretrained ImageNet-1k weights.
+        pretrained_cfg: Pass in an external pretrained_cfg for model.
+        pretrained_cfg_overlay: Replace key-values in base pretrained_cfg with these.
+        checkpoint_path: Path of checkpoint to load _after_ the model is initialized.
+        scriptable: Set layer config so that model is jit scriptable (not working for all models yet).
+        exportable: Set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet).
+        no_jit: Set layer config so that model doesn't utilize jit scripted layers (so far activations only).
 
     Keyword Args:
-        drop_rate (float): dropout rate for training (default: 0.0)
-        global_pool (str): global pool type (default: 'avg')
-        **: other kwargs are consumed by builder or model __init__()
+        drop_rate (float): Classifier dropout rate for training.
+        drop_path_rate (float): Stochastic depth drop rate for training.
+        global_pool (str): Classifier global pooling type.
+
+    Example:
+
+    ```py
+    >>> from timm import create_model
+
+    >>> # Create a MobileNetV3-Large model with no pretrained weights.
+    >>> model = create_model('mobilenetv3_large_100')
+
+    >>> # Create a MobileNetV3-Large model with pretrained weights.
+    >>> model = create_model('mobilenetv3_large_100', pretrained=True)
+    >>> model.num_classes
+    1000
+
+    >>> # Create a MobileNetV3-Large model with pretrained weights and a new head with 10 classes.
+    >>> model = create_model('mobilenetv3_large_100', pretrained=True, num_classes=10)
+    >>> model.num_classes
+    10
+    ```
     """
     # Parameters that aren't supported by all models or are intended to only override model defaults if set
     # should default to None in command line args/cfg. Remove them if they are present and not set so that
     # non-supporting models don't break and default args remain in effect.
     kwargs = {k: v for k, v in kwargs.items() if v is not None}
 
     model_source, model_name = parse_model_name(model_name)
     if model_source == 'hf-hub':
         assert not pretrained_cfg, 'pretrained_cfg should not be set when sourcing model from Hugging Face Hub.'
         # For model names specified in the form `hf-hub:path/architecture_name@revision`,
         # load model weights + pretrained_cfg from Hugging Face hub.
         pretrained_cfg, model_name = load_model_config_from_hf(model_name)
     else:
         model_name, pretrained_tag = split_model_name_tag(model_name)
-        if not pretrained_cfg:
+        if pretrained_tag and not pretrained_cfg:
             # a valid pretrained_cfg argument takes priority over tag in model name
             pretrained_cfg = pretrained_tag
 
     if not is_model(model_name):
         raise RuntimeError('Unknown model (%s)' % model_name)
 
     create_fn = model_entrypoint(model_name)
```

### Comparing `timm-0.8.6.dev0/timm/models/_features.py` & `timm-0.9.0/timm/models/_features.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,18 +7,21 @@
 https://github.com/pytorch/vision/blob/d88d8961ae51507d0cb680329d985b1488b1b76b/torchvision/models/_utils.py
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 from collections import OrderedDict, defaultdict
 from copy import deepcopy
 from functools import partial
-from typing import Dict, List, Tuple
+from typing import Dict, List, Sequence, Tuple, Union
 
 import torch
 import torch.nn as nn
+from torch.utils.checkpoint import checkpoint
+
+from timm.layers import Format
 
 
 __all__ = ['FeatureInfo', 'FeatureHooks', 'FeatureDictNet', 'FeatureListNet', 'FeatureHookNet']
 
 
 class FeatureInfo:
 
@@ -84,34 +87,41 @@
         return len(self.info)
 
 
 class FeatureHooks:
     """ Feature Hook Helper
 
     This module helps with the setup and extraction of hooks for extracting features from
-    internal nodes in a model by node name. This works quite well in eager Python but needs
-    redesign for torchscript.
+    internal nodes in a model by node name.
+
+    FIXME This works well in eager Python but needs redesign for torchscript.
     """
 
-    def __init__(self, hooks, named_modules, out_map=None, default_hook_type='forward'):
+    def __init__(
+            self,
+            hooks: Sequence[str],
+            named_modules: dict,
+            out_map: Sequence[Union[int, str]] = None,
+            default_hook_type: str = 'forward',
+    ):
         # setup feature hooks
+        self._feature_outputs = defaultdict(OrderedDict)
         modules = {k: v for k, v in named_modules}
         for i, h in enumerate(hooks):
             hook_name = h['module']
             m = modules[hook_name]
             hook_id = out_map[i] if out_map else hook_name
             hook_fn = partial(self._collect_output_hook, hook_id)
             hook_type = h.get('hook_type', default_hook_type)
             if hook_type == 'forward_pre':
                 m.register_forward_pre_hook(hook_fn)
             elif hook_type == 'forward':
                 m.register_forward_hook(hook_fn)
             else:
                 assert False, "Unsupported hook type"
-        self._feature_outputs = defaultdict(OrderedDict)
 
     def _collect_output_hook(self, hook_id, *args):
         x = args[-1]  # tensor we want is last argument, output for fwd, input for fwd_pre
         if isinstance(x, tuple):
             x = x[0]  # unwrap input tuple
         self._feature_outputs[x.device][hook_id] = x
 
@@ -163,31 +173,40 @@
     order as they are used. There should be no reuse of the same nn.Module more than once, including
     trivial modules like `self.relu = nn.ReLU`.
 
     Only submodules that are directly assigned to the model class (`model.feature1`) or at most
     one Sequential container deep (`model.features.1`, with flatten_sequent=True) can be captured.
     All Sequential containers that are directly assigned to the original model will have their
     modules assigned to this module with the name `model.features.1` being changed to `model.features_1`
-
-    Arguments:
-        model (nn.Module): model from which we will extract the features
-        out_indices (tuple[int]): model output indices to extract features for
-        out_map (sequence): list or tuple specifying desired return id for each out index,
-            otherwise str(index) is used
-        feature_concat (bool): whether to concatenate intermediate features that are lists or tuples
-            vs select element [0]
-        flatten_sequential (bool): whether to flatten sequential modules assigned to model
     """
     def __init__(
-            self, model,
-            out_indices=(0, 1, 2, 3, 4), out_map=None, feature_concat=False, flatten_sequential=False):
+            self,
+            model: nn.Module,
+            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),
+            out_map: Sequence[Union[int, str]] = None,
+            output_fmt: str = 'NCHW',
+            feature_concat: bool = False,
+            flatten_sequential: bool = False,
+    ):
+        """
+        Args:
+            model: Model from which to extract features.
+            out_indices: Output indices of the model features to extract.
+            out_map: Return id mapping for each output index, otherwise str(index) is used.
+            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting
+                first element e.g. `x[0]`
+            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)
+        """
         super(FeatureDictNet, self).__init__()
         self.feature_info = _get_feature_info(model, out_indices)
+        self.output_fmt = Format(output_fmt)
         self.concat = feature_concat
+        self.grad_checkpointing = False
         self.return_layers = {}
+
         return_layers = _get_return_layers(self.feature_info, out_map)
         modules = _module_list(model, flatten_sequential=flatten_sequential)
         remaining = set(return_layers.keys())
         layers = OrderedDict()
         for new_name, old_name, module in modules:
             layers[new_name] = module
             if old_name in remaining:
@@ -196,18 +215,29 @@
                 remaining.remove(old_name)
             if not remaining:
                 break
         assert not remaining and len(self.return_layers) == len(return_layers), \
             f'Return layers ({remaining}) are not present in model'
         self.update(layers)
 
+    def set_grad_checkpointing(self, enable: bool = True):
+        self.grad_checkpointing = enable
+
     def _collect(self, x) -> (Dict[str, torch.Tensor]):
         out = OrderedDict()
-        for name, module in self.items():
-            x = module(x)
+        for i, (name, module) in enumerate(self.items()):
+            if self.grad_checkpointing and not torch.jit.is_scripting():
+                # Skipping checkpoint of first module because need a gradient at input
+                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled
+                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues
+                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)
+                x = module(x) if first_or_last_module else checkpoint(module, x)
+            else:
+                x = module(x)
+
             if name in self.return_layers:
                 out_id = self.return_layers[name]
                 if isinstance(x, (tuple, list)):
                     # If model tap is a tuple or list, concat or select first element
                     # FIXME this may need to be more generic / flexible for some nets
                     out[out_id] = torch.cat(x, 1) if self.concat else x[0]
                 else:
@@ -217,23 +247,39 @@
     def forward(self, x) -> Dict[str, torch.Tensor]:
         return self._collect(x)
 
 
 class FeatureListNet(FeatureDictNet):
     """ Feature extractor with list return
 
-    See docstring for FeatureDictNet above, this class exists only to appease Torchscript typing constraints.
-    In eager Python we could have returned List[Tensor] vs Dict[id, Tensor] based on a member bool.
+    A specialization of FeatureDictNet that always returns features as a list (values() of dict).
     """
     def __init__(
-            self, model,
-            out_indices=(0, 1, 2, 3, 4), out_map=None, feature_concat=False, flatten_sequential=False):
-        super(FeatureListNet, self).__init__(
-            model, out_indices=out_indices, out_map=out_map, feature_concat=feature_concat,
-            flatten_sequential=flatten_sequential)
+            self,
+            model: nn.Module,
+            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),
+            output_fmt: str = 'NCHW',
+            feature_concat: bool = False,
+            flatten_sequential: bool = False,
+    ):
+        """
+        Args:
+            model: Model from which to extract features.
+            out_indices: Output indices of the model features to extract.
+            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting
+                first element e.g. `x[0]`
+            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)
+        """
+        super().__init__(
+            model,
+            out_indices=out_indices,
+            output_fmt=output_fmt,
+            feature_concat=feature_concat,
+            flatten_sequential=flatten_sequential,
+        )
 
     def forward(self, x) -> (List[torch.Tensor]):
         return list(self._collect(x).values())
 
 
 class FeatureHookNet(nn.ModuleDict):
     """ FeatureHookNet
@@ -245,43 +291,77 @@
 
     If `no_rewrite` is False, the model will be re-written as in the
     FeatureList/FeatureDict case by folding first to second (Sequential only) level modules into this one.
 
     FIXME this does not currently work with Torchscript, see FeatureHooks class
     """
     def __init__(
-            self, model,
-            out_indices=(0, 1, 2, 3, 4), out_map=None, out_as_dict=False, no_rewrite=False,
-            feature_concat=False, flatten_sequential=False, default_hook_type='forward'):
-        super(FeatureHookNet, self).__init__()
+            self,
+            model: nn.Module,
+            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),
+            out_map: Sequence[Union[int, str]] = None,
+            return_dict: bool = False,
+            output_fmt: str = 'NCHW',
+            no_rewrite: bool = False,
+            flatten_sequential: bool = False,
+            default_hook_type: str = 'forward',
+    ):
+        """
+
+        Args:
+            model: Model from which to extract features.
+            out_indices: Output indices of the model features to extract.
+            out_map: Return id mapping for each output index, otherwise str(index) is used.
+            return_dict: Output features as a dict.
+            no_rewrite: Enforce that model is not re-written if True, ie no modules are removed / changed.
+                flatten_sequential arg must also be False if this is set True.
+            flatten_sequential: Re-write modules by flattening first two levels of nn.Sequential containers.
+            default_hook_type: The default hook type to use if not specified in model.feature_info.
+        """
+        super().__init__()
         assert not torch.jit.is_scripting()
         self.feature_info = _get_feature_info(model, out_indices)
-        self.out_as_dict = out_as_dict
+        self.return_dict = return_dict
+        self.output_fmt = Format(output_fmt)
+        self.grad_checkpointing = False
+
         layers = OrderedDict()
         hooks = []
         if no_rewrite:
             assert not flatten_sequential
             if hasattr(model, 'reset_classifier'):  # make sure classifier is removed?
                 model.reset_classifier(0)
             layers['body'] = model
             hooks.extend(self.feature_info.get_dicts())
         else:
             modules = _module_list(model, flatten_sequential=flatten_sequential)
-            remaining = {f['module']: f['hook_type'] if 'hook_type' in f else default_hook_type
-                         for f in self.feature_info.get_dicts()}
+            remaining = {
+                f['module']: f['hook_type'] if 'hook_type' in f else default_hook_type
+                for f in self.feature_info.get_dicts()
+            }
             for new_name, old_name, module in modules:
                 layers[new_name] = module
                 for fn, fm in module.named_modules(prefix=old_name):
                     if fn in remaining:
                         hooks.append(dict(module=fn, hook_type=remaining[fn]))
                         del remaining[fn]
                 if not remaining:
                     break
             assert not remaining, f'Return layers ({remaining}) are not present in model'
         self.update(layers)
         self.hooks = FeatureHooks(hooks, model.named_modules(), out_map=out_map)
 
+    def set_grad_checkpointing(self, enable: bool = True):
+        self.grad_checkpointing = enable
+
     def forward(self, x):
-        for name, module in self.items():
-            x = module(x)
+        for i, (name, module) in enumerate(self.items()):
+            if self.grad_checkpointing and not torch.jit.is_scripting():
+                # Skipping checkpoint of first module because need a gradient at input
+                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled
+                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues
+                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)
+                x = module(x) if first_or_last_module else checkpoint(module, x)
+            else:
+                x = module(x)
         out = self.hooks.get_output(x.device)
-        return out if self.out_as_dict else list(out.values())
+        return out if self.return_dict else list(out.values())
```

### Comparing `timm-0.8.6.dev0/timm/models/_features_fx.py` & `timm-0.9.0/timm/models/_features_fx.py`

 * *Files 22% similar despite different names*

```diff
@@ -15,14 +15,19 @@
     has_fx_feature_extraction = False
 
 # Layers we went to treat as leaf modules
 from timm.layers import Conv2dSame, ScaledStdConv2dSame, CondConv2d, StdConv2dSame
 from timm.layers.non_local_attn import BilinearAttnTransform
 from timm.layers.pool2d_same import MaxPool2dSame, AvgPool2dSame
 
+__all__ = ['register_notrace_module', 'is_notrace_module', 'get_notrace_modules',
+           'register_notrace_function', 'is_notrace_function', 'get_notrace_functions',
+           'create_feature_extractor', 'FeatureGraphNet', 'GraphExtractNet']
+
+
 # NOTE: By default, any modules from timm.models.layers that we want to treat as leaf modules go here
 # BUT modules from timm.models should use the registration mechanism below
 _leaf_modules = {
     BilinearAttnTransform,  # reason: flow control t <= 1
     # Reason: get_same_padding has a max which raises a control flow error
     Conv2dSame, MaxPool2dSame, ScaledStdConv2dSame, StdConv2dSame, AvgPool2dSame,
     CondConv2d,  # reason: TypeError: F.conv2d received Proxy in groups=self.groups * B (because B = x.shape[0])
@@ -31,38 +36,50 @@
 try:
     from timm.layers import InplaceAbn
     _leaf_modules.add(InplaceAbn)
 except ImportError:
     pass
 
 
-__all__ = ['register_notrace_module', 'register_notrace_function', 'create_feature_extractor',
-           'FeatureGraphNet', 'GraphExtractNet']
-
-
 def register_notrace_module(module: Type[nn.Module]):
     """
     Any module not under timm.models.layers should get this decorator if we don't want to trace through it.
     """
     _leaf_modules.add(module)
     return module
 
 
+def is_notrace_module(module: Type[nn.Module]):
+    return module in _leaf_modules
+
+
+def get_notrace_modules():
+    return list(_leaf_modules)
+
+
 # Functions we want to autowrap (treat them as leaves)
 _autowrap_functions = set()
 
 
 def register_notrace_function(func: Callable):
     """
     Decorator for functions which ought not to be traced through
     """
     _autowrap_functions.add(func)
     return func
 
 
+def is_notrace_function(func: Callable):
+    return func in _autowrap_functions
+
+
+def get_notrace_functions():
+    return list(_autowrap_functions)
+
+
 def create_feature_extractor(model: nn.Module, return_nodes: Union[Dict[str, str], List[str]]):
     assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'
     return _create_feature_extractor(
         model, return_nodes,
         tracer_kwargs={'leaf_modules': list(_leaf_modules), 'autowrap_functions': list(_autowrap_functions)}
     )
```

### Comparing `timm-0.8.6.dev0/timm/models/_helpers.py` & `timm-0.9.0/timm/models/_helpers.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,50 @@
 """ Model creation / weight loading / state_dict helpers
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import logging
 import os
 from collections import OrderedDict
+from typing import Any, Callable, Dict, Optional, Union
 
 import torch
-
-import timm.models._builder
+try:
+    import safetensors.torch
+    _has_safetensors = True
+except ImportError:
+    _has_safetensors = False
 
 _logger = logging.getLogger(__name__)
 
-__all__ = ['clean_state_dict', 'load_state_dict', 'load_checkpoint', 'remap_checkpoint', 'resume_checkpoint']
+__all__ = ['clean_state_dict', 'load_state_dict', 'load_checkpoint', 'remap_state_dict', 'resume_checkpoint']
 
 
-def clean_state_dict(state_dict):
+def clean_state_dict(state_dict: Dict[str, Any]) -> Dict[str, Any]:
     # 'clean' checkpoint by removing .module prefix from state dict if it exists from parallel training
-    cleaned_state_dict = OrderedDict()
+    cleaned_state_dict = {}
     for k, v in state_dict.items():
         name = k[7:] if k.startswith('module.') else k
         cleaned_state_dict[name] = v
     return cleaned_state_dict
 
 
-def load_state_dict(checkpoint_path, use_ema=True):
+def load_state_dict(
+        checkpoint_path: str,
+        use_ema: bool = True,
+        device: Union[str, torch.device] = 'cpu',
+) -> Dict[str, Any]:
     if checkpoint_path and os.path.isfile(checkpoint_path):
-        checkpoint = torch.load(checkpoint_path, map_location='cpu')
+        # Check if safetensors or not and load weights accordingly
+        if str(checkpoint_path).endswith(".safetensors"):
+            assert _has_safetensors, "`pip install safetensors` to use .safetensors"
+            checkpoint = safetensors.torch.load_file(checkpoint_path, device=device)
+        else:
+            checkpoint = torch.load(checkpoint_path, map_location=device)
+
         state_dict_key = ''
         if isinstance(checkpoint, dict):
             if use_ema and checkpoint.get('state_dict_ema', None) is not None:
                 state_dict_key = 'state_dict_ema'
             elif use_ema and checkpoint.get('model_ema', None) is not None:
                 state_dict_key = 'model_ema'
             elif 'state_dict' in checkpoint:
@@ -41,46 +55,67 @@
         _logger.info("Loaded {} from checkpoint '{}'".format(state_dict_key, checkpoint_path))
         return state_dict
     else:
         _logger.error("No checkpoint found at '{}'".format(checkpoint_path))
         raise FileNotFoundError()
 
 
-def load_checkpoint(model, checkpoint_path, use_ema=True, strict=True, remap=False):
+def load_checkpoint(
+        model: torch.nn.Module,
+        checkpoint_path: str,
+        use_ema: bool = True,
+        device: Union[str, torch.device] = 'cpu',
+        strict: bool = True,
+        remap: bool = False,
+        filter_fn: Optional[Callable] = None,
+):
     if os.path.splitext(checkpoint_path)[-1].lower() in ('.npz', '.npy'):
         # numpy checkpoint, try to load via model specific load_pretrained fn
         if hasattr(model, 'load_pretrained'):
-            timm.models._model_builder.load_pretrained(checkpoint_path)
+            model.load_pretrained(checkpoint_path)
         else:
             raise NotImplementedError('Model cannot load numpy checkpoint')
         return
-    state_dict = load_state_dict(checkpoint_path, use_ema)
+
+    state_dict = load_state_dict(checkpoint_path, use_ema, device=device)
     if remap:
-        state_dict = remap_checkpoint(model, state_dict)
+        state_dict = remap_state_dict(state_dict, model)
+    elif filter_fn:
+        state_dict = filter_fn(state_dict, model)
     incompatible_keys = model.load_state_dict(state_dict, strict=strict)
     return incompatible_keys
 
 
-def remap_checkpoint(model, state_dict, allow_reshape=True):
+def remap_state_dict(
+        state_dict: Dict[str, Any],
+        model: torch.nn.Module,
+        allow_reshape: bool = True
+):
     """ remap checkpoint by iterating over state dicts in order (ignoring original keys).
     This assumes models (and originating state dict) were created with params registered in same order.
     """
     out_dict = {}
     for (ka, va), (kb, vb) in zip(model.state_dict().items(), state_dict.items()):
-        assert va.numel == vb.numel, f'Tensor size mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'
+        assert va.numel() == vb.numel(), f'Tensor size mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'
         if va.shape != vb.shape:
             if allow_reshape:
                 vb = vb.reshape(va.shape)
             else:
                 assert False,  f'Tensor shape mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'
         out_dict[ka] = vb
     return out_dict
 
 
-def resume_checkpoint(model, checkpoint_path, optimizer=None, loss_scaler=None, log_info=True):
+def resume_checkpoint(
+        model: torch.nn.Module,
+        checkpoint_path: str,
+        optimizer: torch.optim.Optimizer = None,
+        loss_scaler: Any = None,
+        log_info: bool = True,
+):
     resume_epoch = None
     if os.path.isfile(checkpoint_path):
         checkpoint = torch.load(checkpoint_path, map_location='cpu')
         if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
             if log_info:
                 _logger.info('Restoring model state from checkpoint...')
             state_dict = clean_state_dict(checkpoint['state_dict'])
```

### Comparing `timm-0.8.6.dev0/timm/models/_hub.py` & `timm-0.9.0/timm/models/_hub.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,24 +1,36 @@
 import hashlib
 import json
 import logging
 import os
+import sys
 from functools import partial
 from pathlib import Path
 from tempfile import TemporaryDirectory
-from typing import Optional, Union
+from typing import Iterable, Optional, Union
 
 import torch
 from torch.hub import HASH_REGEX, download_url_to_file, urlparse
 
 try:
     from torch.hub import get_dir
 except ImportError:
     from torch.hub import _get_torch_home as get_dir
 
+try:
+    import safetensors.torch
+    _has_safetensors = True
+except ImportError:
+    _has_safetensors = False
+
+if sys.version_info >= (3, 8):
+    from typing import Literal
+else:
+    from typing_extensions import Literal
+
 from timm import __version__
 from timm.models._pretrained import filter_pretrained_cfg
 
 try:
     from huggingface_hub import (
         create_repo, get_hf_file_metadata,
         hf_hub_download, hf_hub_url,
@@ -31,14 +43,20 @@
     _has_hf_hub = False
 
 _logger = logging.getLogger(__name__)
 
 __all__ = ['get_cache_dir', 'download_cached_file', 'has_hf_hub', 'hf_split', 'load_model_config_from_hf',
            'load_state_dict_from_hf', 'save_for_hf', 'push_to_hf_hub']
 
+# Default name for a weights file hosted on the Huggingface Hub.
+HF_WEIGHTS_NAME = "pytorch_model.bin"  # default pytorch pkl
+HF_SAFE_WEIGHTS_NAME = "model.safetensors"  # safetensors version
+HF_OPEN_CLIP_WEIGHTS_NAME = "open_clip_pytorch_model.bin"  # default pytorch pkl
+HF_OPEN_CLIP_SAFE_WEIGHTS_NAME = "open_clip_model.safetensors"  # safetensors version
+
 
 def get_cache_dir(child_dir=''):
     """
     Returns the location of the directory where models are cached (and creates it if necessary).
     """
     # Issue warning to move data if old env is set
     if os.getenv('TORCH_MODEL_ZOO'):
@@ -92,15 +110,15 @@
     if not _has_hf_hub and necessary:
         # if no HF Hub module installed, and it is necessary to continue, raise error
         raise RuntimeError(
             'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.')
     return _has_hf_hub
 
 
-def hf_split(hf_id):
+def hf_split(hf_id: str):
     # FIXME I may change @ -> # and be parsed as fragment in a URI model name scheme
     rev_split = hf_id.split('@')
     assert 0 < len(rev_split) <= 2, 'hf_hub id should only contain one @ character to identify revision.'
     hf_model_id = rev_split[0]
     hf_revision = rev_split[-1] if len(rev_split) > 1 else None
     return hf_model_id, hf_revision
 
@@ -123,98 +141,142 @@
     hf_config = load_cfg_from_json(cached_file)
     if 'pretrained_cfg' not in hf_config:
         # old form, pull pretrain_cfg out of the base dict
         pretrained_cfg = hf_config
         hf_config = {}
         hf_config['architecture'] = pretrained_cfg.pop('architecture')
         hf_config['num_features'] = pretrained_cfg.pop('num_features', None)
-        if 'labels' in pretrained_cfg:
-            hf_config['label_name'] = pretrained_cfg.pop('labels')
+        if 'labels' in pretrained_cfg:  # deprecated name for 'label_names'
+            pretrained_cfg['label_names'] = pretrained_cfg.pop('labels')
         hf_config['pretrained_cfg'] = pretrained_cfg
 
     # NOTE currently discarding parent config as only arch name and pretrained_cfg used in timm right now
     pretrained_cfg = hf_config['pretrained_cfg']
     pretrained_cfg['hf_hub_id'] = model_id  # insert hf_hub id for pretrained weight load during model creation
     pretrained_cfg['source'] = 'hf-hub'
+
+    # model should be created with base config num_classes if its exist
     if 'num_classes' in hf_config:
-        # model should be created with parent num_classes if they exist
         pretrained_cfg['num_classes'] = hf_config['num_classes']
-    model_name = hf_config['architecture']
 
+    # label meta-data in base config overrides saved pretrained_cfg on load
+    if 'label_names' in hf_config:
+        pretrained_cfg['label_names'] = hf_config.pop('label_names')
+    if 'label_descriptions' in hf_config:
+        pretrained_cfg['label_descriptions'] = hf_config.pop('label_descriptions')
+
+    model_name = hf_config['architecture']
     return pretrained_cfg, model_name
 
 
-def load_state_dict_from_hf(model_id: str, filename: str = 'pytorch_model.bin'):
+def load_state_dict_from_hf(model_id: str, filename: str = HF_WEIGHTS_NAME):
     assert has_hf_hub(True)
-    cached_file = download_from_hf(model_id, filename)
-    state_dict = torch.load(cached_file, map_location='cpu')
-    return state_dict
-
+    hf_model_id, hf_revision = hf_split(model_id)
 
-def save_config_for_hf(model, config_path, model_config=None):
+    # Look for .safetensors alternatives and load from it if it exists
+    if _has_safetensors:
+        for safe_filename in _get_safe_alternatives(filename):
+            try:
+                cached_safe_file = hf_hub_download(repo_id=hf_model_id, filename=safe_filename, revision=hf_revision)
+                _logger.info(
+                    f"[{model_id}] Safe alternative available for '{filename}' "
+                    f"(as '{safe_filename}'). Loading weights using safetensors.")
+                return safetensors.torch.load_file(cached_safe_file, device="cpu")
+            except EntryNotFoundError:
+                pass
+
+    # Otherwise, load using pytorch.load
+    cached_file = hf_hub_download(hf_model_id, filename=filename, revision=hf_revision)
+    _logger.debug(f"[{model_id}] Safe alternative not found for '{filename}'. Loading weights using default pytorch.")
+    return torch.load(cached_file, map_location='cpu')
+
+
+def save_config_for_hf(
+        model,
+        config_path: str,
+        model_config: Optional[dict] = None
+):
     model_config = model_config or {}
     hf_config = {}
     pretrained_cfg = filter_pretrained_cfg(model.pretrained_cfg, remove_source=True, remove_null=True)
     # set some values at root config level
     hf_config['architecture'] = pretrained_cfg.pop('architecture')
     hf_config['num_classes'] = model_config.get('num_classes', model.num_classes)
     hf_config['num_features'] = model_config.get('num_features', model.num_features)
     global_pool_type = model_config.get('global_pool', getattr(model, 'global_pool', None))
     if isinstance(global_pool_type, str) and global_pool_type:
         hf_config['global_pool'] = global_pool_type
 
     if 'labels' in model_config:
         _logger.warning(
-            "'labels' as a config field for timm models is deprecated. Please use 'label_name' and 'display_name'. "
-            "Using provided 'label' field as 'label_name'.")
-        model_config['label_name'] = model_config.pop('labels')
-
-    label_name = model_config.pop('label_name', None)
-    if label_name:
-        assert isinstance(label_name, (dict, list, tuple))
+            "'labels' as a config field for is deprecated. Please use 'label_names' and 'label_descriptions'."
+            " Renaming provided 'labels' field to 'label_names'.")
+        model_config.setdefault('label_names', model_config.pop('labels'))
+
+    label_names = model_config.pop('label_names', None)
+    if label_names:
+        assert isinstance(label_names, (dict, list, tuple))
         # map label id (classifier index) -> unique label name (ie synset for ImageNet, MID for OpenImages)
         # can be a dict id: name if there are id gaps, or tuple/list if no gaps.
-        hf_config['label_name'] = model_config['label_name']
+        hf_config['label_names'] = label_names
 
-    display_name = model_config.pop('display_name', None)
-    if display_name:
-        assert isinstance(display_name, dict)
-        # map label_name -> user interface display name
-        hf_config['display_name'] = model_config['display_name']
+    label_descriptions = model_config.pop('label_descriptions', None)
+    if label_descriptions:
+        assert isinstance(label_descriptions, dict)
+        # maps label names -> descriptions
+        hf_config['label_descriptions'] = label_descriptions
 
     hf_config['pretrained_cfg'] = pretrained_cfg
     hf_config.update(model_config)
 
     with config_path.open('w') as f:
         json.dump(hf_config, f, indent=2)
 
 
-def save_for_hf(model, save_directory, model_config=None):
+def save_for_hf(
+        model,
+        save_directory: str,
+        model_config: Optional[dict] = None,
+        safe_serialization: Union[bool, Literal["both"]] = False,
+):
     assert has_hf_hub(True)
     save_directory = Path(save_directory)
     save_directory.mkdir(exist_ok=True, parents=True)
 
-    weights_path = save_directory / 'pytorch_model.bin'
-    torch.save(model.state_dict(), weights_path)
+    # Save model weights, either safely (using safetensors), or using legacy pytorch approach or both.
+    tensors = model.state_dict()
+    if safe_serialization is True or safe_serialization == "both":
+        assert _has_safetensors, "`pip install safetensors` to use .safetensors"
+        safetensors.torch.save_file(tensors, save_directory / HF_SAFE_WEIGHTS_NAME)
+    if safe_serialization is False or safe_serialization == "both":
+        torch.save(tensors, save_directory / HF_WEIGHTS_NAME)
 
     config_path = save_directory / 'config.json'
     save_config_for_hf(model, config_path, model_config=model_config)
 
 
 def push_to_hf_hub(
-    model,
-    repo_id: str,
-    commit_message: str = 'Add model',
-    token: Optional[str] = None,
-    revision: Optional[str] = None,
-    private: bool = False,
-    create_pr: bool = False,
-    model_config: Optional[dict] = None,
-    model_card: Optional[dict] = None,
+        model,
+        repo_id: str,
+        commit_message: str = 'Add model',
+        token: Optional[str] = None,
+        revision: Optional[str] = None,
+        private: bool = False,
+        create_pr: bool = False,
+        model_config: Optional[dict] = None,
+        model_card: Optional[dict] = None,
+        safe_serialization: Union[bool, Literal["both"]] = False,
 ):
+    """
+    Arguments:
+        (...)
+        safe_serialization (`bool` or `"both"`, *optional*, defaults to `False`):
+            Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).
+            Can be set to `"both"` in order to push both safe and unsafe weights.
+    """
     # Create repo if it doesn't exist yet
     repo_url = create_repo(repo_id, token=token, private=private, exist_ok=True)
 
     # Infer complete repo_id from repo_url
     # Can be different from the input `repo_id` if repo_owner was implicit
     _, repo_owner, repo_name = repo_type_and_id_from_hf_id(repo_url)
     repo_id = f"{repo_owner}/{repo_name}"
@@ -225,38 +287,96 @@
         has_readme = True
     except EntryNotFoundError:
         has_readme = False
 
     # Dump model and push to Hub
     with TemporaryDirectory() as tmpdir:
         # Save model weights and config.
-        save_for_hf(model, tmpdir, model_config=model_config)
+        save_for_hf(model, tmpdir, model_config=model_config, safe_serialization=safe_serialization)
 
         # Add readme if it does not exist
         if not has_readme:
             model_card = model_card or {}
             model_name = repo_id.split('/')[-1]
             readme_path = Path(tmpdir) / "README.md"
-            readme_text = "---\n"
-            readme_text += "tags:\n- image-classification\n- timm\n"
-            readme_text += "library_tag: timm\n"
-            readme_text += f"license: {model_card.get('license', 'apache-2.0')}\n"
-            readme_text += "---\n"
-            readme_text += f"# Model card for {model_name}\n"
-            if 'description' in model_card:
-                readme_text += f"\n{model_card['description']}\n"
-            if 'details' in model_card:
-                readme_text += f"\n## Model Details\n"
-                for k, v in model_card['details'].items():
-                    readme_text += f"- **{k}:** {v}\n"
-            if 'citation' in model_card:
-                readme_text += f"\n## Citation\n```\n{model_card['citation']}```\n"
+            readme_text = generate_readme(model_card, model_name)
             readme_path.write_text(readme_text)
 
         # Upload model and return
         return upload_folder(
             repo_id=repo_id,
             folder_path=tmpdir,
             revision=revision,
             create_pr=create_pr,
             commit_message=commit_message,
         )
+
+
+def generate_readme(model_card: dict, model_name: str):
+    readme_text = "---\n"
+    readme_text += "tags:\n- image-classification\n- timm\n"
+    readme_text += "library_name: timm\n"
+    readme_text += f"license: {model_card.get('license', 'apache-2.0')}\n"
+    if 'details' in model_card and 'Dataset' in model_card['details']:
+        readme_text += 'datasets:\n'
+        if isinstance(model_card['details']['Dataset'], (tuple, list)):
+            for d in model_card['details']['Dataset']:
+                readme_text += f"- {d.lower()}\n"
+        else:
+            readme_text += f"- {model_card['details']['Dataset'].lower()}\n"
+        if 'Pretrain Dataset' in model_card['details']:
+            if isinstance(model_card['details']['Pretrain Dataset'], (tuple, list)):
+                for d in model_card['details']['Pretrain Dataset']:
+                    readme_text += f"- {d.lower()}\n"
+            else:
+                readme_text += f"- {model_card['details']['Pretrain Dataset'].lower()}\n"
+    readme_text += "---\n"
+    readme_text += f"# Model card for {model_name}\n"
+    if 'description' in model_card:
+        readme_text += f"\n{model_card['description']}\n"
+    if 'details' in model_card:
+        readme_text += f"\n## Model Details\n"
+        for k, v in model_card['details'].items():
+            if isinstance(v, (list, tuple)):
+                readme_text += f"- **{k}:**\n"
+                for vi in v:
+                    readme_text += f"  - {vi}\n"
+            elif isinstance(v, dict):
+                readme_text += f"- **{k}:**\n"
+                for ki, vi in v.items():
+                    readme_text += f"  - {ki}: {vi}\n"
+            else:
+                readme_text += f"- **{k}:** {v}\n"
+    if 'usage' in model_card:
+        readme_text += f"\n## Model Usage\n"
+        readme_text += model_card['usage']
+        readme_text += '\n'
+
+    if 'comparison' in model_card:
+        readme_text += f"\n## Model Comparison\n"
+        readme_text += model_card['comparison']
+        readme_text += '\n'
+
+    if 'citation' in model_card:
+        readme_text += f"\n## Citation\n"
+        if not isinstance(model_card['citation'], (list, tuple)):
+            citations = [model_card['citation']]
+        else:
+            citations = model_card['citation']
+        for c in citations:
+            readme_text += f"```bibtex\n{c}\n```\n"
+    return readme_text
+
+
+def _get_safe_alternatives(filename: str) -> Iterable[str]:
+    """Returns potential safetensors alternatives for a given filename.
+
+    Use case:
+        When downloading a model from the Huggingface Hub, we first look if a .safetensors file exists and if yes, we use it.
+        Main use case is filename "pytorch_model.bin" => check for "model.safetensors" or "pytorch_model.safetensors".
+    """
+    if filename == HF_WEIGHTS_NAME:
+        yield HF_SAFE_WEIGHTS_NAME
+    # if filename == HF_OPEN_CLIP_WEIGHTS_NAME:  # FIXME tracking safetensors yet
+    #     yield HF_OPEN_CLIP_SAFE_WEIGHTS_NAME
+    if filename not in (HF_WEIGHTS_NAME, HF_OPEN_CLIP_WEIGHTS_NAME) and filename.endswith(".bin"):
+        yield filename[:-4] + ".safetensors"
```

### Comparing `timm-0.8.6.dev0/timm/models/_manipulate.py` & `timm-0.9.0/timm/models/_manipulate.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,70 +1,85 @@
 import collections.abc
 import math
 import re
 from collections import defaultdict
 from itertools import chain
-from typing import Callable, Union, Dict
+from typing import Any, Callable, Dict, Iterator, Tuple, Type, Union
 
 import torch
 from torch import nn as nn
 from torch.utils.checkpoint import checkpoint
 
 __all__ = ['model_parameters', 'named_apply', 'named_modules', 'named_modules_with_params', 'adapt_input_conv',
            'group_with_matcher', 'group_modules', 'group_parameters', 'flatten_modules', 'checkpoint_seq']
 
 
-def model_parameters(model, exclude_head=False):
+def model_parameters(model: nn.Module, exclude_head: bool = False):
     if exclude_head:
         # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering
         return [p for p in model.parameters()][:-2]
     else:
         return model.parameters()
 
 
-def named_apply(fn: Callable, module: nn.Module, name='', depth_first=True, include_root=False) -> nn.Module:
+def named_apply(
+        fn: Callable,
+        module: nn.Module, name='',
+        depth_first: bool = True,
+        include_root: bool = False,
+) -> nn.Module:
     if not depth_first and include_root:
         fn(module=module, name=name)
     for child_name, child_module in module.named_children():
         child_name = '.'.join((name, child_name)) if name else child_name
         named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)
     if depth_first and include_root:
         fn(module=module, name=name)
     return module
 
 
-def named_modules(module: nn.Module, name='', depth_first=True, include_root=False):
+def named_modules(
+        module: nn.Module,
+        name: str = '',
+        depth_first: bool = True,
+        include_root: bool = False,
+):
     if not depth_first and include_root:
         yield name, module
     for child_name, child_module in module.named_children():
         child_name = '.'.join((name, child_name)) if name else child_name
         yield from named_modules(
             module=child_module, name=child_name, depth_first=depth_first, include_root=True)
     if depth_first and include_root:
         yield name, module
 
 
-def named_modules_with_params(module: nn.Module, name='', depth_first=True, include_root=False):
+def named_modules_with_params(
+        module: nn.Module,
+        name: str = '',
+        depth_first: bool = True,
+        include_root: bool = False,
+):
     if module._parameters and not depth_first and include_root:
         yield name, module
     for child_name, child_module in module.named_children():
         child_name = '.'.join((name, child_name)) if name else child_name
         yield from named_modules_with_params(
             module=child_module, name=child_name, depth_first=depth_first, include_root=True)
     if module._parameters and depth_first and include_root:
         yield name, module
 
 
 MATCH_PREV_GROUP = (99999,)
 
 
 def group_with_matcher(
-        named_objects,
+        named_objects: Iterator[Tuple[str, Any]],
         group_matcher: Union[Dict, Callable],
-        output_values: bool = False,
+        return_values: bool = False,
         reverse: bool = False
 ):
     if isinstance(group_matcher, dict):
         # dictionary matcher contains a dict of raw-string regex expr that must be compiled
         compiled = []
         for group_ordinal, (group_name, mspec) in enumerate(group_matcher.items()):
             if mspec is None:
@@ -92,57 +107,62 @@
             if not isinstance(ord, collections.abc.Iterable):
                 return ord,
             return tuple(ord)
 
     # map layers into groups via ordinals (ints or tuples of ints) from matcher
     grouping = defaultdict(list)
     for k, v in named_objects:
-        grouping[_get_grouping(k)].append(v if output_values else k)
+        grouping[_get_grouping(k)].append(v if return_values else k)
 
     # remap to integers
     layer_id_to_param = defaultdict(list)
     lid = -1
     for k in sorted(filter(lambda x: x is not None, grouping.keys())):
         if lid < 0 or k[-1] != MATCH_PREV_GROUP[0]:
             lid += 1
         layer_id_to_param[lid].extend(grouping[k])
 
     if reverse:
-        assert not output_values, "reverse mapping only sensible for name output"
+        assert not return_values, "reverse mapping only sensible for name output"
         # output reverse mapping
         param_to_layer_id = {}
         for lid, lm in layer_id_to_param.items():
             for n in lm:
                 param_to_layer_id[n] = lid
         return param_to_layer_id
 
     return layer_id_to_param
 
 
 def group_parameters(
         module: nn.Module,
         group_matcher,
-        output_values=False,
-        reverse=False,
+        return_values: bool = False,
+        reverse: bool = False,
 ):
     return group_with_matcher(
-        module.named_parameters(), group_matcher, output_values=output_values, reverse=reverse)
+        module.named_parameters(), group_matcher, return_values=return_values, reverse=reverse)
 
 
 def group_modules(
         module: nn.Module,
         group_matcher,
-        output_values=False,
-        reverse=False,
+        return_values: bool = False,
+        reverse: bool = False,
 ):
     return group_with_matcher(
-        named_modules_with_params(module), group_matcher, output_values=output_values, reverse=reverse)
+        named_modules_with_params(module), group_matcher, return_values=return_values, reverse=reverse)
 
 
-def flatten_modules(named_modules, depth=1, prefix='', module_types='sequential'):
+def flatten_modules(
+        named_modules: Iterator[Tuple[str, nn.Module]],
+        depth: int = 1,
+        prefix: Union[str, Tuple[str, ...]] = '',
+        module_types: Union[str, Tuple[Type[nn.Module]]] = 'sequential',
+):
     prefix_is_tuple = isinstance(prefix, tuple)
     if isinstance(module_types, str):
         if module_types == 'container':
             module_types = (nn.Sequential, nn.ModuleList, nn.ModuleDict)
         else:
             module_types = (nn.Sequential,)
     for name, module in named_modules:
```

### Comparing `timm-0.8.6.dev0/timm/models/_pretrained.py` & `timm-0.9.0/timm/models/_pretrained.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 import copy
 from collections import deque, defaultdict
 from dataclasses import dataclass, field, replace, asdict
 from typing import Any, Deque, Dict, Tuple, Optional, Union
 
 
-__all__ = ['PretrainedCfg', 'filter_pretrained_cfg', 'DefaultCfg', 'split_model_name_tag', 'generate_default_cfgs']
+__all__ = ['PretrainedCfg', 'filter_pretrained_cfg', 'DefaultCfg']
 
 
 @dataclass
 class PretrainedCfg:
     """
     """
-    # weight locations
-    url: Optional[Union[str, Tuple[str, str]]] = None
-    file: Optional[str] = None
-    hf_hub_id: Optional[str] = None
-    hf_hub_filename: Optional[str] = None
+    # weight source locations
+    url: Optional[Union[str, Tuple[str, str]]] = None  # remote URL
+    file: Optional[str] = None  # local / shared filesystem path
+    state_dict: Optional[Dict[str, Any]] = None  # in-memory state dict
+    hf_hub_id: Optional[str] = None  # Hugging Face Hub model id ('organization/model')
+    hf_hub_filename: Optional[str] = None  # Hugging Face Hub filename (overrides default)
 
     source: Optional[str] = None  # source of cfg / weight location used (url, file, hf-hub)
     architecture: Optional[str] = None  # architecture variant can be set when not implicit
     tag: Optional[str] = None  # pretrained tag of source
     custom_load: bool = False  # use custom model specific model.load_pretrained() (ie for npz files)
 
     # input / data config
@@ -30,17 +31,19 @@
     interpolation: str = 'bicubic'
     crop_pct: float = 0.875
     test_crop_pct: Optional[float] = None
     crop_mode: str = 'center'
     mean: Tuple[float, ...] = (0.485, 0.456, 0.406)
     std: Tuple[float, ...] = (0.229, 0.224, 0.225)
 
-    # head config
+    # head / classifier config and meta-data
     num_classes: int = 1000
     label_offset: Optional[int] = None
+    label_names: Optional[Tuple[str]] = None
+    label_descriptions: Optional[Dict[str, str]] = None
 
     # model attributes that vary with above or required for pretrained adaptation
     pool_size: Optional[Tuple[int, ...]] = None
     test_pool_size: Optional[Tuple[int, ...]] = None
     first_conv: Optional[str] = None
     classifier: Optional[str] = None
 
@@ -85,45 +88,7 @@
     def default(self):
         return self.cfgs[self.tags[0]]
 
     @property
     def default_with_tag(self):
         tag = self.tags[0]
         return tag, self.cfgs[tag]
-
-
-def split_model_name_tag(model_name: str, no_tag=''):
-    model_name, *tag_list = model_name.split('.', 1)
-    tag = tag_list[0] if tag_list else no_tag
-    return model_name, tag
-
-
-def generate_default_cfgs(cfgs: Dict[str, Union[Dict[str, Any], PretrainedCfg]]):
-    out = defaultdict(DefaultCfg)
-    default_set = set()  # no tag and tags ending with * are prioritized as default
-
-    for k, v in cfgs.items():
-        if isinstance(v, dict):
-            v = PretrainedCfg(**v)
-        has_weights = v.has_weights
-
-        model, tag = split_model_name_tag(k)
-        is_default_set = model in default_set
-        priority = (has_weights and not tag) or (tag.endswith('*') and not is_default_set)
-        tag = tag.strip('*')
-
-        default_cfg = out[model]
-
-        if priority:
-            default_cfg.tags.appendleft(tag)
-            default_set.add(model)
-        elif has_weights and not default_cfg.is_pretrained:
-            default_cfg.tags.appendleft(tag)
-        else:
-            default_cfg.tags.append(tag)
-
-        if has_weights:
-            default_cfg.is_pretrained = True
-
-        default_cfg.cfgs[tag] = v
-
-    return out
```

### Comparing `timm-0.8.6.dev0/timm/models/_prune.py` & `timm-0.9.0/timm/models/_prune.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import os
+import pkgutil
 from copy import deepcopy
 
 from torch import nn as nn
 
 from timm.layers import Conv2dSame, BatchNormAct2d, Linear
 
 __all__ = ['extract_layer', 'set_layer', 'adapt_model_from_string', 'adapt_model_from_file']
@@ -104,10 +105,9 @@
     new_module.eval()
     parent_module.eval()
 
     return new_module
 
 
 def adapt_model_from_file(parent_module, model_variant):
-    adapt_file = os.path.join(os.path.dirname(__file__), '_pruned', model_variant + '.txt')
-    with open(adapt_file, 'r') as f:
-        return adapt_model_from_string(parent_module, f.read().strip())
+    adapt_data = pkgutil.get_data(__name__, os.path.join('_pruned', model_variant + '.txt'))
+    return adapt_model_from_string(parent_module, adapt_data.decode('utf-8').strip())
```

### Comparing `timm-0.8.6.dev0/timm/models/_registry.py` & `timm-0.9.0/timm/models/_registry.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,50 +1,93 @@
 """ Model Registry
 Hacked together by / Copyright 2020 Ross Wightman
 """
 
 import fnmatch
 import re
 import sys
+import warnings
 from collections import defaultdict, deque
 from copy import deepcopy
 from dataclasses import replace
-from typing import List, Optional, Union, Tuple
+from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Sequence, Union, Tuple
 
-from ._pretrained import PretrainedCfg, DefaultCfg, split_model_name_tag
+from ._pretrained import PretrainedCfg, DefaultCfg
 
 __all__ = [
+    'split_model_name_tag', 'get_arch_name', 'register_model', 'generate_default_cfgs',
     'list_models', 'list_pretrained', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',
-    'get_pretrained_cfg_value', 'is_model_pretrained', 'get_arch_name']
+    'get_pretrained_cfg_value', 'is_model_pretrained'
+]
 
-_module_to_models = defaultdict(set)  # dict of sets to check membership of model in module
-_model_to_module = {}  # mapping of model names to module names
-_model_entrypoints = {}  # mapping of model names to architecture entrypoint fns
-_model_has_pretrained = set()  # set of model names that have pretrained weight url present
-_model_default_cfgs = dict()  # central repo for model arch -> default cfg objects
-_model_pretrained_cfgs = dict()  # central repo for model arch.tag -> pretrained cfgs
-_model_with_tags = defaultdict(list)  # shortcut to map each model arch to all model + tag names
+_module_to_models: Dict[str, Set[str]] = defaultdict(set)  # dict of sets to check membership of model in module
+_model_to_module: Dict[str, str] = {}  # mapping of model names to module names
+_model_entrypoints: Dict[str, Callable[..., Any]] = {}  # mapping of model names to architecture entrypoint fns
+_model_has_pretrained: Set[str] = set()  # set of model names that have pretrained weight url present
+_model_default_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch -> default cfg objects
+_model_pretrained_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch.tag -> pretrained cfgs
+_model_with_tags: Dict[str, List[str]] = defaultdict(list)  # shortcut to map each model arch to all model + tag names
+_module_to_deprecated_models: Dict[str, Dict[str, Optional[str]]] = defaultdict(dict)
+_deprecated_models: Dict[str, Optional[str]] = {}
+
+
+def split_model_name_tag(model_name: str, no_tag: str = '') -> Tuple[str, str]:
+    model_name, *tag_list = model_name.split('.', 1)
+    tag = tag_list[0] if tag_list else no_tag
+    return model_name, tag
 
 
-def get_arch_name(model_name: str) -> Tuple[str, Optional[str]]:
+def get_arch_name(model_name: str) -> str:
     return split_model_name_tag(model_name)[0]
 
 
-def register_model(fn):
+def generate_default_cfgs(cfgs: Dict[str, Union[Dict[str, Any], PretrainedCfg]]):
+    out = defaultdict(DefaultCfg)
+    default_set = set()  # no tag and tags ending with * are prioritized as default
+
+    for k, v in cfgs.items():
+        if isinstance(v, dict):
+            v = PretrainedCfg(**v)
+        has_weights = v.has_weights
+
+        model, tag = split_model_name_tag(k)
+        is_default_set = model in default_set
+        priority = (has_weights and not tag) or (tag.endswith('*') and not is_default_set)
+        tag = tag.strip('*')
+
+        default_cfg = out[model]
+
+        if priority:
+            default_cfg.tags.appendleft(tag)
+            default_set.add(model)
+        elif has_weights and not default_cfg.is_pretrained:
+            default_cfg.tags.appendleft(tag)
+        else:
+            default_cfg.tags.append(tag)
+
+        if has_weights:
+            default_cfg.is_pretrained = True
+
+        default_cfg.cfgs[tag] = v
+
+    return out
+
+
+def register_model(fn: Callable[..., Any]) -> Callable[..., Any]:
     # lookup containing module
     mod = sys.modules[fn.__module__]
     module_name_split = fn.__module__.split('.')
     module_name = module_name_split[-1] if len(module_name_split) else ''
 
     # add model to __all__ in module
     model_name = fn.__name__
     if hasattr(mod, '__all__'):
         mod.__all__.append(model_name)
     else:
-        mod.__all__ = [model_name]
+        mod.__all__ = [model_name]  # type: ignore
 
     # add entries to registry dict/sets
     _model_entrypoints[model_name] = fn
     _model_to_module[model_name] = module_name
     _module_to_models[module_name].add(model_name)
     if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:
         # this will catch all models that have entrypoint matching cfg key, but miss any aliasing
@@ -83,144 +126,203 @@
                 _model_with_tags[model_name].append(model_name)  # has empty tag (to slowly remove these instances)
 
         _model_default_cfgs[model_name] = default_cfg
 
     return fn
 
 
-def _natural_key(string_):
+def _deprecated_model_shim(deprecated_name: str, current_fn: Callable = None, current_tag: str = ''):
+    def _fn(pretrained=False, **kwargs):
+        assert current_fn is not None,  f'Model {deprecated_name} has been removed with no replacement.'
+        current_name = '.'.join([current_fn.__name__, current_tag]) if current_tag else current_fn.__name__
+        warnings.warn(f'Mapping deprecated model name {deprecated_name} to current {current_name}.', stacklevel=2)
+        pretrained_cfg = kwargs.pop('pretrained_cfg', None)
+        return current_fn(pretrained=pretrained, pretrained_cfg=pretrained_cfg or current_tag, **kwargs)
+    return _fn
+
+
+def register_model_deprecations(module_name: str, deprecation_map: Dict[str, Optional[str]]):
+    mod = sys.modules[module_name]
+    module_name_split = module_name.split('.')
+    module_name = module_name_split[-1] if len(module_name_split) else ''
+
+    for deprecated, current in deprecation_map.items():
+        if hasattr(mod, '__all__'):
+            mod.__all__.append(deprecated)
+        current_fn = None
+        current_tag = ''
+        if current:
+            current_name, current_tag = split_model_name_tag(current)
+            current_fn = getattr(mod, current_name)
+        deprecated_entrypoint_fn = _deprecated_model_shim(deprecated, current_fn, current_tag)
+        setattr(mod, deprecated, deprecated_entrypoint_fn)
+        _model_entrypoints[deprecated] = deprecated_entrypoint_fn
+        _model_to_module[deprecated] = module_name
+        _module_to_models[module_name].add(deprecated)
+        _deprecated_models[deprecated] = current
+        _module_to_deprecated_models[module_name][deprecated] = current
+
+
+def _natural_key(string_: str) -> List[Union[int, str]]:
+    """See https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/"""
     return [int(s) if s.isdigit() else s for s in re.split(r'(\d+)', string_.lower())]
 
 
+def _expand_filter(filter: str):
+    """ expand a 'base_filter' to 'base_filter.*' if no tag portion"""
+    filter_base, filter_tag = split_model_name_tag(filter)
+    if not filter_tag:
+        return ['.'.join([filter_base, '*']), filter]
+    else:
+        return [filter]
+
+
 def list_models(
         filter: Union[str, List[str]] = '',
         module: str = '',
-        pretrained=False,
-        exclude_filters: str = '',
+        pretrained: bool = False,
+        exclude_filters: Union[str, List[str]] = '',
         name_matches_cfg: bool = False,
         include_tags: Optional[bool] = None,
-):
+) -> List[str]:
     """ Return list of available model names, sorted alphabetically
 
     Args:
-        filter (str) - Wildcard filter string that works with fnmatch
-        module (str) - Limit model selection to a specific submodule (ie 'vision_transformer')
-        pretrained (bool) - Include only models with valid pretrained weights if True
-        exclude_filters (str or list[str]) - Wildcard filters to exclude models after including them with filter
-        name_matches_cfg (bool) - Include only models w/ model_name matching default_cfg name (excludes some aliases)
-        include_tags (Optional[boo]) - Include pretrained tags in model names (model.tag). If None, defaults
+        filter - Wildcard filter string that works with fnmatch
+        module - Limit model selection to a specific submodule (ie 'vision_transformer')
+        pretrained - Include only models with valid pretrained weights if True
+        exclude_filters - Wildcard filters to exclude models after including them with filter
+        name_matches_cfg - Include only models w/ model_name matching default_cfg name (excludes some aliases)
+        include_tags - Include pretrained tags in model names (model.tag). If None, defaults
             set to True when pretrained=True else False (default: None)
+
+    Returns:
+        models - The sorted list of models
+
     Example:
         model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'
         model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module
     """
+    if filter:
+        include_filters = filter if isinstance(filter, (tuple, list)) else [filter]
+    else:
+        include_filters = []
+
     if include_tags is None:
         # FIXME should this be default behaviour? or default to include_tags=True?
         include_tags = pretrained
 
-    if module:
-        all_models = list(_module_to_models[module])
-    else:
-        all_models = _model_entrypoints.keys()
+    all_models: Set[str] = _module_to_models[module] if module else set(_model_entrypoints.keys())
+    all_models = all_models - _deprecated_models.keys()  # remove deprecated models from listings
 
     if include_tags:
         # expand model names to include names w/ pretrained tags
-        models_with_tags = []
+        models_with_tags: Set[str] = set()
         for m in all_models:
-            models_with_tags.extend(_model_with_tags[m])
+            models_with_tags.update(_model_with_tags[m])
         all_models = models_with_tags
+        # expand include and exclude filters to include a '.*' for proper match if no tags in filter
+        include_filters = [ef for f in include_filters for ef in _expand_filter(f)]
+        exclude_filters = [ef for f in exclude_filters for ef in _expand_filter(f)]
 
-    if filter:
-        models = []
-        include_filters = filter if isinstance(filter, (tuple, list)) else [filter]
+    if include_filters:
+        models: Set[str] = set()
         for f in include_filters:
             include_models = fnmatch.filter(all_models, f)  # include these models
             if len(include_models):
-                models = set(models).union(include_models)
+                models = models.union(include_models)
     else:
         models = all_models
 
     if exclude_filters:
         if not isinstance(exclude_filters, (tuple, list)):
             exclude_filters = [exclude_filters]
         for xf in exclude_filters:
             exclude_models = fnmatch.filter(models, xf)  # exclude these models
             if len(exclude_models):
-                models = set(models).difference(exclude_models)
+                models = models.difference(exclude_models)
 
     if pretrained:
         models = _model_has_pretrained.intersection(models)
 
     if name_matches_cfg:
         models = set(_model_pretrained_cfgs).intersection(models)
 
-    return list(sorted(models, key=_natural_key))
+    return sorted(models, key=_natural_key)
 
 
 def list_pretrained(
         filter: Union[str, List[str]] = '',
         exclude_filters: str = '',
-):
+) -> List[str]:
     return list_models(
         filter=filter,
         pretrained=True,
         exclude_filters=exclude_filters,
         include_tags=True,
     )
 
 
-def is_model(model_name):
+def get_deprecated_models(module: str = '') -> Dict[str, str]:
+    all_deprecated = _module_to_deprecated_models[module] if module else _deprecated_models
+    return deepcopy(all_deprecated)
+
+
+def is_model(model_name: str) -> bool:
     """ Check if a model name exists
     """
     arch_name = get_arch_name(model_name)
     return arch_name in _model_entrypoints
 
 
-def model_entrypoint(model_name, module_filter: Optional[str] = None):
+def model_entrypoint(model_name: str, module_filter: Optional[str] = None) -> Callable[..., Any]:
     """Fetch a model entrypoint for specified model name
     """
     arch_name = get_arch_name(model_name)
     if module_filter and arch_name not in _module_to_models.get(module_filter, {}):
         raise RuntimeError(f'Model ({model_name} not found in module {module_filter}.')
     return _model_entrypoints[arch_name]
 
 
-def list_modules():
+def list_modules() -> List[str]:
     """ Return list of module names that contain models / model entrypoints
     """
     modules = _module_to_models.keys()
-    return list(sorted(modules))
+    return sorted(modules)
 
 
-def is_model_in_modules(model_name, module_names):
+def is_model_in_modules(
+        model_name: str, module_names: Union[Tuple[str, ...], List[str], Set[str]]
+) -> bool:
     """Check if a model exists within a subset of modules
+
     Args:
-        model_name (str) - name of model to check
-        module_names (tuple, list, set) - names of modules to search in
+        model_name - name of model to check
+        module_names - names of modules to search in
     """
     arch_name = get_arch_name(model_name)
     assert isinstance(module_names, (tuple, list, set))
     return any(arch_name in _module_to_models[n] for n in module_names)
 
 
-def is_model_pretrained(model_name):
+def is_model_pretrained(model_name: str) -> bool:
     return model_name in _model_has_pretrained
 
 
-def get_pretrained_cfg(model_name, allow_unregistered=True):
+def get_pretrained_cfg(model_name: str, allow_unregistered: bool = True) -> Optional[PretrainedCfg]:
     if model_name in _model_pretrained_cfgs:
         return deepcopy(_model_pretrained_cfgs[model_name])
     arch_name, tag = split_model_name_tag(model_name)
     if arch_name in _model_default_cfgs:
         # if model arch exists, but the tag is wrong, error out
         raise RuntimeError(f'Invalid pretrained tag ({tag}) for {arch_name}.')
     if allow_unregistered:
         # if model arch doesn't exist, it has no pretrained_cfg registered, allow a default to be created
         return None
     raise RuntimeError(f'Model architecture ({arch_name}) has no pretrained cfg registered.')
 
 
-def get_pretrained_cfg_value(model_name, cfg_key):
+def get_pretrained_cfg_value(model_name: str, cfg_key: str) -> Optional[Any]:
     """ Get a specific model default_cfg value by key. None if key doesn't exist.
     """
     cfg = get_pretrained_cfg(model_name, allow_unregistered=False)
     return getattr(cfg, cfg_key, None)
```

### Comparing `timm-0.8.6.dev0/timm/models/beit.py` & `timm-0.9.0/timm/models/beit.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,25 +17,14 @@
 author={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},
 year={2022},
 eprint={2208.06366},
 archivePrefix={arXiv},
 primaryClass={cs.CV}
 }
 
-EVA from https://github.com/baaivision/EVA , paper: https://arxiv.org/abs/2211.07636
-
-@article{EVA,
-  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
-  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang,
-  Tiejun and Wang, Xinlong and Cao, Yue},
-  journal={arXiv preprint arXiv:2211.07636},
-  year={2022}
-}
-
-
 At this point only the 1k fine-tuned classification weights and model configs have been added,
 see original source above for pre-training models and procedure.
 
 Modifications by / Copyright 2021 Ross Wightman, original copyrights below
 """
 # --------------------------------------------------------
 # BEIT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)
@@ -45,30 +34,28 @@
 # By Hangbo Bao
 # Based on timm and DeiT code bases
 # https://github.com/rwightman/pytorch-image-models/tree/master/timm
 # https://github.com/facebookresearch/deit/
 # https://github.com/facebookresearch/dino
 # --------------------------------------------------------'
 
-# EVA models Copyright (c) 2022 BAAI-Vision
-
 import math
 from functools import partial
-from typing import Optional, Tuple
+from typing import Callable, Final, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.utils.checkpoint import checkpoint
 
-from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
-from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_
+from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
+from timm.layers import PatchEmbed, Mlp, SwiGLU, LayerNorm, DropPath, trunc_normal_, use_fused_attn
+
 from ._builder import build_model_with_cfg
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model
 from .vision_transformer import checkpoint_filter_fn
 
 __all__ = ['Beit']
 
 
 def gen_relative_position_index(window_size: Tuple[int, int]) -> torch.Tensor:
     num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3
@@ -89,24 +76,34 @@
     relative_position_index[0, 0:] = num_relative_distance - 3
     relative_position_index[0:, 0] = num_relative_distance - 2
     relative_position_index[0, 0] = num_relative_distance - 1
     return relative_position_index
 
 
 class Attention(nn.Module):
+    fused_attn: Final[bool]
+
     def __init__(
-            self, dim, num_heads=8, qkv_bias=False, attn_drop=0.,
-            proj_drop=0., window_size=None, attn_head_dim=None):
+            self,
+            dim: int,
+            num_heads: int = 8,
+            qkv_bias: bool = False,
+            attn_drop: float = 0.,
+            proj_drop: float = 0.,
+            window_size: Optional[Tuple[int, int]] = None,
+            attn_head_dim: Optional[int] = None,
+    ):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         if attn_head_dim is not None:
             head_dim = attn_head_dim
         all_head_dim = head_dim * self.num_heads
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)
         if qkv_bias:
             self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
             self.register_buffer('k_bias', torch.zeros(all_head_dim), persistent=False)
             self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
         else:
@@ -139,63 +136,113 @@
 
     def forward(self, x, shared_rel_pos_bias: Optional[torch.Tensor] = None):
         B, N, C = x.shape
 
         qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias)) if self.q_bias is not None else None
         qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
         qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
-
-        q = q * self.scale
-        attn = (q @ k.transpose(-2, -1))
+        q, k, v = qkv.unbind(0)  # B, num_heads, N, head_dim
 
-        if self.relative_position_bias_table is not None:
-            attn = attn + self._get_rel_pos_bias()
-        if shared_rel_pos_bias is not None:
-            attn = attn + shared_rel_pos_bias
+        if self.fused_attn:
+            rel_pos_bias = None
+            if self.relative_position_bias_table is not None:
+                rel_pos_bias = self._get_rel_pos_bias()
+                if shared_rel_pos_bias is not None:
+                    rel_pos_bias = rel_pos_bias + shared_rel_pos_bias
+            elif shared_rel_pos_bias is not None:
+                rel_pos_bias = shared_rel_pos_bias
+
+            x = F.scaled_dot_product_attention(
+                q, k, v,
+                attn_mask=rel_pos_bias,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = (q @ k.transpose(-2, -1))
 
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+            if self.relative_position_bias_table is not None:
+                attn = attn + self._get_rel_pos_bias()
+            if shared_rel_pos_bias is not None:
+                attn = attn + shared_rel_pos_bias
+
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)
+        x = x.transpose(1, 2).reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class Block(nn.Module):
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
-            drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,
-            window_size=None, attn_head_dim=None):
+            self,
+            dim: int,
+            num_heads: int,
+            qkv_bias: bool = False,
+            mlp_ratio: float = 4.,
+            scale_mlp: bool = False,
+            swiglu_mlp: bool = False,
+            proj_drop: float = 0.,
+            attn_drop: float = 0.,
+            drop_path: float = 0.,
+            init_values: Optional[float] = None,
+            act_layer: Callable = nn.GELU,
+            norm_layer: Callable = LayerNorm,
+            window_size: Optional[Tuple[int, int]] = None,
+            attn_head_dim: Optional[int] = None,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = Attention(
-            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,
-            window_size=window_size, attn_head_dim=attn_head_dim)
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            window_size=window_size,
+            attn_head_dim=attn_head_dim,
+        )
         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+
         self.norm2 = norm_layer(dim)
-        mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        if swiglu_mlp:
+            self.mlp = SwiGLU(
+                in_features=dim,
+                hidden_features=int(dim * mlp_ratio),
+                norm_layer=norm_layer if scale_mlp else None,
+                drop=proj_drop,
+            )
+        else:
+            self.mlp = Mlp(
+                in_features=dim,
+                hidden_features=int(dim * mlp_ratio),
+                act_layer=act_layer,
+                norm_layer=norm_layer if scale_mlp else None,
+                drop=proj_drop,
+            )
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         if init_values:
             self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))
             self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))
         else:
             self.gamma_1, self.gamma_2 = None, None
 
     def forward(self, x, shared_rel_pos_bias: Optional[torch.Tensor] = None):
         if self.gamma_1 is None:
-            x = x + self.drop_path(self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))
-            x = x + self.drop_path(self.mlp(self.norm2(x)))
+            x = x + self.drop_path1(self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))
+            x = x + self.drop_path2(self.mlp(self.norm2(x)))
         else:
-            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))
-            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
+            x = x + self.drop_path1(self.gamma_1 * self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))
+            x = x + self.drop_path2(self.gamma_2 * self.mlp(self.norm2(x)))
         return x
 
 
 class RelativePositionBias(nn.Module):
 
     def __init__(self, window_size, num_heads):
         super().__init__()
@@ -213,56 +260,96 @@
 
 
 class Beit(nn.Module):
     """ Vision Transformer with support for patch or hybrid CNN input stage
     """
 
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, global_pool='avg',
-            embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.,
-            attn_drop_rate=0., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
-            init_values=None, use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,
-            head_init_scale=0.001):
+            self,
+            img_size: Union[int, Tuple[int, int]] = 224,
+            patch_size: Union[int, Tuple[int, int]] = 16,
+            in_chans: int = 3,
+            num_classes: int = 1000,
+            global_pool: str = 'avg',
+            embed_dim: int = 768,
+            depth: int = 12,
+            num_heads: int = 12,
+            qkv_bias: bool = True,
+            mlp_ratio: float = 4.,
+            swiglu_mlp: bool = False,
+            scale_mlp: bool = False,
+            drop_rate: float = 0.,
+            pos_drop_rate: float = 0.,
+            proj_drop_rate: float = 0.,
+            attn_drop_rate: float = 0.,
+            drop_path_rate: float = 0.,
+            norm_layer: Callable = LayerNorm,
+            init_values: Optional[float] = None,
+            use_abs_pos_emb: bool = True,
+            use_rel_pos_bias: bool = False,
+            use_shared_rel_pos_bias: bool = False,
+            head_init_scale: float = 0.001,
+    ):
         super().__init__()
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
+        self.num_prefix_tokens = 1
         self.grad_checkpointing = False
 
         self.patch_embed = PatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+        )
         num_patches = self.patch_embed.num_patches
 
         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
         # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
         self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) if use_abs_pos_emb else None
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         if use_shared_rel_pos_bias:
-            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.grid_size, num_heads=num_heads)
+            self.rel_pos_bias = RelativePositionBias(
+                window_size=self.patch_embed.grid_size,
+                num_heads=num_heads,
+            )
         else:
             self.rel_pos_bias = None
 
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
         self.blocks = nn.ModuleList([
             Block(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                init_values=init_values, window_size=self.patch_embed.grid_size if use_rel_pos_bias else None)
+                dim=embed_dim,
+                num_heads=num_heads,
+                qkv_bias=qkv_bias,
+                mlp_ratio=mlp_ratio,
+                scale_mlp=scale_mlp,
+                swiglu_mlp=swiglu_mlp,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+                init_values=init_values,
+                window_size=self.patch_embed.grid_size if use_rel_pos_bias else None,
+            )
             for i in range(depth)])
+
         use_fc_norm = self.global_pool == 'avg'
         self.norm = nn.Identity() if use_fc_norm else norm_layer(embed_dim)
-        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else None
+        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
         self.apply(self._init_weights)
         if self.pos_embed is not None:
             trunc_normal_(self.pos_embed, std=.02)
         trunc_normal_(self.cls_token, std=.02)
-        # trunc_normal_(self.mask_token, std=.02)
+
         self.fix_init_weight()
         if isinstance(self.head, nn.Linear):
             trunc_normal_(self.head.weight, std=.02)
             self.head.weight.data.mul_(head_init_scale)
             self.head.bias.data.mul_(head_init_scale)
 
     def fix_init_weight(self):
@@ -325,19 +412,18 @@
                 x = checkpoint(blk, x, shared_rel_pos_bias=rel_pos_bias)
             else:
                 x = blk(x, shared_rel_pos_bias=rel_pos_bias)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.fc_norm is not None:
-            x = x[:, 1:].mean(dim=1)
-            x = self.fc_norm(x)
-        else:
-            x = x[:, 0]
+        if self.global_pool:
+            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
+        x = self.fc_norm(x)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -387,50 +473,39 @@
     ),
 
     'beitv2_base_patch16_224.in1k_ft_in22k_in1k': _cfg(
         url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft21kto1k.pth',
         hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD
     ),
+    'beitv2_base_patch16_224.in1k_ft_in1k': _cfg(
+        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft1k.pth',
+        hf_hub_id='timm/',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD
+    ),
     'beitv2_base_patch16_224.in1k_ft_in22k': _cfg(
         url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft21k.pth',
         hf_hub_id='timm/',
         num_classes=21841, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD
     ),
     'beitv2_large_patch16_224.in1k_ft_in22k_in1k': _cfg(
         url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft21kto1k.pth',
         hf_hub_id='timm/',
         crop_pct=0.95, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD
     ),
+    'beitv2_large_patch16_224.in1k_ft_in1k': _cfg(
+        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft1k.pth',
+        hf_hub_id='timm/',
+        crop_pct=0.95, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD
+    ),
     'beitv2_large_patch16_224.in1k_ft_in22k': _cfg(
         url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft21k.pth',
         hf_hub_id='timm/',
         num_classes=21841, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD
     ),
-
-    'eva_giant_patch14_224.clip_ft_in1k': _cfg(
-        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_clip_vis_enc_sz224_ftcls_89p1.pt',
-        hf_hub_id='timm/',
-        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0,
-    ),
-    'eva_giant_patch14_336.clip_ft_in1k': _cfg(
-        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_clip_vis_enc_sz336_ftcls_89p4.pt',
-        hf_hub_id='timm/',
-        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
-        input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),
-    'eva_giant_patch14_336.m30m_ft_in22k_in1k': _cfg(
-        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_21k_1k_336px_psz14_ema_89p6.pt',
-        hf_hub_id='timm/',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,
-        input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),
-    'eva_giant_patch14_560.m30m_ft_in22k_in1k': _cfg(
-        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_21k_1k_560px_psz14_ema_89p7.pt',
-        hf_hub_id='timm/',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,
-        input_size=(3, 560, 560), crop_pct=1.0, crop_mode='squash'),
 })
 
 
 def _beit_checkpoint_filter_fn(state_dict, model):
     if 'module' in state_dict:
         # beit v2 didn't strip module
         state_dict = state_dict['module']
@@ -446,94 +521,67 @@
         # FIXME an updated filter fn needed to interpolate rel pos emb if fine tuning to diff model sizes
         pretrained_filter_fn=_beit_checkpoint_filter_fn,
         **kwargs)
     return model
 
 
 @register_model
-def beit_base_patch16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beit_base_patch16_224(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=0.1, **kwargs)
-    model = _create_beit('beit_base_patch16_224', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=0.1)
+    model = _create_beit('beit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def beit_base_patch16_384(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beit_base_patch16_384(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=0.1, **kwargs)
-    model = _create_beit('beit_base_patch16_384', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=0.1)
+    model = _create_beit('beit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def beit_large_patch16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beit_large_patch16_224(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         patch_size=16, embed_dim=1024, depth=24, num_heads=16,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5,  **kwargs)
-    model = _create_beit('beit_large_patch16_224', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)
+    model = _create_beit('beit_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def beit_large_patch16_384(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beit_large_patch16_384(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5, **kwargs)
-    model = _create_beit('beit_large_patch16_384', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)
+    model = _create_beit('beit_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def beit_large_patch16_512(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beit_large_patch16_512(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         img_size=512, patch_size=16, embed_dim=1024, depth=24, num_heads=16,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5, **kwargs)
-    model = _create_beit('beit_large_patch16_512', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)
+    model = _create_beit('beit_large_patch16_512', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def beitv2_base_patch16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beitv2_base_patch16_224(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5, **kwargs)
-    model = _create_beit('beitv2_base_patch16_224', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)
+    model = _create_beit('beitv2_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def beitv2_large_patch16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def beitv2_large_patch16_224(pretrained=False, **kwargs) -> Beit:
+    model_args = dict(
         patch_size=16, embed_dim=1024, depth=24, num_heads=16,
-        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5,  **kwargs)
-    model = _create_beit('beitv2_large_patch16_224', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def eva_giant_patch14_224(pretrained=False, **kwargs):
-    """ EVA-g model https://arxiv.org/abs/2211.07636 """
-    model_kwargs = dict(
-        patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408, **kwargs)
-    model = _create_beit('eva_giant_patch14_224', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def eva_giant_patch14_336(pretrained=False, **kwargs):
-    """ EVA-g model https://arxiv.org/abs/2211.07636 """
-    model_kwargs = dict(
-        patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408, **kwargs)
-    model = _create_beit('eva_giant_patch14_336', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def eva_giant_patch14_560(pretrained=False, **kwargs):
-    """ EVA-g model https://arxiv.org/abs/2211.07636 """
-    model_kwargs = dict(
-        patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408, **kwargs)
-    model = _create_beit('eva_giant_patch14_560', pretrained=pretrained, **model_kwargs)
+        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)
+    model = _create_beit('beitv2_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/byoanet.py` & `timm-0.9.0/timm/models/byoanet.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,84 +10,20 @@
 
 Consider all of the models definitions here as experimental WIP and likely to change.
 
 Hacked together by / copyright Ross Wightman, 2021.
 """
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .byobnet import ByoBlockCfg, ByoModelCfg, ByobNet, interleave_blocks
 
 __all__ = []
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.95, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',
-        'fixed_input_size': False, 'min_input_size': (3, 224, 224),
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # GPU-Efficient (ResNet) weights
-    'botnet26t_256': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/botnet26t_c1_256-167a0e9f.pth',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
-    'sebotnet33ts_256': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/sebotnet33ts_a1h2_256-957e3c3e.pth',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
-    'botnet50ts_256': _cfg(
-        url='',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
-    'eca_botnext26ts_256': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_botnext26ts_c_256-95a898f6.pth',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
-
-    'halonet_h1': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256)),
-    'halonet26t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halonet26t_a1h_256-3083328c.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256)),
-    'sehalonet33ts': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/sehalonet33ts_256-87e053f9.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),
-    'halonet50ts': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halonet50ts_a1h2_256-f3a3daee.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),
-    'eca_halonext26ts': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_halonext26ts_c_256-06906299.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),
-
-    'lambda_resnet26t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet26t_c_256-e5a5c857.pth',
-        min_input_size=(3, 128, 128), input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
-    'lambda_resnet50ts': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet50ts_a1h_256-b87370f7.pth',
-        min_input_size=(3, 128, 128), input_size=(3, 256, 256), pool_size=(8, 8)),
-    'lambda_resnet26rpt_256': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet26rpt_c_256-ab00292d.pth',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
-
-    'haloregnetz_b': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/haloregnetz_c_raa_256-c8ad7616.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
-        first_conv='stem.conv', input_size=(3, 224, 224), pool_size=(7, 7), min_input_size=(3, 224, 224), crop_pct=0.94),
-
-    'lamhalobotnet50ts_256': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lamhalobotnet50ts_a1h2_256-fe3d9445.pth',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
-    'halo2botnet50ts_256': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halo2botnet50ts_a1h2_256-fd9c11a3.pth',
-        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
-}
-
-
 model_cfgs = dict(
 
     botnet26t=ByoModelCfg(
         blocks=(
             ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
             ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=0, br=0.25),
             interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=0, br=0.25),
@@ -325,118 +261,195 @@
 
 
 def _create_byoanet(variant, cfg_variant=None, pretrained=False, **kwargs):
     return build_model_with_cfg(
         ByobNet, variant, pretrained,
         model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],
         feature_cfg=dict(flatten_sequential=True),
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.95, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',
+        'fixed_input_size': False, 'min_input_size': (3, 224, 224),
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    # GPU-Efficient (ResNet) weights
+    'botnet26t_256.c1_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/botnet26t_c1_256-167a0e9f.pth',
+        hf_hub_id='timm/',
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
+    'sebotnet33ts_256.a1h_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/sebotnet33ts_a1h2_256-957e3c3e.pth',
+        hf_hub_id='timm/',
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
+    'botnet50ts_256.untrained': _cfg(
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
+    'eca_botnext26ts_256.c1_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_botnext26ts_c_256-95a898f6.pth',
+        hf_hub_id='timm/',
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
+
+    'halonet_h1.untrained': _cfg(input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256)),
+    'halonet26t.a1h_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halonet26t_a1h_256-3083328c.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256)),
+    'sehalonet33ts.ra2_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/sehalonet33ts_256-87e053f9.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),
+    'halonet50ts.a1h_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halonet50ts_a1h2_256-f3a3daee.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),
+    'eca_halonext26ts.c1_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_halonext26ts_c_256-06906299.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),
+
+    'lambda_resnet26t.c1_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet26t_c_256-e5a5c857.pth',
+        hf_hub_id='timm/',
+        min_input_size=(3, 128, 128), input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
+    'lambda_resnet50ts.a1h_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet50ts_a1h_256-b87370f7.pth',
+        hf_hub_id='timm/',
+        min_input_size=(3, 128, 128), input_size=(3, 256, 256), pool_size=(8, 8)),
+    'lambda_resnet26rpt_256.c1_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet26rpt_c_256-ab00292d.pth',
+        hf_hub_id='timm/',
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
+
+    'haloregnetz_b.ra3_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/haloregnetz_c_raa_256-c8ad7616.pth',
+        hf_hub_id='timm/',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        first_conv='stem.conv', input_size=(3, 224, 224), pool_size=(7, 7), min_input_size=(3, 224, 224), crop_pct=0.94),
+
+    'lamhalobotnet50ts_256.a1h_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lamhalobotnet50ts_a1h2_256-fe3d9445.pth',
+        hf_hub_id='timm/',
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
+    'halo2botnet50ts_256.a1h_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halo2botnet50ts_a1h2_256-fd9c11a3.pth',
+        hf_hub_id='timm/',
+        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),
+})
 
 
 @register_model
-def botnet26t_256(pretrained=False, **kwargs):
+def botnet26t_256(pretrained=False, **kwargs) -> ByobNet:
     """ Bottleneck Transformer w/ ResNet26-T backbone.
     """
     kwargs.setdefault('img_size', 256)
     return _create_byoanet('botnet26t_256', 'botnet26t', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def sebotnet33ts_256(pretrained=False, **kwargs):
+def sebotnet33ts_256(pretrained=False, **kwargs) -> ByobNet:
     """ Bottleneck Transformer w/ a ResNet33-t backbone, SE attn for non Halo blocks, SiLU,
     """
     return _create_byoanet('sebotnet33ts_256', 'sebotnet33ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def botnet50ts_256(pretrained=False, **kwargs):
+def botnet50ts_256(pretrained=False, **kwargs) -> ByobNet:
     """ Bottleneck Transformer w/ ResNet50-T backbone, silu act.
     """
     kwargs.setdefault('img_size', 256)
     return _create_byoanet('botnet50ts_256', 'botnet50ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_botnext26ts_256(pretrained=False, **kwargs):
+def eca_botnext26ts_256(pretrained=False, **kwargs) -> ByobNet:
     """ Bottleneck Transformer w/ ResNet26-T backbone, silu act.
     """
     kwargs.setdefault('img_size', 256)
     return _create_byoanet('eca_botnext26ts_256', 'eca_botnext26ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def halonet_h1(pretrained=False, **kwargs):
+def halonet_h1(pretrained=False, **kwargs) -> ByobNet:
     """ HaloNet-H1. Halo attention in all stages as per the paper.
     NOTE: This runs very slowly!
     """
     return _create_byoanet('halonet_h1', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def halonet26t(pretrained=False, **kwargs):
+def halonet26t(pretrained=False, **kwargs) -> ByobNet:
     """ HaloNet w/ a ResNet26-t backbone. Halo attention in final two stages
     """
     return _create_byoanet('halonet26t', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def sehalonet33ts(pretrained=False, **kwargs):
+def sehalonet33ts(pretrained=False, **kwargs) -> ByobNet:
     """ HaloNet w/ a ResNet33-t backbone, SE attn for non Halo blocks, SiLU, 1-2 Halo in stage 2,3,4.
     """
     return _create_byoanet('sehalonet33ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def halonet50ts(pretrained=False, **kwargs):
+def halonet50ts(pretrained=False, **kwargs) -> ByobNet:
     """ HaloNet w/ a ResNet50-t backbone, silu act. Halo attention in final two stages
     """
     return _create_byoanet('halonet50ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_halonext26ts(pretrained=False, **kwargs):
+def eca_halonext26ts(pretrained=False, **kwargs) -> ByobNet:
     """ HaloNet w/ a ResNet26-t backbone, silu act. Halo attention in final two stages
     """
     return _create_byoanet('eca_halonext26ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def lambda_resnet26t(pretrained=False, **kwargs):
+def lambda_resnet26t(pretrained=False, **kwargs) -> ByobNet:
     """ Lambda-ResNet-26-T. Lambda layers w/ conv pos in last two stages.
     """
     return _create_byoanet('lambda_resnet26t', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def lambda_resnet50ts(pretrained=False, **kwargs):
+def lambda_resnet50ts(pretrained=False, **kwargs) -> ByobNet:
     """ Lambda-ResNet-50-TS. SiLU act. Lambda layers w/ conv pos in last two stages.
     """
     return _create_byoanet('lambda_resnet50ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def lambda_resnet26rpt_256(pretrained=False, **kwargs):
+def lambda_resnet26rpt_256(pretrained=False, **kwargs) -> ByobNet:
     """ Lambda-ResNet-26-R-T. Lambda layers w/ rel pos embed in last two stages.
     """
     kwargs.setdefault('img_size', 256)
     return _create_byoanet('lambda_resnet26rpt_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def haloregnetz_b(pretrained=False, **kwargs):
+def haloregnetz_b(pretrained=False, **kwargs) -> ByobNet:
     """ Halo + RegNetZ
     """
     return _create_byoanet('haloregnetz_b', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def lamhalobotnet50ts_256(pretrained=False, **kwargs):
+def lamhalobotnet50ts_256(pretrained=False, **kwargs) -> ByobNet:
     """ Combo Attention (Lambda + Halo + Bot) Network
     """
     return _create_byoanet('lamhalobotnet50ts_256', 'lamhalobotnet50ts', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def halo2botnet50ts_256(pretrained=False, **kwargs):
+def halo2botnet50ts_256(pretrained=False, **kwargs) -> ByobNet:
     """ Combo Attention (Halo + Halo + Bot) Network
     """
     return _create_byoanet('halo2botnet50ts_256', 'halo2botnet50ts', pretrained=pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/byobnet.py` & `timm-0.9.0/timm/models/byobnet.py`

 * *Files 7% similar despite different names*

```diff
@@ -33,145 +33,19 @@
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, ConvNormAct, BatchNormAct2d, DropPath, AvgPool2dSame, \
     create_conv2d, get_act_layer, get_norm_act_layer, get_attn, make_divisible, to_2tuple, EvoNorm2dS0a
 from ._builder import build_model_with_cfg
 from ._manipulate import named_apply, checkpoint_seq
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model
 
 __all__ = ['ByobNet', 'ByoModelCfg', 'ByoBlockCfg', 'create_byob_stem', 'create_block']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-def _cfgr(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
-        'crop_pct': 0.9, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # GPU-Efficient (ResNet) weights
-    'gernet_s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-ger-weights/gernet_s-756b4751.pth'),
-    'gernet_m': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-ger-weights/gernet_m-0873c53a.pth'),
-    'gernet_l': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-ger-weights/gernet_l-f31e2e8d.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8)),
-
-    # RepVGG weights
-    'repvgg_a2': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_a2-c1ee6d2b.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b0': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b0-80ac3f1b.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b1': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b1-77ca2989.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b1g4': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b1g4-abde5d92.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b2': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b2-25b7494e.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b2g4': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b2g4-165a85f2.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b3': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b3-199bc50d.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-    'repvgg_b3g4': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-repvgg-weights/repvgg_b3g4-73c370bf.pth',
-        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv')),
-
-    # experimental configs
-    'resnet51q': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet51q_ra2-d47dcc76.pth',
-        first_conv='stem.conv1', input_size=(3, 256, 256), pool_size=(8, 8),
-        test_input_size=(3, 288, 288), crop_pct=1.0),
-    'resnet61q': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet61q_ra2-6afc536c.pth',
-        test_input_size=(3, 288, 288), crop_pct=1.0),
-
-    'resnext26ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnext26ts_256_ra2-8bbd9106.pth'),
-    'gcresnext26ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnext26ts_256-e414378b.pth'),
-    'seresnext26ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/seresnext26ts_256-6f0d74a3.pth'),
-    'eca_resnext26ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_resnext26ts_256-5a1d030f.pth'),
-    'bat_resnext26ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/bat_resnext26ts_256-fa6fd595.pth',
-        min_input_size=(3, 256, 256)),
-
-    'resnet32ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet32ts_256-aacf5250.pth'),
-    'resnet33ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet33ts_256-e91b09a4.pth'),
-    'gcresnet33ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnet33ts_256-0e0cd345.pth'),
-    'seresnet33ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/seresnet33ts_256-f8ad44d9.pth'),
-    'eca_resnet33ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_resnet33ts_256-8f98face.pth'),
-
-    'gcresnet50t': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnet50t_256-96374d1c.pth'),
-
-    'gcresnext50ts': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnext50ts_256-3e0f515e.pth'),
-
-    # experimental models, likely to change ot be removed
-    'regnetz_b16': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_b_raa-677d9606.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
-        input_size=(3, 224, 224), pool_size=(7, 7), test_input_size=(3, 288, 288), first_conv='stem.conv', crop_pct=0.94),
-    'regnetz_c16': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_c_rab2_256-a54bf36a.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), test_input_size=(3, 320, 320), first_conv='stem.conv', crop_pct=0.94),
-    'regnetz_d32': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_d_rab_256-b8073a89.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), test_input_size=(3, 320, 320), crop_pct=0.95),
-    'regnetz_d8': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_d8_bh-afc03c55.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), test_input_size=(3, 320, 320), crop_pct=1.0),
-    'regnetz_e8': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_e8_bh-aace8e6e.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), test_input_size=(3, 320, 320), crop_pct=1.0),
-
-    'regnetz_b16_evos': _cfgr(
-        url='',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
-        input_size=(3, 224, 224), pool_size=(7, 7), test_input_size=(3, 288, 288), first_conv='stem.conv',
-        crop_pct=0.94),
-    'regnetz_c16_evos': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_c16_evos_ch-d8311942.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), test_input_size=(3, 320, 320), first_conv='stem.conv', crop_pct=0.95),
-    'regnetz_d8_evos': _cfgr(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_d8_evos_ch-2bc12646.pth',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), test_input_size=(3, 320, 320), crop_pct=0.95),
-}
-
-
 @dataclass
 class ByoBlockCfg:
     type: Union[str, nn.Module]
     d: int  # block depth (number of block repeats in stage)
     c: int  # number of output channels for each block in stage
     s: int = 2  # stride of stage (first block)
     gs: Optional[Union[int, Callable]] = None  # group-size of blocks in stage, conv is depthwise if gs == 1
@@ -234,679 +108,14 @@
     blocks = []
     for i in range(d):
         block_type = types[1] if i in every else types[0]
         blocks += [ByoBlockCfg(type=block_type, d=1, **kwargs)]
     return tuple(blocks)
 
 
-model_cfgs = dict(
-    gernet_l=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='basic', d=1, c=128, s=2, gs=0, br=1.),
-            ByoBlockCfg(type='basic', d=2, c=192, s=2, gs=0, br=1.),
-            ByoBlockCfg(type='bottle', d=6, c=640, s=2, gs=0, br=1 / 4),
-            ByoBlockCfg(type='bottle', d=5, c=640, s=2, gs=1, br=3.),
-            ByoBlockCfg(type='bottle', d=4, c=640, s=1, gs=1, br=3.),
-        ),
-        stem_chs=32,
-        stem_pool=None,
-        num_features=2560,
-    ),
-    gernet_m=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='basic', d=1, c=128, s=2, gs=0, br=1.),
-            ByoBlockCfg(type='basic', d=2, c=192, s=2, gs=0, br=1.),
-            ByoBlockCfg(type='bottle', d=6, c=640, s=2, gs=0, br=1 / 4),
-            ByoBlockCfg(type='bottle', d=4, c=640, s=2, gs=1, br=3.),
-            ByoBlockCfg(type='bottle', d=1, c=640, s=1, gs=1, br=3.),
-        ),
-        stem_chs=32,
-        stem_pool=None,
-        num_features=2560,
-    ),
-    gernet_s=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='basic', d=1, c=48, s=2, gs=0, br=1.),
-            ByoBlockCfg(type='basic', d=3, c=48, s=2, gs=0, br=1.),
-            ByoBlockCfg(type='bottle', d=7, c=384, s=2, gs=0, br=1 / 4),
-            ByoBlockCfg(type='bottle', d=2, c=560, s=2, gs=1, br=3.),
-            ByoBlockCfg(type='bottle', d=1, c=256, s=1, gs=1, br=3.),
-        ),
-        stem_chs=13,
-        stem_pool=None,
-        num_features=1920,
-    ),
-
-    repvgg_a2=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(d=(2, 4, 14, 1), wf=(1.5, 1.5, 1.5, 2.75)),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b0=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(1., 1., 1., 2.5)),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b1=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(2., 2., 2., 4.)),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b1g4=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(2., 2., 2., 4.), groups=4),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b2=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(2.5, 2.5, 2.5, 5.)),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b2g4=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(2.5, 2.5, 2.5, 5.), groups=4),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b3=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(3., 3., 3., 5.)),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-    repvgg_b3g4=ByoModelCfg(
-        blocks=_rep_vgg_bcfg(wf=(3., 3., 3., 5.), groups=4),
-        stem_type='rep',
-        stem_chs=64,
-    ),
-
-    # 4 x conv stem w/ 2 act, no maxpool, 2,4,6,4 repeats, group size 32 in first 3 blocks
-    # DW convs in last block, 2048 pre-FC, silu act  
-    resnet51q=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=6, c=1536, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=4, c=1536, s=2, gs=1, br=1.0),
-        ),
-        stem_chs=128,
-        stem_type='quad2',
-        stem_pool=None,
-        num_features=2048,
-        act_layer='silu',
-    ),
-
-    # 4 x conv stem w/ 4 act, no maxpool, 1,4,6,4 repeats, edge block first, group size 32 in next 2 blocks
-    # DW convs in last block, 4 conv for each bottle block, 2048 pre-FC, silu act  
-    resnet61q=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='edge', d=1, c=256, s=1, gs=0, br=1.0, block_kwargs=dict()),
-            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=6, c=1536, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=4, c=1536, s=2, gs=1, br=1.0),
-        ),
-        stem_chs=128,
-        stem_type='quad',
-        stem_pool=None,
-        num_features=2048,
-        act_layer='silu',
-        block_kwargs=dict(extra_conv=True),
-    ),
-
-    # A series of ResNeXt-26 models w/ one of none, GC, SE, ECA, BAT attn, group size 32, SiLU act,
-    # and a tiered stem w/ maxpool
-    resnext26ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='maxpool',
-        act_layer='silu',
-    ),
-    gcresnext26ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='maxpool',
-        act_layer='silu',
-        attn_layer='gca',
-    ),
-    seresnext26ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='maxpool',
-        act_layer='silu',
-        attn_layer='se',
-    ),
-    eca_resnext26ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='maxpool',
-        act_layer='silu',
-        attn_layer='eca',
-    ),
-    bat_resnext26ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='maxpool',
-        act_layer='silu',
-        attn_layer='bat',
-        attn_kwargs=dict(block_size=8)
-    ),
-
-    # ResNet-32 (2, 3, 3, 2) models w/ no attn, no groups, SiLU act, no pre-fc feat layer, tiered stem w/o maxpool
-    resnet32ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        num_features=0,
-        act_layer='silu',
-    ),
-
-    # ResNet-33 (2, 3, 3, 2) models w/ no attn, no groups, SiLU act, 1280 pre-FC feat, tiered stem w/o maxpool
-    resnet33ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        num_features=1280,
-        act_layer='silu',
-    ),
-
-    # A series of ResNet-33 (2, 3, 3, 2) models w/ one of GC, SE, ECA attn, no groups, SiLU act, 1280 pre-FC feat 
-    # and a tiered stem w/ no maxpool
-    gcresnet33ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        num_features=1280,
-        act_layer='silu',
-        attn_layer='gca',
-    ),
-    seresnet33ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        num_features=1280,
-        act_layer='silu',
-        attn_layer='se',
-    ),
-    eca_resnet33ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
-            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        num_features=1280,
-        act_layer='silu',
-        attn_layer='eca',
-    ),
-
-    gcresnet50t=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=3, c=256, s=1, br=0.25),
-            ByoBlockCfg(type='bottle', d=4, c=512, s=2, br=0.25),
-            ByoBlockCfg(type='bottle', d=6, c=1024, s=2, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=2048, s=2, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        attn_layer='gca',
-    ),
-
-    gcresnext50ts=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=6, c=1024, s=2, gs=32, br=0.25),
-            ByoBlockCfg(type='bottle', d=3, c=2048, s=2, gs=32, br=0.25),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='maxpool',
-        # stem_pool=None,
-        act_layer='silu',
-        attn_layer='gca',
-    ),
-
-    # experimental models, closer to a RegNetZ than a ResNet. Similar to EfficientNets but w/ groups instead of DW
-    regnetz_b16=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),
-            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),
-            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=3),
-            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=3),
-        ),
-        stem_chs=32,
-        stem_pool='',
-        downsample='',
-        num_features=1536,
-        act_layer='silu',
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-    regnetz_c16=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=4),
-            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=4),
-            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=4),
-            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=4),
-        ),
-        stem_chs=32,
-        stem_pool='',
-        downsample='',
-        num_features=1536,
-        act_layer='silu',
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-    regnetz_d32=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=32, br=4),
-            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=32, br=4),
-            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=32, br=4),
-            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=32, br=4),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        downsample='',
-        num_features=1792,
-        act_layer='silu',
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-    regnetz_d8=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=8, br=4),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        downsample='',
-        num_features=1792,
-        act_layer='silu',
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-    regnetz_e8=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=3, c=96, s=1, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=8, c=192, s=2, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=16, c=384, s=2, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=8, br=4),
-        ),
-        stem_chs=64,
-        stem_type='tiered',
-        stem_pool='',
-        downsample='',
-        num_features=2048,
-        act_layer='silu',
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-
-    # experimental EvoNorm configs
-    regnetz_b16_evos=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),
-            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),
-            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=3),
-            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=3),
-        ),
-        stem_chs=32,
-        stem_pool='',
-        downsample='',
-        num_features=1536,
-        act_layer='silu',
-        norm_layer=partial(EvoNorm2dS0a, group_size=16),
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-    regnetz_c16_evos=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=4),
-            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=4),
-            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=4),
-            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=4),
-        ),
-        stem_chs=32,
-        stem_pool='',
-        downsample='',
-        num_features=1536,
-        act_layer='silu',
-        norm_layer=partial(EvoNorm2dS0a, group_size=16),
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-    regnetz_d8_evos=ByoModelCfg(
-        blocks=(
-            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=8, br=4),
-            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=8, br=4),
-        ),
-        stem_chs=64,
-        stem_type='deep',
-        stem_pool='',
-        downsample='',
-        num_features=1792,
-        act_layer='silu',
-        norm_layer=partial(EvoNorm2dS0a, group_size=16),
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.25),
-        block_kwargs=dict(bottle_in=True, linear_out=True),
-    ),
-)
-
-@register_model
-def gernet_l(pretrained=False, **kwargs):
-    """ GEResNet-Large (GENet-Large from official impl)
-    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
-    """
-    return _create_byobnet('gernet_l', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def gernet_m(pretrained=False, **kwargs):
-    """ GEResNet-Medium (GENet-Normal from official impl)
-    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
-    """
-    return _create_byobnet('gernet_m', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def gernet_s(pretrained=False, **kwargs):
-    """ EResNet-Small (GENet-Small from official impl)
-    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
-    """
-    return _create_byobnet('gernet_s', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_a2(pretrained=False, **kwargs):
-    """ RepVGG-A2
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_a2', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b0(pretrained=False, **kwargs):
-    """ RepVGG-B0
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b0', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b1(pretrained=False, **kwargs):
-    """ RepVGG-B1
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b1', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b1g4(pretrained=False, **kwargs):
-    """ RepVGG-B1g4
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b1g4', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b2(pretrained=False, **kwargs):
-    """ RepVGG-B2
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b2', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b2g4(pretrained=False, **kwargs):
-    """ RepVGG-B2g4
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b2g4', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b3(pretrained=False, **kwargs):
-    """ RepVGG-B3
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b3', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def repvgg_b3g4(pretrained=False, **kwargs):
-    """ RepVGG-B3g4
-    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
-    """
-    return _create_byobnet('repvgg_b3g4', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def resnet51q(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('resnet51q', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def resnet61q(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('resnet61q', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def resnext26ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('resnext26ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def gcresnext26ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('gcresnext26ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def seresnext26ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('seresnext26ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def eca_resnext26ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('eca_resnext26ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def bat_resnext26ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('bat_resnext26ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def resnet32ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('resnet32ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def resnet33ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('resnet33ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def gcresnet33ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('gcresnet33ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def seresnet33ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('seresnet33ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def eca_resnet33ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('eca_resnet33ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def gcresnet50t(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('gcresnet50t', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def gcresnext50ts(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('gcresnext50ts', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_b16(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_b16', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_c16(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_c16', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_d32(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_d32', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_d8(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_d8', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_e8(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_e8', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_b16_evos(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_b16_evos', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_c16_evos(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_c16_evos', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def regnetz_d8_evos(pretrained=False, **kwargs):
-    """
-    """
-    return _create_byobnet('regnetz_d8_evos', pretrained=pretrained, **kwargs)
-
-
 def expand_blocks_cfg(stage_blocks_cfg: Union[ByoBlockCfg, Sequence[ByoBlockCfg]]) -> List[ByoBlockCfg]:
     if not isinstance(stage_blocks_cfg, Sequence):
         stage_blocks_cfg = (stage_blocks_cfg,)
     block_cfgs = []
     for i, cfg in enumerate(stage_blocks_cfg):
         block_cfgs += [replace(cfg, d=1) for _ in range(cfg.d)]
     return block_cfgs
@@ -927,15 +136,23 @@
     norm_act: Callable = BatchNormAct2d
     act: Callable = nn.ReLU
     attn: Optional[Callable] = None
     self_attn: Optional[Callable] = None
 
 
 class DownsampleAvg(nn.Module):
-    def __init__(self, in_chs, out_chs, stride=1, dilation=1, apply_act=False, layers: LayerFn = None):
+    def __init__(
+            self,
+            in_chs: int,
+            out_chs: int,
+            stride: int = 1,
+            dilation: int = 1,
+            apply_act: bool = False,
+            layers: LayerFn = None,
+    ):
         """ AvgPool Downsampling as in 'D' ResNet variants."""
         super(DownsampleAvg, self).__init__()
         layers = layers or LayerFn()
         avg_stride = stride if dilation == 1 else 1
         if stride > 1 or dilation > 1:
             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
@@ -943,15 +160,23 @@
             self.pool = nn.Identity()
         self.conv = layers.conv_norm_act(in_chs, out_chs, 1, apply_act=apply_act)
 
     def forward(self, x):
         return self.conv(self.pool(x))
 
 
-def create_shortcut(downsample_type, layers: LayerFn, in_chs, out_chs, stride, dilation, **kwargs):
+def create_shortcut(
+        downsample_type: str,
+        in_chs: int,
+        out_chs: int,
+        stride: int,
+        dilation: Tuple[int, int],
+        layers: LayerFn,
+        **kwargs,
+):
     assert downsample_type in ('avg', 'conv1x1', '')
     if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:
         if not downsample_type:
             return None  # no shortcut
         elif downsample_type == 'avg':
             return DownsampleAvg(in_chs, out_chs, stride=stride, dilation=dilation[0], **kwargs)
         else:
@@ -962,41 +187,44 @@
 
 class BasicBlock(nn.Module):
     """ ResNet Basic Block - kxk + kxk
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=1,
-            dilation=(1, 1),
-            group_size=None,
-            bottle_ratio=1.0,
-            downsample='avg',
-            attn_last=True,
-            linear_out=False,
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 1,
+            dilation: Tuple[int, int] = (1, 1),
+            group_size: Optional[int] = None,
+            bottle_ratio: float = 1.0,
+            downsample: str = 'avg',
+            attn_last: bool = True,
+            linear_out: bool = False,
             layers: LayerFn = None,
-            drop_block=None,
-            drop_path_rate=0.,
+            drop_block: Callable = None,
+            drop_path_rate: float = 0.,
     ):
         super(BasicBlock, self).__init__()
         layers = layers or LayerFn()
         mid_chs = make_divisible(out_chs * bottle_ratio)
         groups = num_groups(group_size, mid_chs)
 
         self.shortcut = create_shortcut(
-            downsample, in_chs=in_chs, out_chs=out_chs, stride=stride, dilation=dilation,
-            apply_act=False, layers=layers)
+            downsample, in_chs, out_chs,
+            stride=stride, dilation=dilation, apply_act=False, layers=layers,
+        )
 
         self.conv1_kxk = layers.conv_norm_act(in_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0])
         self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)
         self.conv2_kxk = layers.conv_norm_act(
-            mid_chs, out_chs, kernel_size, dilation=dilation[1], groups=groups, drop_layer=drop_block, apply_act=False)
+            mid_chs, out_chs, kernel_size,
+            dilation=dilation[1], groups=groups, drop_layer=drop_block, apply_act=False,
+        )
         self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
         self.act = nn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv2_kxk.bn, 'weight', None) is not None:
             nn.init.zeros_(self.conv2_kxk.bn.weight)
@@ -1017,44 +245,48 @@
 
 class BottleneckBlock(nn.Module):
     """ ResNet-like Bottleneck Block - 1x1 - kxk - 1x1
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=1,
-            dilation=(1, 1),
-            bottle_ratio=1.,
-            group_size=None,
-            downsample='avg',
-            attn_last=False,
-            linear_out=False,
-            extra_conv=False,
-            bottle_in=False,
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 1,
+            dilation: Tuple[int, int] = (1, 1),
+            bottle_ratio: float = 1.,
+            group_size: Optional[int] = None,
+            downsample: str = 'avg',
+            attn_last: bool = False,
+            linear_out: bool = False,
+            extra_conv: bool = False,
+            bottle_in: bool = False,
             layers: LayerFn = None,
-            drop_block=None,
-            drop_path_rate=0.,
+            drop_block: Callable = None,
+            drop_path_rate: float = 0.,
     ):
         super(BottleneckBlock, self).__init__()
         layers = layers or LayerFn()
         mid_chs = make_divisible((in_chs if bottle_in else out_chs) * bottle_ratio)
         groups = num_groups(group_size, mid_chs)
 
         self.shortcut = create_shortcut(
-            downsample, in_chs=in_chs, out_chs=out_chs, stride=stride, dilation=dilation,
-            apply_act=False, layers=layers)
+            downsample, in_chs, out_chs,
+            stride=stride, dilation=dilation, apply_act=False, layers=layers,
+        )
 
         self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1)
         self.conv2_kxk = layers.conv_norm_act(
-            mid_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block)
+            mid_chs, mid_chs, kernel_size,
+            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block,
+        )
         if extra_conv:
-            self.conv2b_kxk = layers.conv_norm_act(mid_chs, mid_chs, kernel_size, dilation=dilation[1], groups=groups)
+            self.conv2b_kxk = layers.conv_norm_act(
+                mid_chs, mid_chs, kernel_size, dilation=dilation[1], groups=groups)
         else:
             self.conv2b_kxk = nn.Identity()
         self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)
         self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False)
         self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
         self.act = nn.Identity() if linear_out else layers.act(inplace=True)
@@ -1089,42 +321,44 @@
 
     If one does want to use a lot of these blocks w/ stride, I'd recommend using the EdgeBlock (3x3 /w stride + 1x1)
     for more optimal compute.
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=1,
-            dilation=(1, 1),
-            bottle_ratio=1.0,
-            group_size=None,
-            downsample='avg',
-            attn_last=True,
-            linear_out=False,
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 1,
+            dilation: Tuple[int, int] = (1, 1),
+            bottle_ratio: float = 1.0,
+            group_size: Optional[int] = None,
+            downsample: str = 'avg',
+            attn_last: bool = True,
+            linear_out: bool = False,
             layers: LayerFn = None,
-            drop_block=None,
-            drop_path_rate=0.,
+            drop_block: Callable = None,
+            drop_path_rate: float = 0.,
     ):
         super(DarkBlock, self).__init__()
         layers = layers or LayerFn()
         mid_chs = make_divisible(out_chs * bottle_ratio)
         groups = num_groups(group_size, mid_chs)
 
         self.shortcut = create_shortcut(
-            downsample, in_chs=in_chs, out_chs=out_chs, stride=stride, dilation=dilation,
-            apply_act=False, layers=layers)
+            downsample, in_chs, out_chs,
+            stride=stride, dilation=dilation, apply_act=False, layers=layers,
+        )
 
         self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1)
         self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)
         self.conv2_kxk = layers.conv_norm_act(
-            mid_chs, out_chs, kernel_size, stride=stride, dilation=dilation[0],
-            groups=groups, drop_layer=drop_block, apply_act=False)
+            mid_chs, out_chs, kernel_size,
+            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block, apply_act=False,
+        )
         self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
         self.act = nn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv2_kxk.bn, 'weight', None) is not None:
             nn.init.zeros_(self.conv2_kxk.bn.weight)
@@ -1152,39 +386,42 @@
     intended to be used with either expansion or bottleneck contraction, and can use DW/group/non-grouped convs.
 
     FIXME is there a more common 3x3 + 1x1 conv block to name this after?
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=1,
-            dilation=(1, 1),
-            bottle_ratio=1.0,
-            group_size=None,
-            downsample='avg',
-            attn_last=False,
-            linear_out=False,
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 1,
+            dilation: Tuple[int, int] = (1, 1),
+            bottle_ratio: float = 1.0,
+            group_size: Optional[int] = None,
+            downsample: str = 'avg',
+            attn_last: bool = False,
+            linear_out: bool = False,
             layers: LayerFn = None,
-            drop_block=None,
-            drop_path_rate=0.,
+            drop_block: Callable = None,
+            drop_path_rate: float = 0.,
     ):
         super(EdgeBlock, self).__init__()
         layers = layers or LayerFn()
         mid_chs = make_divisible(out_chs * bottle_ratio)
         groups = num_groups(group_size, mid_chs)
 
         self.shortcut = create_shortcut(
-            downsample, in_chs=in_chs, out_chs=out_chs, stride=stride, dilation=dilation,
-            apply_act=False, layers=layers)
+            downsample, in_chs, out_chs,
+            stride=stride, dilation=dilation, apply_act=False, layers=layers,
+        )
 
         self.conv1_kxk = layers.conv_norm_act(
-            in_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block)
+            in_chs, mid_chs, kernel_size,
+            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block,
+        )
         self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)
         self.conv2_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False)
         self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
         self.act = nn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
@@ -1212,35 +449,36 @@
     Adapted from impl at https://github.com/DingXiaoH/RepVGG
 
     This version does not currently support the deploy optimization. It is currently fixed in 'train' mode.
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=1,
-            dilation=(1, 1),
-            bottle_ratio=1.0,
-            group_size=None,
-            downsample='',
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 1,
+            dilation: Tuple[int, int] = (1, 1),
+            bottle_ratio: float = 1.0,
+            group_size: Optional[int] = None,
+            downsample: str = '',
             layers: LayerFn = None,
-            drop_block=None,
-            drop_path_rate=0.,
+            drop_block: Callable = None,
+            drop_path_rate: float = 0.,
     ):
         super(RepVggBlock, self).__init__()
         layers = layers or LayerFn()
         groups = num_groups(group_size, in_chs)
 
         use_ident = in_chs == out_chs and stride == 1 and dilation[0] == dilation[1]
         self.identity = layers.norm_act(out_chs, apply_act=False) if use_ident else None
         self.conv_kxk = layers.conv_norm_act(
-            in_chs, out_chs, kernel_size, stride=stride, dilation=dilation[0],
-            groups=groups, drop_layer=drop_block, apply_act=False)
+            in_chs, out_chs, kernel_size,
+            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block, apply_act=False,
+        )
         self.conv_1x1 = layers.conv_norm_act(in_chs, out_chs, 1, stride=stride, groups=groups, apply_act=False)
         self.attn = nn.Identity() if layers.attn is None else layers.attn(out_chs)
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else nn.Identity()
         self.act = layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         # NOTE this init overrides that base model init with specific changes for the block type
@@ -1265,45 +503,47 @@
 
 class SelfAttnBlock(nn.Module):
     """ ResNet-like Bottleneck Block - 1x1 - optional kxk - self attn - 1x1
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=1,
-            dilation=(1, 1),
-            bottle_ratio=1.,
-            group_size=None,
-            downsample='avg',
-            extra_conv=False,
-            linear_out=False,
-            bottle_in=False,
-            post_attn_na=True,
-            feat_size=None,
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 1,
+            dilation: Tuple[int, int] = (1, 1),
+            bottle_ratio: float = 1.,
+            group_size: Optional[int] = None,
+            downsample: str = 'avg',
+            extra_conv: bool = False,
+            linear_out: bool = False,
+            bottle_in: bool = False,
+            post_attn_na: bool = True,
+            feat_size: Optional[Tuple[int, int]] = None,
             layers: LayerFn = None,
-            drop_block=None,
-            drop_path_rate=0.,
+            drop_block: Callable = None,
+            drop_path_rate: float = 0.,
     ):
         super(SelfAttnBlock, self).__init__()
         assert layers is not None
         mid_chs = make_divisible((in_chs if bottle_in else out_chs) * bottle_ratio)
         groups = num_groups(group_size, mid_chs)
 
         self.shortcut = create_shortcut(
-            downsample, in_chs=in_chs, out_chs=out_chs, stride=stride, dilation=dilation,
-            apply_act=False, layers=layers)
+            downsample, in_chs, out_chs,
+            stride=stride, dilation=dilation, apply_act=False, layers=layers,
+        )
 
         self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1)
         if extra_conv:
             self.conv2_kxk = layers.conv_norm_act(
-                mid_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0],
-                groups=groups, drop_layer=drop_block)
+                mid_chs, mid_chs, kernel_size,
+                stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block,
+            )
             stride = 1  # striding done via conv if enabled
         else:
             self.conv2_kxk = nn.Identity()
         opt_kwargs = {} if feat_size is None else dict(feat_size=feat_size)
         # FIXME need to dilate self attn to have dilated network support, moop moop
         self.self_attn = layers.self_attn(mid_chs, stride=stride, **opt_kwargs)
         self.post_attn = layers.norm_act(mid_chs) if post_attn_na else nn.Identity()
@@ -1325,14 +565,15 @@
         x = self.post_attn(x)
         x = self.conv3_1x1(x)
         x = self.drop_path(x)
         if self.shortcut is not None:
             x = x + self.shortcut(shortcut)
         return self.act(x)
 
+
 _block_registry = dict(
     basic=BasicBlock,
     bottle=BottleneckBlock,
     dark=DarkBlock,
     edge=EdgeBlock,
     rep=RepVggBlock,
     self_attn=SelfAttnBlock,
@@ -1350,22 +591,22 @@
     return _block_registry[block](**kwargs)
 
 
 class Stem(nn.Sequential):
 
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            kernel_size=3,
-            stride=4,
-            pool='maxpool',
-            num_rep=3,
-            num_act=None,
-            chs_decay=0.5,
+            in_chs: int,
+            out_chs: int,
+            kernel_size: int = 3,
+            stride: int = 4,
+            pool: str = 'maxpool',
+            num_rep: int = 3,
+            num_act: Optional[int] = None,
+            chs_decay: float = 0.5,
             layers: LayerFn = None,
     ):
         super().__init__()
         assert stride in (2, 4)
         layers = layers or LayerFn()
 
         if isinstance(out_chs, (list, tuple)):
@@ -1404,19 +645,19 @@
             prev_feat = 'pool'
 
         self.feature_info.append(dict(num_chs=prev_chs, reduction=curr_stride, module=prev_feat))
         assert curr_stride == stride
 
 
 def create_byob_stem(
-        in_chs,
-        out_chs,
-        stem_type='',
-        pool_type='',
-        feat_prefix='stem',
+        in_chs: int,
+        out_chs: int,
+        stem_type: str = '',
+        pool_type: str = '',
+        feat_prefix: str = 'stem',
         layers: LayerFn = None,
 ):
     layers = layers or LayerFn()
     assert stem_type in ('', 'quad', 'quad2', 'tiered', 'deep', 'rep', '7x7', '3x3')
     if 'quad' in stem_type:
         # based on NFNet stem, stack of 4 3x3 convs
         num_act = 2 if 'quad2' in stem_type else None
@@ -1582,37 +823,36 @@
     dataclass cfg definition w/ factory functions for module instantiation.
 
     Current assumption is that both stem and blocks are in conv-bn-act order (w/ block ending in act).
     """
     def __init__(
             self,
             cfg: ByoModelCfg,
-            num_classes=1000,
-            in_chans=3,
-            global_pool='avg',
-            output_stride=32,
-            img_size=None,
-            drop_rate=0.,
-            drop_path_rate=0.,
-            zero_init_last=True,
+            num_classes: int = 1000,
+            in_chans: int = 3,
+            global_pool: str = 'avg',
+            output_stride: int = 32,
+            img_size: Optional[Union[int, Tuple[int, int]]] = None,
+            drop_rate: float = 0.,
+            drop_path_rate: float =0.,
+            zero_init_last: bool = True,
             **kwargs,
     ):
         """
-
         Args:
-            cfg (ByoModelCfg): Model architecture configuration
-            num_classes (int): Number of classifier classes (default: 1000)
-            in_chans (int): Number of input channels (default: 3)
-            global_pool (str): Global pooling type (default: 'avg')
-            output_stride (int): Output stride of network, one of (8, 16, 32) (default: 32)
-            img_size (Union[int, Tuple[int]): Image size for fixed image size models (i.e. self-attn)
-            drop_rate (float): Dropout rate (default: 0.)
-            drop_path_rate (float): Stochastic depth drop-path rate (default: 0.)
-            zero_init_last (bool): Zero-init last weight of residual path
-            kwargs (dict): Extra kwargs overlayed onto cfg
+            cfg: Model architecture configuration.
+            num_classes: Number of classifier classes.
+            in_chans: Number of input channels.
+            global_pool: Global pooling type.
+            output_stride: Output stride of network, one of (8, 16, 32).
+            img_size: Image size for fixed image size models (i.e. self-attn).
+            drop_rate: Classifier dropout rate.
+            drop_path_rate: Stochastic depth drop-path rate.
+            zero_init_last: Zero-init last weight of residual path.
+            **kwargs: Extra kwargs overlayed onto cfg.
         """
         super().__init__()
         self.num_classes = num_classes
         self.drop_rate = drop_rate
         self.grad_checkpointing = False
 
         cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg
@@ -1624,28 +864,39 @@
         self.feature_info = []
         stem_chs = int(round((cfg.stem_chs or cfg.blocks[0].c) * cfg.width_factor))
         self.stem, stem_feat = create_byob_stem(in_chans, stem_chs, cfg.stem_type, cfg.stem_pool, layers=layers)
         self.feature_info.extend(stem_feat[:-1])
         feat_size = reduce_feat_size(feat_size, stride=stem_feat[-1]['reduction'])
 
         self.stages, stage_feat = create_byob_stages(
-            cfg, drop_path_rate, output_stride, stem_feat[-1], layers=layers, feat_size=feat_size)
+            cfg,
+            drop_path_rate,
+            output_stride,
+            stem_feat[-1],
+            layers=layers,
+            feat_size=feat_size,
+        )
         self.feature_info.extend(stage_feat[:-1])
 
         prev_chs = stage_feat[-1]['num_chs']
         if cfg.num_features:
             self.num_features = int(round(cfg.width_factor * cfg.num_features))
             self.final_conv = layers.conv_norm_act(prev_chs, self.num_features, 1)
         else:
             self.num_features = prev_chs
             self.final_conv = nn.Identity()
         self.feature_info += [
             dict(num_chs=self.num_features, reduction=stage_feat[-1]['reduction'], module='final_conv')]
 
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=self.drop_rate,
+        )
 
         # init weights
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         matcher = dict(
@@ -1662,15 +913,15 @@
         self.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool='avg'):
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x)
         else:
             x = self.stages(x)
@@ -1700,13 +951,834 @@
     elif isinstance(module, nn.BatchNorm2d):
         nn.init.ones_(module.weight)
         nn.init.zeros_(module.bias)
     elif hasattr(module, 'init_weights'):
         module.init_weights(zero_init_last=zero_init_last)
 
 
+model_cfgs = dict(
+    gernet_l=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='basic', d=1, c=128, s=2, gs=0, br=1.),
+            ByoBlockCfg(type='basic', d=2, c=192, s=2, gs=0, br=1.),
+            ByoBlockCfg(type='bottle', d=6, c=640, s=2, gs=0, br=1 / 4),
+            ByoBlockCfg(type='bottle', d=5, c=640, s=2, gs=1, br=3.),
+            ByoBlockCfg(type='bottle', d=4, c=640, s=1, gs=1, br=3.),
+        ),
+        stem_chs=32,
+        stem_pool=None,
+        num_features=2560,
+    ),
+    gernet_m=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='basic', d=1, c=128, s=2, gs=0, br=1.),
+            ByoBlockCfg(type='basic', d=2, c=192, s=2, gs=0, br=1.),
+            ByoBlockCfg(type='bottle', d=6, c=640, s=2, gs=0, br=1 / 4),
+            ByoBlockCfg(type='bottle', d=4, c=640, s=2, gs=1, br=3.),
+            ByoBlockCfg(type='bottle', d=1, c=640, s=1, gs=1, br=3.),
+        ),
+        stem_chs=32,
+        stem_pool=None,
+        num_features=2560,
+    ),
+    gernet_s=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='basic', d=1, c=48, s=2, gs=0, br=1.),
+            ByoBlockCfg(type='basic', d=3, c=48, s=2, gs=0, br=1.),
+            ByoBlockCfg(type='bottle', d=7, c=384, s=2, gs=0, br=1 / 4),
+            ByoBlockCfg(type='bottle', d=2, c=560, s=2, gs=1, br=3.),
+            ByoBlockCfg(type='bottle', d=1, c=256, s=1, gs=1, br=3.),
+        ),
+        stem_chs=13,
+        stem_pool=None,
+        num_features=1920,
+    ),
+
+    repvgg_a2=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(d=(2, 4, 14, 1), wf=(1.5, 1.5, 1.5, 2.75)),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b0=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(1., 1., 1., 2.5)),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b1=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(2., 2., 2., 4.)),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b1g4=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(2., 2., 2., 4.), groups=4),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b2=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(2.5, 2.5, 2.5, 5.)),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b2g4=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(2.5, 2.5, 2.5, 5.), groups=4),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b3=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(3., 3., 3., 5.)),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+    repvgg_b3g4=ByoModelCfg(
+        blocks=_rep_vgg_bcfg(wf=(3., 3., 3., 5.), groups=4),
+        stem_type='rep',
+        stem_chs=64,
+    ),
+
+    # 4 x conv stem w/ 2 act, no maxpool, 2,4,6,4 repeats, group size 32 in first 3 blocks
+    # DW convs in last block, 2048 pre-FC, silu act
+    resnet51q=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=6, c=1536, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=4, c=1536, s=2, gs=1, br=1.0),
+        ),
+        stem_chs=128,
+        stem_type='quad2',
+        stem_pool=None,
+        num_features=2048,
+        act_layer='silu',
+    ),
+
+    # 4 x conv stem w/ 4 act, no maxpool, 1,4,6,4 repeats, edge block first, group size 32 in next 2 blocks
+    # DW convs in last block, 4 conv for each bottle block, 2048 pre-FC, silu act
+    resnet61q=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='edge', d=1, c=256, s=1, gs=0, br=1.0, block_kwargs=dict()),
+            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=6, c=1536, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=4, c=1536, s=2, gs=1, br=1.0),
+        ),
+        stem_chs=128,
+        stem_type='quad',
+        stem_pool=None,
+        num_features=2048,
+        act_layer='silu',
+        block_kwargs=dict(extra_conv=True),
+    ),
+
+    # A series of ResNeXt-26 models w/ one of none, GC, SE, ECA, BAT attn, group size 32, SiLU act,
+    # and a tiered stem w/ maxpool
+    resnext26ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='maxpool',
+        act_layer='silu',
+    ),
+    gcresnext26ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='maxpool',
+        act_layer='silu',
+        attn_layer='gca',
+    ),
+    seresnext26ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='maxpool',
+        act_layer='silu',
+        attn_layer='se',
+    ),
+    eca_resnext26ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='maxpool',
+        act_layer='silu',
+        attn_layer='eca',
+    ),
+    bat_resnext26ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='maxpool',
+        act_layer='silu',
+        attn_layer='bat',
+        attn_kwargs=dict(block_size=8)
+    ),
+
+    # ResNet-32 (2, 3, 3, 2) models w/ no attn, no groups, SiLU act, no pre-fc feat layer, tiered stem w/o maxpool
+    resnet32ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        num_features=0,
+        act_layer='silu',
+    ),
+
+    # ResNet-33 (2, 3, 3, 2) models w/ no attn, no groups, SiLU act, 1280 pre-FC feat, tiered stem w/o maxpool
+    resnet33ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        num_features=1280,
+        act_layer='silu',
+    ),
+
+    # A series of ResNet-33 (2, 3, 3, 2) models w/ one of GC, SE, ECA attn, no groups, SiLU act, 1280 pre-FC feat
+    # and a tiered stem w/ no maxpool
+    gcresnet33ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        num_features=1280,
+        act_layer='silu',
+        attn_layer='gca',
+    ),
+    seresnet33ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        num_features=1280,
+        act_layer='silu',
+        attn_layer='se',
+    ),
+    eca_resnet33ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),
+            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        num_features=1280,
+        act_layer='silu',
+        attn_layer='eca',
+    ),
+
+    gcresnet50t=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=3, c=256, s=1, br=0.25),
+            ByoBlockCfg(type='bottle', d=4, c=512, s=2, br=0.25),
+            ByoBlockCfg(type='bottle', d=6, c=1024, s=2, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=2048, s=2, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        attn_layer='gca',
+    ),
+
+    gcresnext50ts=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=6, c=1024, s=2, gs=32, br=0.25),
+            ByoBlockCfg(type='bottle', d=3, c=2048, s=2, gs=32, br=0.25),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='maxpool',
+        act_layer='silu',
+        attn_layer='gca',
+    ),
+
+    # experimental models, closer to a RegNetZ than a ResNet. Similar to EfficientNets but w/ groups instead of DW
+    regnetz_b16=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),
+            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),
+            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=3),
+            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=3),
+        ),
+        stem_chs=32,
+        stem_pool='',
+        downsample='',
+        num_features=1536,
+        act_layer='silu',
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+    regnetz_c16=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=4),
+            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=4),
+            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=4),
+            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=4),
+        ),
+        stem_chs=32,
+        stem_pool='',
+        downsample='',
+        num_features=1536,
+        act_layer='silu',
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+    regnetz_d32=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=32, br=4),
+            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=32, br=4),
+            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=32, br=4),
+            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=32, br=4),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        downsample='',
+        num_features=1792,
+        act_layer='silu',
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+    regnetz_d8=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=8, br=4),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        downsample='',
+        num_features=1792,
+        act_layer='silu',
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+    regnetz_e8=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=3, c=96, s=1, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=8, c=192, s=2, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=16, c=384, s=2, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=8, br=4),
+        ),
+        stem_chs=64,
+        stem_type='tiered',
+        stem_pool='',
+        downsample='',
+        num_features=2048,
+        act_layer='silu',
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+
+    # experimental EvoNorm configs
+    regnetz_b16_evos=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),
+            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),
+            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=3),
+            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=3),
+        ),
+        stem_chs=32,
+        stem_pool='',
+        downsample='',
+        num_features=1536,
+        act_layer='silu',
+        norm_layer=partial(EvoNorm2dS0a, group_size=16),
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+    regnetz_c16_evos=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=4),
+            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=4),
+            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=4),
+            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=4),
+        ),
+        stem_chs=32,
+        stem_pool='',
+        downsample='',
+        num_features=1536,
+        act_layer='silu',
+        norm_layer=partial(EvoNorm2dS0a, group_size=16),
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+    regnetz_d8_evos=ByoModelCfg(
+        blocks=(
+            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=8, br=4),
+            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=8, br=4),
+        ),
+        stem_chs=64,
+        stem_type='deep',
+        stem_pool='',
+        downsample='',
+        num_features=1792,
+        act_layer='silu',
+        norm_layer=partial(EvoNorm2dS0a, group_size=16),
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.25),
+        block_kwargs=dict(bottle_in=True, linear_out=True),
+    ),
+)
+
+
 def _create_byobnet(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
         ByobNet, variant, pretrained,
         model_cfg=model_cfgs[variant],
         feature_cfg=dict(flatten_sequential=True),
         **kwargs)
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+def _cfgr(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
+        'crop_pct': 0.9, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    # GPU-Efficient (ResNet) weights
+    'gernet_s.idstcv_in1k': _cfg(hf_hub_id='timm/'),
+    'gernet_m.idstcv_in1k': _cfg(hf_hub_id='timm/'),
+    'gernet_l.idstcv_in1k': _cfg(hf_hub_id='timm/', input_size=(3, 256, 256), pool_size=(8, 8)),
+
+    # RepVGG weights
+    'repvgg_a2.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b0.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b1.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b1g4.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b2.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b2g4.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b3.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+    'repvgg_b3g4.rvgg_in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),
+
+    # experimental ResNet configs
+    'resnet51q.ra2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet51q_ra2-d47dcc76.pth',
+        first_conv='stem.conv1', input_size=(3, 256, 256), pool_size=(8, 8),
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'resnet61q.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet61q_ra2-6afc536c.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    # ResNeXt-26 models with different attention in Bottleneck blocks
+    'resnext26ts.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnext26ts_256_ra2-8bbd9106.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'seresnext26ts.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/seresnext26ts_256-6f0d74a3.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'gcresnext26ts.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnext26ts_256-e414378b.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'eca_resnext26ts.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_resnext26ts_256-5a1d030f.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'bat_resnext26ts.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/bat_resnext26ts_256-fa6fd595.pth',
+        min_input_size=(3, 256, 256)),
+
+    # ResNet-32 / 33 models with different attention in Bottleneck blocks
+    'resnet32ts.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet32ts_256-aacf5250.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'resnet33ts.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet33ts_256-e91b09a4.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'gcresnet33ts.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnet33ts_256-0e0cd345.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'seresnet33ts.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/seresnet33ts_256-f8ad44d9.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'eca_resnet33ts.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_resnet33ts_256-8f98face.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    'gcresnet50t.ra2_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnet50t_256-96374d1c.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    'gcresnext50ts.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnext50ts_256-3e0f515e.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    # custom `timm` specific RegNetZ inspired models w/ different sizing from paper
+    'regnetz_b16.ra3_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_b_raa-677d9606.pth',
+        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        input_size=(3, 224, 224), pool_size=(7, 7), crop_pct=0.94, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'regnetz_c16.ra3_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_c_rab2_256-a54bf36a.pth',
+        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),
+    'regnetz_d32.ra3_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_d_rab_256-b8073a89.pth',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.95, test_input_size=(3, 320, 320)),
+    'regnetz_d8.ra3_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_d8_bh-afc03c55.pth',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),
+    'regnetz_e8.ra3_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_e8_bh-aace8e6e.pth',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),
+
+    'regnetz_b16_evos.untrained': _cfgr(
+        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        input_size=(3, 224, 224), pool_size=(7, 7), crop_pct=0.95, test_input_size=(3, 288, 288)),
+    'regnetz_c16_evos.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_c16_evos_ch-d8311942.pth',
+        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        crop_pct=0.95, test_input_size=(3, 320, 320)),
+    'regnetz_d8_evos.ch_in1k': _cfgr(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_d8_evos_ch-2bc12646.pth',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0),
+})
+
+
+@register_model
+def gernet_l(pretrained=False, **kwargs) -> ByobNet:
+    """ GEResNet-Large (GENet-Large from official impl)
+    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
+    """
+    return _create_byobnet('gernet_l', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def gernet_m(pretrained=False, **kwargs) -> ByobNet:
+    """ GEResNet-Medium (GENet-Normal from official impl)
+    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
+    """
+    return _create_byobnet('gernet_m', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def gernet_s(pretrained=False, **kwargs) -> ByobNet:
+    """ EResNet-Small (GENet-Small from official impl)
+    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
+    """
+    return _create_byobnet('gernet_s', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_a2(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-A2
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_a2', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b0(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B0
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b0', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b1(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B1
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b1', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b1g4(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B1g4
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b1g4', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b2(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B2
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b2', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b2g4(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B2g4
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b2g4', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b3(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B3
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b3', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def repvgg_b3g4(pretrained=False, **kwargs) -> ByobNet:
+    """ RepVGG-B3g4
+    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
+    """
+    return _create_byobnet('repvgg_b3g4', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def resnet51q(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('resnet51q', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def resnet61q(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('resnet61q', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def resnext26ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('resnext26ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def gcresnext26ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('gcresnext26ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def seresnext26ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('seresnext26ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def eca_resnext26ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('eca_resnext26ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def bat_resnext26ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('bat_resnext26ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def resnet32ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('resnet32ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def resnet33ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('resnet33ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def gcresnet33ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('gcresnet33ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def seresnet33ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('seresnet33ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def eca_resnet33ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('eca_resnet33ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def gcresnet50t(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('gcresnet50t', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def gcresnext50ts(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('gcresnext50ts', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_b16(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_b16', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_c16(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_c16', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_d32(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_d32', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_d8(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_d8', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_e8(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_e8', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_b16_evos(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_b16_evos', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_c16_evos(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_c16_evos', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def regnetz_d8_evos(pretrained=False, **kwargs) -> ByobNet:
+    """
+    """
+    return _create_byobnet('regnetz_d8_evos', pretrained=pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/cait.py` & `timm-0.9.0/timm/models/cait.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,121 +10,102 @@
 # All rights reserved.
 from functools import partial
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_
+from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, use_fused_attn
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 384, 384), 'pool_size': None,
-        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    cait_xxs24_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/XXS24_224.pth',
-        input_size=(3, 224, 224),
-    ),
-    cait_xxs24_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/XXS24_384.pth',
-    ),
-    cait_xxs36_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/XXS36_224.pth',
-        input_size=(3, 224, 224),
-    ),
-    cait_xxs36_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/XXS36_384.pth',
-    ),
-    cait_xs24_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/XS24_384.pth',
-    ),
-    cait_s24_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/S24_224.pth',
-        input_size=(3, 224, 224),
-    ),
-    cait_s24_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/S24_384.pth',
-    ),
-    cait_s36_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/S36_384.pth',
-    ),
-    cait_m36_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/M36_384.pth',
-    ),
-    cait_m48_448=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/M48_448.pth',
-        input_size=(3, 448, 448),
-    ),
-)
-
-
 class ClassAttn(nn.Module):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
-    # with slight modifications to do CA 
+    # with slight modifications to do CA
+    fused_attn: torch.jit.Final[bool]
+
     def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.q = nn.Linear(dim, dim, bias=qkv_bias)
         self.k = nn.Linear(dim, dim, bias=qkv_bias)
         self.v = nn.Linear(dim, dim, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
 
     def forward(self, x):
         B, N, C = x.shape
         q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
         k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
-
-        q = q * self.scale
         v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
 
-        attn = (q @ k.transpose(-2, -1))
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        if self.fused_attn:
+            x_cls = torch.nn.functional.scaled_dot_product_attention(
+                q, k, v,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x_cls = attn @ v
 
-        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)
+        x_cls = x_cls.transpose(1, 2).reshape(B, 1, C)
         x_cls = self.proj(x_cls)
         x_cls = self.proj_drop(x_cls)
 
         return x_cls
 
 
 class LayerScaleBlockClassAttn(nn.Module):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add CA and LayerScale
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
-            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_block=ClassAttn,
-            mlp_block=Mlp, init_values=1e-4):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            attn_block=ClassAttn,
+            mlp_block=Mlp,
+            init_values=1e-4,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = attn_block(
-            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = mlp_block(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
         self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))
         self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))
 
     def forward(self, x, x_cls):
         u = torch.cat((x_cls, x), dim=1)
         x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))
         x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))
@@ -154,15 +135,15 @@
         self.proj_drop = nn.Dropout(proj_drop)
 
     def forward(self, x):
         B, N, C = x.shape
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
         q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]
 
-        attn = (q @ k.transpose(-2, -1))
+        attn = q @ k.transpose(-2, -1)
 
         attn = self.proj_l(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
 
         attn = attn.softmax(dim=-1)
 
         attn = self.proj_w(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
         attn = self.attn_drop(attn)
@@ -173,41 +154,75 @@
         return x
 
 
 class LayerScaleBlock(nn.Module):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add layerScale
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
-            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_block=TalkingHeadAttn,
-            mlp_block=Mlp, init_values=1e-4):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            attn_block=TalkingHeadAttn,
+            mlp_block=Mlp,
+            init_values=1e-4,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = attn_block(
-            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = mlp_block(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
         self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))
         self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))
 
     def forward(self, x):
         x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))
         x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
         return x
 
 
 class Cait(nn.Module):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to adapt to our cait models
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, global_pool='token',
-            embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True,
-            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,
+            self,
+            img_size=224,
+            patch_size=16,
+            in_chans=3,
+            num_classes=1000,
+            global_pool='token',
+            embed_dim=768,
+            depth=12,
+            num_heads=12,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
             block_layers=LayerScaleBlock,
             block_layers_token=LayerScaleBlockClassAttn,
             patch_layer=PatchEmbed,
             norm_layer=partial(nn.LayerNorm, eps=1e-6),
             act_layer=nn.GELU,
             attn_block=TalkingHeadAttn,
             mlp_block=Mlp,
@@ -222,41 +237,58 @@
 
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_features = self.embed_dim = embed_dim
         self.grad_checkpointing = False
 
         self.patch_embed = patch_layer(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+        )
 
         num_patches = self.patch_embed.num_patches
 
         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
         self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         dpr = [drop_path_rate for i in range(depth)]
-        self.blocks = nn.Sequential(*[
-            block_layers(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                act_layer=act_layer, attn_block=attn_block, mlp_block=mlp_block, init_values=init_values)
-            for i in range(depth)])
-
-        self.blocks_token_only = nn.ModuleList([
-            block_layers_token(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio_token_only, qkv_bias=qkv_bias,
-                drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=norm_layer,
-                act_layer=act_layer, attn_block=attn_block_token_only,
-                mlp_block=mlp_block_token_only, init_values=init_values)
-            for i in range(depth_token_only)])
+        self.blocks = nn.Sequential(*[block_layers(
+            dim=embed_dim,
+            num_heads=num_heads,
+            mlp_ratio=mlp_ratio,
+            qkv_bias=qkv_bias,
+            proj_drop=proj_drop_rate,
+            attn_drop=attn_drop_rate,
+            drop_path=dpr[i],
+            norm_layer=norm_layer,
+            act_layer=act_layer,
+            attn_block=attn_block,
+            mlp_block=mlp_block,
+            init_values=init_values,
+        ) for i in range(depth)])
+
+        self.blocks_token_only = nn.ModuleList([block_layers_token(
+            dim=embed_dim,
+            num_heads=num_heads,
+            mlp_ratio=mlp_ratio_token_only,
+            qkv_bias=qkv_bias,
+            norm_layer=norm_layer,
+            act_layer=act_layer,
+            attn_block=attn_block_token_only,
+            mlp_block=mlp_block_token_only,
+            init_values=init_values,
+        ) for _ in range(depth_token_only)])
 
         self.norm = norm_layer(embed_dim)
 
         self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
         trunc_normal_(self.pos_embed, std=.02)
         trunc_normal_(self.cls_token, std=.02)
         self.apply(self._init_weights)
 
     def _init_weights(self, m):
@@ -318,14 +350,15 @@
         x = torch.cat((cls_tokens, x), dim=1)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool:
             x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -340,81 +373,143 @@
 
 
 def _create_cait(variant, pretrained=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
     model = build_model_with_cfg(
-        Cait, variant, pretrained,
+        Cait,
+        variant,
+        pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
-        **kwargs)
+        **kwargs,
+    )
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 384, 384), 'pool_size': None,
+        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'cait_xxs24_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/XXS24_224.pth',
+        input_size=(3, 224, 224),
+    ),
+    'cait_xxs24_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/XXS24_384.pth',
+    ),
+    'cait_xxs36_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/XXS36_224.pth',
+        input_size=(3, 224, 224),
+    ),
+    'cait_xxs36_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/XXS36_384.pth',
+    ),
+    'cait_xs24_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/XS24_384.pth',
+    ),
+    'cait_s24_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/S24_224.pth',
+        input_size=(3, 224, 224),
+    ),
+    'cait_s24_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/S24_384.pth',
+    ),
+    'cait_s36_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/S36_384.pth',
+    ),
+    'cait_m36_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/M36_384.pth',
+    ),
+    'cait_m48_448.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/M48_448.pth',
+        input_size=(3, 448, 448),
+    ),
+})
+
+
 @register_model
-def cait_xxs24_224(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_xxs24_224', pretrained=pretrained, **model_args)
+def cait_xxs24_224(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5)
+    model = _create_cait('cait_xxs24_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_xxs24_384(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_xxs24_384', pretrained=pretrained, **model_args)
+def cait_xxs24_384(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5)
+    model = _create_cait('cait_xxs24_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_xxs36_224(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_xxs36_224', pretrained=pretrained, **model_args)
+def cait_xxs36_224(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5)
+    model = _create_cait('cait_xxs36_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_xxs36_384(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_xxs36_384', pretrained=pretrained, **model_args)
+def cait_xxs36_384(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5)
+    model = _create_cait('cait_xxs36_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_xs24_384(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=288, depth=24, num_heads=6, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_xs24_384', pretrained=pretrained, **model_args)
+def cait_xs24_384(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=288, depth=24, num_heads=6, init_values=1e-5)
+    model = _create_cait('cait_xs24_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_s24_224(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_s24_224', pretrained=pretrained, **model_args)
+def cait_s24_224(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5)
+    model = _create_cait('cait_s24_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_s24_384(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5, **kwargs)
-    model = _create_cait('cait_s24_384', pretrained=pretrained, **model_args)
+def cait_s24_384(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5)
+    model = _create_cait('cait_s24_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_s36_384(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=8, init_values=1e-6, **kwargs)
-    model = _create_cait('cait_s36_384', pretrained=pretrained, **model_args)
+def cait_s36_384(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=8, init_values=1e-6)
+    model = _create_cait('cait_s36_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_m36_384(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=768, depth=36, num_heads=16, init_values=1e-6, **kwargs)
-    model = _create_cait('cait_m36_384', pretrained=pretrained, **model_args)
+def cait_m36_384(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=768, depth=36, num_heads=16, init_values=1e-6)
+    model = _create_cait('cait_m36_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def cait_m48_448(pretrained=False, **kwargs):
-    model_args = dict(patch_size=16, embed_dim=768, depth=48, num_heads=16, init_values=1e-6, **kwargs)
-    model = _create_cait('cait_m48_448', pretrained=pretrained, **model_args)
+def cait_m48_448(pretrained=False, **kwargs) -> Cait:
+    model_args = dict(patch_size=16, embed_dim=768, depth=48, num_heads=16, init_values=1e-6)
+    model = _create_cait('cait_m48_448', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/coat.py` & `timm-0.9.0/timm/models/coat.py`

 * *Files 11% similar despite different names*

```diff
@@ -11,54 +11,24 @@
 from typing import Tuple, List, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert
+from timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, LayerNorm
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['CoaT']
 
 
-def _cfg_coat(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed1.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'coat_tiny': _cfg_coat(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_tiny-473c2a20.pth'
-    ),
-    'coat_mini': _cfg_coat(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_mini-2c6baf49.pth'
-    ),
-    'coat_lite_tiny': _cfg_coat(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_lite_tiny-461b07a7.pth'
-    ),
-    'coat_lite_mini': _cfg_coat(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_lite_mini-d7842000.pth'
-    ),
-    'coat_lite_small': _cfg_coat(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_lite_small-fea1d5a1.pth'
-    ),
-}
-
-
 class ConvRelPosEnc(nn.Module):
     """ Convolutional relative position encoding. """
-    def __init__(self, Ch, h, window):
+    def __init__(self, head_chs, num_heads, window):
         """
         Initialization.
             Ch: Channels per head.
             h: Number of heads.
             window: Window size(s) in convolutional relative positional encoding. It can have two forms:
                 1. An integer of window size, which assigns all attention heads with the same window s
                     size in ConvRelPosEnc.
@@ -66,63 +36,73 @@
                     e.g. {window size 1: #attention head split 1, window size 2: #attention head split 2})
                     It will apply different window size to the attention head splits.
         """
         super().__init__()
 
         if isinstance(window, int):
             # Set the same window size for all attention heads.
-            window = {window: h}
+            window = {window: num_heads}
             self.window = window
         elif isinstance(window, dict):
             self.window = window
         else:
             raise ValueError()            
         
         self.conv_list = nn.ModuleList()
         self.head_splits = []
         for cur_window, cur_head_split in window.items():
             dilation = 1
             # Determine padding size.
             # Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338
             padding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2
-            cur_conv = nn.Conv2d(cur_head_split*Ch, cur_head_split*Ch,
+            cur_conv = nn.Conv2d(
+                cur_head_split * head_chs,
+                cur_head_split * head_chs,
                 kernel_size=(cur_window, cur_window), 
                 padding=(padding_size, padding_size),
                 dilation=(dilation, dilation),                          
-                groups=cur_head_split*Ch,
+                groups=cur_head_split * head_chs,
             )
             self.conv_list.append(cur_conv)
             self.head_splits.append(cur_head_split)
-        self.channel_splits = [x*Ch for x in self.head_splits]
+        self.channel_splits = [x * head_chs for x in self.head_splits]
 
     def forward(self, q, v, size: Tuple[int, int]):
-        B, h, N, Ch = q.shape
+        B, num_heads, N, C = q.shape
         H, W = size
         _assert(N == 1 + H * W, '')
 
         # Convolutional relative position encoding.
         q_img = q[:, :, 1:, :]  # [B, h, H*W, Ch]
         v_img = v[:, :, 1:, :]  # [B, h, H*W, Ch]
 
-        v_img = v_img.transpose(-1, -2).reshape(B, h * Ch, H, W)
+        v_img = v_img.transpose(-1, -2).reshape(B, num_heads * C, H, W)
         v_img_list = torch.split(v_img, self.channel_splits, dim=1)  # Split according to channels
         conv_v_img_list = []
         for i, conv in enumerate(self.conv_list):
             conv_v_img_list.append(conv(v_img_list[i]))
         conv_v_img = torch.cat(conv_v_img_list, dim=1)
-        conv_v_img = conv_v_img.reshape(B, h, Ch, H * W).transpose(-1, -2)
+        conv_v_img = conv_v_img.reshape(B, num_heads, C, H * W).transpose(-1, -2)
 
         EV_hat = q_img * conv_v_img
         EV_hat = F.pad(EV_hat, (0, 0, 1, 0, 0, 0))  # [B, h, N, Ch].
         return EV_hat
 
 
 class FactorAttnConvRelPosEnc(nn.Module):
     """ Factorized attention with convolutional relative position encoding class. """
-    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., shared_crpe=None):
+    def __init__(
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+            shared_crpe=None,
+    ):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
 
         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)  # Note: attn_drop is actually not used.
@@ -133,15 +113,15 @@
         self.crpe = shared_crpe
 
     def forward(self, x, size: Tuple[int, int]):
         B, N, C = x.shape
 
         # Generate Q, K, V.
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, h, N, Ch]
+        q, k, v = qkv.unbind(0)  # [B, h, N, Ch]
 
         # Factorized attention.
         k_softmax = k.softmax(dim=2)
         factor_att = k_softmax.transpose(-1, -2) @ v
         factor_att = q @ factor_att
 
         # Convolutional relative position encoding.
@@ -184,30 +164,53 @@
 
         return x
 
 
 class SerialBlock(nn.Module):
     """ Serial block class.
         Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module. """
-    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
-                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, shared_cpe=None, shared_crpe=None):
+    def __init__(
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            shared_cpe=None,
+            shared_crpe=None,
+    ):
         super().__init__()
 
         # Conv-Attention.
         self.cpe = shared_cpe
 
         self.norm1 = norm_layer(dim)
         self.factoratt_crpe = FactorAttnConvRelPosEnc(
-            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, shared_crpe=shared_crpe)
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            shared_crpe=shared_crpe,
+        )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         # MLP.
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
     def forward(self, x, size: Tuple[int, int]):
         # Conv-Attention.
         x = self.cpe(x, size)
         cur = self.norm1(x)
         cur = self.factoratt_crpe(cur, size)
         x = x + self.drop_path(cur) 
@@ -218,46 +221,73 @@
         x = x + self.drop_path(cur)
 
         return x
 
 
 class ParallelBlock(nn.Module):
     """ Parallel block class. """
-    def __init__(self, dims, num_heads, mlp_ratios=[], qkv_bias=False, drop=0., attn_drop=0.,
-                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, shared_crpes=None):
+    def __init__(
+            self,
+            dims,
+            num_heads,
+            mlp_ratios=[],
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            shared_crpes=None,
+    ):
         super().__init__()
 
         # Conv-Attention.
         self.norm12 = norm_layer(dims[1])
         self.norm13 = norm_layer(dims[2])
         self.norm14 = norm_layer(dims[3])
         self.factoratt_crpe2 = FactorAttnConvRelPosEnc(
-            dims[1], num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, 
-            shared_crpe=shared_crpes[1]
+            dims[1],
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            shared_crpe=shared_crpes[1],
         )
         self.factoratt_crpe3 = FactorAttnConvRelPosEnc(
-            dims[2], num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, 
-            shared_crpe=shared_crpes[2]
+            dims[2],
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            shared_crpe=shared_crpes[2],
         )
         self.factoratt_crpe4 = FactorAttnConvRelPosEnc(
-            dims[3], num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, 
-            shared_crpe=shared_crpes[3]
+            dims[3],
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            shared_crpe=shared_crpes[3],
         )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         # MLP.
         self.norm22 = norm_layer(dims[1])
         self.norm23 = norm_layer(dims[2])
         self.norm24 = norm_layer(dims[3])
         # In parallel block, we assume dimensions are the same and share the linear transformation.
         assert dims[1] == dims[2] == dims[3]
         assert mlp_ratios[1] == mlp_ratios[2] == mlp_ratios[3]
         mlp_hidden_dim = int(dims[1] * mlp_ratios[1])
         self.mlp2 = self.mlp3 = self.mlp4 = Mlp(
-            in_features=dims[1], hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+            in_features=dims[1],
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
     def upsample(self, x, factor: float, size: Tuple[int, int]):
         """ Feature map up-sampling. """
         return self.interpolate(x, scale_factor=factor, size=size)
 
     def downsample(self, x, factor: float, size: Tuple[int, int]):
         """ Feature map down-sampling. """
@@ -270,15 +300,20 @@
         _assert(N == 1 + H * W, '')
 
         cls_token = x[:, :1, :]
         img_tokens = x[:, 1:, :]
         
         img_tokens = img_tokens.transpose(1, 2).reshape(B, C, H, W)
         img_tokens = F.interpolate(
-            img_tokens, scale_factor=scale_factor, recompute_scale_factor=False, mode='bilinear', align_corners=False)
+            img_tokens,
+            scale_factor=scale_factor,
+            recompute_scale_factor=False,
+            mode='bilinear',
+            align_corners=False,
+        )
         img_tokens = img_tokens.reshape(B, C, -1).transpose(1, 2)
         
         out = torch.cat((cls_token, img_tokens), dim=1)
 
         return out
 
     def forward(self, x1, x2, x3, x4, sizes: List[Tuple[int, int]]):
@@ -315,18 +350,35 @@
 
         return x1, x2, x3, x4
 
 
 class CoaT(nn.Module):
     """ CoaT class. """
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=(0, 0, 0, 0),
-            serial_depths=(0, 0, 0, 0), parallel_depth=0, num_heads=0, mlp_ratios=(0, 0, 0, 0), qkv_bias=True,
-            drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),
-            return_interm_layers=False, out_features=None, crpe_window=None, global_pool='token'):
+            self,
+            img_size=224,
+            patch_size=16,
+            in_chans=3,
+            num_classes=1000,
+            embed_dims=(64, 128, 320, 512),
+            serial_depths=(3, 4, 6, 3),
+            parallel_depth=0,
+            num_heads=8,
+            mlp_ratios=(4, 4, 4, 4),
+            qkv_bias=True,
+            drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            norm_layer=LayerNorm,
+            return_interm_layers=False,
+            out_features=None,
+            crpe_window=None,
+            global_pool='token',
+    ):
         super().__init__()
         assert global_pool in ('token', 'avg')
         crpe_window = crpe_window or {3: 2, 5: 3, 7: 3}
         self.return_interm_layers = return_interm_layers
         self.out_features = out_features
         self.embed_dims = embed_dims
         self.num_features = embed_dims[-1]
@@ -357,71 +409,88 @@
         # Convolutional position encodings.
         self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)
         self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)
         self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)
         self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)
 
         # Convolutional relative position encodings.
-        self.crpe1 = ConvRelPosEnc(Ch=embed_dims[0] // num_heads, h=num_heads, window=crpe_window)
-        self.crpe2 = ConvRelPosEnc(Ch=embed_dims[1] // num_heads, h=num_heads, window=crpe_window)
-        self.crpe3 = ConvRelPosEnc(Ch=embed_dims[2] // num_heads, h=num_heads, window=crpe_window)
-        self.crpe4 = ConvRelPosEnc(Ch=embed_dims[3] // num_heads, h=num_heads, window=crpe_window)
+        self.crpe1 = ConvRelPosEnc(head_chs=embed_dims[0] // num_heads, num_heads=num_heads, window=crpe_window)
+        self.crpe2 = ConvRelPosEnc(head_chs=embed_dims[1] // num_heads, num_heads=num_heads, window=crpe_window)
+        self.crpe3 = ConvRelPosEnc(head_chs=embed_dims[2] // num_heads, num_heads=num_heads, window=crpe_window)
+        self.crpe4 = ConvRelPosEnc(head_chs=embed_dims[3] // num_heads, num_heads=num_heads, window=crpe_window)
 
         # Disable stochastic depth.
         dpr = drop_path_rate
         assert dpr == 0.0
-        
+        skwargs = dict(
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            proj_drop=proj_drop_rate,
+            attn_drop=attn_drop_rate,
+            drop_path=dpr,
+            norm_layer=norm_layer,
+        )
+
         # Serial blocks 1.
         self.serial_blocks1 = nn.ModuleList([
             SerialBlock(
-                dim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, 
-                shared_cpe=self.cpe1, shared_crpe=self.crpe1
+                dim=embed_dims[0],
+                mlp_ratio=mlp_ratios[0],
+                shared_cpe=self.cpe1,
+                shared_crpe=self.crpe1,
+                **skwargs,
             )
             for _ in range(serial_depths[0])]
         )
 
         # Serial blocks 2.
         self.serial_blocks2 = nn.ModuleList([
             SerialBlock(
-                dim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, 
-                shared_cpe=self.cpe2, shared_crpe=self.crpe2
+                dim=embed_dims[1],
+                mlp_ratio=mlp_ratios[1],
+                shared_cpe=self.cpe2,
+                shared_crpe=self.crpe2,
+                **skwargs,
             )
             for _ in range(serial_depths[1])]
         )
 
         # Serial blocks 3.
         self.serial_blocks3 = nn.ModuleList([
             SerialBlock(
-                dim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, 
-                shared_cpe=self.cpe3, shared_crpe=self.crpe3
+                dim=embed_dims[2],
+                mlp_ratio=mlp_ratios[2],
+                shared_cpe=self.cpe3,
+                shared_crpe=self.crpe3,
+                **skwargs,
             )
             for _ in range(serial_depths[2])]
         )
 
         # Serial blocks 4.
         self.serial_blocks4 = nn.ModuleList([
             SerialBlock(
-                dim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, 
-                shared_cpe=self.cpe4, shared_crpe=self.crpe4
+                dim=embed_dims[3],
+                mlp_ratio=mlp_ratios[3],
+                shared_cpe=self.cpe4,
+                shared_crpe=self.crpe4,
+                **skwargs,
             )
             for _ in range(serial_depths[3])]
         )
 
         # Parallel blocks.
         self.parallel_depth = parallel_depth
         if self.parallel_depth > 0:
             self.parallel_blocks = nn.ModuleList([
                 ParallelBlock(
-                    dims=embed_dims, num_heads=num_heads, mlp_ratios=mlp_ratios, qkv_bias=qkv_bias,
-                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,
-                    shared_crpes=(self.crpe1, self.crpe2, self.crpe3, self.crpe4)
+                    dims=embed_dims,
+                    mlp_ratios=mlp_ratios,
+                    shared_crpes=(self.crpe1, self.crpe2, self.crpe3, self.crpe4),
+                    **skwargs,
                 )
                 for _ in range(parallel_depth)]
             )
         else:
             self.parallel_blocks = None
 
         # Classification head(s).
@@ -433,18 +502,20 @@
                 self.norm2 = self.norm3 = None
             self.norm4 = norm_layer(embed_dims[3])
 
             if self.parallel_depth > 0:
                 # CoaT series: Aggregate features of last three scales for classification.
                 assert embed_dims[1] == embed_dims[2] == embed_dims[3]
                 self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)
+                self.head_drop = nn.Dropout(drop_rate)
                 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
             else:
                 # CoaT-Lite series: Use feature of last scale for classification.
                 self.aggregate = None
+                self.head_drop = nn.Dropout(drop_rate)
                 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
         # Initialize weights.
         trunc_normal_(self.cls_token1, std=.02)
         trunc_normal_(self.cls_token2, std=.02)
         trunc_normal_(self.cls_token3, std=.02)
         trunc_normal_(self.cls_token4, std=.02)
@@ -583,14 +654,15 @@
             if self.global_pool == 'avg':
                 x = torch.cat([xl[:, 1:].mean(dim=1, keepdim=True) for xl in x_feat], dim=1)  # [B, 3, C]
             else:
                 x = torch.stack([xl[:, 0] for xl in x_feat], dim=1)  # [B, 3, C]
             x = self.aggregate(x).squeeze(dim=1)  # Shape: [B, C]
         else:
             x = x_feat[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x_feat[:, 0]
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x) -> torch.Tensor:
         if not torch.jit.is_scripting() and self.return_interm_layers:
             # Return intermediate features (for down-stream tasks).
             return self.forward_features(x)
         else:
@@ -610,14 +682,15 @@
 def remove_cls(x):
     """ Remove CLS token. """
     return x[:, 1:, :]
 
 
 def checkpoint_filter_fn(state_dict, model):
     out_dict = {}
+    state_dict = state_dict.get('model', state_dict)
     for k, v in state_dict.items():
         # original model had unused norm layers, removing them requires filtering pretrained checkpoints
         if k.startswith('norm1') or \
                 (model.norm2 is None and k.startswith('norm2')) or \
                 (model.norm3 is None and k.startswith('norm3')):
             continue
         out_dict[k] = v
@@ -625,56 +698,104 @@
 
 
 def _create_coat(variant, pretrained=False, default_cfg=None, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
     model = build_model_with_cfg(
-        CoaT, variant, pretrained,
+        CoaT,
+        variant,
+        pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
-        **kwargs)
+        **kwargs,
+    )
+    return model
+
+
+def _cfg_coat(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed1.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'coat_tiny.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_mini.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_small.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_lite_tiny.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_lite_mini.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_lite_small.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_lite_medium.in1k': _cfg_coat(hf_hub_id='timm/'),
+    'coat_lite_medium_384.in1k': _cfg_coat(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash',
+    ),
+})
+
+
+@register_model
+def coat_tiny(pretrained=False, **kwargs) -> CoaT:
+    model_cfg = dict(
+        patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6)
+    model = _create_coat('coat_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))
+    return model
+
+
+@register_model
+def coat_mini(pretrained=False, **kwargs) -> CoaT:
+    model_cfg = dict(
+        patch_size=4, embed_dims=[152, 216, 216, 216], serial_depths=[2, 2, 2, 2], parallel_depth=6)
+    model = _create_coat('coat_mini', pretrained=pretrained, **dict(model_cfg, **kwargs))
+    return model
+
+
+@register_model
+def coat_small(pretrained=False, **kwargs) -> CoaT:
+    model_cfg = dict(
+        patch_size=4, embed_dims=[152, 320, 320, 320], serial_depths=[2, 2, 2, 2], parallel_depth=6, **kwargs)
+    model = _create_coat('coat_small', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def coat_tiny(pretrained=False, **kwargs):
+def coat_lite_tiny(pretrained=False, **kwargs) -> CoaT:
     model_cfg = dict(
-        patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6,
-        num_heads=8, mlp_ratios=[4, 4, 4, 4], **kwargs)
-    model = _create_coat('coat_tiny', pretrained=pretrained, **model_cfg)
+        patch_size=4, embed_dims=[64, 128, 256, 320], serial_depths=[2, 2, 2, 2], mlp_ratios=[8, 8, 4, 4])
+    model = _create_coat('coat_lite_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def coat_mini(pretrained=False, **kwargs):
+def coat_lite_mini(pretrained=False, **kwargs) -> CoaT:
     model_cfg = dict(
-        patch_size=4, embed_dims=[152, 216, 216, 216], serial_depths=[2, 2, 2, 2], parallel_depth=6,
-        num_heads=8, mlp_ratios=[4, 4, 4, 4], **kwargs)
-    model = _create_coat('coat_mini', pretrained=pretrained, **model_cfg)
+        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[2, 2, 2, 2], mlp_ratios=[8, 8, 4, 4])
+    model = _create_coat('coat_lite_mini', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def coat_lite_tiny(pretrained=False, **kwargs):
+def coat_lite_small(pretrained=False, **kwargs) -> CoaT:
     model_cfg = dict(
-        patch_size=4, embed_dims=[64, 128, 256, 320], serial_depths=[2, 2, 2, 2], parallel_depth=0,
-        num_heads=8, mlp_ratios=[8, 8, 4, 4], **kwargs)
-    model = _create_coat('coat_lite_tiny', pretrained=pretrained, **model_cfg)
+        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[3, 4, 6, 3], mlp_ratios=[8, 8, 4, 4])
+    model = _create_coat('coat_lite_small', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def coat_lite_mini(pretrained=False, **kwargs):
+def coat_lite_medium(pretrained=False, **kwargs) -> CoaT:
     model_cfg = dict(
-        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[2, 2, 2, 2], parallel_depth=0,
-        num_heads=8, mlp_ratios=[8, 8, 4, 4], **kwargs)
-    model = _create_coat('coat_lite_mini', pretrained=pretrained, **model_cfg)
+        patch_size=4, embed_dims=[128, 256, 320, 512], serial_depths=[3, 6, 10, 8])
+    model = _create_coat('coat_lite_medium', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def coat_lite_small(pretrained=False, **kwargs):
+def coat_lite_medium_384(pretrained=False, **kwargs) -> CoaT:
     model_cfg = dict(
-        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[3, 4, 6, 3], parallel_depth=0,
-        num_heads=8, mlp_ratios=[8, 8, 4, 4], **kwargs)
-    model = _create_coat('coat_lite_small', pretrained=pretrained, **model_cfg)
+        img_size=384, patch_size=4, embed_dims=[128, 256, 320, 512], serial_depths=[3, 6, 10, 8])
+    model = _create_coat('coat_lite_medium_384', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/convit.py` & `timm-0.9.0/timm/models/convit.py`

 * *Files 12% similar despite different names*

```diff
@@ -24,49 +24,35 @@
 
 from functools import partial
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import DropPath, trunc_normal_, PatchEmbed, Mlp
+from timm.layers import DropPath, trunc_normal_, PatchEmbed, Mlp, LayerNorm
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_module
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .vision_transformer_hybrid import HybridEmbed
 
 
-__all__ = ['ConViT']
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # ConViT
-    'convit_tiny': _cfg(
-        url="https://dl.fbaipublicfiles.com/convit/convit_tiny.pth"),
-    'convit_small': _cfg(
-        url="https://dl.fbaipublicfiles.com/convit/convit_small.pth"),
-    'convit_base': _cfg(
-        url="https://dl.fbaipublicfiles.com/convit/convit_base.pth")
-}
+__all__ = ['ConVit']
 
 
 @register_notrace_module  # reason: FX can't symbolically trace control flow in forward method
 class GPSA(nn.Module):
     def __init__(
-            self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., locality_strength=1.):
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+            locality_strength=1.,
+    ):
         super().__init__()
         self.num_heads = num_heads
         self.dim = dim
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
         self.locality_strength = locality_strength
 
@@ -141,15 +127,22 @@
         rel_indices[:, :, :, 1] = indy.unsqueeze(0)
         rel_indices[:, :, :, 0] = indx.unsqueeze(0)
         device = self.qk.weight.device
         return rel_indices.to(device)
 
 
 class MHSA(nn.Module):
-    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
+    def __init__(
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+    ):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
 
         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)
@@ -191,44 +184,90 @@
         x = self.proj_drop(x)
         return x
 
 
 class Block(nn.Module):
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
-            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_gpsa=True, **kwargs):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=LayerNorm,
+            use_gpsa=True,
+            locality_strength=1.,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.use_gpsa = use_gpsa
         if self.use_gpsa:
             self.attn = GPSA(
-                dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, **kwargs)
+                dim,
+                num_heads=num_heads,
+                qkv_bias=qkv_bias,
+                attn_drop=attn_drop,
+                proj_drop=proj_drop,
+                locality_strength=locality_strength,
+            )
         else:
-            self.attn = MHSA(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+            self.attn = MHSA(
+                dim,
+                num_heads=num_heads,
+                qkv_bias=qkv_bias,
+                attn_drop=attn_drop,
+                proj_drop=proj_drop,
+            )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
     def forward(self, x):
         x = x + self.drop_path(self.attn(self.norm1(x)))
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
-class ConViT(nn.Module):
+class ConVit(nn.Module):
     """ Vision Transformer with support for patch or hybrid CNN input stage
     """
 
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, global_pool='token',
-            embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, drop_rate=0., attn_drop_rate=0.,
-            drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm,
-            local_up_to_layer=3, locality_strength=1., use_pos_embed=True):
+            self,
+            img_size=224,
+            patch_size=16,
+            in_chans=3,
+            num_classes=1000,
+            global_pool='token',
+            embed_dim=768,
+            depth=12,
+            num_heads=12,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            hybrid_backbone=None,
+            norm_layer=LayerNorm,
+            local_up_to_layer=3,
+            locality_strength=1.,
+            use_pos_embed=True,
+    ):
         super().__init__()
         assert global_pool in ('', 'avg', 'token')
         embed_dim *= num_heads
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.local_up_to_layer = local_up_to_layer
         self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
@@ -236,42 +275,48 @@
         self.use_pos_embed = use_pos_embed
 
         if hybrid_backbone is not None:
             self.patch_embed = HybridEmbed(
                 hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)
         else:
             self.patch_embed = PatchEmbed(
-                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
+                img_size=img_size,
+                patch_size=patch_size,
+                in_chans=in_chans,
+                embed_dim=embed_dim,
+            )
         num_patches = self.patch_embed.num_patches
         self.num_patches = num_patches
 
         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         if self.use_pos_embed:
             self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
             trunc_normal_(self.pos_embed, std=.02)
 
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
         self.blocks = nn.ModuleList([
             Block(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                use_gpsa=True,
-                locality_strength=locality_strength)
-            if i < local_up_to_layer else
-            Block(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                use_gpsa=False)
-            for i in range(depth)])
+                dim=embed_dim,
+                num_heads=num_heads,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+                use_gpsa=i < local_up_to_layer,
+                locality_strength=locality_strength,
+            ) for i in range(depth)])
         self.norm = norm_layer(embed_dim)
 
         # Classifier head
         self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
         trunc_normal_(self.cls_token, std=.02)
         self.apply(self._init_weights)
         for n, m in self.named_modules():
             if hasattr(m, 'local_init'):
                 m.local_init()
@@ -323,47 +368,63 @@
             x = blk(x)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool:
             x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_convit(variant, pretrained=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
-    return build_model_with_cfg(ConViT, variant, pretrained, **kwargs)
+    return build_model_with_cfg(ConVit, variant, pretrained, **kwargs)
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    # ConViT
+    'convit_tiny.fb_in1k': _cfg(hf_hub_id='timm/'),
+    'convit_small.fb_in1k': _cfg(hf_hub_id='timm/'),
+    'convit_base.fb_in1k': _cfg(hf_hub_id='timm/')
+})
 
 
 @register_model
-def convit_tiny(pretrained=False, **kwargs):
+def convit_tiny(pretrained=False, **kwargs) -> ConVit:
     model_args = dict(
-        local_up_to_layer=10, locality_strength=1.0, embed_dim=48,
-        num_heads=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    model = _create_convit(variant='convit_tiny', pretrained=pretrained, **model_args)
+        local_up_to_layer=10, locality_strength=1.0, embed_dim=48, num_heads=4)
+    model = _create_convit(variant='convit_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convit_small(pretrained=False, **kwargs):
+def convit_small(pretrained=False, **kwargs) -> ConVit:
     model_args = dict(
-        local_up_to_layer=10, locality_strength=1.0, embed_dim=48,
-        num_heads=9, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    model = _create_convit(variant='convit_small', pretrained=pretrained, **model_args)
+        local_up_to_layer=10, locality_strength=1.0, embed_dim=48, num_heads=9)
+    model = _create_convit(variant='convit_small', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convit_base(pretrained=False, **kwargs):
+def convit_base(pretrained=False, **kwargs) -> ConVit:
     model_args = dict(
-        local_up_to_layer=10, locality_strength=1.0, embed_dim=48,
-        num_heads=16, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    model = _create_convit(variant='convit_base', pretrained=pretrained, **model_args)
+        local_up_to_layer=10, locality_strength=1.0, embed_dim=48, num_heads=16)
+    model = _create_convit(variant='convit_base', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/convmixer.py` & `timm-0.9.0/timm/models/convmixer.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,52 +2,44 @@
 
 """
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq
 
 __all__ = ['ConvMixer']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .96, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'classifier': 'head',
-        'first_conv': 'stem.0',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'convmixer_1536_20': _cfg(url='https://github.com/tmp-iclr/convmixer/releases/download/timm-v1.0/convmixer_1536_20_ks9_p7.pth.tar'),
-    'convmixer_768_32': _cfg(url='https://github.com/tmp-iclr/convmixer/releases/download/timm-v1.0/convmixer_768_32_ks7_p7_relu.pth.tar'),
-    'convmixer_1024_20_ks9_p14': _cfg(url='https://github.com/tmp-iclr/convmixer/releases/download/timm-v1.0/convmixer_1024_20_ks9_p14.pth.tar')
-}
-
-
 class Residual(nn.Module):
     def __init__(self, fn):
         super().__init__()
         self.fn = fn
 
     def forward(self, x):
         return self.fn(x) + x
 
 
 class ConvMixer(nn.Module):
     def __init__(
-            self, dim, depth, kernel_size=9, patch_size=7, in_chans=3, num_classes=1000, global_pool='avg',
-            act_layer=nn.GELU, **kwargs):
+            self,
+            dim,
+            depth,
+            kernel_size=9,
+            patch_size=7,
+            in_chans=3,
+            num_classes=1000,
+            global_pool='avg',
+            drop_rate=0.,
+            act_layer=nn.GELU,
+            **kwargs,
+    ):
         super().__init__()
         self.num_classes = num_classes
         self.num_features = dim
         self.grad_checkpointing = False
 
         self.stem = nn.Sequential(
             nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size),
@@ -63,14 +55,15 @@
                     )),
                     nn.Conv2d(dim, dim, kernel_size=1),
                     act_layer(),
                     nn.BatchNorm2d(dim)
             ) for i in range(depth)]
         )
         self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(dim, num_classes) if num_classes > 0 else nn.Identity()
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         matcher = dict(stem=r'^stem', blocks=r'^blocks\.(\d+)')
         return matcher
 
@@ -94,35 +87,55 @@
             x = checkpoint_seq(self.blocks, x)
         else:
             x = self.blocks(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.pooling(x)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_convmixer(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(ConvMixer, variant, pretrained, **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .96, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'classifier': 'head',
+        'first_conv': 'stem.0',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'convmixer_1536_20.in1k': _cfg(hf_hub_id='timm/'),
+    'convmixer_768_32.in1k': _cfg(hf_hub_id='timm/'),
+    'convmixer_1024_20_ks9_p14.in1k': _cfg(hf_hub_id='timm/')
+})
+
+
+
 @register_model
-def convmixer_1536_20(pretrained=False, **kwargs):
+def convmixer_1536_20(pretrained=False, **kwargs) -> ConvMixer:
     model_args = dict(dim=1536, depth=20, kernel_size=9, patch_size=7, **kwargs)
     return _create_convmixer('convmixer_1536_20', pretrained, **model_args)
 
 
 @register_model
-def convmixer_768_32(pretrained=False, **kwargs):
+def convmixer_768_32(pretrained=False, **kwargs) -> ConvMixer:
     model_args = dict(dim=768, depth=32, kernel_size=7, patch_size=7, act_layer=nn.ReLU, **kwargs)
     return _create_convmixer('convmixer_768_32', pretrained, **model_args)
 
 
 @register_model
-def convmixer_1024_20_ks9_p14(pretrained=False, **kwargs):
+def convmixer_1024_20_ks9_p14(pretrained=False, **kwargs) -> ConvMixer:
     model_args = dict(dim=1024, depth=20, kernel_size=9, patch_size=14, **kwargs)
     return _create_convmixer('convmixer_1024_20_ks9_p14', pretrained, **model_args)
```

### Comparing `timm-0.8.6.dev0/timm/models/convnext.py` & `timm-0.9.0/timm/models/convnext.py`

 * *Files 16% similar despite different names*

```diff
@@ -35,73 +35,120 @@
 # All rights reserved.
 # This source code is licensed under the license found in the
 # LICENSE file in the root directory of this source tree (Attribution-NonCommercial 4.0 International (CC BY-NC 4.0))
 # No code was used directly from ConvNeXt-V2, however the weights are CC BY-NC 4.0 so beware if using commercially.
 
 from collections import OrderedDict
 from functools import partial
+from typing import Callable, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 
-from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import trunc_normal_, SelectAdaptivePool2d, DropPath, Mlp, GlobalResponseNormMlp, \
+from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
+from timm.layers import trunc_normal_, AvgPool2dSame, DropPath, Mlp, GlobalResponseNormMlp, \
     LayerNorm2d, LayerNorm, create_conv2d, get_act_layer, make_divisible, to_ntuple
+from timm.layers import NormMlpClassifierHead, ClassifierHead
 from ._builder import build_model_with_cfg
 from ._manipulate import named_apply, checkpoint_seq
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['ConvNeXt']  # model_registry will add each entrypoint fn to this
 
 
+class Downsample(nn.Module):
+
+    def __init__(self, in_chs, out_chs, stride=1, dilation=1):
+        super().__init__()
+        avg_stride = stride if dilation == 1 else 1
+        if stride > 1 or dilation > 1:
+            avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
+            self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
+        else:
+            self.pool = nn.Identity()
+
+        if in_chs != out_chs:
+            self.conv = create_conv2d(in_chs, out_chs, 1, stride=1)
+        else:
+            self.conv = nn.Identity()
+
+    def forward(self, x):
+        x = self.pool(x)
+        x = self.conv(x)
+        return x
+
+
 class ConvNeXtBlock(nn.Module):
     """ ConvNeXt Block
     There are two equivalent implementations:
       (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)
       (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back
 
     Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate
     choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear
     is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time & w/ different HW.
-
-    Args:
-        in_chs (int): Number of input channels.
-        drop_path (float): Stochastic depth rate. Default: 0.0
-        ls_init_value (float): Init value for Layer Scale. Default: 1e-6.
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs=None,
-            kernel_size=7,
-            stride=1,
-            dilation=1,
-            mlp_ratio=4,
-            conv_mlp=False,
-            conv_bias=True,
-            use_grn=False,
-            ls_init_value=1e-6,
-            act_layer='gelu',
-            norm_layer=None,
-            drop_path=0.,
+            in_chs: int,
+            out_chs: Optional[int] = None,
+            kernel_size: int = 7,
+            stride: int = 1,
+            dilation: Union[int, Tuple[int, int]] = (1, 1),
+            mlp_ratio: float = 4,
+            conv_mlp: bool = False,
+            conv_bias: bool = True,
+            use_grn: bool = False,
+            ls_init_value: Optional[float] = 1e-6,
+            act_layer: Union[str, Callable] = 'gelu',
+            norm_layer: Optional[Callable] = None,
+            drop_path: float = 0.,
     ):
+        """
+
+        Args:
+            in_chs: Block input channels.
+            out_chs: Block output channels (same as in_chs if None).
+            kernel_size: Depthwise convolution kernel size.
+            stride: Stride of depthwise convolution.
+            dilation: Tuple specifying input and output dilation of block.
+            mlp_ratio: MLP expansion ratio.
+            conv_mlp: Use 1x1 convolutions for MLP and a NCHW compatible norm layer if True.
+            conv_bias: Apply bias for all convolution (linear) layers.
+            use_grn: Use GlobalResponseNorm in MLP (from ConvNeXt-V2)
+            ls_init_value: Layer-scale init values, layer-scale applied if not None.
+            act_layer: Activation layer.
+            norm_layer: Normalization layer (defaults to LN if not specified).
+            drop_path: Stochastic depth probability.
+        """
         super().__init__()
         out_chs = out_chs or in_chs
+        dilation = to_ntuple(2)(dilation)
         act_layer = get_act_layer(act_layer)
         if not norm_layer:
             norm_layer = LayerNorm2d if conv_mlp else LayerNorm
         mlp_layer = partial(GlobalResponseNormMlp if use_grn else Mlp, use_conv=conv_mlp)
         self.use_conv_mlp = conv_mlp
         self.conv_dw = create_conv2d(
-            in_chs, out_chs, kernel_size=kernel_size, stride=stride, dilation=dilation, depthwise=True, bias=conv_bias)
+            in_chs,
+            out_chs,
+            kernel_size=kernel_size,
+            stride=stride,
+            dilation=dilation[0],
+            depthwise=True,
+            bias=conv_bias,
+        )
         self.norm = norm_layer(out_chs)
         self.mlp = mlp_layer(out_chs, int(mlp_ratio * out_chs), act_layer=act_layer)
         self.gamma = nn.Parameter(ls_init_value * torch.ones(out_chs)) if ls_init_value is not None else None
+        if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:
+            self.shortcut = Downsample(in_chs, out_chs, stride=stride, dilation=dilation[0])
+        else:
+            self.shortcut = nn.Identity()
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
     def forward(self, x):
         shortcut = x
         x = self.conv_dw(x)
         if self.use_conv_mlp:
             x = self.norm(x)
@@ -110,15 +157,15 @@
             x = x.permute(0, 2, 3, 1)
             x = self.norm(x)
             x = self.mlp(x)
             x = x.permute(0, 3, 1, 2)
         if self.gamma is not None:
             x = x.mul(self.gamma.reshape(1, -1, 1, 1))
 
-        x = self.drop_path(x) + shortcut
+        x = self.drop_path(x) + self.shortcut(shortcut)
         return x
 
 
 class ConvNeXtStage(nn.Module):
 
     def __init__(
             self,
@@ -142,16 +189,22 @@
 
         if in_chs != out_chs or stride > 1 or dilation[0] != dilation[1]:
             ds_ks = 2 if stride > 1 or dilation[0] != dilation[1] else 1
             pad = 'same' if dilation[1] > 1 else 0  # same padding needed if dilation used
             self.downsample = nn.Sequential(
                 norm_layer(in_chs),
                 create_conv2d(
-                    in_chs, out_chs, kernel_size=ds_ks, stride=stride,
-                    dilation=dilation[0], padding=pad, bias=conv_bias),
+                    in_chs,
+                    out_chs,
+                    kernel_size=ds_ks,
+                    stride=stride,
+                    dilation=dilation[0],
+                    padding=pad,
+                    bias=conv_bias,
+                ),
             )
             in_chs = out_chs
         else:
             self.downsample = nn.Identity()
 
         drop_path_rates = drop_path_rates or [0.] * depth
         stage_blocks = []
@@ -184,77 +237,85 @@
 class ConvNeXt(nn.Module):
     r""" ConvNeXt
         A PyTorch impl of : `A ConvNet for the 2020s`  - https://arxiv.org/pdf/2201.03545.pdf
     """
 
     def __init__(
             self,
-            in_chans=3,
-            num_classes=1000,
-            global_pool='avg',
-            output_stride=32,
-            depths=(3, 3, 9, 3),
-            dims=(96, 192, 384, 768),
-            kernel_sizes=7,
-            ls_init_value=1e-6,
-            stem_type='patch',
-            patch_size=4,
-            head_init_scale=1.,
-            head_norm_first=False,
-            conv_mlp=False,
-            conv_bias=True,
-            use_grn=False,
-            act_layer='gelu',
-            norm_layer=None,
-            drop_rate=0.,
-            drop_path_rate=0.,
+            in_chans: int = 3,
+            num_classes: int = 1000,
+            global_pool: str = 'avg',
+            output_stride: int = 32,
+            depths: Tuple[int, ...] = (3, 3, 9, 3),
+            dims: Tuple[int, ...] = (96, 192, 384, 768),
+            kernel_sizes: Union[int, Tuple[int, ...]] = 7,
+            ls_init_value: Optional[float] = 1e-6,
+            stem_type: str = 'patch',
+            patch_size: int = 4,
+            head_init_scale: float = 1.,
+            head_norm_first: bool = False,
+            head_hidden_size: Optional[int] = None,
+            conv_mlp: bool = False,
+            conv_bias: bool = True,
+            use_grn: bool = False,
+            act_layer: Union[str, Callable] = 'gelu',
+            norm_layer: Optional[Union[str, Callable]] = None,
+            norm_eps: Optional[float] = None,
+            drop_rate: float = 0.,
+            drop_path_rate: float = 0.,
     ):
         """
         Args:
-            in_chans (int): Number of input image channels (default: 3)
-            num_classes (int): Number of classes for classification head (default: 1000)
-            global_pool (str): Global pooling type (default: 'avg')
-            output_stride (int): Output stride of network, one of (8, 16, 32) (default: 32)
-            depths (tuple(int)): Number of blocks at each stage. (default: [3, 3, 9, 3])
-            dims (tuple(int)): Feature dimension at each stage. (default: [96, 192, 384, 768])
-            kernel_sizes (Union[int, List[int]]: Depthwise convolution kernel-sizes for each stage (default: 7)
-            ls_init_value (float): Init value for Layer Scale (default: 1e-6)
-            stem_type (str): Type of stem (default: 'patch')
-            patch_size (int): Stem patch size for patch stem (default: 4)
-            head_init_scale (float): Init scaling value for classifier weights and biases (default: 1)
-            head_norm_first (bool): Apply normalization before global pool + head (default: False)
-            conv_mlp (bool): Use 1x1 conv in MLP, improves speed for small networks w/ chan last (default: False)
-            conv_bias (bool): Use bias layers w/ all convolutions (default: True)
-            use_grn (bool): Use Global Response Norm (ConvNeXt-V2) in MLP (default: False)
-            act_layer (Union[str, nn.Module]): Activation Layer
-            norm_layer (Union[str, nn.Module]): Normalization Layer
-            drop_rate (float): Head dropout rate (default: 0.)
-            drop_path_rate (float): Stochastic depth rate (default: 0.)
+            in_chans: Number of input image channels.
+            num_classes: Number of classes for classification head.
+            global_pool: Global pooling type.
+            output_stride: Output stride of network, one of (8, 16, 32).
+            depths: Number of blocks at each stage.
+            dims: Feature dimension at each stage.
+            kernel_sizes: Depthwise convolution kernel-sizes for each stage.
+            ls_init_value: Init value for Layer Scale, disabled if None.
+            stem_type: Type of stem.
+            patch_size: Stem patch size for patch stem.
+            head_init_scale: Init scaling value for classifier weights and biases.
+            head_norm_first: Apply normalization before global pool + head.
+            head_hidden_size: Size of MLP hidden layer in head if not None and head_norm_first == False.
+            conv_mlp: Use 1x1 conv in MLP, improves speed for small networks w/ chan last.
+            conv_bias: Use bias layers w/ all convolutions.
+            use_grn: Use Global Response Norm (ConvNeXt-V2) in MLP.
+            act_layer: Activation layer type.
+            norm_layer: Normalization layer type.
+            drop_rate: Head pre-classifier dropout rate.
+            drop_path_rate: Stochastic depth drop rate.
         """
         super().__init__()
         assert output_stride in (8, 16, 32)
         kernel_sizes = to_ntuple(4)(kernel_sizes)
         if norm_layer is None:
             norm_layer = LayerNorm2d
             norm_layer_cl = norm_layer if conv_mlp else LayerNorm
+            if norm_eps is not None:
+                norm_layer = partial(norm_layer, eps=norm_eps)
+                norm_layer_cl = partial(norm_layer_cl, eps=norm_eps)
         else:
             assert conv_mlp,\
                 'If a norm_layer is specified, conv MLP must be used so all norm expect rank-4, channels-first input'
             norm_layer_cl = norm_layer
+            if norm_eps is not None:
+                norm_layer_cl = partial(norm_layer_cl, eps=norm_eps)
 
         self.num_classes = num_classes
         self.drop_rate = drop_rate
         self.feature_info = []
 
         assert stem_type in ('patch', 'overlap', 'overlap_tiered')
         if stem_type == 'patch':
             # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4
             self.stem = nn.Sequential(
                 nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size, bias=conv_bias),
-                norm_layer(dims[0])
+                norm_layer(dims[0]),
             )
             stem_stride = patch_size
         else:
             mid_chs = make_divisible(dims[0] // 2) if 'tiered' in stem_type else dims[0]
             self.stem = nn.Sequential(
                 nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias),
                 nn.Conv2d(mid_chs, dims[0], kernel_size=3, stride=2, padding=1, bias=conv_bias),
@@ -297,22 +358,34 @@
             # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2
             self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{i}')]
         self.stages = nn.Sequential(*stages)
         self.num_features = prev_chs
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
         # otherwise pool -> norm -> fc, the default ConvNeXt ordering (pretrained FB weights)
-        self.norm_pre = norm_layer(self.num_features) if head_norm_first else nn.Identity()
-        self.head = nn.Sequential(OrderedDict([
-                ('global_pool', SelectAdaptivePool2d(pool_type=global_pool)),
-                ('norm', nn.Identity() if head_norm_first else norm_layer(self.num_features)),
-                ('flatten', nn.Flatten(1) if global_pool else nn.Identity()),
-                ('drop', nn.Dropout(self.drop_rate)),
-                ('fc', nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())]))
-
+        if head_norm_first:
+            assert not head_hidden_size
+            self.norm_pre = norm_layer(self.num_features)
+            self.head = ClassifierHead(
+                self.num_features,
+                num_classes,
+                pool_type=global_pool,
+                drop_rate=self.drop_rate,
+            )
+        else:
+            self.norm_pre = nn.Identity()
+            self.head = NormMlpClassifierHead(
+                self.num_features,
+                num_classes,
+                hidden_size=head_hidden_size,
+                pool_type=global_pool,
+                drop_rate=self.drop_rate,
+                norm_layer=norm_layer,
+                act_layer='gelu',
+            )
         named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
             blocks=r'^stages\.(\d+)' if coarse else [
@@ -328,32 +401,24 @@
             s.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes=0, global_pool=None):
-        if global_pool is not None:
-            self.head.global_pool = SelectAdaptivePool2d(pool_type=global_pool)
-            self.head.flatten = nn.Flatten(1) if global_pool else nn.Identity()
-        self.head.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm_pre(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        # NOTE nn.Sequential in head broken down since can't call head[:-1](x) in torchscript :(
-        x = self.head.global_pool(x)
-        x = self.head.norm(x)
-        x = self.head.flatten(x)
-        x = self.head.drop(x)
-        return x if pre_logits else self.head.fc(x)
+        return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
@@ -372,15 +437,28 @@
 
 def checkpoint_filter_fn(state_dict, model):
     """ Remap FB checkpoints -> timm """
     if 'head.norm.weight' in state_dict or 'norm_pre.weight' in state_dict:
         return state_dict  # non-FB checkpoint
     if 'model' in state_dict:
         state_dict = state_dict['model']
+
     out_dict = {}
+    if 'visual.trunk.stem.0.weight' in state_dict:
+        out_dict = {k.replace('visual.trunk.', ''): v for k, v in state_dict.items() if k.startswith('visual.trunk.')}
+        if 'visual.head.proj.weight' in state_dict:
+            out_dict['head.fc.weight'] = state_dict['visual.head.proj.weight']
+            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])
+        elif 'visual.head.mlp.fc1.weight' in state_dict:
+            out_dict['head.pre_logits.fc.weight'] = state_dict['visual.head.mlp.fc1.weight']
+            out_dict['head.pre_logits.fc.bias'] = state_dict['visual.head.mlp.fc1.bias']
+            out_dict['head.fc.weight'] = state_dict['visual.head.mlp.fc2.weight']
+            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.mlp.fc2.weight'].shape[0])
+        return out_dict
+
     import re
     for k, v in state_dict.items():
         k = k.replace('downsample_layers.0.', 'stem.')
         k = re.sub(r'stages.([0-9]+).([0-9]+)', r'stages.\1.blocks.\2', k)
         k = re.sub(r'downsample_layers.([0-9]+).([0-9]+)', r'stages.\1.downsample.\2', k)
         k = k.replace('dwconv', 'conv_dw')
         k = k.replace('pwconv', 'mlp.fc')
@@ -391,14 +469,15 @@
         k = k.replace('head.', 'head.fc.')
         if k.startswith('norm.'):
             k = k.replace('norm', 'head.norm')
         if v.ndim == 2 and 'head' not in k:
             model_shape = model.state_dict()[k].shape
             v = v.reshape(model_shape)
         out_dict[k] = v
+
     return out_dict
 
 
 def _create_convnext(variant, pretrained=False, **kwargs):
     if kwargs.get('pretrained_cfg', '') == 'fcmae':
         # NOTE fcmae pretrained weights have no classifier or final norm-layer (`head.norm`)
         # This is workaround loading with num_classes=0 w/o removing norm-layer.
@@ -435,14 +514,21 @@
         'origin_url': 'https://github.com/facebookresearch/ConvNeXt-V2',
         **kwargs
     }
 
 
 default_cfgs = generate_default_cfgs({
     # timm specific variants
+    'convnext_tiny.in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'convnext_small.in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
     'convnext_atto.d2_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_atto_d2-01bb0f51.pth',
         hf_hub_id='timm/',
         test_input_size=(3, 288, 288), test_crop_pct=0.95),
     'convnext_atto_ols.a2_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_atto_ols_a2-78d1c8f3.pth',
         hf_hub_id='timm/',
@@ -474,50 +560,32 @@
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_nano_ols_d1h-ae424a9a.pth',
         hf_hub_id='timm/',
         crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
     'convnext_tiny_hnf.a2h_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_tiny_hnf_a2h-ab7e9df2.pth',
         hf_hub_id='timm/',
         crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'convnext_tiny.in12k_ft_in1k': _cfg(
+
+    'convnext_tiny.in12k_ft_in1k_384': _cfg(
         hf_hub_id='timm/',
-        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'convnext_small.in12k_ft_in1k': _cfg(
+       input_size=(3, 384, 384), pool_size=(12, 12),  crop_pct=1.0, crop_mode='squash'),
+    'convnext_small.in12k_ft_in1k_384': _cfg(
         hf_hub_id='timm/',
-        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0,  crop_mode='squash'),
 
     'convnext_nano.in12k': _cfg(
         hf_hub_id='timm/',
         crop_pct=0.95, num_classes=11821),
     'convnext_tiny.in12k': _cfg(
         hf_hub_id='timm/',
         crop_pct=0.95, num_classes=11821),
     'convnext_small.in12k': _cfg(
         hf_hub_id='timm/',
         crop_pct=0.95, num_classes=11821),
 
-    'convnext_tiny.fb_in1k': _cfg(
-        url="https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth",
-        hf_hub_id='timm/',
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'convnext_small.fb_in1k': _cfg(
-        url="https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth",
-        hf_hub_id='timm/',
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'convnext_base.fb_in1k': _cfg(
-        url="https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth",
-        hf_hub_id='timm/',
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'convnext_large.fb_in1k': _cfg(
-        url="https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth",
-        hf_hub_id='timm/',
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'convnext_xlarge.untrained': _cfg(),
-    'convnext_xxlarge.untrained': _cfg(),
-
     'convnext_tiny.fb_in22k_ft_in1k': _cfg(
         url='https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_224.pth',
         hf_hub_id='timm/',
         test_input_size=(3, 288, 288), test_crop_pct=1.0),
     'convnext_small.fb_in22k_ft_in1k': _cfg(
         url='https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_224.pth',
         hf_hub_id='timm/',
@@ -531,14 +599,31 @@
         hf_hub_id='timm/',
         test_input_size=(3, 288, 288), test_crop_pct=1.0),
     'convnext_xlarge.fb_in22k_ft_in1k': _cfg(
         url='https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_224_ema.pth',
         hf_hub_id='timm/',
         test_input_size=(3, 288, 288), test_crop_pct=1.0),
 
+    'convnext_tiny.fb_in1k': _cfg(
+        url="https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth",
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'convnext_small.fb_in1k': _cfg(
+        url="https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth",
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'convnext_base.fb_in1k': _cfg(
+        url="https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth",
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'convnext_large.fb_in1k': _cfg(
+        url="https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth",
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
     'convnext_tiny.fb_in22k_ft_in1k_384': _cfg(
         url='https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_384.pth',
         hf_hub_id='timm/',
         input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'convnext_small.fb_in22k_ft_in1k_384': _cfg(
         url='https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_384.pth',
         hf_hub_id='timm/',
@@ -681,203 +766,329 @@
         num_classes=0),
     'convnextv2_huge.fcmae': _cfgv2(
         url="https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_huge_1k_224_fcmae.pt",
         hf_hub_id='timm/',
         num_classes=0),
 
     'convnextv2_small.untrained': _cfg(),
+
+    # CLIP weights, fine-tuned on in1k or in12k + in1k
+    'convnext_base.clip_laion2b_augreg_ft_in12k_in1k': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),
+    'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0),
+    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+
+    'convnext_base.clip_laion2b_augreg_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),
+    'convnext_base.clip_laiona_augreg_ft_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'convnext_large_mlp.clip_laion2b_augreg_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0
+    ),
+    'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'
+    ),
+    'convnext_xxlarge.clip_laion2b_soup_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),
+
+    'convnext_base.clip_laion2b_augreg_ft_in12k': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),
+    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0),
+    'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384': _cfg(
+        hf_hub_id='timm/',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+
+    # CLIP original image tower weights
+    'convnext_base.clip_laion2b': _cfg(
+        hf_hub_id='laion/CLIP-convnext_base_w-laion2B-s13B-b82K',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=640),
+    'convnext_base.clip_laion2b_augreg': _cfg(
+        hf_hub_id='laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=640),
+    'convnext_base.clip_laiona': _cfg(
+        hf_hub_id='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=640),
+    'convnext_base.clip_laiona_320': _cfg(
+        hf_hub_id='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=640),
+    'convnext_base.clip_laiona_augreg_320': _cfg(
+        hf_hub_id='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=640),
+    'convnext_large_mlp.clip_laion2b_augreg': _cfg(
+        hf_hub_id='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=768),
+    'convnext_large_mlp.clip_laion2b_ft_320': _cfg(
+        hf_hub_id='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=768),
+    'convnext_large_mlp.clip_laion2b_ft_soup_320': _cfg(
+        hf_hub_id='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=768),
+    'convnext_xxlarge.clip_laion2b_soup': _cfg(
+        hf_hub_id='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=1024),
+    'convnext_xxlarge.clip_laion2b_rewind': _cfg(
+        hf_hub_id='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=1024),
 })
 
 
 @register_model
-def convnext_atto(pretrained=False, **kwargs):
+def convnext_atto(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
-    model_args = dict(
-        depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True, **kwargs)
-    model = _create_convnext('convnext_atto', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True)
+    model = _create_convnext('convnext_atto', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_atto_ols(pretrained=False, **kwargs):
+def convnext_atto_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant with overlapping 3x3 conv stem, wider than non-ols femto above, current param count 3.7M
-    model_args = dict(
-        depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True, stem_type='overlap_tiered', **kwargs)
-    model = _create_convnext('convnext_atto_ols', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True, stem_type='overlap_tiered')
+    model = _create_convnext('convnext_atto_ols', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_femto(pretrained=False, **kwargs):
+def convnext_femto(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant
-    model_args = dict(
-        depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True, **kwargs)
-    model = _create_convnext('convnext_femto', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True)
+    model = _create_convnext('convnext_femto', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_femto_ols(pretrained=False, **kwargs):
+def convnext_femto_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant
-    model_args = dict(
-        depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True, stem_type='overlap_tiered', **kwargs)
-    model = _create_convnext('convnext_femto_ols', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True, stem_type='overlap_tiered')
+    model = _create_convnext('convnext_femto_ols', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_pico(pretrained=False, **kwargs):
+def convnext_pico(pretrained=False, **kwargs) -> ConvNeXt:
     # timm pico variant
-    model_args = dict(
-        depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True, **kwargs)
-    model = _create_convnext('convnext_pico', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True)
+    model = _create_convnext('convnext_pico', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_pico_ols(pretrained=False, **kwargs):
+def convnext_pico_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # timm nano variant with overlapping 3x3 conv stem
-    model_args = dict(
-        depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True,  stem_type='overlap_tiered', **kwargs)
-    model = _create_convnext('convnext_pico_ols', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True,  stem_type='overlap_tiered')
+    model = _create_convnext('convnext_pico_ols', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_nano(pretrained=False, **kwargs):
+def convnext_nano(pretrained=False, **kwargs) -> ConvNeXt:
     # timm nano variant with standard stem and head
-    model_args = dict(
-        depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True, **kwargs)
-    model = _create_convnext('convnext_nano', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True)
+    model = _create_convnext('convnext_nano', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_nano_ols(pretrained=False, **kwargs):
+def convnext_nano_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # experimental nano variant with overlapping conv stem
-    model_args = dict(
-        depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True, stem_type='overlap', **kwargs)
-    model = _create_convnext('convnext_nano_ols', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True, stem_type='overlap')
+    model = _create_convnext('convnext_nano_ols', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_tiny_hnf(pretrained=False, **kwargs):
+def convnext_tiny_hnf(pretrained=False, **kwargs) -> ConvNeXt:
     # experimental tiny variant with norm before pooling in head (head norm first)
-    model_args = dict(
-        depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), head_norm_first=True, conv_mlp=True, **kwargs)
-    model = _create_convnext('convnext_tiny_hnf', pretrained=pretrained, **model_args)
+    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), head_norm_first=True, conv_mlp=True)
+    model = _create_convnext('convnext_tiny_hnf', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_tiny(pretrained=False, **kwargs):
-    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), **kwargs)
-    model = _create_convnext('convnext_tiny', pretrained=pretrained, **model_args)
+def convnext_tiny(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768))
+    model = _create_convnext('convnext_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_small(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)
-    model = _create_convnext('convnext_small', pretrained=pretrained, **model_args)
+def convnext_small(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768])
+    model = _create_convnext('convnext_small', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_base(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)
-    model = _create_convnext('convnext_base', pretrained=pretrained, **model_args)
+def convnext_base(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024])
+    model = _create_convnext('convnext_base', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_large(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)
-    model = _create_convnext('convnext_large', pretrained=pretrained, **model_args)
+def convnext_large(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536])
+    model = _create_convnext('convnext_large', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_xlarge(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)
-    model = _create_convnext('convnext_xlarge', pretrained=pretrained, **model_args)
+def convnext_large_mlp(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], head_hidden_size=1536)
+    model = _create_convnext('convnext_large_mlp', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnext_xxlarge(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 4, 30, 3], dims=[384, 768, 1536, 3072], **kwargs)
-    model = _create_convnext('convnext_xxlarge', pretrained=pretrained, **model_args)
+def convnext_xlarge(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048])
+    model = _create_convnext('convnext_xlarge', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_atto(pretrained=False, **kwargs):
+def convnext_xxlarge(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 4, 30, 3], dims=[384, 768, 1536, 3072], norm_eps=kwargs.pop('norm_eps', 1e-5))
+    model = _create_convnext('convnext_xxlarge', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def convnextv2_atto(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
     model_args = dict(
-        depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), use_grn=True, ls_init_value=None, conv_mlp=True, **kwargs)
-    model = _create_convnext('convnextv2_atto', pretrained=pretrained, **model_args)
+        depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), use_grn=True, ls_init_value=None, conv_mlp=True)
+    model = _create_convnext('convnextv2_atto', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_femto(pretrained=False, **kwargs):
+def convnextv2_femto(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant
     model_args = dict(
-        depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), use_grn=True, ls_init_value=None, conv_mlp=True, **kwargs)
-    model = _create_convnext('convnextv2_femto', pretrained=pretrained, **model_args)
+        depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), use_grn=True, ls_init_value=None, conv_mlp=True)
+    model = _create_convnext('convnextv2_femto', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_pico(pretrained=False, **kwargs):
+def convnextv2_pico(pretrained=False, **kwargs) -> ConvNeXt:
     # timm pico variant
     model_args = dict(
-        depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), use_grn=True, ls_init_value=None, conv_mlp=True, **kwargs)
-    model = _create_convnext('convnextv2_pico', pretrained=pretrained, **model_args)
+        depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), use_grn=True, ls_init_value=None, conv_mlp=True)
+    model = _create_convnext('convnextv2_pico', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_nano(pretrained=False, **kwargs):
+def convnextv2_nano(pretrained=False, **kwargs) -> ConvNeXt:
     # timm nano variant with standard stem and head
     model_args = dict(
-        depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), use_grn=True, ls_init_value=None, conv_mlp=True, **kwargs)
-    model = _create_convnext('convnextv2_nano', pretrained=pretrained, **model_args)
+        depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), use_grn=True, ls_init_value=None, conv_mlp=True)
+    model = _create_convnext('convnextv2_nano', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_tiny(pretrained=False, **kwargs):
-    model_args = dict(
-        depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), use_grn=True, ls_init_value=None, **kwargs)
-    model = _create_convnext('convnextv2_tiny', pretrained=pretrained, **model_args)
+def convnextv2_tiny(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), use_grn=True, ls_init_value=None)
+    model = _create_convnext('convnextv2_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_small(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], use_grn=True, ls_init_value=None, **kwargs)
-    model = _create_convnext('convnextv2_small', pretrained=pretrained, **model_args)
+def convnextv2_small(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], use_grn=True, ls_init_value=None)
+    model = _create_convnext('convnextv2_small', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_base(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], use_grn=True, ls_init_value=None, **kwargs)
-    model = _create_convnext('convnextv2_base', pretrained=pretrained, **model_args)
+def convnextv2_base(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], use_grn=True, ls_init_value=None)
+    model = _create_convnext('convnextv2_base', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_large(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], use_grn=True, ls_init_value=None, **kwargs)
-    model = _create_convnext('convnextv2_large', pretrained=pretrained, **model_args)
+def convnextv2_large(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], use_grn=True, ls_init_value=None)
+    model = _create_convnext('convnextv2_large', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def convnextv2_huge(pretrained=False, **kwargs):
-    model_args = dict(depths=[3, 3, 27, 3], dims=[352, 704, 1408, 2816], use_grn=True, ls_init_value=None, **kwargs)
-    model = _create_convnext('convnextv2_huge', pretrained=pretrained, **model_args)
-    return model
+def convnextv2_huge(pretrained=False, **kwargs) -> ConvNeXt:
+    model_args = dict(depths=[3, 3, 27, 3], dims=[352, 704, 1408, 2816], use_grn=True, ls_init_value=None)
+    model = _create_convnext('convnextv2_huge', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+register_model_deprecations(__name__, {
+    'convnext_tiny_in22ft1k': 'convnext_tiny.fb_in22k_ft_in1k',
+    'convnext_small_in22ft1k': 'convnext_small.fb_in22k_ft_in1k',
+    'convnext_base_in22ft1k': 'convnext_base.fb_in22k_ft_in1k',
+    'convnext_large_in22ft1k': 'convnext_large.fb_in22k_ft_in1k',
+    'convnext_xlarge_in22ft1k': 'convnext_xlarge.fb_in22k_ft_in1k',
+    'convnext_tiny_384_in22ft1k': 'convnext_tiny.fb_in22k_ft_in1k_384',
+    'convnext_small_384_in22ft1k': 'convnext_small.fb_in22k_ft_in1k_384',
+    'convnext_base_384_in22ft1k': 'convnext_base.fb_in22k_ft_in1k_384',
+    'convnext_large_384_in22ft1k': 'convnext_large.fb_in22k_ft_in1k_384',
+    'convnext_xlarge_384_in22ft1k': 'convnext_xlarge.fb_in22k_ft_in1k_384',
+    'convnext_tiny_in22k': 'convnext_tiny.fb_in22k',
+    'convnext_small_in22k': 'convnext_small.fb_in22k',
+    'convnext_base_in22k': 'convnext_base.fb_in22k',
+    'convnext_large_in22k': 'convnext_large.fb_in22k',
+    'convnext_xlarge_in22k': 'convnext_xlarge.fb_in22k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/crossvit.py` & `timm-0.9.0/timm/models/crossvit.py`

 * *Files 22% similar despite different names*

```diff
@@ -32,62 +32,18 @@
 import torch.hub
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, to_2tuple, trunc_normal_, _assert
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .vision_transformer import Block
 
-__all__ = ['CrossViT']  # model_registry will add each entrypoint fn to this
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 240, 240), 'pool_size': None, 'crop_pct': 0.875,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,
-        'first_conv': ('patch_embed.0.proj', 'patch_embed.1.proj'),
-        'classifier': ('head.0', 'head.1'),
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'crossvit_15_240': _cfg(url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_15_224.pth'),
-    'crossvit_15_dagger_240': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_15_dagger_224.pth',
-        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),
-    ),
-    'crossvit_15_dagger_408': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_15_dagger_384.pth',
-        input_size=(3, 408, 408), first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'), crop_pct=1.0,
-    ),
-    'crossvit_18_240': _cfg(url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_18_224.pth'),
-    'crossvit_18_dagger_240': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_18_dagger_224.pth',
-        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),
-    ),
-    'crossvit_18_dagger_408': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_18_dagger_384.pth',
-        input_size=(3, 408, 408), first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'), crop_pct=1.0,
-    ),
-    'crossvit_9_240': _cfg(url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_9_224.pth'),
-    'crossvit_9_dagger_240': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_9_dagger_224.pth',
-        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),
-    ),
-    'crossvit_base_240': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_base_224.pth'),
-    'crossvit_small_240': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_small_224.pth'),
-    'crossvit_tiny_240': _cfg(
-        url='https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_tiny_224.pth'),
-}
+__all__ = ['CrossVit']  # model_registry will add each entrypoint fn to this
 
 
 class PatchEmbed(nn.Module):
     """ Image to Patch Embedding
     """
 
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=False):
@@ -126,20 +82,27 @@
         _assert(W == self.img_size[1],
                 f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).")
         x = self.proj(x).flatten(2).transpose(1, 2)
         return x
 
 
 class CrossAttention(nn.Module):
-    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
+    def __init__(
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+    ):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
-        self.scale = qk_scale or head_dim ** -0.5
+        self.scale = head_dim ** -0.5
 
         self.wq = nn.Linear(dim, dim, bias=qkv_bias)
         self.wk = nn.Linear(dim, dim, bias=qkv_bias)
         self.wv = nn.Linear(dim, dim, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
@@ -162,44 +125,77 @@
         x = self.proj_drop(x)
         return x
 
 
 class CrossAttentionBlock(nn.Module):
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
-            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = CrossAttention(
-            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
     def forward(self, x):
         x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))
         return x
 
 
 class MultiScaleBlock(nn.Module):
 
-    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, qkv_bias=False, drop=0., attn_drop=0.,
-                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+    def __init__(
+            self,
+            dim,
+            patches,
+            depth,
+            num_heads,
+            mlp_ratio,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
 
         num_branches = len(dim)
         self.num_branches = num_branches
         # different branch could have different embedding size, the first one is the base
         self.blocks = nn.ModuleList()
         for d in range(num_branches):
             tmp = []
             for i in range(depth[d]):
                 tmp.append(Block(
-                    dim=dim[d], num_heads=num_heads[d], mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias,
-                    drop=drop, attn_drop=attn_drop, drop_path=drop_path[i], norm_layer=norm_layer))
+                    dim=dim[d],
+                    num_heads=num_heads[d],
+                    mlp_ratio=mlp_ratio[d],
+                    qkv_bias=qkv_bias,
+                    proj_drop=proj_drop,
+                    attn_drop=attn_drop,
+                    drop_path=drop_path[i],
+                    norm_layer=norm_layer,
+                ))
             if len(tmp) != 0:
                 self.blocks.append(nn.Sequential(*tmp))
 
         if len(self.blocks) == 0:
             self.blocks = None
 
         self.projs = nn.ModuleList()
@@ -213,22 +209,36 @@
         self.fusion = nn.ModuleList()
         for d in range(num_branches):
             d_ = (d + 1) % num_branches
             nh = num_heads[d_]
             if depth[-1] == 0:  # backward capability:
                 self.fusion.append(
                     CrossAttentionBlock(
-                        dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias,
-                        drop=drop, attn_drop=attn_drop, drop_path=drop_path[-1], norm_layer=norm_layer))
+                        dim=dim[d_],
+                        num_heads=nh,
+                        mlp_ratio=mlp_ratio[d],
+                        qkv_bias=qkv_bias,
+                        proj_drop=proj_drop,
+                        attn_drop=attn_drop,
+                        drop_path=drop_path[-1],
+                        norm_layer=norm_layer,
+                    ))
             else:
                 tmp = []
                 for _ in range(depth[-1]):
                     tmp.append(CrossAttentionBlock(
-                        dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias,
-                        drop=drop, attn_drop=attn_drop, drop_path=drop_path[-1], norm_layer=norm_layer))
+                        dim=dim[d_],
+                        num_heads=nh,
+                        mlp_ratio=mlp_ratio[d],
+                        qkv_bias=qkv_bias,
+                        proj_drop=proj_drop,
+                        attn_drop=attn_drop,
+                        drop_path=drop_path[-1],
+                        norm_layer=norm_layer,
+                    ))
                 self.fusion.append(nn.Sequential(*tmp))
 
         self.revert_projs = nn.ModuleList()
         for d in range(num_branches):
             if dim[(d + 1) % num_branches] == dim[d] and False:
                 tmp = [nn.Identity()]
             else:
@@ -279,23 +289,39 @@
             cu, cl = int(round((H - ss[0]) / 2.)), int(round((W - ss[1]) / 2.))
             x = x[:, :, cu:cu + ss[0], cl:cl + ss[1]]
         else:
             x = torch.nn.functional.interpolate(x, size=ss, mode='bicubic', align_corners=False)
     return x
 
 
-class CrossViT(nn.Module):
+class CrossVit(nn.Module):
     """ Vision Transformer with support for patch or hybrid CNN input stage
     """
 
     def __init__(
-            self, img_size=224, img_scale=(1.0, 1.0), patch_size=(8, 16), in_chans=3, num_classes=1000,
-            embed_dim=(192, 384), depth=((1, 3, 1), (1, 3, 1), (1, 3, 1)), num_heads=(6, 12), mlp_ratio=(2., 2., 4.),
-            multi_conv=False, crop_scale=False, qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,
-            norm_layer=partial(nn.LayerNorm, eps=1e-6), global_pool='token',
+            self,
+            img_size=224,
+            img_scale=(1.0, 1.0),
+            patch_size=(8, 16),
+            in_chans=3,
+            num_classes=1000,
+            embed_dim=(192, 384),
+            depth=((1, 3, 1), (1, 3, 1), (1, 3, 1)),
+            num_heads=(6, 12),
+            mlp_ratio=(2., 2., 4.),
+            multi_conv=False,
+            crop_scale=False,
+            qkv_bias=True,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            norm_layer=partial(nn.LayerNorm, eps=1e-6),
+            global_pool='token',
     ):
         super().__init__()
         assert global_pool in ('token', 'avg')
 
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.img_size = to_2tuple(img_size)
@@ -311,32 +337,48 @@
         # hard-coded for torch jit script
         for i in range(self.num_branches):
             setattr(self, f'pos_embed_{i}', nn.Parameter(torch.zeros(1, 1 + num_patches[i], embed_dim[i])))
             setattr(self, f'cls_token_{i}', nn.Parameter(torch.zeros(1, 1, embed_dim[i])))
 
         for im_s, p, d in zip(self.img_size_scaled, patch_size, embed_dim):
             self.patch_embed.append(
-                PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_chans, embed_dim=d, multi_conv=multi_conv))
+                PatchEmbed(
+                    img_size=im_s,
+                    patch_size=p,
+                    in_chans=in_chans,
+                    embed_dim=d,
+                    multi_conv=multi_conv,
+                ))
 
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         total_depth = sum([sum(x[-2:]) for x in depth])
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule
         dpr_ptr = 0
         self.blocks = nn.ModuleList()
         for idx, block_cfg in enumerate(depth):
             curr_depth = max(block_cfg[:-1]) + block_cfg[-1]
             dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]
             blk = MultiScaleBlock(
-                embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,
-                qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_, norm_layer=norm_layer)
+                embed_dim,
+                num_patches,
+                block_cfg,
+                num_heads=num_heads,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr_,
+                norm_layer=norm_layer,
+            )
             dpr_ptr += curr_depth
             self.blocks.append(blk)
 
         self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.ModuleList([
             nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity()
             for i in range(self.num_branches)])
 
         for i in range(self.num_branches):
             trunc_normal_(getattr(self, f'pos_embed_{i}'), std=.02)
             trunc_normal_(getattr(self, f'cls_token_{i}'), std=.02)
@@ -407,14 +449,15 @@
 
         # NOTE: was before branch token section, move to here to assure all branch token are before layer norm
         xs = [norm(xs[i]) for i, norm in enumerate(self.norm)]
         return xs
 
     def forward_head(self, xs: List[torch.Tensor], pre_logits: bool = False) -> torch.Tensor:
         xs = [x[:, 1:].mean(dim=1) for x in xs] if self.global_pool == 'avg' else [x[:, 0] for x in xs]
+        xs = [self.head_drop(x) for x in xs]
         if pre_logits or isinstance(self.head[0], nn.Identity):
             return torch.cat([x for x in xs], dim=1)
         return torch.mean(torch.stack([head(xs[i]) for i, head in enumerate(self.head)], dim=0), dim=0)
 
     def forward(self, x):
         xs = self.forward_features(x)
         x = self.forward_head(xs)
@@ -432,109 +475,153 @@
                 new_key = key.replace(".", "_")
             else:
                 new_key = key
             new_state_dict[new_key] = state_dict[key]
         return new_state_dict
 
     return build_model_with_cfg(
-        CrossViT, variant, pretrained,
+        CrossVit,
+        variant,
+        pretrained,
         pretrained_filter_fn=pretrained_filter_fn,
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 240, 240), 'pool_size': None, 'crop_pct': 0.875,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,
+        'first_conv': ('patch_embed.0.proj', 'patch_embed.1.proj'),
+        'classifier': ('head.0', 'head.1'),
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'crossvit_15_240.in1k': _cfg(hf_hub_id='timm/'),
+    'crossvit_15_dagger_240.in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),
+    ),
+    'crossvit_15_dagger_408.in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 408, 408), first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'), crop_pct=1.0,
+    ),
+    'crossvit_18_240.in1k': _cfg(hf_hub_id='timm/'),
+    'crossvit_18_dagger_240.in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),
+    ),
+    'crossvit_18_dagger_408.in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 408, 408), first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'), crop_pct=1.0,
+    ),
+    'crossvit_9_240.in1k': _cfg(hf_hub_id='timm/'),
+    'crossvit_9_dagger_240.in1k': _cfg(
+        hf_hub_id='timm/',
+        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),
+    ),
+    'crossvit_base_240.in1k': _cfg(hf_hub_id='timm/'),
+    'crossvit_small_240.in1k': _cfg(hf_hub_id='timm/'),
+    'crossvit_tiny_240.in1k': _cfg(hf_hub_id='timm/'),
+})
 
 
 @register_model
-def crossvit_tiny_240(pretrained=False, **kwargs):
+def crossvit_tiny_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[96, 192], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],
-        num_heads=[3, 3], mlp_ratio=[4, 4, 1], **kwargs)
-    model = _create_crossvit(variant='crossvit_tiny_240', pretrained=pretrained, **model_args)
+        num_heads=[3, 3], mlp_ratio=[4, 4, 1])
+    model = _create_crossvit(variant='crossvit_tiny_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_small_240(pretrained=False, **kwargs):
+def crossvit_small_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],
-        num_heads=[6, 6], mlp_ratio=[4, 4, 1], **kwargs)
-    model = _create_crossvit(variant='crossvit_small_240', pretrained=pretrained, **model_args)
+        num_heads=[6, 6], mlp_ratio=[4, 4, 1])
+    model = _create_crossvit(variant='crossvit_small_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_base_240(pretrained=False, **kwargs):
+def crossvit_base_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[384, 768], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],
-        num_heads=[12, 12], mlp_ratio=[4, 4, 1], **kwargs)
-    model = _create_crossvit(variant='crossvit_base_240', pretrained=pretrained, **model_args)
+        num_heads=[12, 12], mlp_ratio=[4, 4, 1])
+    model = _create_crossvit(variant='crossvit_base_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_9_240(pretrained=False, **kwargs):
+def crossvit_9_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[128, 256], depth=[[1, 3, 0], [1, 3, 0], [1, 3, 0]],
-        num_heads=[4, 4], mlp_ratio=[3, 3, 1], **kwargs)
-    model = _create_crossvit(variant='crossvit_9_240', pretrained=pretrained, **model_args)
+        num_heads=[4, 4], mlp_ratio=[3, 3, 1])
+    model = _create_crossvit(variant='crossvit_9_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_15_240(pretrained=False, **kwargs):
+def crossvit_15_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 5, 0], [1, 5, 0], [1, 5, 0]],
-        num_heads=[6, 6], mlp_ratio=[3, 3, 1], **kwargs)
-    model = _create_crossvit(variant='crossvit_15_240', pretrained=pretrained, **model_args)
+        num_heads=[6, 6], mlp_ratio=[3, 3, 1])
+    model = _create_crossvit(variant='crossvit_15_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_18_240(pretrained=False, **kwargs):
+def crossvit_18_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224 / 240), patch_size=[12, 16], embed_dim=[224, 448], depth=[[1, 6, 0], [1, 6, 0], [1, 6, 0]],
         num_heads=[7, 7], mlp_ratio=[3, 3, 1], **kwargs)
-    model = _create_crossvit(variant='crossvit_18_240', pretrained=pretrained, **model_args)
+    model = _create_crossvit(variant='crossvit_18_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_9_dagger_240(pretrained=False, **kwargs):
+def crossvit_9_dagger_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224 / 240), patch_size=[12, 16], embed_dim=[128, 256], depth=[[1, 3, 0], [1, 3, 0], [1, 3, 0]],
-        num_heads=[4, 4], mlp_ratio=[3, 3, 1], multi_conv=True, **kwargs)
-    model = _create_crossvit(variant='crossvit_9_dagger_240', pretrained=pretrained, **model_args)
+        num_heads=[4, 4], mlp_ratio=[3, 3, 1], multi_conv=True)
+    model = _create_crossvit(variant='crossvit_9_dagger_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_15_dagger_240(pretrained=False, **kwargs):
+def crossvit_15_dagger_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 5, 0], [1, 5, 0], [1, 5, 0]],
-        num_heads=[6, 6], mlp_ratio=[3, 3, 1], multi_conv=True, **kwargs)
-    model = _create_crossvit(variant='crossvit_15_dagger_240', pretrained=pretrained, **model_args)
+        num_heads=[6, 6], mlp_ratio=[3, 3, 1], multi_conv=True)
+    model = _create_crossvit(variant='crossvit_15_dagger_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_15_dagger_408(pretrained=False, **kwargs):
+def crossvit_15_dagger_408(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 384/408), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 5, 0], [1, 5, 0], [1, 5, 0]],
-        num_heads=[6, 6], mlp_ratio=[3, 3, 1], multi_conv=True, **kwargs)
-    model = _create_crossvit(variant='crossvit_15_dagger_408', pretrained=pretrained, **model_args)
+        num_heads=[6, 6], mlp_ratio=[3, 3, 1], multi_conv=True)
+    model = _create_crossvit(variant='crossvit_15_dagger_408', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_18_dagger_240(pretrained=False, **kwargs):
+def crossvit_18_dagger_240(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[224, 448], depth=[[1, 6, 0], [1, 6, 0], [1, 6, 0]],
-        num_heads=[7, 7], mlp_ratio=[3, 3, 1], multi_conv=True, **kwargs)
-    model = _create_crossvit(variant='crossvit_18_dagger_240', pretrained=pretrained, **model_args)
+        num_heads=[7, 7], mlp_ratio=[3, 3, 1], multi_conv=True)
+    model = _create_crossvit(variant='crossvit_18_dagger_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def crossvit_18_dagger_408(pretrained=False, **kwargs):
+def crossvit_18_dagger_408(pretrained=False, **kwargs) -> CrossVit:
     model_args = dict(
         img_scale=(1.0, 384/408), patch_size=[12, 16], embed_dim=[224, 448], depth=[[1, 6, 0], [1, 6, 0], [1, 6, 0]],
-        num_heads=[7, 7], mlp_ratio=[3, 3, 1], multi_conv=True, **kwargs)
-    model = _create_crossvit(variant='crossvit_18_dagger_408', pretrained=pretrained, **model_args)
+        num_heads=[7, 7], mlp_ratio=[3, 3, 1], multi_conv=True)
+    model = _create_crossvit(variant='crossvit_18_dagger_408', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/cspnet.py` & `timm-0.9.0/timm/models/cspnet.py`

 * *Files 6% similar despite different names*

```diff
@@ -19,94 +19,19 @@
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, ConvNormAct, ConvNormActAa, DropPath, get_attn, create_act_layer, make_divisible
 from ._builder import build_model_with_cfg
 from ._manipulate import named_apply, MATCH_PREV_GROUP
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['CspNet']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
-        'crop_pct': 0.887, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'cspresnet50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnet50_ra-d3e8d487.pth'),
-    'cspresnet50d': _cfg(url=''),
-    'cspresnet50w': _cfg(url=''),
-    'cspresnext50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnext50_ra_224-648b4713.pth',
-    ),
-    'cspdarknet53': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspdarknet53_ra_256-d05c7c21.pth'),
-
-    'darknet17': _cfg(url=''),
-    'darknet21': _cfg(url=''),
-    'sedarknet21': _cfg(url=''),
-    'darknet53': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/darknet53_256_c2ns-3aeff817.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'darknetaa53': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/darknetaa53_c2ns-5c28ec8a.pth',
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-
-    'cs3darknet_s': _cfg(
-        url='', interpolation='bicubic'),
-    'cs3darknet_m': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_m_c2ns-43f06604.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95,
-    ),
-    'cs3darknet_l': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_l_c2ns-16220c5d.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
-    'cs3darknet_x': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_x_c2ns-4e4490aa.pth',
-        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
-
-    'cs3darknet_focus_s': _cfg(
-        url='', interpolation='bicubic'),
-    'cs3darknet_focus_m': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_focus_m_c2ns-e23bed41.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
-    'cs3darknet_focus_l': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_focus_l_c2ns-65ef8888.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
-    'cs3darknet_focus_x': _cfg(
-        url='', interpolation='bicubic'),
-
-    'cs3sedarknet_l': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3sedarknet_l_c2ns-e8d1dc13.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
-    'cs3sedarknet_x': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3sedarknet_x_c2ns-b4d0abc0.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),
-
-    'cs3sedarknet_xdw': _cfg(
-        url='', interpolation='bicubic'),
-
-    'cs3edgenet_x': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3edgenet_x_c2-2e1610a9.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    'cs3se_edgenet_x': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3se_edgenet_x_c2ns-76f8e3ac.pth',
-        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0),
-}
-
-
 @dataclass
 class CspStemCfg:
     out_chs: Union[int, Tuple[int, ...]] = 32
     stride: Union[int, Tuple[int, ...]] = 2
     kernel_size: int = 3
     padding: Union[int, str] = ''
     pool: Optional[str] = ''
@@ -203,175 +128,14 @@
             stage_type='cs3',
             block_type=block_type,
         ),
         act_layer=act_layer,
     )
 
 
-model_cfgs = dict(
-    cspresnet50=CspModelCfg(
-        stem=CspStemCfg(out_chs=64, kernel_size=7, stride=4, pool='max'),
-        stages=CspStagesCfg(
-            depth=(3, 3, 5, 2),
-            out_chs=(128, 256, 512, 1024),
-            stride=(1, 2),
-            expand_ratio=2.,
-            bottle_ratio=0.5,
-            cross_linear=True,
-        ),
-    ),
-    cspresnet50d=CspModelCfg(
-        stem=CspStemCfg(out_chs=(32, 32, 64), kernel_size=3, stride=4, pool='max'),
-        stages=CspStagesCfg(
-            depth=(3, 3, 5, 2),
-            out_chs=(128, 256, 512, 1024),
-            stride=(1,) + (2,),
-            expand_ratio=2.,
-            bottle_ratio=0.5,
-            block_ratio=1.,
-            cross_linear=True,
-        ),
-    ),
-    cspresnet50w=CspModelCfg(
-        stem=CspStemCfg(out_chs=(32, 32, 64), kernel_size=3, stride=4, pool='max'),
-        stages=CspStagesCfg(
-            depth=(3, 3, 5, 2),
-            out_chs=(256, 512, 1024, 2048),
-            stride=(1,) + (2,),
-            expand_ratio=1.,
-            bottle_ratio=0.25,
-            block_ratio=0.5,
-            cross_linear=True,
-        ),
-    ),
-    cspresnext50=CspModelCfg(
-        stem=CspStemCfg(out_chs=64, kernel_size=7, stride=4, pool='max'),
-        stages=CspStagesCfg(
-            depth=(3, 3, 5, 2),
-            out_chs=(256, 512, 1024, 2048),
-            stride=(1,) + (2,),
-            groups=32,
-            expand_ratio=1.,
-            bottle_ratio=1.,
-            block_ratio=0.5,
-            cross_linear=True,
-        ),
-    ),
-    cspdarknet53=CspModelCfg(
-        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
-        stages=CspStagesCfg(
-            depth=(1, 2, 8, 8, 4),
-            out_chs=(64, 128, 256, 512, 1024),
-            stride=2,
-            expand_ratio=(2.,) + (1.,),
-            bottle_ratio=(0.5,) + (1.,),
-            block_ratio=(1.,) + (0.5,),
-            down_growth=True,
-            block_type='dark',
-        ),
-    ),
-    darknet17=CspModelCfg(
-        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
-        stages=CspStagesCfg(
-            depth=(1,) * 5,
-            out_chs=(64, 128, 256, 512, 1024),
-            stride=(2,),
-            bottle_ratio=(0.5,),
-            block_ratio=(1.,),
-            stage_type='dark',
-            block_type='dark',
-        ),
-    ),
-    darknet21=CspModelCfg(
-        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
-        stages=CspStagesCfg(
-            depth=(1, 1, 1, 2, 2),
-            out_chs=(64, 128, 256, 512, 1024),
-            stride=(2,),
-            bottle_ratio=(0.5,),
-            block_ratio=(1.,),
-            stage_type='dark',
-            block_type='dark',
-
-        ),
-    ),
-    sedarknet21=CspModelCfg(
-        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
-        stages=CspStagesCfg(
-            depth=(1, 1, 1, 2, 2),
-            out_chs=(64, 128, 256, 512, 1024),
-            stride=2,
-            bottle_ratio=0.5,
-            block_ratio=1.,
-            attn_layer='se',
-            stage_type='dark',
-            block_type='dark',
-
-        ),
-    ),
-    darknet53=CspModelCfg(
-        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
-        stages=CspStagesCfg(
-            depth=(1, 2, 8, 8, 4),
-            out_chs=(64, 128, 256, 512, 1024),
-            stride=2,
-            bottle_ratio=0.5,
-            block_ratio=1.,
-            stage_type='dark',
-            block_type='dark',
-        ),
-    ),
-    darknetaa53=CspModelCfg(
-        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
-        stages=CspStagesCfg(
-            depth=(1, 2, 8, 8, 4),
-            out_chs=(64, 128, 256, 512, 1024),
-            stride=2,
-            bottle_ratio=0.5,
-            block_ratio=1.,
-            avg_down=True,
-            stage_type='dark',
-            block_type='dark',
-        ),
-    ),
-
-    cs3darknet_s=_cs3_cfg(width_multiplier=0.5, depth_multiplier=0.5),
-    cs3darknet_m=_cs3_cfg(width_multiplier=0.75, depth_multiplier=0.67),
-    cs3darknet_l=_cs3_cfg(),
-    cs3darknet_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33),
-
-    cs3darknet_focus_s=_cs3_cfg(width_multiplier=0.5, depth_multiplier=0.5, focus=True),
-    cs3darknet_focus_m=_cs3_cfg(width_multiplier=0.75, depth_multiplier=0.67, focus=True),
-    cs3darknet_focus_l=_cs3_cfg(focus=True),
-    cs3darknet_focus_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33, focus=True),
-
-    cs3sedarknet_l=_cs3_cfg(attn_layer='se', attn_kwargs=dict(rd_ratio=.25)),
-    cs3sedarknet_x=_cs3_cfg(attn_layer='se', width_multiplier=1.25, depth_multiplier=1.33),
-
-    cs3sedarknet_xdw=CspModelCfg(
-        stem=CspStemCfg(out_chs=(32, 64), kernel_size=3, stride=2, pool=''),
-        stages=CspStagesCfg(
-            depth=(3, 6, 12, 4),
-            out_chs=(256, 512, 1024, 2048),
-            stride=2,
-            groups=(1, 1, 256, 512),
-            bottle_ratio=0.5,
-            block_ratio=0.5,
-            attn_layer='se',
-        ),
-        act_layer='silu',
-    ),
-
-    cs3edgenet_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33, bottle_ratio=1.5, block_type='edge'),
-    cs3se_edgenet_x=_cs3_cfg(
-        width_multiplier=1.25, depth_multiplier=1.33, bottle_ratio=1.5, block_type='edge',
-        attn_layer='se', attn_kwargs=dict(rd_ratio=.25)),
-)
-
-
 class BottleneckBlock(nn.Module):
     """ ResNe(X)t Bottleneck Block
     """
 
     def __init__(
             self,
             in_chs,
@@ -909,15 +673,15 @@
         )
         prev_chs = stage_feat_info[-1]['num_chs']
         self.feature_info.extend(stage_feat_info)
 
         # Construct the head
         self.num_features = prev_chs
         self.head = ClassifierHead(
-            in_chs=prev_chs, num_classes=num_classes, pool_type=global_pool, drop_rate=drop_rate)
+            in_features=prev_chs, num_classes=num_classes, pool_type=global_pool, drop_rate=drop_rate)
 
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^stem',
@@ -963,134 +727,380 @@
         nn.init.normal_(module.weight, mean=0.0, std=0.01)
         if module.bias is not None:
             nn.init.zeros_(module.bias)
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
 
+model_cfgs = dict(
+    cspresnet50=CspModelCfg(
+        stem=CspStemCfg(out_chs=64, kernel_size=7, stride=4, pool='max'),
+        stages=CspStagesCfg(
+            depth=(3, 3, 5, 2),
+            out_chs=(128, 256, 512, 1024),
+            stride=(1, 2),
+            expand_ratio=2.,
+            bottle_ratio=0.5,
+            cross_linear=True,
+        ),
+    ),
+    cspresnet50d=CspModelCfg(
+        stem=CspStemCfg(out_chs=(32, 32, 64), kernel_size=3, stride=4, pool='max'),
+        stages=CspStagesCfg(
+            depth=(3, 3, 5, 2),
+            out_chs=(128, 256, 512, 1024),
+            stride=(1,) + (2,),
+            expand_ratio=2.,
+            bottle_ratio=0.5,
+            block_ratio=1.,
+            cross_linear=True,
+        ),
+    ),
+    cspresnet50w=CspModelCfg(
+        stem=CspStemCfg(out_chs=(32, 32, 64), kernel_size=3, stride=4, pool='max'),
+        stages=CspStagesCfg(
+            depth=(3, 3, 5, 2),
+            out_chs=(256, 512, 1024, 2048),
+            stride=(1,) + (2,),
+            expand_ratio=1.,
+            bottle_ratio=0.25,
+            block_ratio=0.5,
+            cross_linear=True,
+        ),
+    ),
+    cspresnext50=CspModelCfg(
+        stem=CspStemCfg(out_chs=64, kernel_size=7, stride=4, pool='max'),
+        stages=CspStagesCfg(
+            depth=(3, 3, 5, 2),
+            out_chs=(256, 512, 1024, 2048),
+            stride=(1,) + (2,),
+            groups=32,
+            expand_ratio=1.,
+            bottle_ratio=1.,
+            block_ratio=0.5,
+            cross_linear=True,
+        ),
+    ),
+    cspdarknet53=CspModelCfg(
+        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
+        stages=CspStagesCfg(
+            depth=(1, 2, 8, 8, 4),
+            out_chs=(64, 128, 256, 512, 1024),
+            stride=2,
+            expand_ratio=(2.,) + (1.,),
+            bottle_ratio=(0.5,) + (1.,),
+            block_ratio=(1.,) + (0.5,),
+            down_growth=True,
+            block_type='dark',
+        ),
+    ),
+    darknet17=CspModelCfg(
+        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
+        stages=CspStagesCfg(
+            depth=(1,) * 5,
+            out_chs=(64, 128, 256, 512, 1024),
+            stride=(2,),
+            bottle_ratio=(0.5,),
+            block_ratio=(1.,),
+            stage_type='dark',
+            block_type='dark',
+        ),
+    ),
+    darknet21=CspModelCfg(
+        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
+        stages=CspStagesCfg(
+            depth=(1, 1, 1, 2, 2),
+            out_chs=(64, 128, 256, 512, 1024),
+            stride=(2,),
+            bottle_ratio=(0.5,),
+            block_ratio=(1.,),
+            stage_type='dark',
+            block_type='dark',
+
+        ),
+    ),
+    sedarknet21=CspModelCfg(
+        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
+        stages=CspStagesCfg(
+            depth=(1, 1, 1, 2, 2),
+            out_chs=(64, 128, 256, 512, 1024),
+            stride=2,
+            bottle_ratio=0.5,
+            block_ratio=1.,
+            attn_layer='se',
+            stage_type='dark',
+            block_type='dark',
+
+        ),
+    ),
+    darknet53=CspModelCfg(
+        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
+        stages=CspStagesCfg(
+            depth=(1, 2, 8, 8, 4),
+            out_chs=(64, 128, 256, 512, 1024),
+            stride=2,
+            bottle_ratio=0.5,
+            block_ratio=1.,
+            stage_type='dark',
+            block_type='dark',
+        ),
+    ),
+    darknetaa53=CspModelCfg(
+        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),
+        stages=CspStagesCfg(
+            depth=(1, 2, 8, 8, 4),
+            out_chs=(64, 128, 256, 512, 1024),
+            stride=2,
+            bottle_ratio=0.5,
+            block_ratio=1.,
+            avg_down=True,
+            stage_type='dark',
+            block_type='dark',
+        ),
+    ),
+
+    cs3darknet_s=_cs3_cfg(width_multiplier=0.5, depth_multiplier=0.5),
+    cs3darknet_m=_cs3_cfg(width_multiplier=0.75, depth_multiplier=0.67),
+    cs3darknet_l=_cs3_cfg(),
+    cs3darknet_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33),
+
+    cs3darknet_focus_s=_cs3_cfg(width_multiplier=0.5, depth_multiplier=0.5, focus=True),
+    cs3darknet_focus_m=_cs3_cfg(width_multiplier=0.75, depth_multiplier=0.67, focus=True),
+    cs3darknet_focus_l=_cs3_cfg(focus=True),
+    cs3darknet_focus_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33, focus=True),
+
+    cs3sedarknet_l=_cs3_cfg(attn_layer='se', attn_kwargs=dict(rd_ratio=.25)),
+    cs3sedarknet_x=_cs3_cfg(attn_layer='se', width_multiplier=1.25, depth_multiplier=1.33),
+
+    cs3sedarknet_xdw=CspModelCfg(
+        stem=CspStemCfg(out_chs=(32, 64), kernel_size=3, stride=2, pool=''),
+        stages=CspStagesCfg(
+            depth=(3, 6, 12, 4),
+            out_chs=(256, 512, 1024, 2048),
+            stride=2,
+            groups=(1, 1, 256, 512),
+            bottle_ratio=0.5,
+            block_ratio=0.5,
+            attn_layer='se',
+        ),
+        act_layer='silu',
+    ),
+
+    cs3edgenet_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33, bottle_ratio=1.5, block_type='edge'),
+    cs3se_edgenet_x=_cs3_cfg(
+        width_multiplier=1.25, depth_multiplier=1.33, bottle_ratio=1.5, block_type='edge',
+        attn_layer='se', attn_kwargs=dict(rd_ratio=.25)),
+)
+
+
 def _create_cspnet(variant, pretrained=False, **kwargs):
     if variant.startswith('darknet') or variant.startswith('cspdarknet'):
         # NOTE: DarkNet is one of few models with stride==1 features w/ 6 out_indices [0..5]
         default_out_indices = (0, 1, 2, 3, 4, 5)
     else:
         default_out_indices = (0, 1, 2, 3, 4)
     out_indices = kwargs.pop('out_indices', default_out_indices)
     return build_model_with_cfg(
         CspNet, variant, pretrained,
         model_cfg=model_cfgs[variant],
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
+        'crop_pct': 0.887, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'cspresnet50.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnet50_ra-d3e8d487.pth'),
+    'cspresnet50d.untrained': _cfg(),
+    'cspresnet50w.untrained': _cfg(),
+    'cspresnext50.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnext50_ra_224-648b4713.pth',
+    ),
+    'cspdarknet53.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspdarknet53_ra_256-d05c7c21.pth'),
+
+    'darknet17.untrained': _cfg(),
+    'darknet21.untrained': _cfg(),
+    'sedarknet21.untrained': _cfg(),
+    'darknet53.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/darknet53_256_c2ns-3aeff817.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'darknetaa53.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/darknetaa53_c2ns-5c28ec8a.pth',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    'cs3darknet_s.untrained': _cfg(interpolation='bicubic'),
+    'cs3darknet_m.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_m_c2ns-43f06604.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95,
+    ),
+    'cs3darknet_l.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_l_c2ns-16220c5d.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'cs3darknet_x.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_x_c2ns-4e4490aa.pth',
+        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    'cs3darknet_focus_s.untrained': _cfg(interpolation='bicubic'),
+    'cs3darknet_focus_m.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_focus_m_c2ns-e23bed41.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'cs3darknet_focus_l.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_focus_l_c2ns-65ef8888.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'cs3darknet_focus_x.untrained': _cfg(interpolation='bicubic'),
+
+    'cs3sedarknet_l.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3sedarknet_l_c2ns-e8d1dc13.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'cs3sedarknet_x.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3sedarknet_x_c2ns-b4d0abc0.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),
+
+    'cs3sedarknet_xdw.untrained': _cfg(interpolation='bicubic'),
+
+    'cs3edgenet_x.c2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3edgenet_x_c2-2e1610a9.pth',
+        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'cs3se_edgenet_x.c2ns_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3se_edgenet_x_c2ns-76f8e3ac.pth',
+        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0),
+})
+
+
 @register_model
-def cspresnet50(pretrained=False, **kwargs):
+def cspresnet50(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cspresnet50', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cspresnet50d(pretrained=False, **kwargs):
+def cspresnet50d(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cspresnet50d', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cspresnet50w(pretrained=False, **kwargs):
+def cspresnet50w(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cspresnet50w', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cspresnext50(pretrained=False, **kwargs):
+def cspresnext50(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cspresnext50', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cspdarknet53(pretrained=False, **kwargs):
+def cspdarknet53(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cspdarknet53', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def darknet17(pretrained=False, **kwargs):
+def darknet17(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('darknet17', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def darknet21(pretrained=False, **kwargs):
+def darknet21(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('darknet21', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def sedarknet21(pretrained=False, **kwargs):
+def sedarknet21(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('sedarknet21', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def darknet53(pretrained=False, **kwargs):
+def darknet53(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('darknet53', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def darknetaa53(pretrained=False, **kwargs):
+def darknetaa53(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('darknetaa53', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_s(pretrained=False, **kwargs):
+def cs3darknet_s(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_s', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_m(pretrained=False, **kwargs):
+def cs3darknet_m(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_m', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_l(pretrained=False, **kwargs):
+def cs3darknet_l(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_l', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_x(pretrained=False, **kwargs):
+def cs3darknet_x(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_x', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_focus_s(pretrained=False, **kwargs):
+def cs3darknet_focus_s(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_focus_s', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_focus_m(pretrained=False, **kwargs):
+def cs3darknet_focus_m(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_focus_m', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_focus_l(pretrained=False, **kwargs):
+def cs3darknet_focus_l(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_focus_l', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3darknet_focus_x(pretrained=False, **kwargs):
+def cs3darknet_focus_x(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3darknet_focus_x', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3sedarknet_l(pretrained=False, **kwargs):
+def cs3sedarknet_l(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3sedarknet_l', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3sedarknet_x(pretrained=False, **kwargs):
+def cs3sedarknet_x(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3sedarknet_x', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3sedarknet_xdw(pretrained=False, **kwargs):
+def cs3sedarknet_xdw(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3sedarknet_xdw', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3edgenet_x(pretrained=False, **kwargs):
+def cs3edgenet_x(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3edgenet_x', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def cs3se_edgenet_x(pretrained=False, **kwargs):
+def cs3se_edgenet_x(pretrained=False, **kwargs) -> CspNet:
     return _create_cspnet('cs3se_edgenet_x', pretrained=pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/deit.py` & `timm-0.9.0/timm/models/deit.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,111 +7,29 @@
 paper: `DeiT III: Revenge of the ViT` - https://arxiv.org/abs/2204.07118
 
 Modifications copyright 2021, Ross Wightman
 """
 # Copyright (c) 2015-present, Facebook, Inc.
 # All rights reserved.
 from functools import partial
+from typing import Sequence, Union
 
 import torch
 from torch import nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
+from timm.layers import resample_abs_pos_embed
 from timm.models.vision_transformer import VisionTransformer, trunc_normal_, checkpoint_filter_fn
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['VisionTransformerDistilled']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # deit models (FB weights)
-    'deit_tiny_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth'),
-    'deit_small_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth'),
-    'deit_base_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth'),
-    'deit_base_patch16_384': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-
-    'deit_tiny_distilled_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth',
-        classifier=('head', 'head_dist')),
-    'deit_small_distilled_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth',
-        classifier=('head', 'head_dist')),
-    'deit_base_distilled_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth',
-        classifier=('head', 'head_dist')),
-    'deit_base_distilled_patch16_384': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth',
-        input_size=(3, 384, 384), crop_pct=1.0,
-        classifier=('head', 'head_dist')),
-
-    'deit3_small_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_224_1k.pth'),
-    'deit3_small_patch16_384': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_384_1k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-    'deit3_medium_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_1k.pth'),
-    'deit3_base_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_224_1k.pth'),
-    'deit3_base_patch16_384': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_384_1k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-    'deit3_large_patch16_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_224_1k.pth'),
-    'deit3_large_patch16_384': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_384_1k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-    'deit3_huge_patch14_224': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_1k.pth'),
-
-    'deit3_small_patch16_224_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_224_21k.pth',
-        crop_pct=1.0),
-    'deit3_small_patch16_384_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_384_21k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-    'deit3_medium_patch16_224_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_21k.pth',
-        crop_pct=1.0),
-    'deit3_base_patch16_224_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_224_21k.pth',
-        crop_pct=1.0),
-    'deit3_base_patch16_384_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_384_21k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-    'deit3_large_patch16_224_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_224_21k.pth',
-        crop_pct=1.0),
-    'deit3_large_patch16_384_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_384_21k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-    'deit3_huge_patch14_224_in21ft1k': _cfg(
-        url='https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_21k_v1.pth',
-        crop_pct=1.0),
-}
-
-
 class VisionTransformerDistilled(VisionTransformer):
     """ Vision Transformer w/ Distillation Token and Head
 
     Distillation token & head support for `DeiT: Data-efficient Image Transformers`
         - https://arxiv.org/abs/2012.12877
     """
 
@@ -151,301 +69,357 @@
         self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
         self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()
 
     @torch.jit.ignore
     def set_distilled_training(self, enable=True):
         self.distilled_training = enable
 
+    def _intermediate_layers(
+            self,
+            x: torch.Tensor,
+            n: Union[int, Sequence] = 1,
+    ):
+        outputs, num_blocks = [], len(self.blocks)
+        take_indices = set(range(num_blocks - n, num_blocks) if isinstance(n, int) else n)
+
+        # forward pass
+        x = self.patch_embed(x)
+        x = torch.cat((
+            self.cls_token.expand(x.shape[0], -1, -1),
+            self.dist_token.expand(x.shape[0], -1, -1),
+            x),
+            dim=1)
+        x = self.pos_drop(x + self.pos_embed)
+        x = self.patch_drop(x)
+        x = self.norm_pre(x)
+        for i, blk in enumerate(self.blocks):
+            x = blk(x)
+            if i in take_indices:
+                outputs.append(x)
+
+        return outputs
+
     def forward_features(self, x) -> torch.Tensor:
         x = self.patch_embed(x)
         x = torch.cat((
             self.cls_token.expand(x.shape[0], -1, -1),
-            self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)
+            self.dist_token.expand(x.shape[0], -1, -1),
+            x),
+            dim=1)
         x = self.pos_drop(x + self.pos_embed)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
             x = self.blocks(x)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:
+        x, x_dist = x[:, 0], x[:, 1]
         if pre_logits:
-            return (x[:, 0] + x[:, 1]) / 2
-        x, x_dist = self.head(x[:, 0]), self.head_dist(x[:, 1])
+            return (x + x_dist) / 2
+        x = self.head(x)
+        x_dist = self.head_dist(x_dist)
         if self.distilled_training and self.training and not torch.jit.is_scripting():
             # only return separate classification predictions when training in distilled mode
             return x, x_dist
         else:
             # during standard train / finetune, inference average the classifier predictions
             return (x + x_dist) / 2
 
 
 def _create_deit(variant, pretrained=False, distilled=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
     model_cls = VisionTransformerDistilled if distilled else VisionTransformer
     model = build_model_with_cfg(
-        model_cls, variant, pretrained,
+        model_cls,
+        variant,
+        pretrained,
         pretrained_filter_fn=partial(checkpoint_filter_fn, adapt_layer_scale=True),
-        **kwargs)
+        **kwargs,
+    )
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    # deit models (FB weights)
+    'deit_tiny_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth'),
+    'deit_small_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth'),
+    'deit_base_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth'),
+    'deit_base_patch16_384.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+
+    'deit_tiny_distilled_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth',
+        classifier=('head', 'head_dist')),
+    'deit_small_distilled_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth',
+        classifier=('head', 'head_dist')),
+    'deit_base_distilled_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth',
+        classifier=('head', 'head_dist')),
+    'deit_base_distilled_patch16_384.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth',
+        input_size=(3, 384, 384), crop_pct=1.0,
+        classifier=('head', 'head_dist')),
+
+    'deit3_small_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_224_1k.pth'),
+    'deit3_small_patch16_384.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_384_1k.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+    'deit3_medium_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_1k.pth'),
+    'deit3_base_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_224_1k.pth'),
+    'deit3_base_patch16_384.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_384_1k.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+    'deit3_large_patch16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_224_1k.pth'),
+    'deit3_large_patch16_384.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_384_1k.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+    'deit3_huge_patch14_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_1k.pth'),
+
+    'deit3_small_patch16_224.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_224_21k.pth',
+        crop_pct=1.0),
+    'deit3_small_patch16_384.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_384_21k.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+    'deit3_medium_patch16_224.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_21k.pth',
+        crop_pct=1.0),
+    'deit3_base_patch16_224.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_224_21k.pth',
+        crop_pct=1.0),
+    'deit3_base_patch16_384.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_384_21k.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+    'deit3_large_patch16_224.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_224_21k.pth',
+        crop_pct=1.0),
+    'deit3_large_patch16_384.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_384_21k.pth',
+        input_size=(3, 384, 384), crop_pct=1.0),
+    'deit3_huge_patch14_224.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_21k_v1.pth',
+        crop_pct=1.0),
+})
+
+
 @register_model
-def deit_tiny_patch16_224(pretrained=False, **kwargs):
+def deit_tiny_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-tiny model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3, **kwargs)
-    model = _create_deit('deit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
+    model = _create_deit('deit_tiny_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_small_patch16_224(pretrained=False, **kwargs):
+def deit_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-small model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, **kwargs)
-    model = _create_deit('deit_small_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
+    model = _create_deit('deit_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_base_patch16_224(pretrained=False, **kwargs):
+def deit_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT base model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)
-    model = _create_deit('deit_base_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
+    model = _create_deit('deit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_base_patch16_384(pretrained=False, **kwargs):
+def deit_base_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT base model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)
-    model = _create_deit('deit_base_patch16_384', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
+    model = _create_deit('deit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):
+def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs) -> VisionTransformerDistilled:
     """ DeiT-tiny distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3, **kwargs)
+    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
     model = _create_deit(
-        'deit_tiny_distilled_patch16_224', pretrained=pretrained, distilled=True, **model_kwargs)
+        'deit_tiny_distilled_patch16_224', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_small_distilled_patch16_224(pretrained=False, **kwargs):
+def deit_small_distilled_patch16_224(pretrained=False, **kwargs) -> VisionTransformerDistilled:
     """ DeiT-small distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, **kwargs)
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
     model = _create_deit(
-        'deit_small_distilled_patch16_224', pretrained=pretrained, distilled=True, **model_kwargs)
+        'deit_small_distilled_patch16_224', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_base_distilled_patch16_224(pretrained=False, **kwargs):
+def deit_base_distilled_patch16_224(pretrained=False, **kwargs) -> VisionTransformerDistilled:
     """ DeiT-base distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
     model = _create_deit(
-        'deit_base_distilled_patch16_224', pretrained=pretrained, distilled=True, **model_kwargs)
+        'deit_base_distilled_patch16_224', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit_base_distilled_patch16_384(pretrained=False, **kwargs):
+def deit_base_distilled_patch16_384(pretrained=False, **kwargs) -> VisionTransformerDistilled:
     """ DeiT-base distilled model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
     model = _create_deit(
-        'deit_base_distilled_patch16_384', pretrained=pretrained, distilled=True, **model_kwargs)
+        'deit_base_distilled_patch16_384', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_small_patch16_224(pretrained=False, **kwargs):
+def deit3_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 small model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_small_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_small_patch16_384(pretrained=False, **kwargs):
+def deit3_small_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 small model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_small_patch16_384', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_small_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_medium_patch16_224(pretrained=False, **kwargs):
+def deit3_medium_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 medium model @ 224x224 (https://arxiv.org/abs/2012.12877).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=12, num_heads=8, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_medium_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=512, depth=12, num_heads=8, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_medium_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_base_patch16_224(pretrained=False, **kwargs):
+def deit3_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 base model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_base_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_base_patch16_384(pretrained=False, **kwargs):
+def deit3_base_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 base model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_base_patch16_384', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_large_patch16_224(pretrained=False, **kwargs):
+def deit3_large_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 large model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_large_patch16_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_large_patch16_384(pretrained=False, **kwargs):
+def deit3_large_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 large model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_large_patch16_384', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def deit3_huge_patch14_224(pretrained=False, **kwargs):
+def deit3_huge_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ DeiT-3 base model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
     ImageNet-1k weights from https://github.com/facebookresearch/deit.
     """
-    model_kwargs = dict(
-        patch_size=14, embed_dim=1280, depth=32, num_heads=16, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_huge_patch14_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, no_embed_class=True, init_values=1e-6)
+    model = _create_deit('deit3_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
-@register_model
-def deit3_small_patch16_224_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 small model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_small_patch16_224_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_small_patch16_384_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 small model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_small_patch16_384_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_medium_patch16_224_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 medium model @ 224x224 (https://arxiv.org/abs/2012.12877).
-    ImageNet-1k weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=12, num_heads=8, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_medium_patch16_224_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_base_patch16_224_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 base model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_base_patch16_224_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_base_patch16_384_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 base model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_base_patch16_384_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_large_patch16_224_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 large model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_large_patch16_224_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_large_patch16_384_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 large model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_large_patch16_384_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def deit3_huge_patch14_224_in21ft1k(pretrained=False, **kwargs):
-    """ DeiT-3 base model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).
-    ImageNet-21k pretrained weights from https://github.com/facebookresearch/deit.
-    """
-    model_kwargs = dict(
-        patch_size=14, embed_dim=1280, depth=32, num_heads=16, no_embed_class=True, init_values=1e-6, **kwargs)
-    model = _create_deit('deit3_huge_patch14_224_in21ft1k', pretrained=pretrained, **model_kwargs)
-    return model
+register_model_deprecations(__name__, {
+    'deit3_small_patch16_224_in21ft1k': 'deit3_small_patch16_224.fb_in22k_ft_in1k',
+    'deit3_small_patch16_384_in21ft1k': 'deit3_small_patch16_384.fb_in22k_ft_in1k',
+    'deit3_medium_patch16_224_in21ft1k': 'deit3_medium_patch16_224.fb_in22k_ft_in1k',
+    'deit3_base_patch16_224_in21ft1k': 'deit3_base_patch16_224.fb_in22k_ft_in1k',
+    'deit3_base_patch16_384_in21ft1k': 'deit3_base_patch16_384.fb_in22k_ft_in1k',
+    'deit3_large_patch16_224_in21ft1k': 'deit3_large_patch16_224.fb_in22k_ft_in1k',
+    'deit3_large_patch16_384_in21ft1k': 'deit3_large_patch16_384.fb_in22k_ft_in1k',
+    'deit3_huge_patch14_224_in21ft1k': 'deit3_huge_patch14_224.fb_in22k_ft_in1k'
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/densenet.py` & `timm-0.9.0/timm/models/densenet.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,56 +11,38 @@
 import torch.utils.checkpoint as cp
 from torch.jit.annotations import List
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import BatchNormAct2d, get_norm_act_layer, BlurPool2d, create_classifier
 from ._builder import build_model_with_cfg
 from ._manipulate import MATCH_PREV_GROUP
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['DenseNet']
 
 
-def _cfg(url=''):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'features.conv0', 'classifier': 'classifier',
-    }
-
-
-default_cfgs = {
-    'densenet121': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet121_ra-50efcf5c.pth'),
-    'densenet121d': _cfg(url=''),
-    'densenetblur121d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenetblur121d_ra-100dcfbc.pth'),
-    'densenet169': _cfg(url='https://download.pytorch.org/models/densenet169-b2777c0a.pth'),
-    'densenet201': _cfg(url='https://download.pytorch.org/models/densenet201-c1103571.pth'),
-    'densenet161': _cfg(url='https://download.pytorch.org/models/densenet161-8d451a50.pth'),
-    'densenet264': _cfg(url=''),
-    'densenet264d_iabn': _cfg(url=''),
-    'tv_densenet121': _cfg(url='https://download.pytorch.org/models/densenet121-a639ec97.pth'),
-}
-
-
 class DenseLayer(nn.Module):
     def __init__(
-            self, num_input_features, growth_rate, bn_size, norm_layer=BatchNormAct2d,
-            drop_rate=0., memory_efficient=False):
+            self,
+            num_input_features,
+            growth_rate,
+            bn_size,
+            norm_layer=BatchNormAct2d,
+            drop_rate=0.,
+            grad_checkpointing=False,
+    ):
         super(DenseLayer, self).__init__()
         self.add_module('norm1', norm_layer(num_input_features)),
         self.add_module('conv1', nn.Conv2d(
             num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)),
         self.add_module('norm2', norm_layer(bn_size * growth_rate)),
         self.add_module('conv2', nn.Conv2d(
             bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),
         self.drop_rate = float(drop_rate)
-        self.memory_efficient = memory_efficient
+        self.grad_checkpointing = grad_checkpointing
 
     def bottleneck_fn(self, xs):
         # type: (List[torch.Tensor]) -> torch.Tensor
         concated_features = torch.cat(xs, 1)
         bottleneck_output = self.conv1(self.norm1(concated_features))  # noqa: T484
         return bottleneck_output
 
@@ -94,15 +76,15 @@
     # allowing it to take either a List[Tensor] or single Tensor
     def forward(self, x):  # noqa: F811
         if isinstance(x, torch.Tensor):
             prev_features = [x]
         else:
             prev_features = x
 
-        if self.memory_efficient and self.any_requires_grad(prev_features):
+        if self.grad_checkpointing and self.any_requires_grad(prev_features):
             if torch.jit.is_scripting():
                 raise Exception("Memory Efficient not supported in JIT")
             bottleneck_output = self.call_checkpoint_bottleneck(prev_features)
         else:
             bottleneck_output = self.bottleneck_fn(prev_features)
 
         new_features = self.conv2(self.norm2(bottleneck_output))
@@ -118,38 +100,44 @@
             self,
             num_layers,
             num_input_features,
             bn_size,
             growth_rate,
             norm_layer=BatchNormAct2d,
             drop_rate=0.,
-            memory_efficient=False,
+            grad_checkpointing=False,
     ):
         super(DenseBlock, self).__init__()
         for i in range(num_layers):
             layer = DenseLayer(
                 num_input_features + i * growth_rate,
                 growth_rate=growth_rate,
                 bn_size=bn_size,
                 norm_layer=norm_layer,
                 drop_rate=drop_rate,
-                memory_efficient=memory_efficient,
+                grad_checkpointing=grad_checkpointing,
             )
             self.add_module('denselayer%d' % (i + 1), layer)
 
     def forward(self, init_features):
         features = [init_features]
         for name, layer in self.items():
             new_features = layer(features)
             features.append(new_features)
         return torch.cat(features, 1)
 
 
 class DenseTransition(nn.Sequential):
-    def __init__(self, num_input_features, num_output_features, norm_layer=BatchNormAct2d, aa_layer=None):
+    def __init__(
+            self,
+            num_input_features,
+            num_output_features,
+            norm_layer=BatchNormAct2d,
+            aa_layer=None,
+    ):
         super(DenseTransition, self).__init__()
         self.add_module('norm', norm_layer(num_input_features))
         self.add_module('conv', nn.Conv2d(
             num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))
         if aa_layer is not None:
             self.add_module('pool', aa_layer(num_output_features, stride=2))
         else:
@@ -161,15 +149,16 @@
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`_
 
     Args:
         growth_rate (int) - how many filters to add each layer (`k` in paper)
         block_config (list of 4 ints) - how many layers in each pooling block
         bn_size (int) - multiplicative factor for number of bottle neck layers
           (i.e. bn_size * k features in the bottleneck layer)
-        drop_rate (float) - dropout rate after each dense layer
+        drop_rate (float) - dropout rate before classifier layer
+        proj_drop_rate (float) - dropout rate after each dense layer
         num_classes (int) - number of classification classes
         memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,
           but slower. Default: *False*. See `"paper" <https://arxiv.org/pdf/1707.06990.pdf>`_
     """
 
     def __init__(
             self,
@@ -179,20 +168,20 @@
             in_chans=3,
             global_pool='avg',
             bn_size=4,
             stem_type='',
             act_layer='relu',
             norm_layer='batchnorm2d',
             aa_layer=None,
-            drop_rate=0,
+            drop_rate=0.,
+            proj_drop_rate=0.,
             memory_efficient=False,
             aa_stem_only=True,
     ):
         self.num_classes = num_classes
-        self.drop_rate = drop_rate
         super(DenseNet, self).__init__()
         norm_layer = get_norm_act_layer(norm_layer, act_layer=act_layer)
 
         # Stem
         deep_stem = 'deep' in stem_type  # 3x3 deep stem
         num_init_features = growth_rate * 2
         if aa_layer is None:
@@ -230,16 +219,16 @@
         for i, num_layers in enumerate(block_config):
             block = DenseBlock(
                 num_layers=num_layers,
                 num_input_features=num_features,
                 bn_size=bn_size,
                 growth_rate=growth_rate,
                 norm_layer=norm_layer,
-                drop_rate=drop_rate,
-                memory_efficient=memory_efficient
+                drop_rate=proj_drop_rate,
+                grad_checkpointing=memory_efficient,
             )
             module_name = f'denseblock{(i + 1)}'
             self.features.add_module(module_name, block)
             num_features = num_features + num_layers * growth_rate
             transition_aa_layer = None if aa_stem_only else aa_layer
             if i != len(block_config) - 1:
                 self.feature_info += [
@@ -257,16 +246,22 @@
         # Final batch norm
         self.features.add_module('norm5', norm_layer(num_features))
 
         self.feature_info += [dict(num_chs=num_features, reduction=current_stride, module='features.norm5')]
         self.num_features = num_features
 
         # Linear layer
-        self.global_pool, self.classifier = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool)
+        global_pool, classifier = create_classifier(
+            self.num_features,
+            self.num_classes,
+            pool_type=global_pool,
+        )
+        self.global_pool = global_pool
+        self.head_drop = nn.Dropout(drop_rate)
+        self.classifier = classifier
 
         # Official init from torch repo.
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(m.weight)
             elif isinstance(m, nn.BatchNorm2d):
                 nn.init.constant_(m.weight, 1)
@@ -282,31 +277,35 @@
                 (r'^features\.denseblock(\d+)\.denselayer(\d+)', None),
                 (r'^features\.transition(\d+)', MATCH_PREV_GROUP)  # FIXME combine with previous denselayer
             ]
         )
         return matcher
 
     @torch.jit.ignore
+    def set_grad_checkpointing(self, enable=True):
+        for b in self.features.modules():
+            if isinstance(b, DenseLayer):
+                b.grad_checkpointing = enable
+
+    @torch.jit.ignore
     def get_classifier(self):
         return self.classifier
 
     def reset_classifier(self, num_classes, global_pool='avg'):
         self.num_classes = num_classes
         self.global_pool, self.classifier = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
         return self.features(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.global_pool(x)
-        # both classifier and block drop?
-        # if self.drop_rate > 0.:
-        #     x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         x = self.classifier(x)
         return x
 
 
 def _filter_torchvision_pretrained(state_dict):
     pattern = re.compile(
         r'^(.*denselayer\d+\.(?:norm|relu|conv))\.((?:[12])\.(?:weight|bias|running_mean|running_var))$')
@@ -320,102 +319,100 @@
     return state_dict
 
 
 def _create_densenet(variant, growth_rate, block_config, pretrained, **kwargs):
     kwargs['growth_rate'] = growth_rate
     kwargs['block_config'] = block_config
     return build_model_with_cfg(
-        DenseNet, variant, pretrained,
-        feature_cfg=dict(flatten_sequential=True), pretrained_filter_fn=_filter_torchvision_pretrained,
-        **kwargs)
+        DenseNet,
+        variant,
+        pretrained,
+        feature_cfg=dict(flatten_sequential=True),
+        pretrained_filter_fn=_filter_torchvision_pretrained,
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'features.conv0', 'classifier': 'classifier', **kwargs,
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'densenet121.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'densenetblur121d.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'densenet264d.untrained': _cfg(),
+    'densenet121.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'densenet169.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'densenet201.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'densenet161.tv_in1k': _cfg(hf_hub_id='timm/'),
+})
 
 
 @register_model
-def densenet121(pretrained=False, **kwargs):
+def densenet121(pretrained=False, **kwargs) -> DenseNet:
     r"""Densenet-121 model from
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model = _create_densenet(
         'densenet121', growth_rate=32, block_config=(6, 12, 24, 16), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def densenetblur121d(pretrained=False, **kwargs):
-    r"""Densenet-121 model from
+def densenetblur121d(pretrained=False, **kwargs) -> DenseNet:
+    r"""Densenet-121 w/ blur-pooling & 3-layer 3x3 stem
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model = _create_densenet(
         'densenetblur121d', growth_rate=32, block_config=(6, 12, 24, 16), pretrained=pretrained,
         stem_type='deep', aa_layer=BlurPool2d, **kwargs)
     return model
 
 
 @register_model
-def densenet121d(pretrained=False, **kwargs):
-    r"""Densenet-121 model from
-    `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
-    """
-    model = _create_densenet(
-        'densenet121d', growth_rate=32, block_config=(6, 12, 24, 16), stem_type='deep',
-        pretrained=pretrained, **kwargs)
-    return model
-
-
-@register_model
-def densenet169(pretrained=False, **kwargs):
+def densenet169(pretrained=False, **kwargs) -> DenseNet:
     r"""Densenet-169 model from
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model = _create_densenet(
         'densenet169', growth_rate=32, block_config=(6, 12, 32, 32), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def densenet201(pretrained=False, **kwargs):
+def densenet201(pretrained=False, **kwargs) -> DenseNet:
     r"""Densenet-201 model from
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model = _create_densenet(
         'densenet201', growth_rate=32, block_config=(6, 12, 48, 32), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def densenet161(pretrained=False, **kwargs):
+def densenet161(pretrained=False, **kwargs) -> DenseNet:
     r"""Densenet-161 model from
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model = _create_densenet(
         'densenet161', growth_rate=48, block_config=(6, 12, 36, 24), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def densenet264(pretrained=False, **kwargs):
+def densenet264d(pretrained=False, **kwargs) -> DenseNet:
     r"""Densenet-264 model from
     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model = _create_densenet(
-        'densenet264', growth_rate=48, block_config=(6, 12, 64, 48), pretrained=pretrained, **kwargs)
+        'densenet264d', growth_rate=48, block_config=(6, 12, 64, 48), stem_type='deep', pretrained=pretrained, **kwargs)
     return model
 
-
-@register_model
-def densenet264d_iabn(pretrained=False, **kwargs):
-    r"""Densenet-264 model with deep stem and Inplace-ABN
-    """
-    model = _create_densenet(
-        'densenet264d_iabn', growth_rate=48, block_config=(6, 12, 64, 48), stem_type='deep',
-        norm_layer='iabn', act_layer='leaky_relu', pretrained=pretrained, **kwargs)
-    return model
-
-
-@register_model
-def tv_densenet121(pretrained=False, **kwargs):
-    r"""Densenet-121 model with original Torchvision weights, from
-    `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
-    """
-    model = _create_densenet(
-        'tv_densenet121', growth_rate=32, block_config=(6, 12, 24, 16), pretrained=pretrained, **kwargs)
-    return model
```

### Comparing `timm-0.8.6.dev0/timm/models/dla.py` & `timm-0.9.0/timm/models/dla.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 """ Deep Layer Aggregation and DLA w/ Res2Net
-DLA original adapted from Official Pytorch impl at:
+DLA original adapted from Official Pytorch impl at: https://github.com/ucbdrive/dla
 DLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484
 
 Res2Net additions from: https://github.com/gasvn/Res2Net/
 Res2Net Paper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169
 """
 import math
 from typing import List, Optional
@@ -11,63 +11,36 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['DLA']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'base_layer.0', 'classifier': 'fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'dla34': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla34-2b83ff04.pth'),
-    'dla46_c': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla46_c-9b68d685.pth'),
-    'dla46x_c': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla46x_c-6bc5b5c8.pth'),
-    'dla60x_c': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla60x_c-a38e054a.pth'),
-    'dla60': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla60-9e91bd4d.pth'),
-    'dla60x': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla60x-6818f6bb.pth'),
-    'dla102': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla102-21f57b54.pth'),
-    'dla102x': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla102x-7ec0aa2a.pth'),
-    'dla102x2': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla102x2-ac4239c4.pth'),
-    'dla169': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dla169-7c767967.pth'),
-    'dla60_res2net': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net_dla60_4s-d88db7f9.pth'),
-    'dla60_res2next': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2next_dla60_4s-d327927b.pth'),
-}
-
-
 class DlaBasic(nn.Module):
     """DLA Basic"""
 
     def __init__(self, inplanes, planes, stride=1, dilation=1, **_):
         super(DlaBasic, self).__init__()
         self.conv1 = nn.Conv2d(
-            inplanes, planes, kernel_size=3, stride=stride, padding=dilation, bias=False, dilation=dilation)
+            inplanes, planes, kernel_size=3,
+            stride=stride, padding=dilation, bias=False, dilation=dilation)
         self.bn1 = nn.BatchNorm2d(planes)
         self.relu = nn.ReLU(inplace=True)
         self.conv2 = nn.Conv2d(
-            planes, planes, kernel_size=3, stride=1, padding=dilation, bias=False, dilation=dilation)
+            planes, planes, kernel_size=3,
+            stride=1, padding=dilation, bias=False, dilation=dilation)
         self.bn2 = nn.BatchNorm2d(planes)
         self.stride = stride
 
-    def forward(self, x, shortcut=None, children: Optional[List[torch.Tensor]] = None):
+    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
         out = self.conv1(x)
         out = self.bn1(out)
         out = self.relu(out)
 
@@ -89,16 +62,16 @@
         self.stride = stride
         mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)
         mid_planes = mid_planes // self.expansion
 
         self.conv1 = nn.Conv2d(inplanes, mid_planes, kernel_size=1, bias=False)
         self.bn1 = nn.BatchNorm2d(mid_planes)
         self.conv2 = nn.Conv2d(
-            mid_planes, mid_planes, kernel_size=3, stride=stride, padding=dilation,
-            bias=False, dilation=dilation, groups=cardinality)
+            mid_planes, mid_planes, kernel_size=3,
+            stride=stride, padding=dilation, bias=False, dilation=dilation, groups=cardinality)
         self.bn2 = nn.BatchNorm2d(mid_planes)
         self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False)
         self.bn3 = nn.BatchNorm2d(outplanes)
         self.relu = nn.ReLU(inplace=True)
 
     def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
         if shortcut is None:
@@ -139,16 +112,16 @@
         self.bn1 = nn.BatchNorm2d(mid_planes * scale)
 
         num_scale_convs = max(1, scale - 1)
         convs = []
         bns = []
         for _ in range(num_scale_convs):
             convs.append(nn.Conv2d(
-                mid_planes, mid_planes, kernel_size=3, stride=stride,
-                padding=dilation, dilation=dilation, groups=cardinality, bias=False))
+                mid_planes, mid_planes, kernel_size=3,
+                stride=stride, padding=dilation, dilation=dilation, groups=cardinality, bias=False))
             bns.append(nn.BatchNorm2d(mid_planes))
         self.convs = nn.ModuleList(convs)
         self.bns = nn.ModuleList(bns)
         self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1) if self.is_first else None
 
         self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False)
         self.bn3 = nn.BatchNorm2d(outplanes)
@@ -207,16 +180,28 @@
         x = self.relu(x)
 
         return x
 
 
 class DlaTree(nn.Module):
     def __init__(
-            self, levels, block, in_channels, out_channels, stride=1, dilation=1, cardinality=1,
-            base_width=64, level_root=False, root_dim=0, root_kernel_size=1, root_shortcut=False):
+            self,
+            levels,
+            block,
+            in_channels,
+            out_channels,
+            stride=1,
+            dilation=1,
+            cardinality=1,
+            base_width=64,
+            level_root=False,
+            root_dim=0,
+            root_kernel_size=1,
+            root_shortcut=False,
+    ):
         super(DlaTree, self).__init__()
         if root_dim == 0:
             root_dim = 2 * out_channels
         if level_root:
             root_dim += in_channels
         self.downsample = nn.MaxPool2d(stride, stride=stride) if stride > 1 else nn.Identity()
         self.project = nn.Identity()
@@ -231,17 +216,30 @@
                 self.project = nn.Sequential(
                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),
                     nn.BatchNorm2d(out_channels))
             self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_shortcut)
         else:
             cargs.update(dict(root_kernel_size=root_kernel_size, root_shortcut=root_shortcut))
             self.tree1 = DlaTree(
-                levels - 1, block, in_channels, out_channels, stride, root_dim=0, **cargs)
+                levels - 1,
+                block,
+                in_channels,
+                out_channels,
+                stride,
+                root_dim=0,
+                **cargs,
+            )
             self.tree2 = DlaTree(
-                levels - 1, block, out_channels, out_channels, root_dim=root_dim + out_channels, **cargs)
+                levels - 1,
+                block,
+                out_channels,
+                out_channels,
+                root_dim=root_dim + out_channels,
+                **cargs,
+            )
             self.root = None
         self.level_root = level_root
         self.root_dim = root_dim
         self.levels = levels
 
     def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
         if children is None:
@@ -258,28 +256,39 @@
             children.append(x1)
             x = self.tree2(x1, None, children)
         return x
 
 
 class DLA(nn.Module):
     def __init__(
-            self, levels, channels, output_stride=32, num_classes=1000, in_chans=3, global_pool='avg',
-            cardinality=1, base_width=64, block=DlaBottle2neck, shortcut_root=False, drop_rate=0.0):
+            self,
+            levels,
+            channels,
+            output_stride=32,
+            num_classes=1000,
+            in_chans=3,
+            global_pool='avg',
+            cardinality=1,
+            base_width=64,
+            block=DlaBottle2neck,
+            shortcut_root=False,
+            drop_rate=0.0,
+    ):
         super(DLA, self).__init__()
         self.channels = channels
         self.num_classes = num_classes
         self.cardinality = cardinality
         self.base_width = base_width
-        self.drop_rate = drop_rate
         assert output_stride == 32  # FIXME support dilation
 
         self.base_layer = nn.Sequential(
             nn.Conv2d(in_chans, channels[0], kernel_size=7, stride=1, padding=3, bias=False),
             nn.BatchNorm2d(channels[0]),
-            nn.ReLU(inplace=True))
+            nn.ReLU(inplace=True),
+        )
         self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])
         self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)
         cargs = dict(cardinality=cardinality, base_width=base_width, root_shortcut=shortcut_root)
         self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)
         self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)
         self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)
         self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)
@@ -289,16 +298,21 @@
             dict(num_chs=channels[2], reduction=4, module='level2'),
             dict(num_chs=channels[3], reduction=8, module='level3'),
             dict(num_chs=channels[4], reduction=16, module='level4'),
             dict(num_chs=channels[5], reduction=32, module='level5'),
         ]
 
         self.num_features = channels[-1]
-        self.global_pool, self.fc = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)
+        self.global_pool, self.head_drop, self.fc = create_classifier(
+            self.num_features,
+            self.num_classes,
+            pool_type=global_pool,
+            use_conv=True,
+            drop_rate=drop_rate,
+        )
         self.flatten = nn.Flatten(1) if global_pool else nn.Identity()
 
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                 m.weight.data.normal_(0, math.sqrt(2. / n))
             elif isinstance(m, nn.BatchNorm2d):
@@ -306,15 +320,16 @@
                 m.bias.data.zero_()
 
     def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):
         modules = []
         for i in range(convs):
             modules.extend([
                 nn.Conv2d(
-                    inplanes, planes, kernel_size=3, stride=stride if i == 0 else 1,
+                    inplanes, planes, kernel_size=3,
+                    stride=stride if i == 0 else 1,
                     padding=dilation, bias=False, dilation=dilation),
                 nn.BatchNorm2d(planes),
                 nn.ReLU(inplace=True)])
             inplanes = planes
         return nn.Sequential(*modules)
 
     @torch.jit.ignore
@@ -352,123 +367,149 @@
         x = self.level3(x)
         x = self.level4(x)
         x = self.level5(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
-        if self.drop_rate > 0.:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         if pre_logits:
-            return x.flatten(1)
-        else:
-            x = self.fc(x)
             return self.flatten(x)
+        x = self.fc(x)
+        return self.flatten(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_dla(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        DLA, variant, pretrained,
+        DLA,
+        variant,
+        pretrained,
         pretrained_strict=False,
         feature_cfg=dict(out_indices=(1, 2, 3, 4, 5)),
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'base_layer.0', 'classifier': 'fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'dla34.in1k': _cfg(hf_hub_id='timm/'),
+    'dla46_c.in1k': _cfg(hf_hub_id='timm/'),
+    'dla46x_c.in1k': _cfg(hf_hub_id='timm/'),
+    'dla60x_c.in1k': _cfg(hf_hub_id='timm/'),
+    'dla60.in1k': _cfg(hf_hub_id='timm/'),
+    'dla60x.in1k': _cfg(hf_hub_id='timm/'),
+    'dla102.in1k': _cfg(hf_hub_id='timm/'),
+    'dla102x.in1k': _cfg(hf_hub_id='timm/'),
+    'dla102x2.in1k': _cfg(hf_hub_id='timm/'),
+    'dla169.in1k': _cfg(hf_hub_id='timm/'),
+    'dla60_res2net.in1k': _cfg(hf_hub_id='timm/'),
+    'dla60_res2next.in1k': _cfg(hf_hub_id='timm/'),
+})
 
 
 @register_model
-def dla60_res2net(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def dla60_res2net(pretrained=False, **kwargs) -> DLA:
+    model_args = dict(
         levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),
-        block=DlaBottle2neck, cardinality=1, base_width=28, **kwargs)
-    return _create_dla('dla60_res2net', pretrained, **model_kwargs)
+        block=DlaBottle2neck, cardinality=1, base_width=28)
+    return _create_dla('dla60_res2net', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
 def dla60_res2next(pretrained=False,**kwargs):
-    model_kwargs = dict(
+    model_args = dict(
         levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),
-        block=DlaBottle2neck, cardinality=8, base_width=4, **kwargs)
-    return _create_dla('dla60_res2next', pretrained, **model_kwargs)
+        block=DlaBottle2neck, cardinality=8, base_width=4)
+    return _create_dla('dla60_res2next', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla34(pretrained=False, **kwargs):  # DLA-34
-    model_kwargs = dict(
-        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 128, 256, 512],
-        block=DlaBasic, **kwargs)
-    return _create_dla('dla34', pretrained, **model_kwargs)
+def dla34(pretrained=False, **kwargs) -> DLA:  # DLA-34
+    model_args = dict(
+        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 128, 256, 512], block=DlaBasic)
+    return _create_dla('dla34', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla46_c(pretrained=False, **kwargs):  # DLA-46-C
-    model_kwargs = dict(
-        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256],
-        block=DlaBottleneck, **kwargs)
-    return _create_dla('dla46_c', pretrained, **model_kwargs)
+def dla46_c(pretrained=False, **kwargs) -> DLA:  # DLA-46-C
+    model_args = dict(
+        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256], block=DlaBottleneck)
+    return _create_dla('dla46_c', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla46x_c(pretrained=False, **kwargs):  # DLA-X-46-C
-    model_kwargs = dict(
+def dla46x_c(pretrained=False, **kwargs) -> DLA:  # DLA-X-46-C
+    model_args = dict(
         levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256],
-        block=DlaBottleneck, cardinality=32, base_width=4, **kwargs)
-    return _create_dla('dla46x_c', pretrained, **model_kwargs)
+        block=DlaBottleneck, cardinality=32, base_width=4)
+    return _create_dla('dla46x_c', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla60x_c(pretrained=False, **kwargs):  # DLA-X-60-C
-    model_kwargs = dict(
+def dla60x_c(pretrained=False, **kwargs) -> DLA:  # DLA-X-60-C
+    model_args = dict(
         levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 64, 64, 128, 256],
-        block=DlaBottleneck, cardinality=32, base_width=4, **kwargs)
-    return _create_dla('dla60x_c', pretrained, **model_kwargs)
+        block=DlaBottleneck, cardinality=32, base_width=4)
+    return _create_dla('dla60x_c', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla60(pretrained=False, **kwargs):  # DLA-60
-    model_kwargs = dict(
+def dla60(pretrained=False, **kwargs) -> DLA:  # DLA-60
+    model_args = dict(
         levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 128, 256, 512, 1024],
-        block=DlaBottleneck, **kwargs)
-    return _create_dla('dla60', pretrained, **model_kwargs)
+        block=DlaBottleneck)
+    return _create_dla('dla60', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla60x(pretrained=False, **kwargs):  # DLA-X-60
-    model_kwargs = dict(
+def dla60x(pretrained=False, **kwargs) -> DLA:  # DLA-X-60
+    model_args = dict(
         levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 128, 256, 512, 1024],
-        block=DlaBottleneck, cardinality=32, base_width=4, **kwargs)
-    return _create_dla('dla60x', pretrained, **model_kwargs)
+        block=DlaBottleneck, cardinality=32, base_width=4)
+    return _create_dla('dla60x', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla102(pretrained=False, **kwargs):  # DLA-102
-    model_kwargs = dict(
+def dla102(pretrained=False, **kwargs) -> DLA:  # DLA-102
+    model_args = dict(
         levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],
-        block=DlaBottleneck, shortcut_root=True, **kwargs)
-    return _create_dla('dla102', pretrained, **model_kwargs)
+        block=DlaBottleneck, shortcut_root=True)
+    return _create_dla('dla102', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla102x(pretrained=False, **kwargs):  # DLA-X-102
-    model_kwargs = dict(
+def dla102x(pretrained=False, **kwargs) -> DLA:  # DLA-X-102
+    model_args = dict(
         levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],
-        block=DlaBottleneck, cardinality=32, base_width=4, shortcut_root=True, **kwargs)
-    return _create_dla('dla102x', pretrained, **model_kwargs)
+        block=DlaBottleneck, cardinality=32, base_width=4, shortcut_root=True)
+    return _create_dla('dla102x', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla102x2(pretrained=False, **kwargs):  # DLA-X-102 64
-    model_kwargs = dict(
+def dla102x2(pretrained=False, **kwargs) -> DLA:  # DLA-X-102 64
+    model_args = dict(
         levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],
-        block=DlaBottleneck, cardinality=64, base_width=4, shortcut_root=True, **kwargs)
-    return _create_dla('dla102x2', pretrained, **model_kwargs)
+        block=DlaBottleneck, cardinality=64, base_width=4, shortcut_root=True)
+    return _create_dla('dla102x2', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def dla169(pretrained=False, **kwargs):  # DLA-169
-    model_kwargs = dict(
+def dla169(pretrained=False, **kwargs) -> DLA:  # DLA-169
+    model_args = dict(
         levels=[1, 1, 2, 3, 5, 1], channels=[16, 32, 128, 256, 512, 1024],
-        block=DlaBottleneck, shortcut_root=True, **kwargs)
-    return _create_dla('dla169', pretrained, **model_kwargs)
+        block=DlaBottleneck, shortcut_root=True)
+    return _create_dla('dla169', pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/dpn.py` & `timm-0.9.0/timm/models/dpn.py`

 * *Files 12% similar despite different names*

```diff
@@ -13,47 +13,19 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DPN_MEAN, IMAGENET_DPN_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import BatchNormAct2d, ConvNormAct, create_conv2d, create_classifier, get_norm_act_layer
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['DPN']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DPN_MEAN, 'std': IMAGENET_DPN_STD,
-        'first_conv': 'features.conv1_1.conv', 'classifier': 'classifier',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'dpn48b': _cfg(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    'dpn68': _cfg(
-        url='https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn68-66bebafa7.pth'),
-    'dpn68b': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/dpn68b_ra-a31ca160.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    'dpn92': _cfg(
-        url='https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn92_extra-b040e4a9b.pth'),
-    'dpn98': _cfg(
-        url='https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn98-5b90dec4d.pth'),
-    'dpn131': _cfg(
-        url='https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn131-71dfe43e0.pth'),
-    'dpn107': _cfg(
-        url='https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn107_extra-1ac7121e2.pth')
-}
-
-
 class CatBnAct(nn.Module):
     def __init__(self, in_chs, norm_layer=BatchNormAct2d):
         super(CatBnAct, self).__init__()
         self.bn = norm_layer(in_chs, eps=0.001)
 
     @torch.jit._overload_method  # noqa: F811
     def forward(self, x):
@@ -294,79 +266,106 @@
         return self.features(x)
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         if self.drop_rate > 0.:
             x = F.dropout(x, p=self.drop_rate, training=self.training)
         if pre_logits:
-            return x.flatten(1)
-        else:
-            x = self.classifier(x)
             return self.flatten(x)
+        x = self.classifier(x)
+        return self.flatten(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_dpn(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        DPN, variant, pretrained,
+        DPN,
+        variant,
+        pretrained,
         feature_cfg=dict(feature_concat=True, flatten_sequential=True),
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DPN_MEAN, 'std': IMAGENET_DPN_STD,
+        'first_conv': 'features.conv1_1.conv', 'classifier': 'classifier',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'dpn48b.untrained': _cfg(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'dpn68.mx_in1k': _cfg(hf_hub_id='timm/'),
+    'dpn68b.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,
+        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'dpn68b.mx_in1k': _cfg(hf_hub_id='timm/'),
+    'dpn92.mx_in1k': _cfg(hf_hub_id='timm/'),
+    'dpn98.mx_in1k': _cfg(hf_hub_id='timm/'),
+    'dpn131.mx_in1k': _cfg(hf_hub_id='timm/'),
+    'dpn107.mx_in1k': _cfg(hf_hub_id='timm/')
+})
 
 
 @register_model
-def dpn48b(pretrained=False, **kwargs):
+def dpn48b(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         small=True, num_init_features=10, k_r=128, groups=32,
         b=True, k_sec=(3, 4, 6, 3), inc_sec=(16, 32, 32, 64), act_layer='silu')
     return _create_dpn('dpn48b', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def dpn68(pretrained=False, **kwargs):
+def dpn68(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         small=True, num_init_features=10, k_r=128, groups=32,
         k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64))
     return _create_dpn('dpn68', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def dpn68b(pretrained=False, **kwargs):
+def dpn68b(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         small=True, num_init_features=10, k_r=128, groups=32,
         b=True, k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64))
     return _create_dpn('dpn68b', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def dpn92(pretrained=False, **kwargs):
+def dpn92(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         num_init_features=64, k_r=96, groups=32,
         k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128))
     return _create_dpn('dpn92', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def dpn98(pretrained=False, **kwargs):
+def dpn98(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         num_init_features=96, k_r=160, groups=40,
         k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128))
     return _create_dpn('dpn98', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def dpn131(pretrained=False, **kwargs):
+def dpn131(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         num_init_features=128, k_r=160, groups=40,
         k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128))
     return _create_dpn('dpn131', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def dpn107(pretrained=False, **kwargs):
+def dpn107(pretrained=False, **kwargs) -> DPN:
     model_kwargs = dict(
         num_init_features=128, k_r=200, groups=50,
         k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128))
     return _create_dpn('dpn107', pretrained=pretrained, **dict(model_kwargs, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/edgenext.py` & `timm-0.9.0/timm/models/edgenext.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,80 +13,45 @@
 from typing import Tuple
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import trunc_normal_tf_, DropPath, LayerNorm2d, Mlp, SelectAdaptivePool2d, create_conv2d
+from timm.layers import trunc_normal_tf_, DropPath, LayerNorm2d, Mlp, SelectAdaptivePool2d, create_conv2d, \
+    use_fused_attn
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_module
 from ._manipulate import named_apply, checkpoint_seq
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['EdgeNeXt']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
-        'crop_pct': 0.9, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.0', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    edgenext_xx_small=_cfg(
-        url="https://github.com/mmaaz60/EdgeNeXt/releases/download/v1.0/edgenext_xx_small.pth",
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    edgenext_x_small=_cfg(
-        url="https://github.com/mmaaz60/EdgeNeXt/releases/download/v1.0/edgenext_x_small.pth",
-        test_input_size=(3, 288, 288), test_crop_pct=1.0),
-    # edgenext_small=_cfg(
-    #     url="https://github.com/mmaaz60/EdgeNeXt/releases/download/v1.0/edgenext_small.pth"),
-    edgenext_small=_cfg(  # USI weights
-        url="https://github.com/mmaaz60/EdgeNeXt/releases/download/v1.1/edgenext_small_usi.pth",
-        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,
-    ),
-    # edgenext_base=_cfg(
-    #     url="https://github.com/mmaaz60/EdgeNeXt/releases/download/v1.2/edgenext_base_usi.pth"),
-    edgenext_base=_cfg(  # USI weights
-        url="https://github.com/mmaaz60/EdgeNeXt/releases/download/v1.2/edgenext_base_usi.pth",
-        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,
-    ),
-
-    edgenext_small_rw=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/edgenext_small_rw-sw-b00041bb.pth',
-        test_input_size=(3, 320, 320), test_crop_pct=1.0,
-    ),
-)
-
-
 @register_notrace_module  # reason: FX can't symbolically trace torch.arange in forward method
 class PositionalEncodingFourier(nn.Module):
     def __init__(self, hidden_dim=32, dim=768, temperature=10000):
         super().__init__()
         self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1)
         self.scale = 2 * math.pi
         self.temperature = temperature
         self.hidden_dim = hidden_dim
         self.dim = dim
 
     def forward(self, shape: Tuple[int, int, int]):
-        inv_mask = ~torch.zeros(shape).to(device=self.token_projection.weight.device, dtype=torch.bool)
-        y_embed = inv_mask.cumsum(1, dtype=torch.float32)
-        x_embed = inv_mask.cumsum(2, dtype=torch.float32)
+        device = self.token_projection.weight.device
+        dtype = self.token_projection.weight.dtype
+        inv_mask = ~torch.zeros(shape).to(device=device, dtype=torch.bool)
+        y_embed = inv_mask.cumsum(1, dtype=dtype)
+        x_embed = inv_mask.cumsum(2, dtype=dtype)
         eps = 1e-6
         y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
         x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale
 
-        dim_t = torch.arange(self.hidden_dim, dtype=torch.float32, device=inv_mask.device)
+        dim_t = torch.arange(self.hidden_dim, dtype=dtype, device=device)
         dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / self.hidden_dim)
 
         pos_x = x_embed[:, :, :, None] / dim_t
         pos_y = y_embed[:, :, :, None] / dim_t
         pos_x = torch.stack(
             (pos_x[:, :, :, 0::2].sin(),
              pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
@@ -163,17 +128,17 @@
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 4, 1)
         q, k, v = qkv.unbind(0)
 
         # NOTE, this is NOT spatial attn, q, k, v are B, num_heads, C, L -->  C x C attn map
         attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)) * self.temperature
         attn = attn.softmax(dim=-1)
         attn = self.attn_drop(attn)
+        x = (attn @ v)
 
-        x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)
-
+        x = x.permute(0, 3, 1, 2).reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
     @torch.jit.ignore
     def no_weight_decay(self):
         return {'temperature'}
@@ -515,58 +480,95 @@
         EdgeNeXt, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
         **kwargs)
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
+        'crop_pct': 0.9, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.0', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'edgenext_xx_small.in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'edgenext_x_small.in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'edgenext_small.usi_in1k': _cfg(  # USI weights
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,
+    ),
+    'edgenext_base.usi_in1k': _cfg(  # USI weights
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,
+    ),
+    'edgenext_base.in21k_ft_in1k': _cfg(  # USI weights
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,
+    ),
+    'edgenext_small_rw.sw_in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 320, 320), test_crop_pct=1.0,
+    ),
+})
+
+
 @register_model
-def edgenext_xx_small(pretrained=False, **kwargs):
+def edgenext_xx_small(pretrained=False, **kwargs) -> EdgeNeXt:
     # 1.33M & 260.58M @ 256 resolution
     # 71.23% Top-1 accuracy
     # No AA, Color Jitter=0.4, No Mixup & Cutmix, DropPath=0.0, BS=4096, lr=0.006, multi-scale-sampler
     # Jetson FPS=51.66 versus 47.67 for MobileViT_XXS
     # For A100: FPS @ BS=1: 212.13 & @ BS=256: 7042.06 versus FPS @ BS=1: 96.68 & @ BS=256: 4624.71 for MobileViT_XXS
     model_kwargs = dict(depths=(2, 2, 6, 2), dims=(24, 48, 88, 168), heads=(4, 4, 4, 4), **kwargs)
     return _create_edgenext('edgenext_xx_small', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def edgenext_x_small(pretrained=False, **kwargs):
+def edgenext_x_small(pretrained=False, **kwargs) -> EdgeNeXt:
     # 2.34M & 538.0M @ 256 resolution
     # 75.00% Top-1 accuracy
     # No AA, No Mixup & Cutmix, DropPath=0.0, BS=4096, lr=0.006, multi-scale-sampler
     # Jetson FPS=31.61 versus 28.49 for MobileViT_XS
     # For A100: FPS @ BS=1: 179.55 & @ BS=256: 4404.95 versus FPS @ BS=1: 94.55 & @ BS=256: 2361.53 for MobileViT_XS
     model_kwargs = dict(depths=(3, 3, 9, 3), dims=(32, 64, 100, 192), heads=(4, 4, 4, 4), **kwargs)
     return _create_edgenext('edgenext_x_small', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def edgenext_small(pretrained=False, **kwargs):
+def edgenext_small(pretrained=False, **kwargs) -> EdgeNeXt:
     # 5.59M & 1260.59M @ 256 resolution
     # 79.43% Top-1 accuracy
     # AA=True, No Mixup & Cutmix, DropPath=0.1, BS=4096, lr=0.006, multi-scale-sampler
     # Jetson FPS=20.47 versus 18.86 for MobileViT_S
     # For A100: FPS @ BS=1: 172.33 & @ BS=256: 3010.25 versus FPS @ BS=1: 93.84 & @ BS=256: 1785.92 for MobileViT_S
     model_kwargs = dict(depths=(3, 3, 9, 3), dims=(48, 96, 160, 304),  **kwargs)
     return _create_edgenext('edgenext_small', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def edgenext_base(pretrained=False, **kwargs):
+def edgenext_base(pretrained=False, **kwargs) -> EdgeNeXt:
     # 18.51M & 3840.93M @ 256 resolution
     # 82.5% (normal) 83.7% (USI) Top-1 accuracy
     # AA=True, Mixup & Cutmix, DropPath=0.1, BS=4096, lr=0.006, multi-scale-sampler
     # Jetson FPS=xx.xx versus xx.xx for MobileViT_S
     # For A100: FPS @ BS=1: xxx.xx & @ BS=256: xxxx.xx
     model_kwargs = dict(depths=[3, 3, 9, 3], dims=[80, 160, 288, 584], **kwargs)
     return _create_edgenext('edgenext_base', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def edgenext_small_rw(pretrained=False, **kwargs):
+def edgenext_small_rw(pretrained=False, **kwargs) -> EdgeNeXt:
     model_kwargs = dict(
         depths=(3, 3, 9, 3), dims=(48, 96, 192, 384),
         downsample_block=True, conv_bias=False, stem_type='overlap', **kwargs)
     return _create_edgenext('edgenext_small_rw', pretrained=pretrained, **model_kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/efficientformer.py` & `timm-0.9.0/timm/models/efficientformer.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,42 +16,20 @@
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, trunc_normal_, to_2tuple, Mlp
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._manipulate import checkpoint_seq
+from ._registry import generate_default_cfgs, register_model
 
 __all__ = ['EfficientFormer']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None, 'fixed_input_size': True,
-        'crop_pct': .95, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv1', 'classifier': ('head', 'head_dist'),
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    efficientformer_l1=_cfg(
-        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/efficientformer_l1_1000d_224-5b08fab0.pth",
-    ),
-    efficientformer_l3=_cfg(
-        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/efficientformer_l3_300d_224-6816624f.pth",
-    ),
-    efficientformer_l7=_cfg(
-        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/efficientformer_l7_300d_224-e957ab75.pth",
-    ),
-)
-
 EfficientFormer_width = {
     'l1': (48, 96, 224, 448),
     'l3': (64, 128, 320, 512),
     'l7': (96, 192, 384, 768),
 }
 
 EfficientFormer_depth = {
@@ -95,15 +73,15 @@
     @torch.no_grad()
     def train(self, mode=True):
         super().train(mode)
         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
     def get_attention_biases(self, device: torch.device) -> torch.Tensor:
-        if self.training:
+        if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
             device_key = str(device)
             if device_key not in self.attention_bias_cache:
                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
@@ -229,23 +207,28 @@
 
     def __init__(
             self,
             dim,
             mlp_ratio=4.,
             act_layer=nn.GELU,
             norm_layer=nn.LayerNorm,
-            drop=0.,
+            proj_drop=0.,
             drop_path=0.,
             layer_scale_init_value=1e-5
     ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.token_mixer = Attention(dim)
         self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.ls1 = LayerScale(dim, layer_scale_init_value)
         self.ls2 = LayerScale(dim, layer_scale_init_value)
 
     def forward(self, x):
         x = x + self.drop_path(self.ls1(self.token_mixer(self.norm1(x))))
@@ -269,30 +252,36 @@
     def __init__(
             self,
             dim,
             pool_size=3,
             mlp_ratio=4.,
             act_layer=nn.GELU,
             norm_layer=nn.BatchNorm2d,
-            drop=0.,
+            proj_drop=0.,
             drop_path=0.,
             layer_scale_init_value=1e-5
     ):
         super().__init__()
         self.token_mixer = Pooling(pool_size=pool_size)
-        self.mlp = ConvMlpWithNorm(
-            dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, norm_layer=norm_layer, drop=drop)
-
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.ls1 = LayerScale2d(dim, layer_scale_init_value)
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+
+        self.mlp = ConvMlpWithNorm(
+            dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            norm_layer=norm_layer,
+            drop=proj_drop,
+        )
         self.ls2 = LayerScale2d(dim, layer_scale_init_value)
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
     def forward(self, x):
-        x = x + self.drop_path(self.ls1(self.token_mixer(x)))
-        x = x + self.drop_path(self.ls2(self.mlp(x)))
+        x = x + self.drop_path1(self.ls1(self.token_mixer(x)))
+        x = x + self.drop_path2(self.ls2(self.mlp(x)))
         return x
 
 
 class EfficientFormerStage(nn.Module):
 
     def __init__(
             self,
@@ -302,15 +291,15 @@
             downsample=True,
             num_vit=1,
             pool_size=3,
             mlp_ratio=4.,
             act_layer=nn.GELU,
             norm_layer=nn.BatchNorm2d,
             norm_layer_cl=nn.LayerNorm,
-            drop=.0,
+            proj_drop=.0,
             drop_path=0.,
             layer_scale_init_value=1e-5,
 ):
         super().__init__()
         self.grad_checkpointing = False
 
         if downsample:
@@ -329,38 +318,41 @@
             if num_vit and num_vit > remain_idx:
                 blocks.append(
                     MetaBlock1d(
                         dim,
                         mlp_ratio=mlp_ratio,
                         act_layer=act_layer,
                         norm_layer=norm_layer_cl,
-                        drop=drop,
+                        proj_drop=proj_drop,
                         drop_path=drop_path[block_idx],
                         layer_scale_init_value=layer_scale_init_value,
                     ))
             else:
                 blocks.append(
                     MetaBlock2d(
                         dim,
                         pool_size=pool_size,
                         mlp_ratio=mlp_ratio,
                         act_layer=act_layer,
                         norm_layer=norm_layer,
-                        drop=drop,
+                        proj_drop=proj_drop,
                         drop_path=drop_path[block_idx],
                         layer_scale_init_value=layer_scale_init_value,
                     ))
                 if num_vit and num_vit == remain_idx:
                     blocks.append(Flat())
 
         self.blocks = nn.Sequential(*blocks)
 
     def forward(self, x):
         x = self.downsample(x)
-        x = self.blocks(x)
+        if self.grad_checkpointing and not torch.jit.is_scripting():
+            x = checkpoint_seq(self.blocks, x)
+        else:
+            x = self.blocks(x)
         return x
 
 
 class EfficientFormer(nn.Module):
 
     def __init__(
             self,
@@ -374,14 +366,15 @@
             mlp_ratios=4,
             pool_size=3,
             layer_scale_init_value=1e-5,
             act_layer=nn.GELU,
             norm_layer=nn.BatchNorm2d,
             norm_layer_cl=nn.LayerNorm,
             drop_rate=0.,
+            proj_drop_rate=0.,
             drop_path_rate=0.,
             **kwargs
     ):
         super().__init__()
         self.num_classes = num_classes
         self.global_pool = global_pool
 
@@ -400,26 +393,27 @@
                 downsample=downsamples[i],
                 num_vit=num_vit if i == 3 else 0,
                 pool_size=pool_size,
                 mlp_ratio=mlp_ratios,
                 act_layer=act_layer,
                 norm_layer_cl=norm_layer_cl,
                 norm_layer=norm_layer,
-                drop=drop_rate,
+                proj_drop=proj_drop_rate,
                 drop_path=dpr[i],
                 layer_scale_init_value=layer_scale_init_value,
             )
             prev_dim = embed_dims[i]
             stages.append(stage)
 
         self.stages = nn.Sequential(*stages)
 
         # Classifier head
         self.num_features = embed_dims[-1]
         self.norm = norm_layer_cl(self.num_features)
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
         # assuming model is always distilled (valid for current checkpoints, will split def if that changes)
         self.head_dist = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()
         self.distilled_training = False  # must set this True to train w/ distillation token
 
         self.apply(self._init_weights)
 
@@ -467,14 +461,15 @@
         x = self.stages(x)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool == 'avg':
             x = x.mean(dim=1)
+        x = self.head_drop(x)
         if pre_logits:
             return x
         x, x_dist = self.head(x), self.head_dist(x)
         if self.distilled_training and self.training and not torch.jit.is_scripting():
             # only return separate classification predictions when training in distilled mode
             return x, x_dist
         else:
@@ -510,44 +505,68 @@
 
         k = re.sub(r'layer_scale_([0-9])', r'ls\1.gamma', k)
         k = k.replace('dist_head', 'head_dist')
         out_dict[k] = v
     return out_dict
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None, 'fixed_input_size': True,
+        'crop_pct': .95, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv1', 'classifier': ('head', 'head_dist'),
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'efficientformer_l1.snap_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+    ),
+    'efficientformer_l3.snap_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+    ),
+    'efficientformer_l7.snap_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+    ),
+})
+
+
 def _create_efficientformer(variant, pretrained=False, **kwargs):
     model = build_model_with_cfg(
         EfficientFormer, variant, pretrained,
         pretrained_filter_fn=_checkpoint_filter_fn,
         **kwargs)
     return model
 
 
 @register_model
-def efficientformer_l1(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def efficientformer_l1(pretrained=False, **kwargs) -> EfficientFormer:
+    model_args = dict(
         depths=EfficientFormer_depth['l1'],
         embed_dims=EfficientFormer_width['l1'],
         num_vit=1,
-        **kwargs)
-    return _create_efficientformer('efficientformer_l1', pretrained=pretrained, **model_kwargs)
+    )
+    return _create_efficientformer('efficientformer_l1', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def efficientformer_l3(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def efficientformer_l3(pretrained=False, **kwargs) -> EfficientFormer:
+    model_args = dict(
         depths=EfficientFormer_depth['l3'],
         embed_dims=EfficientFormer_width['l3'],
         num_vit=4,
-        **kwargs)
-    return _create_efficientformer('efficientformer_l3', pretrained=pretrained, **model_kwargs)
+    )
+    return _create_efficientformer('efficientformer_l3', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def efficientformer_l7(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def efficientformer_l7(pretrained=False, **kwargs) -> EfficientFormer:
+    model_args = dict(
         depths=EfficientFormer_depth['l7'],
         embed_dims=EfficientFormer_width['l7'],
         num_vit=8,
-        **kwargs)
-    return _create_efficientformer('efficientformer_l7', pretrained=pretrained, **model_kwargs)
+    )
+    return _create_efficientformer('efficientformer_l7', pretrained=pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/efficientnet.py` & `timm-0.9.0/timm/models/efficientnet.py`

 * *Files 6% similar despite different names*

```diff
@@ -37,25 +37,25 @@
 """
 from functools import partial
 from typing import List
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+from torch.utils.checkpoint import checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import create_conv2d, create_classifier, get_norm_act_layer, GroupNormAct
 from ._builder import build_model_with_cfg, pretrained_cfg_for_features
 from ._efficientnet_blocks import SqueezeExcite
 from ._efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights, \
     round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT
 from ._features import FeatureInfo, FeatureHooks
 from ._manipulate import checkpoint_seq
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['EfficientNet', 'EfficientNetFeatures']
 
 
 class EfficientNet(nn.Module):
     """ EfficientNet
 
@@ -207,14 +207,15 @@
     ):
         super(EfficientNetFeatures, self).__init__()
         act_layer = act_layer or nn.ReLU
         norm_layer = norm_layer or nn.BatchNorm2d
         norm_act_layer = get_norm_act_layer(norm_layer, act_layer)
         se_layer = se_layer or SqueezeExcite
         self.drop_rate = drop_rate
+        self.grad_checkpointing = False
 
         # Stem
         if not fix_stem:
             stem_size = round_chs_fn(stem_size)
         self.conv_stem = create_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)
         self.bn1 = norm_act_layer(stem_size, inplace=True)
 
@@ -237,23 +238,30 @@
 
         # Register feature extraction hooks with FeatureHooks helper
         self.feature_hooks = None
         if feature_location != 'bottleneck':
             hooks = self.feature_info.get_dicts(keys=('module', 'hook_type'))
             self.feature_hooks = FeatureHooks(hooks, self.named_modules())
 
+    @torch.jit.ignore
+    def set_grad_checkpointing(self, enable=True):
+        self.grad_checkpointing = enable
+
     def forward(self, x) -> List[torch.Tensor]:
         x = self.conv_stem(x)
         x = self.bn1(x)
         if self.feature_hooks is None:
             features = []
             if 0 in self._stage_out_idx:
                 features.append(x)  # add stem out
             for i, b in enumerate(self.blocks):
-                x = b(x)
+                if self.grad_checkpointing and not torch.jit.is_scripting():
+                    x = checkpoint(b, x)
+                else:
+                    x = b(x)
                 if i + 1 in self._stage_out_idx:
                     features.append(x)
             return features
         else:
             self.blocks(x)
             out = self.feature_hooks.get_output(x.device)
             return list(out.values())
@@ -949,18 +957,18 @@
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth',
         hf_hub_id='timm/',
         input_size=(3, 288, 288), pool_size=(9, 9), test_input_size=(3, 320, 320), crop_pct=1.0),
     'efficientnet_b4.ra2_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b4_ra2_320-7eb33cd5.pth',
         hf_hub_id='timm/',
         input_size=(3, 320, 320), pool_size=(10, 10), test_input_size=(3, 384, 384), crop_pct=1.0),
-    'efficientnet_b5.in12k_ft_in1k': _cfg(
+    'efficientnet_b5.sw_in12k_ft_in1k': _cfg(
         hf_hub_id='timm/',
         input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, crop_mode='squash'),
-    'efficientnet_b5.in12k': _cfg(
+    'efficientnet_b5.sw_in12k': _cfg(
         hf_hub_id='timm/',
         input_size=(3, 416, 416), pool_size=(13, 13), crop_pct=0.95, num_classes=11821),
     'efficientnet_b6.untrained': _cfg(
         url='', input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),
     'efficientnet_b7.untrained': _cfg(
         url='', input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),
     'efficientnet_b8.untrained': _cfg(
@@ -1051,50 +1059,54 @@
     'efficientnetv2_m.untrained': _cfg(
         input_size=(3, 320, 320), test_input_size=(3, 416, 416), pool_size=(10, 10), crop_pct=1.0),
     'efficientnetv2_l.untrained': _cfg(
         input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0),
     'efficientnetv2_xl.untrained': _cfg(
         input_size=(3, 384, 384), test_input_size=(3, 512, 512), pool_size=(12, 12), crop_pct=1.0),
 
-    'tf_efficientnet_b0.aa_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_aa-827b6e33.pth',
+    'tf_efficientnet_b0.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth',
         hf_hub_id='timm/',
         input_size=(3, 224, 224)),
-    'tf_efficientnet_b1.aa_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_aa-ea7a6ee0.pth',
+    'tf_efficientnet_b1.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ns-99dd0c41.pth',
         hf_hub_id='timm/',
         input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),
-    'tf_efficientnet_b2.aa_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth',
+    'tf_efficientnet_b2.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ns-00306e48.pth',
         hf_hub_id='timm/',
         input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),
-    'tf_efficientnet_b3.aa_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_aa-84b4657e.pth',
+    'tf_efficientnet_b3.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ns-9d44bf68.pth',
         hf_hub_id='timm/',
         input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),
-    'tf_efficientnet_b4.aa_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_aa-818f208c.pth',
+    'tf_efficientnet_b4.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth',
         hf_hub_id='timm/',
         input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),
-    'tf_efficientnet_b5.ra_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ra-9a3e5369.pth',
+    'tf_efficientnet_b5.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ns-6f26d0cf.pth',
         hf_hub_id='timm/',
         input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),
-    'tf_efficientnet_b6.aa_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_aa-80ba17e4.pth',
+    'tf_efficientnet_b6.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ns-51548356.pth',
         hf_hub_id='timm/',
         input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),
-    'tf_efficientnet_b7.ra_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ra-6c08e654.pth',
+    'tf_efficientnet_b7.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth',
         hf_hub_id='timm/',
         input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),
-    'tf_efficientnet_b8.ra_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ra-572d5dd9.pth',
+    'tf_efficientnet_l2.ns_jft_in1k_475': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns_475-bebbd00a.pth',
         hf_hub_id='timm/',
-        input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),
+        input_size=(3, 475, 475), pool_size=(15, 15), crop_pct=0.936),
+    'tf_efficientnet_l2.ns_jft_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns-df73bb44.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 800, 800), pool_size=(25, 25), crop_pct=0.96),
 
     'tf_efficientnet_b0.ap_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth',
         hf_hub_id='timm/',
         mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, input_size=(3, 224, 224)),
     'tf_efficientnet_b1.ap_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ap-44ef0a3d.pth',
@@ -1133,54 +1145,85 @@
         input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),
     'tf_efficientnet_b8.ap_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ap-00e169fa.pth',
         hf_hub_id='timm/',
         mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,
         input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),
 
-    'tf_efficientnet_b0.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth',
+    'tf_efficientnet_b5.ra_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ra-9a3e5369.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),
+    'tf_efficientnet_b7.ra_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ra-6c08e654.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),
+    'tf_efficientnet_b8.ra_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ra-572d5dd9.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),
+
+    'tf_efficientnet_b0.aa_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_aa-827b6e33.pth',
         hf_hub_id='timm/',
         input_size=(3, 224, 224)),
-    'tf_efficientnet_b1.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ns-99dd0c41.pth',
+    'tf_efficientnet_b1.aa_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_aa-ea7a6ee0.pth',
         hf_hub_id='timm/',
         input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),
-    'tf_efficientnet_b2.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ns-00306e48.pth',
+    'tf_efficientnet_b2.aa_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth',
         hf_hub_id='timm/',
         input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),
-    'tf_efficientnet_b3.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ns-9d44bf68.pth',
+    'tf_efficientnet_b3.aa_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_aa-84b4657e.pth',
         hf_hub_id='timm/',
         input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),
-    'tf_efficientnet_b4.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth',
+    'tf_efficientnet_b4.aa_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_aa-818f208c.pth',
         hf_hub_id='timm/',
         input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),
-    'tf_efficientnet_b5.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ns-6f26d0cf.pth',
+    'tf_efficientnet_b5.aa_in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_aa-99018a74.pth',
         hf_hub_id='timm/',
         input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),
-    'tf_efficientnet_b6.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ns-51548356.pth',
+    'tf_efficientnet_b6.aa_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_aa-80ba17e4.pth',
         hf_hub_id='timm/',
         input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),
-    'tf_efficientnet_b7.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth',
+    'tf_efficientnet_b7.aa_in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_aa-076e3472.pth',
         hf_hub_id='timm/',
         input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),
-    'tf_efficientnet_l2.ns_jft_in1k_475': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns_475-bebbd00a.pth',
-        hf_hub_id='timm/',
-        input_size=(3, 475, 475), pool_size=(15, 15), crop_pct=0.936),
-    'tf_efficientnet_l2.ns_jft_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns-df73bb44.pth',
-        hf_hub_id='timm/',
-        input_size=(3, 800, 800), pool_size=(25, 25), crop_pct=0.96),
+
+    'tf_efficientnet_b0.in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0-0af12548.pth',
+        #hf_hub_id='timm/',
+        input_size=(3, 224, 224)),
+    'tf_efficientnet_b1.in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1-5c1377c4.pth',
+        #hf_hub_id='timm/',
+        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),
+    'tf_efficientnet_b2.in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2-e393ef04.pth',
+        #hf_hub_id='timm/',
+        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),
+    'tf_efficientnet_b3.in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3-e3bd6955.pth',
+        #hf_hub_id='timm/',
+        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),
+    'tf_efficientnet_b4.in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4-74ee3bed.pth',
+        #hf_hub_id='timm/',
+        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),
+    'tf_efficientnet_b5.in1k': _cfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5-c6949ce9.pth',
+        #hf_hub_id='timm/',
+        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),
+
 
     'tf_efficientnet_es.in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_es-ca1afbfe.pth',
         hf_hub_id='timm/',
         mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
         input_size=(3, 224, 224), ),
     'tf_efficientnet_em.in1k': _cfg(
@@ -1235,30 +1278,14 @@
         input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904, interpolation='bilinear'),
     'tf_efficientnet_lite4.in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite4-741542c3.pth',
         hf_hub_id='timm/',
         mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
         input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.920, interpolation='bilinear'),
 
-    'tf_efficientnetv2_s.in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s-eb54923e.pth',
-        hf_hub_id='timm/',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
-        input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),
-    'tf_efficientnetv2_m.in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m-cc09e0cd.pth',
-        hf_hub_id='timm/',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
-        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
-    'tf_efficientnetv2_l.in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_l-d664b728.pth',
-        hf_hub_id='timm/',
-        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
-        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
-
     'tf_efficientnetv2_s.in21k_ft_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21ft1k-d7dafa41.pth',
         hf_hub_id='timm/',
         mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
         input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),
     'tf_efficientnetv2_m.in21k_ft_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m_21ft1k-bf41664a.pth',
@@ -1272,14 +1299,30 @@
         input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'tf_efficientnetv2_xl.in21k_ft_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_xl_in21ft1k-06c35c48.pth',
         hf_hub_id='timm/',
         mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
         input_size=(3, 384, 384), test_input_size=(3, 512, 512), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
 
+    'tf_efficientnetv2_s.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s-eb54923e.pth',
+        hf_hub_id='timm/',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),
+    'tf_efficientnetv2_m.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m-cc09e0cd.pth',
+        hf_hub_id='timm/',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+    'tf_efficientnetv2_l.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_l-d664b728.pth',
+        hf_hub_id='timm/',
+        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),
+        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+
     'tf_efficientnetv2_s.in21k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21k-6337ad01.pth',
         hf_hub_id='timm/',
         mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), num_classes=21843,
         input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),
     'tf_efficientnetv2_m.in21k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m_21k-361418a2.pth',
@@ -1366,917 +1409,948 @@
         input_size=(3, 106, 106), pool_size=(4, 4),  # int(224 * 0.475)
         url='https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_e.pth',
         hf_hub_id='timm/'),
 })
 
 
 @register_model
-def mnasnet_050(pretrained=False, **kwargs):
+def mnasnet_050(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet B1, depth multiplier of 0.5. """
     model = _gen_mnasnet_b1('mnasnet_050', 0.5, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mnasnet_075(pretrained=False, **kwargs):
+def mnasnet_075(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet B1, depth multiplier of 0.75. """
     model = _gen_mnasnet_b1('mnasnet_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mnasnet_100(pretrained=False, **kwargs):
+def mnasnet_100(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet B1, depth multiplier of 1.0. """
     model = _gen_mnasnet_b1('mnasnet_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mnasnet_b1(pretrained=False, **kwargs):
+def mnasnet_b1(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet B1, depth multiplier of 1.0. """
     return mnasnet_100(pretrained, **kwargs)
 
 
 @register_model
-def mnasnet_140(pretrained=False, **kwargs):
+def mnasnet_140(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet B1,  depth multiplier of 1.4 """
     model = _gen_mnasnet_b1('mnasnet_140', 1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def semnasnet_050(pretrained=False, **kwargs):
+def semnasnet_050(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet A1 (w/ SE), depth multiplier of 0.5 """
     model = _gen_mnasnet_a1('semnasnet_050', 0.5, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def semnasnet_075(pretrained=False, **kwargs):
+def semnasnet_075(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet A1 (w/ SE),  depth multiplier of 0.75. """
     model = _gen_mnasnet_a1('semnasnet_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def semnasnet_100(pretrained=False, **kwargs):
+def semnasnet_100(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet A1 (w/ SE), depth multiplier of 1.0. """
     model = _gen_mnasnet_a1('semnasnet_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mnasnet_a1(pretrained=False, **kwargs):
+def mnasnet_a1(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet A1 (w/ SE), depth multiplier of 1.0. """
     return semnasnet_100(pretrained, **kwargs)
 
 
 @register_model
-def semnasnet_140(pretrained=False, **kwargs):
+def semnasnet_140(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet A1 (w/ SE), depth multiplier of 1.4. """
     model = _gen_mnasnet_a1('semnasnet_140', 1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mnasnet_small(pretrained=False, **kwargs):
+def mnasnet_small(pretrained=False, **kwargs) -> EfficientNet:
     """ MNASNet Small,  depth multiplier of 1.0. """
     model = _gen_mnasnet_small('mnasnet_small', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_035(pretrained=False, **kwargs):
+def mobilenetv2_035(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 0.35 channel multiplier """
     model = _gen_mobilenet_v2('mobilenetv2_035', 0.35, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_050(pretrained=False, **kwargs):
+def mobilenetv2_050(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 0.5 channel multiplier """
     model = _gen_mobilenet_v2('mobilenetv2_050', 0.5, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_075(pretrained=False, **kwargs):
+def mobilenetv2_075(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 0.75 channel multiplier """
     model = _gen_mobilenet_v2('mobilenetv2_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_100(pretrained=False, **kwargs):
+def mobilenetv2_100(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 1.0 channel multiplier """
     model = _gen_mobilenet_v2('mobilenetv2_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_140(pretrained=False, **kwargs):
+def mobilenetv2_140(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 1.4 channel multiplier """
     model = _gen_mobilenet_v2('mobilenetv2_140', 1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_110d(pretrained=False, **kwargs):
+def mobilenetv2_110d(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 1.1 channel, 1.2 depth multipliers"""
     model = _gen_mobilenet_v2(
         'mobilenetv2_110d', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv2_120d(pretrained=False, **kwargs):
+def mobilenetv2_120d(pretrained=False, **kwargs) -> EfficientNet:
     """ MobileNet V2 w/ 1.2 channel, 1.4 depth multipliers """
     model = _gen_mobilenet_v2(
         'mobilenetv2_120d', 1.2, depth_multiplier=1.4, fix_stem_head=True, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def fbnetc_100(pretrained=False, **kwargs):
+def fbnetc_100(pretrained=False, **kwargs) -> EfficientNet:
     """ FBNet-C """
     if pretrained:
         # pretrained model trained with non-default BN epsilon
         kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     model = _gen_fbnetc('fbnetc_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def spnasnet_100(pretrained=False, **kwargs):
+def spnasnet_100(pretrained=False, **kwargs) -> EfficientNet:
     """ Single-Path NAS Pixel1"""
     model = _gen_spnasnet('spnasnet_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b0(pretrained=False, **kwargs):
+def efficientnet_b0(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B0 """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b1(pretrained=False, **kwargs):
+def efficientnet_b1(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B1 """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b2(pretrained=False, **kwargs):
+def efficientnet_b2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B2 """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b2a(pretrained=False, **kwargs):
+def efficientnet_b2a(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B2 @ 288x288 w/ 1.0 test crop"""
     # WARN this model def is deprecated, different train/test res + test crop handled by default_cfg now
     return efficientnet_b2(pretrained=pretrained, **kwargs)
 
 
 @register_model
-def efficientnet_b3(pretrained=False, **kwargs):
+def efficientnet_b3(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B3 """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b3a(pretrained=False, **kwargs):
+def efficientnet_b3a(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B3 @ 320x320 w/ 1.0 test crop-pct """
     # WARN this model def is deprecated, different train/test res + test crop handled by default_cfg now
     return efficientnet_b3(pretrained=pretrained, **kwargs)
 
 
 @register_model
-def efficientnet_b4(pretrained=False, **kwargs):
+def efficientnet_b4(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B4 """
     # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b5(pretrained=False, **kwargs):
+def efficientnet_b5(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B5 """
     # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b5', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b6(pretrained=False, **kwargs):
+def efficientnet_b6(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B6 """
     # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b6', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b7(pretrained=False, **kwargs):
+def efficientnet_b7(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B7 """
     # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b7', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b8(pretrained=False, **kwargs):
+def efficientnet_b8(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B8 """
     # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b8', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_l2(pretrained=False, **kwargs):
+def efficientnet_l2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-L2."""
     # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_l2', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)
     return model
 
 
 # FIXME experimental group cong / GroupNorm / EvoNorm experiments
 @register_model
-def efficientnet_b0_gn(pretrained=False, **kwargs):
+def efficientnet_b0_gn(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B0 + GroupNorm"""
     model = _gen_efficientnet(
         'efficientnet_b0_gn', norm_layer=partial(GroupNormAct, group_size=8), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b0_g8_gn(pretrained=False, **kwargs):
+def efficientnet_b0_g8_gn(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B0 w/ group conv + GroupNorm"""
     model = _gen_efficientnet(
         'efficientnet_b0_g8_gn', group_size=8, norm_layer=partial(GroupNormAct, group_size=8),
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b0_g16_evos(pretrained=False, **kwargs):
+def efficientnet_b0_g16_evos(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B0 w/ group 16 conv + EvoNorm"""
     model = _gen_efficientnet(
         'efficientnet_b0_g16_evos', group_size=16, channel_divisor=16,
         pretrained=pretrained, **kwargs) #norm_layer=partial(EvoNorm2dS0, group_size=16),
     return model
 
 
 @register_model
-def efficientnet_b3_gn(pretrained=False, **kwargs):
+def efficientnet_b3_gn(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B3 w/ GroupNorm """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b3_gn', channel_multiplier=1.2, depth_multiplier=1.4, channel_divisor=16,
         norm_layer=partial(GroupNormAct, group_size=16), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b3_g8_gn(pretrained=False, **kwargs):
+def efficientnet_b3_g8_gn(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B3 w/ grouped conv + BN"""
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     model = _gen_efficientnet(
         'efficientnet_b3_g8_gn', channel_multiplier=1.2, depth_multiplier=1.4, group_size=8, channel_divisor=16,
         norm_layer=partial(GroupNormAct, group_size=16), pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_es(pretrained=False, **kwargs):
+def efficientnet_es(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge Small. """
     model = _gen_efficientnet_edge(
         'efficientnet_es', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_es_pruned(pretrained=False, **kwargs):
+def efficientnet_es_pruned(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge Small Pruned. For more info: https://github.com/DeGirum/pruned-models/releases/tag/efficientnet_v1.0"""
     model = _gen_efficientnet_edge(
         'efficientnet_es_pruned', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 @register_model
-def efficientnet_em(pretrained=False, **kwargs):
+def efficientnet_em(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge-Medium. """
     model = _gen_efficientnet_edge(
         'efficientnet_em', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_el(pretrained=False, **kwargs):
+def efficientnet_el(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge-Large. """
     model = _gen_efficientnet_edge(
         'efficientnet_el', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 @register_model
-def efficientnet_el_pruned(pretrained=False, **kwargs):
+def efficientnet_el_pruned(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge-Large pruned. For more info: https://github.com/DeGirum/pruned-models/releases/tag/efficientnet_v1.0"""
     model = _gen_efficientnet_edge(
         'efficientnet_el_pruned', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 @register_model
-def efficientnet_cc_b0_4e(pretrained=False, **kwargs):
+def efficientnet_cc_b0_4e(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-CondConv-B0 w/ 8 Experts """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet_condconv(
         'efficientnet_cc_b0_4e', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_cc_b0_8e(pretrained=False, **kwargs):
+def efficientnet_cc_b0_8e(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-CondConv-B0 w/ 8 Experts """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet_condconv(
         'efficientnet_cc_b0_8e', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_cc_b1_8e(pretrained=False, **kwargs):
+def efficientnet_cc_b1_8e(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-CondConv-B1 w/ 8 Experts """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet_condconv(
         'efficientnet_cc_b1_8e', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_lite0(pretrained=False, **kwargs):
+def efficientnet_lite0(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite0 """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet_lite(
         'efficientnet_lite0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_lite1(pretrained=False, **kwargs):
+def efficientnet_lite1(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite1 """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     model = _gen_efficientnet_lite(
         'efficientnet_lite1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_lite2(pretrained=False, **kwargs):
+def efficientnet_lite2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite2 """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     model = _gen_efficientnet_lite(
         'efficientnet_lite2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_lite3(pretrained=False, **kwargs):
+def efficientnet_lite3(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite3 """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     model = _gen_efficientnet_lite(
         'efficientnet_lite3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_lite4(pretrained=False, **kwargs):
+def efficientnet_lite4(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite4 """
     # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2
     model = _gen_efficientnet_lite(
         'efficientnet_lite4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b1_pruned(pretrained=False, **kwargs):
+def efficientnet_b1_pruned(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B1 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     variant = 'efficientnet_b1_pruned'
     model = _gen_efficientnet(
         variant, channel_multiplier=1.0, depth_multiplier=1.1, pruned=True, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b2_pruned(pretrained=False, **kwargs):
+def efficientnet_b2_pruned(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B2 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'efficientnet_b2_pruned', channel_multiplier=1.1, depth_multiplier=1.2, pruned=True,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnet_b3_pruned(pretrained=False, **kwargs):
+def efficientnet_b3_pruned(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B3 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'efficientnet_b3_pruned', channel_multiplier=1.2, depth_multiplier=1.4, pruned=True,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_rw_t(pretrained=False, **kwargs):
+def efficientnetv2_rw_t(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Tiny (Custom variant, tiny not in paper). """
     model = _gen_efficientnetv2_s(
         'efficientnetv2_rw_t', channel_multiplier=0.8, depth_multiplier=0.9, rw=False, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def gc_efficientnetv2_rw_t(pretrained=False, **kwargs):
+def gc_efficientnetv2_rw_t(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Tiny w/ Global Context Attn (Custom variant, tiny not in paper). """
     model = _gen_efficientnetv2_s(
         'gc_efficientnetv2_rw_t', channel_multiplier=0.8, depth_multiplier=0.9,
         rw=False, se_layer='gc', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_rw_s(pretrained=False, **kwargs):
+def efficientnetv2_rw_s(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Small (RW variant).
     NOTE: This is my initial (pre official code release) w/ some differences.
     See efficientnetv2_s and tf_efficientnetv2_s for versions that match the official w/ PyTorch vs TF padding
     """
     model = _gen_efficientnetv2_s('efficientnetv2_rw_s', rw=True, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_rw_m(pretrained=False, **kwargs):
+def efficientnetv2_rw_m(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Medium (RW variant).
     """
     model = _gen_efficientnetv2_s(
         'efficientnetv2_rw_m', channel_multiplier=1.2, depth_multiplier=(1.2,) * 4 + (1.6,) * 2, rw=True,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_s(pretrained=False, **kwargs):
+def efficientnetv2_s(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Small. """
     model = _gen_efficientnetv2_s('efficientnetv2_s', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_m(pretrained=False, **kwargs):
+def efficientnetv2_m(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Medium. """
     model = _gen_efficientnetv2_m('efficientnetv2_m', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_l(pretrained=False, **kwargs):
+def efficientnetv2_l(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Large. """
     model = _gen_efficientnetv2_l('efficientnetv2_l', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def efficientnetv2_xl(pretrained=False, **kwargs):
+def efficientnetv2_xl(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Xtra-Large. """
     model = _gen_efficientnetv2_xl('efficientnetv2_xl', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b0(pretrained=False, **kwargs):
+def tf_efficientnet_b0(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B0. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b1(pretrained=False, **kwargs):
+def tf_efficientnet_b1(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B1. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b2(pretrained=False, **kwargs):
+def tf_efficientnet_b2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B2. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b3(pretrained=False, **kwargs):
+def tf_efficientnet_b3(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B3. Tensorflow compatible variant """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b4(pretrained=False, **kwargs):
+def tf_efficientnet_b4(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B4. Tensorflow compatible variant """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b5(pretrained=False, **kwargs):
+def tf_efficientnet_b5(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B5. Tensorflow compatible variant """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b5', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b6(pretrained=False, **kwargs):
+def tf_efficientnet_b6(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B6. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.5
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b6', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b7(pretrained=False, **kwargs):
+def tf_efficientnet_b7(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B7. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.5
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b7', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_b8(pretrained=False, **kwargs):
+def tf_efficientnet_b8(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-B8. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.5
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_b8', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_l2(pretrained=False, **kwargs):
+def tf_efficientnet_l2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-L2 NoisyStudent. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.5
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet(
         'tf_efficientnet_l2', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_es(pretrained=False, **kwargs):
+def tf_efficientnet_es(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge Small. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_edge(
         'tf_efficientnet_es', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_em(pretrained=False, **kwargs):
+def tf_efficientnet_em(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge-Medium. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_edge(
         'tf_efficientnet_em', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_el(pretrained=False, **kwargs):
+def tf_efficientnet_el(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Edge-Large. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_edge(
         'tf_efficientnet_el', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_cc_b0_4e(pretrained=False, **kwargs):
+def tf_efficientnet_cc_b0_4e(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-CondConv-B0 w/ 4 Experts. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_condconv(
         'tf_efficientnet_cc_b0_4e', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_cc_b0_8e(pretrained=False, **kwargs):
+def tf_efficientnet_cc_b0_8e(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-CondConv-B0 w/ 8 Experts. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_condconv(
         'tf_efficientnet_cc_b0_8e', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_cc_b1_8e(pretrained=False, **kwargs):
+def tf_efficientnet_cc_b1_8e(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-CondConv-B1 w/ 8 Experts. Tensorflow compatible variant """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_condconv(
         'tf_efficientnet_cc_b1_8e', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,
         pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_lite0(pretrained=False, **kwargs):
+def tf_efficientnet_lite0(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite0 """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_lite(
         'tf_efficientnet_lite0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_lite1(pretrained=False, **kwargs):
+def tf_efficientnet_lite1(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite1 """
     # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_lite(
         'tf_efficientnet_lite1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_lite2(pretrained=False, **kwargs):
+def tf_efficientnet_lite2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite2 """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_lite(
         'tf_efficientnet_lite2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_lite3(pretrained=False, **kwargs):
+def tf_efficientnet_lite3(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite3 """
     # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_lite(
         'tf_efficientnet_lite3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnet_lite4(pretrained=False, **kwargs):
+def tf_efficientnet_lite4(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-Lite4 """
     # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnet_lite(
         'tf_efficientnet_lite4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_s(pretrained=False, **kwargs):
+def tf_efficientnetv2_s(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Small. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_s('tf_efficientnetv2_s', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_m(pretrained=False, **kwargs):
+def tf_efficientnetv2_m(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Medium. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_m('tf_efficientnetv2_m', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_l(pretrained=False, **kwargs):
+def tf_efficientnetv2_l(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Large. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_l('tf_efficientnetv2_l', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_xl(pretrained=False, **kwargs):
+def tf_efficientnetv2_xl(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2 Xtra-Large. Tensorflow compatible variant
     """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_xl('tf_efficientnetv2_xl', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_b0(pretrained=False, **kwargs):
+def tf_efficientnetv2_b0(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2-B0. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_base('tf_efficientnetv2_b0', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_b1(pretrained=False, **kwargs):
+def tf_efficientnetv2_b1(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2-B1. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_base(
         'tf_efficientnetv2_b1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_b2(pretrained=False, **kwargs):
+def tf_efficientnetv2_b2(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2-B2. Tensorflow compatible variant  """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_base(
         'tf_efficientnetv2_b2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_efficientnetv2_b3(pretrained=False, **kwargs):
+def tf_efficientnetv2_b3(pretrained=False, **kwargs) -> EfficientNet:
     """ EfficientNet-V2-B3. Tensorflow compatible variant """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_efficientnetv2_base(
         'tf_efficientnetv2_b3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mixnet_s(pretrained=False, **kwargs):
+def mixnet_s(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Small model.
     """
     model = _gen_mixnet_s(
         'mixnet_s', channel_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mixnet_m(pretrained=False, **kwargs):
+def mixnet_m(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Medium model.
     """
     model = _gen_mixnet_m(
         'mixnet_m', channel_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mixnet_l(pretrained=False, **kwargs):
+def mixnet_l(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Large model.
     """
     model = _gen_mixnet_m(
         'mixnet_l', channel_multiplier=1.3, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mixnet_xl(pretrained=False, **kwargs):
+def mixnet_xl(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Extra-Large model.
     Not a paper spec, experimental def by RW w/ depth scaling.
     """
     model = _gen_mixnet_m(
         'mixnet_xl', channel_multiplier=1.6, depth_multiplier=1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mixnet_xxl(pretrained=False, **kwargs):
+def mixnet_xxl(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Double Extra Large model.
     Not a paper spec, experimental def by RW w/ depth scaling.
     """
     model = _gen_mixnet_m(
         'mixnet_xxl', channel_multiplier=2.4, depth_multiplier=1.3, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mixnet_s(pretrained=False, **kwargs):
+def tf_mixnet_s(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Small model. Tensorflow compatible variant
     """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mixnet_s(
         'tf_mixnet_s', channel_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mixnet_m(pretrained=False, **kwargs):
+def tf_mixnet_m(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Medium model. Tensorflow compatible variant
     """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mixnet_m(
         'tf_mixnet_m', channel_multiplier=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mixnet_l(pretrained=False, **kwargs):
+def tf_mixnet_l(pretrained=False, **kwargs) -> EfficientNet:
     """Creates a MixNet Large model. Tensorflow compatible variant
     """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mixnet_m(
         'tf_mixnet_l', channel_multiplier=1.3, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tinynet_a(pretrained=False, **kwargs):
+def tinynet_a(pretrained=False, **kwargs) -> EfficientNet:
     model = _gen_tinynet('tinynet_a', 1.0, 1.2, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tinynet_b(pretrained=False, **kwargs):
+def tinynet_b(pretrained=False, **kwargs) -> EfficientNet:
     model = _gen_tinynet('tinynet_b', 0.75, 1.1, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tinynet_c(pretrained=False, **kwargs):
+def tinynet_c(pretrained=False, **kwargs) -> EfficientNet:
     model = _gen_tinynet('tinynet_c', 0.54, 0.85, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tinynet_d(pretrained=False, **kwargs):
+def tinynet_d(pretrained=False, **kwargs) -> EfficientNet:
     model = _gen_tinynet('tinynet_d', 0.54, 0.695, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tinynet_e(pretrained=False, **kwargs):
+def tinynet_e(pretrained=False, **kwargs) -> EfficientNet:
     model = _gen_tinynet('tinynet_e', 0.51, 0.6, pretrained=pretrained, **kwargs)
     return model
+
+
+register_model_deprecations(__name__, {
+    'tf_efficientnet_b0_ap': 'tf_efficientnet_b0.ap_in1k',
+    'tf_efficientnet_b1_ap': 'tf_efficientnet_b1.ap_in1k',
+    'tf_efficientnet_b2_ap': 'tf_efficientnet_b2.ap_in1k',
+    'tf_efficientnet_b3_ap': 'tf_efficientnet_b3.ap_in1k',
+    'tf_efficientnet_b4_ap': 'tf_efficientnet_b4.ap_in1k',
+    'tf_efficientnet_b5_ap': 'tf_efficientnet_b5.ap_in1k',
+    'tf_efficientnet_b6_ap': 'tf_efficientnet_b6.ap_in1k',
+    'tf_efficientnet_b7_ap': 'tf_efficientnet_b7.ap_in1k',
+    'tf_efficientnet_b8_ap': 'tf_efficientnet_b8.ap_in1k',
+    'tf_efficientnet_b0_ns': 'tf_efficientnet_b0.ns_jft_in1k',
+    'tf_efficientnet_b1_ns': 'tf_efficientnet_b1.ns_jft_in1k',
+    'tf_efficientnet_b2_ns': 'tf_efficientnet_b2.ns_jft_in1k',
+    'tf_efficientnet_b3_ns': 'tf_efficientnet_b3.ns_jft_in1k',
+    'tf_efficientnet_b4_ns': 'tf_efficientnet_b4.ns_jft_in1k',
+    'tf_efficientnet_b5_ns': 'tf_efficientnet_b5.ns_jft_in1k',
+    'tf_efficientnet_b6_ns': 'tf_efficientnet_b6.ns_jft_in1k',
+    'tf_efficientnet_b7_ns': 'tf_efficientnet_b7.ns_jft_in1k',
+    'tf_efficientnet_l2_ns_475': 'tf_efficientnet_l2.ns_jft_in1k_475',
+    'tf_efficientnet_l2_ns': 'tf_efficientnet_l2.ns_jft_in1k',
+    'tf_efficientnetv2_s_in21ft1k': 'tf_efficientnetv2_s.in21k_ft_in1k',
+    'tf_efficientnetv2_m_in21ft1k': 'tf_efficientnetv2_m.in21k_ft_in1k',
+    'tf_efficientnetv2_l_in21ft1k': 'tf_efficientnetv2_l.in21k_ft_in1k',
+    'tf_efficientnetv2_xl_in21ft1k': 'tf_efficientnetv2_xl.in21k_ft_in1k',
+    'tf_efficientnetv2_s_in21k': 'tf_efficientnetv2_s.in21k',
+    'tf_efficientnetv2_m_in21k': 'tf_efficientnetv2_m.in21k',
+    'tf_efficientnetv2_l_in21k': 'tf_efficientnetv2_l.in21k',
+    'tf_efficientnetv2_xl_in21k': 'tf_efficientnetv2_xl.in21k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/gcvit.py` & `timm-0.9.0/timm/models/gcvit.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,49 +25,23 @@
 
 import torch
 import torch.nn as nn
 import torch.utils.checkpoint as checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, to_2tuple, to_ntuple, Mlp, ClassifierHead, LayerNorm2d, \
-    get_attn, get_act_layer, get_norm_layer, _assert
+    get_attn, get_act_layer, get_norm_layer, RelPosBias, _assert
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
 from ._manipulate import named_apply
-from ._registry import register_model
-from .vision_transformer_relpos import RelPosBias  # FIXME move to common location
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['GlobalContextVit']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv1', 'classifier': 'head.fc',
-        'fixed_input_size': True,
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'gcvit_xxtiny': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_xxtiny_224_nvidia-d1d86009.pth'),
-    'gcvit_xtiny': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_xtiny_224_nvidia-274b92b7.pth'),
-    'gcvit_tiny': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_tiny_224_nvidia-ac783954.pth'),
-    'gcvit_small': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_small_224_nvidia-4e98afa2.pth'),
-    'gcvit_base': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_base_224_nvidia-f009139b.pth'),
-}
-
-
 class MbConvBlock(nn.Module):
     """ A depthwise separable / fused mbconv style residual block with SE, `no norm.
     """
     def __init__(
             self,
             in_chs,
             out_chs=None,
@@ -218,15 +192,15 @@
             q = q_global.repeat(B // q_global.shape[0], 1, 1, 1)
             q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
         else:
             qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
             q, k, v = qkv.unbind(0)
         q = q * self.scale
 
-        attn = (q @ k.transpose(-2, -1))
+        attn = q @ k.transpose(-2, -1).contiguous()  # NOTE contiguous() fixes an odd jit bug in PyTorch 2.0
         attn = self.rel_pos(attn)
         attn = attn.softmax(dim=-1)
         attn = self.attn_drop(attn)
 
         x = (attn @ v).transpose(1, 2).reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
@@ -239,17 +213,17 @@
     windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)
     return windows
 
 
 @register_notrace_function  # reason: int argument is a Proxy
 def window_reverse(windows, window_size: Tuple[int, int], img_size: Tuple[int, int]):
     H, W = img_size
-    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))
-    x = windows.view(B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1)
-    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
+    C = windows.shape[-1]
+    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)
+    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)
     return x
 
 
 class LayerScale(nn.Module):
     def __init__(self, dim, init_values=1e-5, inplace=False):
         super().__init__()
         self.inplace = inplace
@@ -538,55 +512,80 @@
 def _create_gcvit(variant, pretrained=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
     model = build_model_with_cfg(GlobalContextVit, variant, pretrained, **kwargs)
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv1', 'classifier': 'head.fc',
+        'fixed_input_size': True,
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'gcvit_xxtiny.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_xxtiny_224_nvidia-d1d86009.pth'),
+    'gcvit_xtiny.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_xtiny_224_nvidia-274b92b7.pth'),
+    'gcvit_tiny.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_tiny_224_nvidia-ac783954.pth'),
+    'gcvit_small.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_small_224_nvidia-4e98afa2.pth'),
+    'gcvit_base.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_base_224_nvidia-f009139b.pth'),
+})
+
+
 @register_model
-def gcvit_xxtiny(pretrained=False, **kwargs):
+def gcvit_xxtiny(pretrained=False, **kwargs) -> GlobalContextVit:
     model_kwargs = dict(
         depths=(2, 2, 6, 2),
         num_heads=(2, 4, 8, 16),
         **kwargs)
     return _create_gcvit('gcvit_xxtiny', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def gcvit_xtiny(pretrained=False, **kwargs):
+def gcvit_xtiny(pretrained=False, **kwargs) -> GlobalContextVit:
     model_kwargs = dict(
         depths=(3, 4, 6, 5),
         num_heads=(2, 4, 8, 16),
         **kwargs)
     return _create_gcvit('gcvit_xtiny', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def gcvit_tiny(pretrained=False, **kwargs):
+def gcvit_tiny(pretrained=False, **kwargs) -> GlobalContextVit:
     model_kwargs = dict(
         depths=(3, 4, 19, 5),
         num_heads=(2, 4, 8, 16),
         **kwargs)
     return _create_gcvit('gcvit_tiny', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def gcvit_small(pretrained=False, **kwargs):
+def gcvit_small(pretrained=False, **kwargs) -> GlobalContextVit:
     model_kwargs = dict(
         depths=(3, 4, 19, 5),
         num_heads=(3, 6, 12, 24),
         embed_dim=96,
         mlp_ratio=2,
         layer_scale=1e-5,
         **kwargs)
     return _create_gcvit('gcvit_small', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def gcvit_base(pretrained=False, **kwargs):
+def gcvit_base(pretrained=False, **kwargs) -> GlobalContextVit:
     model_kwargs = dict(
         depths=(3, 4, 19, 5),
         num_heads=(4, 8, 16, 32),
         embed_dim=128,
         mlp_ratio=2,
         layer_scale=1e-5,
         **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/ghostnet.py` & `timm-0.9.0/timm/models/ghostnet.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,71 +12,70 @@
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d, Linear, make_divisible
 from ._builder import build_model_with_cfg
 from ._efficientnet_blocks import SqueezeExcite, ConvBnAct
 from ._manipulate import checkpoint_seq
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['GhostNet']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv_stem', 'classifier': 'classifier',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'ghostnet_050': _cfg(url=''),
-    'ghostnet_100': _cfg(
-        url='https://github.com/huawei-noah/CV-backbones/releases/download/ghostnet_pth/ghostnet_1x.pth'),
-    'ghostnet_130': _cfg(url=''),
-}
-
-
 _SE_LAYER = partial(SqueezeExcite, gate_layer='hard_sigmoid', rd_round_fn=partial(make_divisible, divisor=4))
 
 
 class GhostModule(nn.Module):
-    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):
+    def __init__(
+            self,
+            in_chs,
+            out_chs,
+            kernel_size=1,
+            ratio=2,
+            dw_size=3,
+            stride=1,
+            relu=True,
+    ):
         super(GhostModule, self).__init__()
-        self.oup = oup
-        init_channels = math.ceil(oup / ratio)
-        new_channels = init_channels * (ratio - 1)
+        self.out_chs = out_chs
+        init_chs = math.ceil(out_chs / ratio)
+        new_chs = init_chs * (ratio - 1)
 
         self.primary_conv = nn.Sequential(
-            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),
-            nn.BatchNorm2d(init_channels),
-            nn.ReLU(inplace=True) if relu else nn.Sequential(),
+            nn.Conv2d(in_chs, init_chs, kernel_size, stride, kernel_size // 2, bias=False),
+            nn.BatchNorm2d(init_chs),
+            nn.ReLU(inplace=True) if relu else nn.Identity(),
         )
 
         self.cheap_operation = nn.Sequential(
-            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
-            nn.BatchNorm2d(new_channels),
-            nn.ReLU(inplace=True) if relu else nn.Sequential(),
+            nn.Conv2d(init_chs, new_chs, dw_size, 1, dw_size//2, groups=init_chs, bias=False),
+            nn.BatchNorm2d(new_chs),
+            nn.ReLU(inplace=True) if relu else nn.Identity(),
         )
 
     def forward(self, x):
         x1 = self.primary_conv(x)
         x2 = self.cheap_operation(x1)
         out = torch.cat([x1, x2], dim=1)
-        return out[:, :self.oup, :, :]
+        return out[:, :self.out_chs, :, :]
 
 
 class GhostBottleneck(nn.Module):
     """ Ghost bottleneck w/ optional SE"""
 
-    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,
-                 stride=1, act_layer=nn.ReLU, se_ratio=0.):
+    def __init__(
+            self,
+            in_chs,
+            mid_chs,
+            out_chs,
+            dw_kernel_size=3,
+            stride=1,
+            act_layer=nn.ReLU,
+            se_ratio=0.,
+    ):
         super(GhostBottleneck, self).__init__()
         has_se = se_ratio is not None and se_ratio > 0.
         self.stride = stride
 
         # Point-wise expansion
         self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)
 
@@ -129,15 +128,23 @@
         
         x += self.shortcut(shortcut)
         return x
 
 
 class GhostNet(nn.Module):
     def __init__(
-            self, cfgs, num_classes=1000, width=1.0, in_chans=3, output_stride=32, global_pool='avg', drop_rate=0.2):
+            self,
+            cfgs,
+            num_classes=1000,
+            width=1.0,
+            in_chans=3,
+            output_stride=32,
+            global_pool='avg',
+            drop_rate=0.2,
+    ):
         super(GhostNet, self).__init__()
         # setting of inverted residual blocks
         assert output_stride == 32, 'only output_stride==32 is valid, dilation not supported'
         self.cfgs = cfgs
         self.num_classes = num_classes
         self.drop_rate = drop_rate
         self.grad_checkpointing = False
@@ -271,31 +278,52 @@
     ]
     model_kwargs = dict(
         cfgs=cfgs,
         width=width,
         **kwargs,
     )
     return build_model_with_cfg(
-        GhostNet, variant, pretrained,
+        GhostNet,
+        variant,
+        pretrained,
         feature_cfg=dict(flatten_sequential=True),
-        **model_kwargs)
+        **model_kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv_stem', 'classifier': 'classifier',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'ghostnet_050.untrained': _cfg(),
+    'ghostnet_100.in1k': _cfg(
+        url='https://github.com/huawei-noah/CV-backbones/releases/download/ghostnet_pth/ghostnet_1x.pth'),
+    'ghostnet_130.untrained': _cfg(),
+})
 
 
 @register_model
-def ghostnet_050(pretrained=False, **kwargs):
+def ghostnet_050(pretrained=False, **kwargs) -> GhostNet:
     """ GhostNet-0.5x """
     model = _create_ghostnet('ghostnet_050', width=0.5, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def ghostnet_100(pretrained=False, **kwargs):
+def ghostnet_100(pretrained=False, **kwargs) -> GhostNet:
     """ GhostNet-1.0x """
     model = _create_ghostnet('ghostnet_100', width=1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def ghostnet_130(pretrained=False, **kwargs):
+def ghostnet_130(pretrained=False, **kwargs) -> GhostNet:
     """ GhostNet-1.3x """
     model = _create_ghostnet('ghostnet_130', width=1.3, pretrained=pretrained, **kwargs)
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/hardcorenas.py` & `timm-0.9.0/timm/models/hardcorenas.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,40 +3,20 @@
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from ._builder import build_model_with_cfg
 from ._builder import pretrained_cfg_for_features
 from ._efficientnet_blocks import SqueezeExcite
 from ._efficientnet_builder import decode_arch_def, resolve_act_layer, resolve_bn_args, round_channels
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .mobilenetv3 import MobileNetV3, MobileNetV3Features
 
 __all__ = []  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv_stem', 'classifier': 'classifier',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'hardcorenas_a': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/hardcorenas_a_green_38ms_75_9-31dc7186.pth'),
-    'hardcorenas_b': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/hardcorenas_b_green_40ms_76_5-32d91ff2.pth'),
-    'hardcorenas_c': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/hardcorenas_c_green_44ms_77_1-631a0983.pth'),
-    'hardcorenas_d': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/hardcorenas_d_green_50ms_77_4-998d9d7a.pth'),
-    'hardcorenas_e': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/hardcorenas_e_green_55ms_77_9-482886a3.pth'),
-    'hardcorenas_f': _cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/hardcorenas_f_green_60ms_78_1-14b9e780.pth'),
-}
-
-
 def _gen_hardcorenas(pretrained, variant, arch_def, **kwargs):
     """Creates a hardcorenas model
 
     Ref impl: https://github.com/Alibaba-MIIL/HardCoReNAS
     Paper: https://arxiv.org/abs/2102.11646
 
     """
@@ -56,94 +36,117 @@
     model_cls = MobileNetV3
     kwargs_filter = None
     if model_kwargs.pop('features_only', False):
         features_only = True
         kwargs_filter = ('num_classes', 'num_features', 'global_pool', 'head_conv', 'head_bias', 'global_pool')
         model_cls = MobileNetV3Features
     model = build_model_with_cfg(
-        model_cls, variant, pretrained,
+        model_cls,
+        variant,
+        pretrained,
         pretrained_strict=not features_only,
         kwargs_filter=kwargs_filter,
-        **model_kwargs)
+        **model_kwargs,
+    )
     if features_only:
         model.default_cfg = pretrained_cfg_for_features(model.default_cfg)
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv_stem', 'classifier': 'classifier',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'hardcorenas_a.miil_green_in1k': _cfg(hf_hub_id='timm/'),
+    'hardcorenas_b.miil_green_in1k': _cfg(hf_hub_id='timm/'),
+    'hardcorenas_c.miil_green_in1k': _cfg(hf_hub_id='timm/'),
+    'hardcorenas_d.miil_green_in1k': _cfg(hf_hub_id='timm/'),
+    'hardcorenas_e.miil_green_in1k': _cfg(hf_hub_id='timm/'),
+    'hardcorenas_f.miil_green_in1k': _cfg(hf_hub_id='timm/'),
+})
+
+
 @register_model
-def hardcorenas_a(pretrained=False, **kwargs):
+def hardcorenas_a(pretrained=False, **kwargs) -> MobileNetV3:
     """ hardcorenas_A """
     arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],
                 ['ir_r1_k5_s2_e3_c40_nre', 'ir_r1_k5_s1_e6_c40_nre_se0.25'],
                 ['ir_r1_k5_s2_e6_c80_se0.25', 'ir_r1_k5_s1_e6_c80_se0.25'],
                 ['ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25'],
                 ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]
     model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_a', arch_def=arch_def, **kwargs)
     return model
 
 
 @register_model
-def hardcorenas_b(pretrained=False, **kwargs):
+def hardcorenas_b(pretrained=False, **kwargs) -> MobileNetV3:
     """ hardcorenas_B """
     arch_def = [['ds_r1_k3_s1_e1_c16_nre'],
                 ['ir_r1_k5_s2_e3_c24_nre', 'ir_r1_k5_s1_e3_c24_nre_se0.25', 'ir_r1_k3_s1_e3_c24_nre'],
                 ['ir_r1_k5_s2_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre'],
                 ['ir_r1_k5_s2_e3_c80', 'ir_r1_k5_s1_e3_c80', 'ir_r1_k3_s1_e3_c80', 'ir_r1_k3_s1_e3_c80'],
                 ['ir_r1_k5_s1_e3_c112', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112'],
                 ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k3_s1_e3_c192_se0.25'],
                 ['cn_r1_k1_s1_c960']]
     model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_b', arch_def=arch_def, **kwargs)
     return model
 
 
 @register_model
-def hardcorenas_c(pretrained=False, **kwargs):
+def hardcorenas_c(pretrained=False, **kwargs) -> MobileNetV3:
     """ hardcorenas_C """
     arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],
                 ['ir_r1_k5_s2_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre',
                  'ir_r1_k5_s1_e3_c40_nre'],
                 ['ir_r1_k5_s2_e4_c80', 'ir_r1_k5_s1_e6_c80_se0.25', 'ir_r1_k3_s1_e3_c80', 'ir_r1_k3_s1_e3_c80'],
                 ['ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112'],
                 ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k3_s1_e3_c192_se0.25'],
                 ['cn_r1_k1_s1_c960']]
     model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_c', arch_def=arch_def, **kwargs)
     return model
 
 
 @register_model
-def hardcorenas_d(pretrained=False, **kwargs):
+def hardcorenas_d(pretrained=False, **kwargs) -> MobileNetV3:
     """ hardcorenas_D """
     arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre_se0.25', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],
                 ['ir_r1_k5_s2_e3_c40_nre_se0.25', 'ir_r1_k5_s1_e4_c40_nre_se0.25', 'ir_r1_k3_s1_e3_c40_nre_se0.25'],
                 ['ir_r1_k5_s2_e4_c80_se0.25', 'ir_r1_k3_s1_e3_c80_se0.25', 'ir_r1_k3_s1_e3_c80_se0.25',
                  'ir_r1_k3_s1_e3_c80_se0.25'],
                 ['ir_r1_k3_s1_e4_c112_se0.25', 'ir_r1_k5_s1_e4_c112_se0.25', 'ir_r1_k3_s1_e3_c112_se0.25',
                  'ir_r1_k5_s1_e3_c112_se0.25'],
                 ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25',
                  'ir_r1_k3_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]
     model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_d', arch_def=arch_def, **kwargs)
     return model
 
 
 @register_model
-def hardcorenas_e(pretrained=False, **kwargs):
+def hardcorenas_e(pretrained=False, **kwargs) -> MobileNetV3:
     """ hardcorenas_E """
     arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre_se0.25', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],
                 ['ir_r1_k5_s2_e6_c40_nre_se0.25', 'ir_r1_k5_s1_e4_c40_nre_se0.25', 'ir_r1_k5_s1_e4_c40_nre_se0.25',
                  'ir_r1_k3_s1_e3_c40_nre_se0.25'], ['ir_r1_k5_s2_e4_c80_se0.25', 'ir_r1_k3_s1_e6_c80_se0.25'],
                 ['ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25',
                  'ir_r1_k5_s1_e3_c112_se0.25'],
                 ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25',
                  'ir_r1_k3_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]
     model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_e', arch_def=arch_def, **kwargs)
     return model
 
 
 @register_model
-def hardcorenas_f(pretrained=False, **kwargs):
+def hardcorenas_f(pretrained=False, **kwargs) -> MobileNetV3:
     """ hardcorenas_F """
     arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre_se0.25', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],
                 ['ir_r1_k5_s2_e6_c40_nre_se0.25', 'ir_r1_k5_s1_e6_c40_nre_se0.25'],
                 ['ir_r1_k5_s2_e6_c80_se0.25', 'ir_r1_k5_s1_e6_c80_se0.25', 'ir_r1_k3_s1_e3_c80_se0.25',
                  'ir_r1_k3_s1_e3_c80_se0.25'],
                 ['ir_r1_k3_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25',
                  'ir_r1_k3_s1_e3_c112_se0.25'],
```

### Comparing `timm-0.8.6.dev0/timm/models/hrnet.py` & `timm-0.9.0/timm/models/hrnet.py`

 * *Files 27% similar despite different names*

```diff
@@ -15,433 +15,418 @@
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
 from ._builder import build_model_with_cfg, pretrained_cfg_for_features
 from ._features import FeatureInfo
-from ._registry import register_model
-from .resnet import BasicBlock, Bottleneck  # leveraging ResNet blocks w/ additional features like SE
+from ._registry import register_model, generate_default_cfgs
+from .resnet import BasicBlock, Bottleneck  # leveraging ResNet block_types w/ additional features like SE
 
 __all__ = ['HighResolutionNet', 'HighResolutionNetFeatures']  # model_registry will add each entrypoint fn to this
 
 _BN_MOMENTUM = 0.1
 _logger = logging.getLogger(__name__)
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv1', 'classifier': 'classifier',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'hrnet_w18_small': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnet_w18_small_v1-f460c6bc.pth'),
-    'hrnet_w18_small_v2': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnet_w18_small_v2-4c50a8cb.pth'),
-    'hrnet_w18': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w18-8cb57bb9.pth'),
-    'hrnet_w30': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w30-8d7f8dab.pth'),
-    'hrnet_w32': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w32-90d8c5fb.pth'),
-    'hrnet_w40': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w40-7cd397a4.pth'),
-    'hrnet_w44': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w44-c9ac8c18.pth'),
-    'hrnet_w48': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w48-abd2e6ab.pth'),
-    'hrnet_w64': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w64-b47cc881.pth'),
-}
-
 cfg_cls = dict(
     hrnet_w18_small=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(1,),
-            NUM_CHANNELS=(32,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(2, 2),
-            NUM_CHANNELS=(16, 32),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(2, 2, 2),
-            NUM_CHANNELS=(16, 32, 64),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(2, 2, 2, 2),
-            NUM_CHANNELS=(16, 32, 64, 128),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(1,),
+            num_channels=(32,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(2, 2),
+            num_channels=(16, 32),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=1,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(2, 2, 2),
+            num_channels=(16, 32, 64),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=1,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(2, 2, 2, 2),
+            num_channels=(16, 32, 64, 128),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w18_small_v2=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(2,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(2, 2),
-            NUM_CHANNELS=(18, 36),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(2, 2, 2),
-            NUM_CHANNELS=(18, 36, 72),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=2,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(2, 2, 2, 2),
-            NUM_CHANNELS=(18, 36, 72, 144),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(2,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(2, 2),
+            num_channels=(18, 36),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=3,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(2, 2, 2),
+            num_channels=(18, 36, 72),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=2,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(2, 2, 2, 2),
+            num_channels=(18, 36, 72, 144),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w18=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(18, 36),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(18, 36, 72),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(18, 36, 72, 144),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(18, 36),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(18, 36, 72),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(18, 36, 72, 144),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w30=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(30, 60),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(30, 60, 120),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(30, 60, 120, 240),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(30, 60),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(30, 60, 120),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(30, 60, 120, 240),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w32=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(32, 64),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(32, 64, 128),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(32, 64, 128, 256),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(32, 64),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(32, 64, 128),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(32, 64, 128, 256),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w40=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(40, 80),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(40, 80, 160),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(40, 80, 160, 320),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(40, 80),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(40, 80, 160),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(40, 80, 160, 320),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w44=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(44, 88),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(44, 88, 176),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(44, 88, 176, 352),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(44, 88),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(44, 88, 176),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(44, 88, 176, 352),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w48=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(48, 96),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(48, 96, 192),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(48, 96, 192, 384),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(48, 96),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(48, 96, 192),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(48, 96, 192, 384),
+            fuse_method='SUM',
         ),
     ),
 
     hrnet_w64=dict(
-        STEM_WIDTH=64,
-        STAGE1=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=1,
-            BLOCK='BOTTLENECK',
-            NUM_BLOCKS=(4,),
-            NUM_CHANNELS=(64,),
-            FUSE_METHOD='SUM',
-        ),
-        STAGE2=dict(
-            NUM_MODULES=1,
-            NUM_BRANCHES=2,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4),
-            NUM_CHANNELS=(64, 128),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE3=dict(
-            NUM_MODULES=4,
-            NUM_BRANCHES=3,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4),
-            NUM_CHANNELS=(64, 128, 256),
-            FUSE_METHOD='SUM'
-        ),
-        STAGE4=dict(
-            NUM_MODULES=3,
-            NUM_BRANCHES=4,
-            BLOCK='BASIC',
-            NUM_BLOCKS=(4, 4, 4, 4),
-            NUM_CHANNELS=(64, 128, 256, 512),
-            FUSE_METHOD='SUM',
+        stem_width=64,
+        stage1=dict(
+            num_modules=1,
+            num_branches=1,
+            block_type='BOTTLENECK',
+            num_blocks=(4,),
+            num_channels=(64,),
+            fuse_method='SUM',
+        ),
+        stage2=dict(
+            num_modules=1,
+            num_branches=2,
+            block_type='BASIC',
+            num_blocks=(4, 4),
+            num_channels=(64, 128),
+            fuse_method='SUM'
+        ),
+        stage3=dict(
+            num_modules=4,
+            num_branches=3,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4),
+            num_channels=(64, 128, 256),
+            fuse_method='SUM'
+        ),
+        stage4=dict(
+            num_modules=3,
+            num_branches=4,
+            block_type='BASIC',
+            num_blocks=(4, 4, 4, 4),
+            num_channels=(64, 128, 256, 512),
+            fuse_method='SUM',
         ),
     )
 )
 
 
 class HighResolutionModule(nn.Module):
-    def __init__(self, num_branches, blocks, num_blocks, num_in_chs,
-                 num_channels, fuse_method, multi_scale_output=True):
+    def __init__(
+            self,
+            num_branches,
+            block_types,
+            num_blocks,
+            num_in_chs,
+            num_channels,
+            fuse_method,
+            multi_scale_output=True,
+    ):
         super(HighResolutionModule, self).__init__()
         self._check_branches(
-            num_branches, blocks, num_blocks, num_in_chs, num_channels)
+            num_branches,
+            block_types,
+            num_blocks,
+            num_in_chs,
+            num_channels,
+        )
 
         self.num_in_chs = num_in_chs
         self.fuse_method = fuse_method
         self.num_branches = num_branches
 
         self.multi_scale_output = multi_scale_output
 
         self.branches = self._make_branches(
-            num_branches, blocks, num_blocks, num_channels)
+            num_branches,
+            block_types,
+            num_blocks,
+            num_channels,
+        )
         self.fuse_layers = self._make_fuse_layers()
         self.fuse_act = nn.ReLU(False)
 
-    def _check_branches(self, num_branches, blocks, num_blocks, num_in_chs, num_channels):
+    def _check_branches(self, num_branches, block_types, num_blocks, num_in_chs, num_channels):
         error_msg = ''
         if num_branches != len(num_blocks):
-            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(num_branches, len(num_blocks))
+            error_msg = 'num_branches({}) <> num_blocks({})'.format(num_branches, len(num_blocks))
         elif num_branches != len(num_channels):
-            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(num_branches, len(num_channels))
+            error_msg = 'num_branches({}) <> num_channels({})'.format(num_branches, len(num_channels))
         elif num_branches != len(num_in_chs):
-            error_msg = 'NUM_BRANCHES({}) <> num_in_chs({})'.format(num_branches, len(num_in_chs))
+            error_msg = 'num_branches({}) <> num_in_chs({})'.format(num_branches, len(num_in_chs))
         if error_msg:
             _logger.error(error_msg)
             raise ValueError(error_msg)
 
-    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):
+    def _make_one_branch(self, branch_index, block_type, num_blocks, num_channels, stride=1):
         downsample = None
-        if stride != 1 or self.num_in_chs[branch_index] != num_channels[branch_index] * block.expansion:
+        if stride != 1 or self.num_in_chs[branch_index] != num_channels[branch_index] * block_type.expansion:
             downsample = nn.Sequential(
                 nn.Conv2d(
-                    self.num_in_chs[branch_index], num_channels[branch_index] * block.expansion,
+                    self.num_in_chs[branch_index], num_channels[branch_index] * block_type.expansion,
                     kernel_size=1, stride=stride, bias=False),
-                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=_BN_MOMENTUM),
+                nn.BatchNorm2d(num_channels[branch_index] * block_type.expansion, momentum=_BN_MOMENTUM),
             )
 
-        layers = [block(self.num_in_chs[branch_index], num_channels[branch_index], stride, downsample)]
-        self.num_in_chs[branch_index] = num_channels[branch_index] * block.expansion
+        layers = [block_type(self.num_in_chs[branch_index], num_channels[branch_index], stride, downsample)]
+        self.num_in_chs[branch_index] = num_channels[branch_index] * block_type.expansion
         for i in range(1, num_blocks[branch_index]):
-            layers.append(block(self.num_in_chs[branch_index], num_channels[branch_index]))
+            layers.append(block_type(self.num_in_chs[branch_index], num_channels[branch_index]))
 
         return nn.Sequential(*layers)
 
-    def _make_branches(self, num_branches, block, num_blocks, num_channels):
+    def _make_branches(self, num_branches, block_type, num_blocks, num_channels):
         branches = []
         for i in range(num_branches):
-            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))
+            branches.append(self._make_one_branch(i, block_type, num_blocks, num_channels))
 
         return nn.ModuleList(branches)
 
     def _make_fuse_layers(self):
         if self.num_branches == 1:
             return nn.Identity()
 
@@ -458,158 +443,209 @@
                         nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))
                 elif j == i:
                     fuse_layer.append(nn.Identity())
                 else:
                     conv3x3s = []
                     for k in range(i - j):
                         if k == i - j - 1:
-                            num_outchannels_conv3x3 = num_in_chs[i]
+                            num_out_chs_conv3x3 = num_in_chs[i]
                             conv3x3s.append(nn.Sequential(
-                                nn.Conv2d(num_in_chs[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),
-                                nn.BatchNorm2d(num_outchannels_conv3x3, momentum=_BN_MOMENTUM)))
+                                nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False),
+                                nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM)
+                            ))
                         else:
-                            num_outchannels_conv3x3 = num_in_chs[j]
+                            num_out_chs_conv3x3 = num_in_chs[j]
                             conv3x3s.append(nn.Sequential(
-                                nn.Conv2d(num_in_chs[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),
-                                nn.BatchNorm2d(num_outchannels_conv3x3, momentum=_BN_MOMENTUM),
-                                nn.ReLU(False)))
+                                nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False),
+                                nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM),
+                                nn.ReLU(False)
+                            ))
                     fuse_layer.append(nn.Sequential(*conv3x3s))
             fuse_layers.append(nn.ModuleList(fuse_layer))
 
         return nn.ModuleList(fuse_layers)
 
     def get_num_in_chs(self):
         return self.num_in_chs
 
-    def forward(self, x: List[torch.Tensor]):
+    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:
         if self.num_branches == 1:
             return [self.branches[0](x[0])]
 
         for i, branch in enumerate(self.branches):
             x[i] = branch(x[i])
 
         x_fuse = []
         for i, fuse_outer in enumerate(self.fuse_layers):
-            y = x[0] if i == 0 else fuse_outer[0](x[0])
-            for j in range(1, self.num_branches):
-                if i == j:
-                    y = y + x[j]
+            y = None
+            for j, f in enumerate(fuse_outer):
+                if y is None:
+                    y = f(x[j])
                 else:
-                    y = y + fuse_outer[j](x[j])
+                    y = y + f(x[j])
             x_fuse.append(self.fuse_act(y))
-
         return x_fuse
 
 
-blocks_dict = {
+class SequentialList(nn.Sequential):
+
+    def __init__(self, *args):
+        super(SequentialList, self).__init__(*args)
+
+    @torch.jit._overload_method  # noqa: F811
+    def forward(self, x):
+        # type: (List[torch.Tensor]) -> (List[torch.Tensor])
+        pass
+
+    @torch.jit._overload_method  # noqa: F811
+    def forward(self, x):
+        # type: (torch.Tensor) -> (List[torch.Tensor])
+        pass
+
+    def forward(self, x) -> List[torch.Tensor]:
+        for module in self:
+            x = module(x)
+        return x
+
+
+@torch.jit.interface
+class ModuleInterface(torch.nn.Module):
+    def forward(self, input: torch.Tensor) -> torch.Tensor: # `input` has a same name in Sequential forward
+        pass
+
+
+block_types_dict = {
     'BASIC': BasicBlock,
     'BOTTLENECK': Bottleneck
 }
 
 
 class HighResolutionNet(nn.Module):
 
-    def __init__(self, cfg, in_chans=3, num_classes=1000, global_pool='avg', drop_rate=0.0, head='classification'):
+    def __init__(
+            self,
+            cfg,
+            in_chans=3,
+            num_classes=1000,
+            output_stride=32,
+            global_pool='avg',
+            drop_rate=0.0,
+            head='classification',
+            **kwargs,
+    ):
         super(HighResolutionNet, self).__init__()
         self.num_classes = num_classes
-        self.drop_rate = drop_rate
+        assert output_stride == 32  # FIXME support dilation
 
-        stem_width = cfg['STEM_WIDTH']
+        cfg.update(**kwargs)
+        stem_width = cfg['stem_width']
         self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False)
         self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM)
         self.act1 = nn.ReLU(inplace=True)
         self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False)
         self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM)
         self.act2 = nn.ReLU(inplace=True)
 
-        self.stage1_cfg = cfg['STAGE1']
-        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]
-        block = blocks_dict[self.stage1_cfg['BLOCK']]
-        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]
-        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)
-        stage1_out_channel = block.expansion * num_channels
-
-        self.stage2_cfg = cfg['STAGE2']
-        num_channels = self.stage2_cfg['NUM_CHANNELS']
-        block = blocks_dict[self.stage2_cfg['BLOCK']]
-        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]
+        self.stage1_cfg = cfg['stage1']
+        num_channels = self.stage1_cfg['num_channels'][0]
+        block_type = block_types_dict[self.stage1_cfg['block_type']]
+        num_blocks = self.stage1_cfg['num_blocks'][0]
+        self.layer1 = self._make_layer(block_type, 64, num_channels, num_blocks)
+        stage1_out_channel = block_type.expansion * num_channels
+
+        self.stage2_cfg = cfg['stage2']
+        num_channels = self.stage2_cfg['num_channels']
+        block_type = block_types_dict[self.stage2_cfg['block_type']]
+        num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]
         self.transition1 = self._make_transition_layer([stage1_out_channel], num_channels)
         self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels)
 
-        self.stage3_cfg = cfg['STAGE3']
-        num_channels = self.stage3_cfg['NUM_CHANNELS']
-        block = blocks_dict[self.stage3_cfg['BLOCK']]
-        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]
+        self.stage3_cfg = cfg['stage3']
+        num_channels = self.stage3_cfg['num_channels']
+        block_type = block_types_dict[self.stage3_cfg['block_type']]
+        num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]
         self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels)
         self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels)
 
-        self.stage4_cfg = cfg['STAGE4']
-        num_channels = self.stage4_cfg['NUM_CHANNELS']
-        block = blocks_dict[self.stage4_cfg['BLOCK']]
-        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]
+        self.stage4_cfg = cfg['stage4']
+        num_channels = self.stage4_cfg['num_channels']
+        block_type = block_types_dict[self.stage4_cfg['block_type']]
+        num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]
         self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels)
         self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, multi_scale_output=True)
 
         self.head = head
         self.head_channels = None  # set if _make_head called
+        head_conv_bias = cfg.pop('head_conv_bias', True)
         if head == 'classification':
             # Classification Head
             self.num_features = 2048
-            self.incre_modules, self.downsamp_modules, self.final_layer = self._make_head(pre_stage_channels)
-            self.global_pool, self.classifier = create_classifier(
-                self.num_features, self.num_classes, pool_type=global_pool)
-        elif head == 'incre':
-            self.num_features = 2048
-            self.incre_modules, _, _ = self._make_head(pre_stage_channels, True)
+            self.incre_modules, self.downsamp_modules, self.final_layer = self._make_head(
+                pre_stage_channels,
+                conv_bias=head_conv_bias,
+            )
+            self.global_pool, self.head_drop, self.classifier = create_classifier(
+                self.num_features,
+                self.num_classes,
+                pool_type=global_pool,
+                drop_rate=drop_rate,
+            )
         else:
-            self.incre_modules = None
-            self.num_features = 256
+            if head == 'incre':
+                self.num_features = 2048
+                self.incre_modules, _, _ = self._make_head(pre_stage_channels, incre_only=True)
+            else:
+                self.num_features = 256
+                self.incre_modules = None
+            self.global_pool = nn.Identity()
+            self.head_drop = nn.Identity()
+            self.classifier = nn.Identity()
 
         curr_stride = 2
         # module names aren't actually valid here, hook or FeatureNet based extraction would not work
         self.feature_info = [dict(num_chs=64, reduction=curr_stride, module='stem')]
         for i, c in enumerate(self.head_channels if self.head_channels else num_channels):
             curr_stride *= 2
-            c = c * 4 if self.head_channels else c  # head block expansion factor of 4
+            c = c * 4 if self.head_channels else c  # head block_type expansion factor of 4
             self.feature_info += [dict(num_chs=c, reduction=curr_stride, module=f'stage{i + 1}')]
 
         self.init_weights()
 
-    def _make_head(self, pre_stage_channels, incre_only=False):
-        head_block = Bottleneck
+    def _make_head(self, pre_stage_channels, incre_only=False, conv_bias=True):
+        head_block_type = Bottleneck
         self.head_channels = [32, 64, 128, 256]
 
         # Increasing the #channels on each resolution
         # from C, 2C, 4C, 8C to 128, 256, 512, 1024
         incre_modules = []
         for i, channels in enumerate(pre_stage_channels):
-            incre_modules.append(self._make_layer(head_block, channels, self.head_channels[i], 1, stride=1))
+            incre_modules.append(self._make_layer(head_block_type, channels, self.head_channels[i], 1, stride=1))
         incre_modules = nn.ModuleList(incre_modules)
         if incre_only:
             return incre_modules, None, None
 
         # downsampling modules
         downsamp_modules = []
         for i in range(len(pre_stage_channels) - 1):
-            in_channels = self.head_channels[i] * head_block.expansion
-            out_channels = self.head_channels[i + 1] * head_block.expansion
+            in_channels = self.head_channels[i] * head_block_type.expansion
+            out_channels = self.head_channels[i + 1] * head_block_type.expansion
             downsamp_module = nn.Sequential(
                 nn.Conv2d(
-                    in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1),
+                    in_channels=in_channels, out_channels=out_channels,
+                    kernel_size=3, stride=2, padding=1, bias=conv_bias),
                 nn.BatchNorm2d(out_channels, momentum=_BN_MOMENTUM),
                 nn.ReLU(inplace=True)
             )
             downsamp_modules.append(downsamp_module)
         downsamp_modules = nn.ModuleList(downsamp_modules)
 
         final_layer = nn.Sequential(
             nn.Conv2d(
-                in_channels=self.head_channels[3] * head_block.expansion,
-                out_channels=self.num_features, kernel_size=1, stride=1, padding=0
-            ),
+                in_channels=self.head_channels[3] * head_block_type.expansion, out_channels=self.num_features,
+                kernel_size=1, stride=1, padding=0, bias=conv_bias),
             nn.BatchNorm2d(self.num_features, momentum=_BN_MOMENTUM),
             nn.ReLU(inplace=True)
         )
 
         return incre_modules, downsamp_modules, final_layer
 
     def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
@@ -625,57 +661,57 @@
                         nn.BatchNorm2d(num_channels_cur_layer[i], momentum=_BN_MOMENTUM),
                         nn.ReLU(inplace=True)))
                 else:
                     transition_layers.append(nn.Identity())
             else:
                 conv3x3s = []
                 for j in range(i + 1 - num_branches_pre):
-                    inchannels = num_channels_pre_layer[-1]
-                    outchannels = num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels
+                    _in_chs = num_channels_pre_layer[-1]
+                    _out_chs = num_channels_cur_layer[i] if j == i - num_branches_pre else _in_chs
                     conv3x3s.append(nn.Sequential(
-                        nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),
-                        nn.BatchNorm2d(outchannels, momentum=_BN_MOMENTUM),
+                        nn.Conv2d(_in_chs, _out_chs, 3, 2, 1, bias=False),
+                        nn.BatchNorm2d(_out_chs, momentum=_BN_MOMENTUM),
                         nn.ReLU(inplace=True)))
                 transition_layers.append(nn.Sequential(*conv3x3s))
 
         return nn.ModuleList(transition_layers)
 
-    def _make_layer(self, block, inplanes, planes, blocks, stride=1):
+    def _make_layer(self, block_type, inplanes, planes, block_types, stride=1):
         downsample = None
-        if stride != 1 or inplanes != planes * block.expansion:
+        if stride != 1 or inplanes != planes * block_type.expansion:
             downsample = nn.Sequential(
-                nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
-                nn.BatchNorm2d(planes * block.expansion, momentum=_BN_MOMENTUM),
+                nn.Conv2d(inplanes, planes * block_type.expansion, kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(planes * block_type.expansion, momentum=_BN_MOMENTUM),
             )
 
-        layers = [block(inplanes, planes, stride, downsample)]
-        inplanes = planes * block.expansion
-        for i in range(1, blocks):
-            layers.append(block(inplanes, planes))
+        layers = [block_type(inplanes, planes, stride, downsample)]
+        inplanes = planes * block_type.expansion
+        for i in range(1, block_types):
+            layers.append(block_type(inplanes, planes))
 
         return nn.Sequential(*layers)
 
     def _make_stage(self, layer_config, num_in_chs, multi_scale_output=True):
-        num_modules = layer_config['NUM_MODULES']
-        num_branches = layer_config['NUM_BRANCHES']
-        num_blocks = layer_config['NUM_BLOCKS']
-        num_channels = layer_config['NUM_CHANNELS']
-        block = blocks_dict[layer_config['BLOCK']]
-        fuse_method = layer_config['FUSE_METHOD']
+        num_modules = layer_config['num_modules']
+        num_branches = layer_config['num_branches']
+        num_blocks = layer_config['num_blocks']
+        num_channels = layer_config['num_channels']
+        block_type = block_types_dict[layer_config['block_type']]
+        fuse_method = layer_config['fuse_method']
 
         modules = []
         for i in range(num_modules):
             # multi_scale_output is only used last module
             reset_multi_scale_output = multi_scale_output or i < num_modules - 1
             modules.append(HighResolutionModule(
-                num_branches, block, num_blocks, num_in_chs, num_channels, fuse_method, reset_multi_scale_output)
+                num_branches, block_type, num_blocks, num_in_chs, num_channels, fuse_method, reset_multi_scale_output)
             )
             num_in_chs = modules[-1].get_num_in_chs()
 
-        return nn.Sequential(*modules), num_in_chs
+        return SequentialList(*modules), num_in_chs
 
     @torch.jit.ignore
     def init_weights(self):
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(
                     m.weight, mode='fan_out', nonlinearity='relu')
@@ -683,15 +719,15 @@
                 nn.init.constant_(m.weight, 1)
                 nn.init.constant_(m.bias, 0)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^conv[12]|bn[12]',
-            blocks=r'^(?:layer|stage|transition)(\d+)' if coarse else [
+            block_types=r'^(?:layer|stage|transition)(\d+)' if coarse else [
                 (r'^layer(\d+)\.(\d+)', None),
                 (r'^stage(\d+)\.(\d+)', None),
                 (r'^transition(\d+)', (99999,)),
             ],
         )
         return matcher
 
@@ -730,25 +766,30 @@
         x = self.bn2(x)
         x = self.act2(x)
 
         # Stages
         yl = self.stages(x)
         if self.incre_modules is None or self.downsamp_modules is None:
             return yl
-        y = self.incre_modules[0](yl[0])
-        for i, down in enumerate(self.downsamp_modules):
-            y = self.incre_modules[i + 1](yl[i + 1]) + down(y)
+
+        y = None
+        for i, incre in enumerate(self.incre_modules):
+            if y is None:
+                y = incre(yl[i])
+            else:
+                down: ModuleInterface = self.downsamp_modules[i - 1]  # needed for torchscript module indexing
+                y = incre(yl[i]) + down.forward(y)
+
         y = self.final_layer(y)
         return y
 
     def forward_head(self, x, pre_logits: bool = False):
         # Classification Head
         x = self.global_pool(x)
-        if self.drop_rate > 0.:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         return x if pre_logits else self.classifier(x)
 
     def forward(self, x):
         y = self.forward_features(x)
         x = self.forward_head(y)
         return x
 
@@ -760,20 +801,37 @@
     It would be more complicated to use the FeatureNet helpers.
 
     The `feature_location=incre` allows grabbing increased channel count features using part of the
     classification head. If `feature_location=''` the default HRNet features are returned. First stem
     conv is used for stride 2 features.
     """
 
-    def __init__(self, cfg, in_chans=3, num_classes=1000, global_pool='avg', drop_rate=0.0,
-                 feature_location='incre', out_indices=(0, 1, 2, 3, 4)):
+    def __init__(
+            self,
+            cfg,
+            in_chans=3,
+            num_classes=1000,
+            output_stride=32,
+            global_pool='avg',
+            drop_rate=0.0,
+            feature_location='incre',
+            out_indices=(0, 1, 2, 3, 4),
+            **kwargs,
+    ):
         assert feature_location in ('incre', '')
         super(HighResolutionNetFeatures, self).__init__(
-            cfg, in_chans=in_chans, num_classes=num_classes, global_pool=global_pool,
-            drop_rate=drop_rate, head=feature_location)
+            cfg,
+            in_chans=in_chans,
+            num_classes=num_classes,
+            output_stride=output_stride,
+            global_pool=global_pool,
+            drop_rate=drop_rate,
+            head=feature_location,
+            **kwargs,
+        )
         self.feature_info = FeatureInfo(self.feature_info, out_indices)
         self._out_idx = {i for i in out_indices}
 
     def forward_features(self, x):
         assert False, 'Not supported'
 
     def forward(self, x) -> List[torch.tensor]:
@@ -791,70 +849,124 @@
             x = [incre(f) for f, incre in zip(x, self.incre_modules)]
         for i, f in enumerate(x):
             if i + 1 in self._out_idx:
                 out.append(f)
         return out
 
 
-def _create_hrnet(variant, pretrained, **model_kwargs):
+def _create_hrnet(variant, pretrained=False, cfg_variant=None, **model_kwargs):
     model_cls = HighResolutionNet
     features_only = False
     kwargs_filter = None
     if model_kwargs.pop('features_only', False):
         model_cls = HighResolutionNetFeatures
         kwargs_filter = ('num_classes', 'global_pool')
         features_only = True
+    cfg_variant = cfg_variant or variant
     model = build_model_with_cfg(
-        model_cls, variant, pretrained,
-        model_cfg=cfg_cls[variant],
+        model_cls,
+        variant,
+        pretrained,
+        model_cfg=cfg_cls[cfg_variant],
         pretrained_strict=not features_only,
         kwargs_filter=kwargs_filter,
-        **model_kwargs)
+        **model_kwargs,
+    )
     if features_only:
         model.pretrained_cfg = pretrained_cfg_for_features(model.default_cfg)
         model.default_cfg = model.pretrained_cfg  # backwards compat
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv1', 'classifier': 'classifier',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'hrnet_w18_small.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w18_small_v2.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w18.ms_aug_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95,
+    ),
+    'hrnet_w18.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w30.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w32.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w40.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w44.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w48.ms_in1k': _cfg(hf_hub_id='timm/'),
+    'hrnet_w64.ms_in1k': _cfg(hf_hub_id='timm/'),
+
+    'hrnet_w18_ssld.paddle_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288)
+    ),
+    'hrnet_w48_ssld.paddle_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288)
+    ),
+})
+
+
 @register_model
-def hrnet_w18_small(pretrained=False, **kwargs):
+def hrnet_w18_small(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w18_small', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w18_small_v2(pretrained=False, **kwargs):
+def hrnet_w18_small_v2(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w18_small_v2', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w18(pretrained=False, **kwargs):
+def hrnet_w18(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w18', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w30(pretrained=False, **kwargs):
+def hrnet_w30(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w30', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w32(pretrained=False, **kwargs):
+def hrnet_w32(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w32', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w40(pretrained=False, **kwargs):
+def hrnet_w40(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w40', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w44(pretrained=False, **kwargs):
+def hrnet_w44(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w44', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w48(pretrained=False, **kwargs):
+def hrnet_w48(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w48', pretrained, **kwargs)
 
 
 @register_model
-def hrnet_w64(pretrained=False, **kwargs):
+def hrnet_w64(pretrained=False, **kwargs) -> HighResolutionNet:
     return _create_hrnet('hrnet_w64', pretrained, **kwargs)
+
+
+@register_model
+def hrnet_w18_ssld(pretrained=False, **kwargs) -> HighResolutionNet:
+    kwargs.setdefault('head_conv_bias', False)
+    return _create_hrnet('hrnet_w18_ssld', cfg_variant='hrnet_w18', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def hrnet_w48_ssld(pretrained=False, **kwargs) -> HighResolutionNet:
+    kwargs.setdefault('head_conv_bias', False)
+    return _create_hrnet('hrnet_w48_ssld', cfg_variant='hrnet_w48', pretrained=pretrained, **kwargs)
+
```

### Comparing `timm-0.8.6.dev0/timm/models/inception_resnet_v2.py` & `timm-0.9.0/timm/models/inception_resnet_v2.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,191 +1,159 @@
 """ Pytorch Inception-Resnet-V2 implementation
 Sourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is
 based upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)
 """
+from functools import partial
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
-from timm.layers import create_classifier
+from timm.layers import create_classifier, ConvNormAct
 from ._builder import build_model_with_cfg
 from ._manipulate import flatten_modules
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 
 __all__ = ['InceptionResnetV2']
 
-default_cfgs = {
-    # ported from http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz
-    'inception_resnet_v2': {
-        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/inception_resnet_v2-940b1cd6.pth',
-        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
-        'crop_pct': 0.8975, 'interpolation': 'bicubic',
-        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
-        'first_conv': 'conv2d_1a.conv', 'classifier': 'classif',
-        'label_offset': 1,  # 1001 classes in pretrained weights
-    },
-    # ported from http://download.tensorflow.org/models/ens_adv_inception_resnet_v2_2017_08_18.tar.gz
-    'ens_adv_inception_resnet_v2': {
-        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ens_adv_inception_resnet_v2-2592a550.pth',
-        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
-        'crop_pct': 0.8975, 'interpolation': 'bicubic',
-        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
-        'first_conv': 'conv2d_1a.conv', 'classifier': 'classif',
-        'label_offset': 1,  # 1001 classes in pretrained weights
-    }
-}
-
-
-class BasicConv2d(nn.Module):
-    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
-        super(BasicConv2d, self).__init__()
-        self.conv = nn.Conv2d(
-            in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
-        self.bn = nn.BatchNorm2d(out_planes, eps=.001)
-        self.relu = nn.ReLU(inplace=False)
-
-    def forward(self, x):
-        x = self.conv(x)
-        x = self.bn(x)
-        x = self.relu(x)
-        return x
-
 
 class Mixed_5b(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=None):
         super(Mixed_5b, self).__init__()
+        conv_block = conv_block or ConvNormAct
 
-        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)
+        self.branch0 = conv_block(192, 96, kernel_size=1, stride=1)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(192, 48, kernel_size=1, stride=1),
-            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)
+            conv_block(192, 48, kernel_size=1, stride=1),
+            conv_block(48, 64, kernel_size=5, stride=1, padding=2)
         )
 
         self.branch2 = nn.Sequential(
-            BasicConv2d(192, 64, kernel_size=1, stride=1),
-            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),
-            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)
+            conv_block(192, 64, kernel_size=1, stride=1),
+            conv_block(64, 96, kernel_size=3, stride=1, padding=1),
+            conv_block(96, 96, kernel_size=3, stride=1, padding=1)
         )
 
         self.branch3 = nn.Sequential(
             nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
-            BasicConv2d(192, 64, kernel_size=1, stride=1)
+            conv_block(192, 64, kernel_size=1, stride=1)
         )
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
         out = torch.cat((x0, x1, x2, x3), 1)
         return out
 
 
 class Block35(nn.Module):
-    def __init__(self, scale=1.0):
+    def __init__(self, scale=1.0, conv_block=None):
         super(Block35, self).__init__()
-
         self.scale = scale
+        conv_block = conv_block or ConvNormAct
 
-        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)
+        self.branch0 = conv_block(320, 32, kernel_size=1, stride=1)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(320, 32, kernel_size=1, stride=1),
-            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)
+            conv_block(320, 32, kernel_size=1, stride=1),
+            conv_block(32, 32, kernel_size=3, stride=1, padding=1)
         )
 
         self.branch2 = nn.Sequential(
-            BasicConv2d(320, 32, kernel_size=1, stride=1),
-            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),
-            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)
+            conv_block(320, 32, kernel_size=1, stride=1),
+            conv_block(32, 48, kernel_size=3, stride=1, padding=1),
+            conv_block(48, 64, kernel_size=3, stride=1, padding=1)
         )
 
         self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)
-        self.relu = nn.ReLU(inplace=False)
+        self.act = nn.ReLU()
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         out = torch.cat((x0, x1, x2), 1)
         out = self.conv2d(out)
         out = out * self.scale + x
-        out = self.relu(out)
+        out = self.act(out)
         return out
 
 
 class Mixed_6a(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=None):
         super(Mixed_6a, self).__init__()
+        conv_block = conv_block or ConvNormAct
 
-        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)
+        self.branch0 = conv_block(320, 384, kernel_size=3, stride=2)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(320, 256, kernel_size=1, stride=1),
-            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),
-            BasicConv2d(256, 384, kernel_size=3, stride=2)
+            conv_block(320, 256, kernel_size=1, stride=1),
+            conv_block(256, 256, kernel_size=3, stride=1, padding=1),
+            conv_block(256, 384, kernel_size=3, stride=2)
         )
 
         self.branch2 = nn.MaxPool2d(3, stride=2)
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         out = torch.cat((x0, x1, x2), 1)
         return out
 
 
 class Block17(nn.Module):
-    def __init__(self, scale=1.0):
+    def __init__(self, scale=1.0, conv_block=None):
         super(Block17, self).__init__()
-
         self.scale = scale
+        conv_block = conv_block or ConvNormAct
 
-        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)
+        self.branch0 = conv_block(1088, 192, kernel_size=1, stride=1)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(1088, 128, kernel_size=1, stride=1),
-            BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),
-            BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))
+            conv_block(1088, 128, kernel_size=1, stride=1),
+            conv_block(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),
+            conv_block(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))
         )
 
         self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)
-        self.relu = nn.ReLU(inplace=False)
+        self.act = nn.ReLU()
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         out = torch.cat((x0, x1), 1)
         out = self.conv2d(out)
         out = out * self.scale + x
-        out = self.relu(out)
+        out = self.act(out)
         return out
 
 
 class Mixed_7a(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=None):
         super(Mixed_7a, self).__init__()
+        conv_block = conv_block or ConvNormAct
 
         self.branch0 = nn.Sequential(
-            BasicConv2d(1088, 256, kernel_size=1, stride=1),
-            BasicConv2d(256, 384, kernel_size=3, stride=2)
+            conv_block(1088, 256, kernel_size=1, stride=1),
+            conv_block(256, 384, kernel_size=3, stride=2)
         )
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(1088, 256, kernel_size=1, stride=1),
-            BasicConv2d(256, 288, kernel_size=3, stride=2)
+            conv_block(1088, 256, kernel_size=1, stride=1),
+            conv_block(256, 288, kernel_size=3, stride=2)
         )
 
         self.branch2 = nn.Sequential(
-            BasicConv2d(1088, 256, kernel_size=1, stride=1),
-            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),
-            BasicConv2d(288, 320, kernel_size=3, stride=2)
+            conv_block(1088, 256, kernel_size=1, stride=1),
+            conv_block(256, 288, kernel_size=3, stride=1, padding=1),
+            conv_block(288, 320, kernel_size=3, stride=2)
         )
 
         self.branch3 = nn.MaxPool2d(3, stride=2)
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
@@ -193,117 +161,94 @@
         x3 = self.branch3(x)
         out = torch.cat((x0, x1, x2, x3), 1)
         return out
 
 
 class Block8(nn.Module):
 
-    def __init__(self, scale=1.0, no_relu=False):
+    def __init__(self, scale=1.0, no_relu=False, conv_block=None):
         super(Block8, self).__init__()
-
         self.scale = scale
+        conv_block = conv_block or ConvNormAct
 
-        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)
+        self.branch0 = conv_block(2080, 192, kernel_size=1, stride=1)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(2080, 192, kernel_size=1, stride=1),
-            BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),
-            BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
+            conv_block(2080, 192, kernel_size=1, stride=1),
+            conv_block(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),
+            conv_block(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
         )
 
         self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)
-        self.relu = None if no_relu else nn.ReLU(inplace=False)
+        self.relu = None if no_relu else nn.ReLU()
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         out = torch.cat((x0, x1), 1)
         out = self.conv2d(out)
         out = out * self.scale + x
         if self.relu is not None:
             out = self.relu(out)
         return out
 
 
 class InceptionResnetV2(nn.Module):
-    def __init__(self, num_classes=1000, in_chans=3, drop_rate=0., output_stride=32, global_pool='avg'):
+    def __init__(
+            self,
+            num_classes=1000,
+            in_chans=3,
+            drop_rate=0.,
+            output_stride=32,
+            global_pool='avg',
+            norm_layer='batchnorm2d',
+            norm_eps=1e-3,
+            act_layer='relu',
+    ):
         super(InceptionResnetV2, self).__init__()
-        self.drop_rate = drop_rate
         self.num_classes = num_classes
         self.num_features = 1536
         assert output_stride == 32
+        conv_block = partial(
+            ConvNormAct,
+            padding=0,
+            norm_layer=norm_layer,
+            act_layer=act_layer,
+            norm_kwargs=dict(eps=norm_eps),
+            act_kwargs=dict(inplace=True),
+        )
 
-        self.conv2d_1a = BasicConv2d(in_chans, 32, kernel_size=3, stride=2)
-        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)
-        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)
+        self.conv2d_1a = conv_block(in_chans, 32, kernel_size=3, stride=2)
+        self.conv2d_2a = conv_block(32, 32, kernel_size=3, stride=1)
+        self.conv2d_2b = conv_block(32, 64, kernel_size=3, stride=1, padding=1)
         self.feature_info = [dict(num_chs=64, reduction=2, module='conv2d_2b')]
 
         self.maxpool_3a = nn.MaxPool2d(3, stride=2)
-        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)
-        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)
+        self.conv2d_3b = conv_block(64, 80, kernel_size=1, stride=1)
+        self.conv2d_4a = conv_block(80, 192, kernel_size=3, stride=1)
         self.feature_info += [dict(num_chs=192, reduction=4, module='conv2d_4a')]
 
         self.maxpool_5a = nn.MaxPool2d(3, stride=2)
-        self.mixed_5b = Mixed_5b()
-        self.repeat = nn.Sequential(
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17),
-            Block35(scale=0.17)
-        )
+        self.mixed_5b = Mixed_5b(conv_block=conv_block)
+        self.repeat = nn.Sequential(*[Block35(scale=0.17, conv_block=conv_block) for _ in range(10)])
         self.feature_info += [dict(num_chs=320, reduction=8, module='repeat')]
 
-        self.mixed_6a = Mixed_6a()
-        self.repeat_1 = nn.Sequential(
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10),
-            Block17(scale=0.10)
-        )
+        self.mixed_6a = Mixed_6a(conv_block=conv_block)
+        self.repeat_1 = nn.Sequential(*[Block17(scale=0.10, conv_block=conv_block) for _ in range(20)])
         self.feature_info += [dict(num_chs=1088, reduction=16, module='repeat_1')]
 
-        self.mixed_7a = Mixed_7a()
-        self.repeat_2 = nn.Sequential(
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20),
-            Block8(scale=0.20)
-        )
-        self.block8 = Block8(no_relu=True)
-        self.conv2d_7b = BasicConv2d(2080, self.num_features, kernel_size=1, stride=1)
+        self.mixed_7a = Mixed_7a(conv_block=conv_block)
+        self.repeat_2 = nn.Sequential(*[Block8(scale=0.20, conv_block=conv_block) for _ in range(9)])
+
+        self.block8 = Block8(no_relu=True, conv_block=conv_block)
+        self.conv2d_7b = conv_block(2080, self.num_features, kernel_size=1, stride=1)
         self.feature_info += [dict(num_chs=self.num_features, reduction=32, module='conv2d_7b')]
 
-        self.global_pool, self.classif = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool, self.head_drop, self.classif = create_classifier(
+            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         module_map = {k: i for i, (k, _) in enumerate(flatten_modules(self.named_children(), prefix=()))}
         module_map.pop(('classif',))
 
         def _matcher(name):
@@ -348,36 +293,49 @@
         x = self.repeat_2(x)
         x = self.block8(x)
         x = self.conv2d_7b(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
-        if self.drop_rate > 0:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         return x if pre_logits else self.classif(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_inception_resnet_v2(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(InceptionResnetV2, variant, pretrained, **kwargs)
 
 
+default_cfgs = generate_default_cfgs({
+    # ported from http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz
+    'inception_resnet_v2.tf_in1k': {
+        'hf_hub_id': 'timm/',
+        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
+        'crop_pct': 0.8975, 'interpolation': 'bicubic',
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'conv2d_1a.conv', 'classifier': 'classif',
+    },
+    # As per https://arxiv.org/abs/1705.07204 and
+    # ported from http://download.tensorflow.org/models/ens_adv_inception_resnet_v2_2017_08_18.tar.gz
+    'inception_resnet_v2.tf_ens_adv_in1k': {
+        'hf_hub_id': 'timm/',
+        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
+        'crop_pct': 0.8975, 'interpolation': 'bicubic',
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'conv2d_1a.conv', 'classifier': 'classif',
+    }
+})
+
+
 @register_model
-def inception_resnet_v2(pretrained=False, **kwargs):
-    r"""InceptionResnetV2 model architecture from the
-    `"InceptionV4, Inception-ResNet..." <https://arxiv.org/abs/1602.07261>` paper.
-    """
+def inception_resnet_v2(pretrained=False, **kwargs) -> InceptionResnetV2:
     return _create_inception_resnet_v2('inception_resnet_v2', pretrained=pretrained, **kwargs)
 
 
-@register_model
-def ens_adv_inception_resnet_v2(pretrained=False, **kwargs):
-    r""" Ensemble Adversarially trained InceptionResnetV2 model architecture
-    As per https://arxiv.org/abs/1705.07204 and
-    https://github.com/tensorflow/models/tree/master/research/adv_imagenet_models.
-    """
-    return _create_inception_resnet_v2('ens_adv_inception_resnet_v2', pretrained=pretrained, **kwargs)
+register_model_deprecations(__name__, {
+    'ens_adv_inception_resnet_v2': 'inception_resnet_v2.tf_ens_adv_in1k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/inception_v3.py` & `timm-0.9.0/timm/models/inception_v3.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,67 +1,33 @@
 """ Inception-V3
 
 Originally from torchvision Inception3 model
 Licensed BSD-Clause 3 https://github.com/pytorch/vision/blob/master/LICENSE
 """
+from functools import partial
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
-from timm.layers import trunc_normal_, create_classifier, Linear
+from timm.layers import trunc_normal_, create_classifier, Linear, ConvNormAct
 from ._builder import build_model_with_cfg
 from ._builder import resolve_pretrained_cfg
 from ._manipulate import flatten_modules
-from ._registry import register_model
-
-__all__ = ['InceptionV3', 'InceptionV3Aux']  # model_registry will add each entrypoint fn to this
-
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
-        'first_conv': 'Conv2d_1a_3x3.conv', 'classifier': 'fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # original PyTorch weights, ported from Tensorflow but modified
-    'inception_v3': _cfg(
-        # NOTE checkpoint has aux logit layer weights
-        url='https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'),
-    # my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)
-    'tf_inception_v3': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_inception_v3-e0069de4.pth',
-        num_classes=1000, label_offset=1),
-    # my port of Tensorflow adversarially trained Inception V3 from
-    # http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz
-    'adv_inception_v3': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/adv_inception_v3-9e27bd63.pth',
-        num_classes=1000, label_offset=1),
-    # from gluon pretrained models, best performing in terms of accuracy/loss metrics
-    # https://gluon-cv.mxnet.io/model_zoo/classification.html
-    'gluon_inception_v3': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_inception_v3-9f746940.pth',
-        mean=IMAGENET_DEFAULT_MEAN,  # also works well with inception defaults
-        std=IMAGENET_DEFAULT_STD,  # also works well with inception defaults
-    )
-}
+__all__ = ['InceptionV3']  # model_registry will add each entrypoint fn to this
 
 
 class InceptionA(nn.Module):
 
     def __init__(self, in_channels, pool_features, conv_block=None):
         super(InceptionA, self).__init__()
-        if conv_block is None:
-            conv_block = BasicConv2d
+        conv_block = conv_block or ConvNormAct
         self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)
 
         self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)
         self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)
 
         self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)
         self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)
@@ -90,16 +56,15 @@
         return torch.cat(outputs, 1)
 
 
 class InceptionB(nn.Module):
 
     def __init__(self, in_channels, conv_block=None):
         super(InceptionB, self).__init__()
-        if conv_block is None:
-            conv_block = BasicConv2d
+        conv_block = conv_block or ConvNormAct
         self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)
 
         self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)
         self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)
         self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)
 
     def _forward(self, x):
@@ -119,16 +84,15 @@
         return torch.cat(outputs, 1)
 
 
 class InceptionC(nn.Module):
 
     def __init__(self, in_channels, channels_7x7, conv_block=None):
         super(InceptionC, self).__init__()
-        if conv_block is None:
-            conv_block = BasicConv2d
+        conv_block = conv_block or ConvNormAct
         self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)
 
         c7 = channels_7x7
         self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)
         self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))
         self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))
 
@@ -164,16 +128,15 @@
         return torch.cat(outputs, 1)
 
 
 class InceptionD(nn.Module):
 
     def __init__(self, in_channels, conv_block=None):
         super(InceptionD, self).__init__()
-        if conv_block is None:
-            conv_block = BasicConv2d
+        conv_block = conv_block or ConvNormAct
         self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)
         self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)
 
         self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)
         self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))
         self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))
         self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)
@@ -196,16 +159,15 @@
         return torch.cat(outputs, 1)
 
 
 class InceptionE(nn.Module):
 
     def __init__(self, in_channels, conv_block=None):
         super(InceptionE, self).__init__()
-        if conv_block is None:
-            conv_block = BasicConv2d
+        conv_block = conv_block or ConvNormAct
         self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)
 
         self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)
         self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))
         self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))
 
         self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)
@@ -244,16 +206,15 @@
         return torch.cat(outputs, 1)
 
 
 class InceptionAux(nn.Module):
 
     def __init__(self, in_channels, num_classes, conv_block=None):
         super(InceptionAux, self).__init__()
-        if conv_block is None:
-            conv_block = BasicConv2d
+        conv_block = conv_block or ConvNormAct
         self.conv0 = conv_block(in_channels, 128, kernel_size=1)
         self.conv1 = conv_block(128, 768, kernel_size=5)
         self.conv1.stddev = 0.01
         self.fc = Linear(768, num_classes)
         self.fc.stddev = 0.001
 
     def forward(self, x):
@@ -270,70 +231,79 @@
         x = torch.flatten(x, 1)
         # N x 768
         x = self.fc(x)
         # N x 1000
         return x
 
 
-class BasicConv2d(nn.Module):
-
-    def __init__(self, in_channels, out_channels, **kwargs):
-        super(BasicConv2d, self).__init__()
-        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
-        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)
-
-    def forward(self, x):
-        x = self.conv(x)
-        x = self.bn(x)
-        return F.relu(x, inplace=True)
-
-
 class InceptionV3(nn.Module):
-    """Inception-V3 with no AuxLogits
-    FIXME two class defs are redundant, but less screwing around with torchsript fussyness and inconsistent returns
+    """Inception-V3
     """
+    aux_logits: torch.jit.Final[bool]
 
-    def __init__(self, num_classes=1000, in_chans=3, drop_rate=0., global_pool='avg', aux_logits=False):
+    def __init__(
+            self,
+            num_classes=1000,
+            in_chans=3,
+            drop_rate=0.,
+            global_pool='avg',
+            aux_logits=False,
+            norm_layer='batchnorm2d',
+            norm_eps=1e-3,
+            act_layer='relu',
+    ):
         super(InceptionV3, self).__init__()
         self.num_classes = num_classes
-        self.drop_rate = drop_rate
         self.aux_logits = aux_logits
-
-        self.Conv2d_1a_3x3 = BasicConv2d(in_chans, 32, kernel_size=3, stride=2)
-        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)
-        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)
+        conv_block = partial(
+            ConvNormAct,
+            padding=0,
+            norm_layer=norm_layer,
+            act_layer=act_layer,
+            norm_kwargs=dict(eps=norm_eps),
+            act_kwargs=dict(inplace=True),
+        )
+
+        self.Conv2d_1a_3x3 = conv_block(in_chans, 32, kernel_size=3, stride=2)
+        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)
+        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)
         self.Pool1 = nn.MaxPool2d(kernel_size=3, stride=2)
-        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)
-        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)
+        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)
+        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)
         self.Pool2 = nn.MaxPool2d(kernel_size=3, stride=2)
-        self.Mixed_5b = InceptionA(192, pool_features=32)
-        self.Mixed_5c = InceptionA(256, pool_features=64)
-        self.Mixed_5d = InceptionA(288, pool_features=64)
-        self.Mixed_6a = InceptionB(288)
-        self.Mixed_6b = InceptionC(768, channels_7x7=128)
-        self.Mixed_6c = InceptionC(768, channels_7x7=160)
-        self.Mixed_6d = InceptionC(768, channels_7x7=160)
-        self.Mixed_6e = InceptionC(768, channels_7x7=192)
+        self.Mixed_5b = InceptionA(192, pool_features=32, conv_block=conv_block)
+        self.Mixed_5c = InceptionA(256, pool_features=64, conv_block=conv_block)
+        self.Mixed_5d = InceptionA(288, pool_features=64, conv_block=conv_block)
+        self.Mixed_6a = InceptionB(288, conv_block=conv_block)
+        self.Mixed_6b = InceptionC(768, channels_7x7=128, conv_block=conv_block)
+        self.Mixed_6c = InceptionC(768, channels_7x7=160, conv_block=conv_block)
+        self.Mixed_6d = InceptionC(768, channels_7x7=160, conv_block=conv_block)
+        self.Mixed_6e = InceptionC(768, channels_7x7=192, conv_block=conv_block)
         if aux_logits:
-            self.AuxLogits = InceptionAux(768, num_classes)
+            self.AuxLogits = InceptionAux(768, num_classes, conv_block=conv_block)
         else:
             self.AuxLogits = None
-        self.Mixed_7a = InceptionD(768)
-        self.Mixed_7b = InceptionE(1280)
-        self.Mixed_7c = InceptionE(2048)
+        self.Mixed_7a = InceptionD(768, conv_block=conv_block)
+        self.Mixed_7b = InceptionE(1280, conv_block=conv_block)
+        self.Mixed_7c = InceptionE(2048, conv_block=conv_block)
         self.feature_info = [
             dict(num_chs=64, reduction=2, module='Conv2d_2b_3x3'),
             dict(num_chs=192, reduction=4, module='Conv2d_4a_3x3'),
             dict(num_chs=288, reduction=8, module='Mixed_5d'),
             dict(num_chs=768, reduction=16, module='Mixed_6e'),
             dict(num_chs=2048, reduction=32, module='Mixed_7c'),
         ]
 
         self.num_features = 2048
-        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool, self.head_drop, self.fc = create_classifier(
+            self.num_features,
+            self.num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+        )
 
         for m in self.modules():
             if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                 stddev = m.stddev if hasattr(m, 'stddev') else 0.1
                 trunc_normal_(m.weight, std=stddev)
             elif isinstance(m, nn.BatchNorm2d):
                 nn.init.constant_(m.weight, 1)
@@ -390,89 +360,96 @@
         x = self.Mixed_7a(x)  # N x 1280 x 8 x 8
         x = self.Mixed_7b(x)  # N x 2048 x 8 x 8
         x = self.Mixed_7c(x)  # N x 2048 x 8 x 8
         return x
 
     def forward_features(self, x):
         x = self.forward_preaux(x)
+        if self.aux_logits:
+            aux = self.AuxLogits(x)
+            x = self.forward_postaux(x)
+            return x, aux
         x = self.forward_postaux(x)
         return x
 
     def forward_head(self, x):
         x = self.global_pool(x)
-        if self.drop_rate > 0:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         x = self.fc(x)
         return x
 
     def forward(self, x):
+        if self.aux_logits:
+            x, aux = self.forward_features(x)
+            x = self.forward_head(x)
+            return x, aux
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-class InceptionV3Aux(InceptionV3):
-    """InceptionV3 with AuxLogits
-    """
-
-    def __init__(self, num_classes=1000, in_chans=3, drop_rate=0., global_pool='avg', aux_logits=True):
-        super(InceptionV3Aux, self).__init__(
-            num_classes, in_chans, drop_rate, global_pool, aux_logits)
-
-    def forward_features(self, x):
-        x = self.forward_preaux(x)
-        aux = self.AuxLogits(x) if self.training else None
-        x = self.forward_postaux(x)
-        return x, aux
-
-    def forward(self, x):
-        x, aux = self.forward_features(x)
-        x = self.forward_head(x)
-        return x, aux
-
-
 def _create_inception_v3(variant, pretrained=False, **kwargs):
     pretrained_cfg = resolve_pretrained_cfg(variant, pretrained_cfg=kwargs.pop('pretrained_cfg', None))
-    aux_logits = kwargs.pop('aux_logits', False)
+    aux_logits = kwargs.get('aux_logits', False)
+    has_aux_logits = False
+    if pretrained_cfg:
+        # only torchvision pretrained weights have aux logits
+        has_aux_logits = pretrained_cfg.tag == 'tv_in1k'
     if aux_logits:
         assert not kwargs.pop('features_only', False)
-        model_cls = InceptionV3Aux
-        load_strict = variant == 'inception_v3'
+        load_strict = has_aux_logits
     else:
-        model_cls = InceptionV3
-        load_strict = variant != 'inception_v3'
+        load_strict = not has_aux_logits
 
     return build_model_with_cfg(
-        model_cls, variant, pretrained,
+        InceptionV3,
+        variant,
+        pretrained,
         pretrained_cfg=pretrained_cfg,
         pretrained_strict=load_strict,
-        **kwargs)
+        **kwargs,
+    )
 
 
-@register_model
-def inception_v3(pretrained=False, **kwargs):
-    # original PyTorch weights, ported from Tensorflow but modified
-    model = _create_inception_v3('inception_v3', pretrained=pretrained, **kwargs)
-    return model
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'Conv2d_1a_3x3.conv', 'classifier': 'fc',
+        **kwargs
+    }
 
 
-@register_model
-def tf_inception_v3(pretrained=False, **kwargs):
+default_cfgs = generate_default_cfgs({
+    # original PyTorch weights, ported from Tensorflow but modified
+    'inception_v3.tv_in1k': _cfg(
+        # NOTE checkpoint has aux logit layer weights
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'),
     # my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)
-    model = _create_inception_v3('tf_inception_v3', pretrained=pretrained, **kwargs)
-    return model
-
-
-@register_model
-def adv_inception_v3(pretrained=False, **kwargs):
+    'inception_v3.tf_in1k': _cfg(hf_hub_id='timm/'),
     # my port of Tensorflow adversarially trained Inception V3 from
     # http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz
-    model = _create_inception_v3('adv_inception_v3', pretrained=pretrained, **kwargs)
-    return model
+    'inception_v3.tf_adv_in1k': _cfg(hf_hub_id='timm/'),
+    # from gluon pretrained models, best performing in terms of accuracy/loss metrics
+    # https://gluon-cv.mxnet.io/model_zoo/classification.html
+    'inception_v3.gluon_in1k': _cfg(
+        hf_hub_id='timm/',
+        mean=IMAGENET_DEFAULT_MEAN,  # also works well with inception defaults
+        std=IMAGENET_DEFAULT_STD,  # also works well with inception defaults
+    )
+})
 
 
 @register_model
-def gluon_inception_v3(pretrained=False, **kwargs):
-    # from gluon pretrained models, best performing in terms of accuracy/loss metrics
-    # https://gluon-cv.mxnet.io/model_zoo/classification.html
-    model = _create_inception_v3('gluon_inception_v3', pretrained=pretrained, **kwargs)
+def inception_v3(pretrained=False, **kwargs) -> InceptionV3:
+    model = _create_inception_v3('inception_v3', pretrained=pretrained, **kwargs)
     return model
+
+
+register_model_deprecations(__name__, {
+    'tf_inception_v3': 'inception_v3.tf_in1k',
+    'adv_inception_v3': 'inception_v3.tf_adv_in1k',
+    'gluon_inception_v3': 'inception_v3.gluon_in1k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/inception_v4.py` & `timm-0.9.0/timm/models/inception_v4.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,227 +1,202 @@
 """ Pytorch Inception-V4 implementation
 Sourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is
 based upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)
 """
+from functools import partial
+
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
-from timm.layers import create_classifier
+from timm.layers import create_classifier, ConvNormAct
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['InceptionV4']
 
-default_cfgs = {
-    'inception_v4': {
-        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/inceptionv4-8e4777a0.pth',
-        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
-        'first_conv': 'features.0.conv', 'classifier': 'last_linear',
-        'label_offset': 1,  # 1001 classes in pretrained weights
-    }
-}
-
-
-class BasicConv2d(nn.Module):
-    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
-        super(BasicConv2d, self).__init__()
-        self.conv = nn.Conv2d(
-            in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
-        self.bn = nn.BatchNorm2d(out_planes, eps=0.001)
-        self.relu = nn.ReLU(inplace=True)
-
-    def forward(self, x):
-        x = self.conv(x)
-        x = self.bn(x)
-        x = self.relu(x)
-        return x
-
 
 class Mixed3a(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(Mixed3a, self).__init__()
         self.maxpool = nn.MaxPool2d(3, stride=2)
-        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)
+        self.conv = conv_block(64, 96, kernel_size=3, stride=2)
 
     def forward(self, x):
         x0 = self.maxpool(x)
         x1 = self.conv(x)
         out = torch.cat((x0, x1), 1)
         return out
 
 
 class Mixed4a(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(Mixed4a, self).__init__()
 
         self.branch0 = nn.Sequential(
-            BasicConv2d(160, 64, kernel_size=1, stride=1),
-            BasicConv2d(64, 96, kernel_size=3, stride=1)
+            conv_block(160, 64, kernel_size=1, stride=1),
+            conv_block(64, 96, kernel_size=3, stride=1)
         )
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(160, 64, kernel_size=1, stride=1),
-            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),
-            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),
-            BasicConv2d(64, 96, kernel_size=(3, 3), stride=1)
+            conv_block(160, 64, kernel_size=1, stride=1),
+            conv_block(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),
+            conv_block(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),
+            conv_block(64, 96, kernel_size=(3, 3), stride=1)
         )
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         out = torch.cat((x0, x1), 1)
         return out
 
 
 class Mixed5a(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(Mixed5a, self).__init__()
-        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)
+        self.conv = conv_block(192, 192, kernel_size=3, stride=2)
         self.maxpool = nn.MaxPool2d(3, stride=2)
 
     def forward(self, x):
         x0 = self.conv(x)
         x1 = self.maxpool(x)
         out = torch.cat((x0, x1), 1)
         return out
 
 
 class InceptionA(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(InceptionA, self).__init__()
-        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)
+        self.branch0 = conv_block(384, 96, kernel_size=1, stride=1)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(384, 64, kernel_size=1, stride=1),
-            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)
+            conv_block(384, 64, kernel_size=1, stride=1),
+            conv_block(64, 96, kernel_size=3, stride=1, padding=1)
         )
 
         self.branch2 = nn.Sequential(
-            BasicConv2d(384, 64, kernel_size=1, stride=1),
-            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),
-            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)
+            conv_block(384, 64, kernel_size=1, stride=1),
+            conv_block(64, 96, kernel_size=3, stride=1, padding=1),
+            conv_block(96, 96, kernel_size=3, stride=1, padding=1)
         )
 
         self.branch3 = nn.Sequential(
             nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
-            BasicConv2d(384, 96, kernel_size=1, stride=1)
+            conv_block(384, 96, kernel_size=1, stride=1)
         )
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
         out = torch.cat((x0, x1, x2, x3), 1)
         return out
 
 
 class ReductionA(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(ReductionA, self).__init__()
-        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)
+        self.branch0 = conv_block(384, 384, kernel_size=3, stride=2)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(384, 192, kernel_size=1, stride=1),
-            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),
-            BasicConv2d(224, 256, kernel_size=3, stride=2)
+            conv_block(384, 192, kernel_size=1, stride=1),
+            conv_block(192, 224, kernel_size=3, stride=1, padding=1),
+            conv_block(224, 256, kernel_size=3, stride=2)
         )
 
         self.branch2 = nn.MaxPool2d(3, stride=2)
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         out = torch.cat((x0, x1, x2), 1)
         return out
 
 
 class InceptionB(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(InceptionB, self).__init__()
-        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)
+        self.branch0 = conv_block(1024, 384, kernel_size=1, stride=1)
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(1024, 192, kernel_size=1, stride=1),
-            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),
-            BasicConv2d(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0))
+            conv_block(1024, 192, kernel_size=1, stride=1),
+            conv_block(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),
+            conv_block(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0))
         )
 
         self.branch2 = nn.Sequential(
-            BasicConv2d(1024, 192, kernel_size=1, stride=1),
-            BasicConv2d(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),
-            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),
-            BasicConv2d(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),
-            BasicConv2d(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3))
+            conv_block(1024, 192, kernel_size=1, stride=1),
+            conv_block(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),
+            conv_block(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),
+            conv_block(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),
+            conv_block(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3))
         )
 
         self.branch3 = nn.Sequential(
             nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
-            BasicConv2d(1024, 128, kernel_size=1, stride=1)
+            conv_block(1024, 128, kernel_size=1, stride=1)
         )
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
         out = torch.cat((x0, x1, x2, x3), 1)
         return out
 
 
 class ReductionB(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(ReductionB, self).__init__()
 
         self.branch0 = nn.Sequential(
-            BasicConv2d(1024, 192, kernel_size=1, stride=1),
-            BasicConv2d(192, 192, kernel_size=3, stride=2)
+            conv_block(1024, 192, kernel_size=1, stride=1),
+            conv_block(192, 192, kernel_size=3, stride=2)
         )
 
         self.branch1 = nn.Sequential(
-            BasicConv2d(1024, 256, kernel_size=1, stride=1),
-            BasicConv2d(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),
-            BasicConv2d(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),
-            BasicConv2d(320, 320, kernel_size=3, stride=2)
+            conv_block(1024, 256, kernel_size=1, stride=1),
+            conv_block(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),
+            conv_block(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),
+            conv_block(320, 320, kernel_size=3, stride=2)
         )
 
         self.branch2 = nn.MaxPool2d(3, stride=2)
 
     def forward(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         out = torch.cat((x0, x1, x2), 1)
         return out
 
 
 class InceptionC(nn.Module):
-    def __init__(self):
+    def __init__(self, conv_block=ConvNormAct):
         super(InceptionC, self).__init__()
 
-        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)
+        self.branch0 = conv_block(1536, 256, kernel_size=1, stride=1)
 
-        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)
-        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))
-        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
-
-        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)
-        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))
-        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))
-        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))
-        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
+        self.branch1_0 = conv_block(1536, 384, kernel_size=1, stride=1)
+        self.branch1_1a = conv_block(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))
+        self.branch1_1b = conv_block(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
+
+        self.branch2_0 = conv_block(1536, 384, kernel_size=1, stride=1)
+        self.branch2_1 = conv_block(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))
+        self.branch2_2 = conv_block(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))
+        self.branch2_3a = conv_block(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))
+        self.branch2_3b = conv_block(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))
 
         self.branch3 = nn.Sequential(
             nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
-            BasicConv2d(1536, 256, kernel_size=1, stride=1)
+            conv_block(1536, 256, kernel_size=1, stride=1)
         )
 
     def forward(self, x):
         x0 = self.branch0(x)
 
         x1_0 = self.branch1_0(x)
         x1_1a = self.branch1_1a(x1_0)
@@ -238,54 +213,61 @@
         x3 = self.branch3(x)
 
         out = torch.cat((x0, x1, x2, x3), 1)
         return out
 
 
 class InceptionV4(nn.Module):
-    def __init__(self, num_classes=1000, in_chans=3, output_stride=32, drop_rate=0., global_pool='avg'):
+    def __init__(
+            self,
+            num_classes=1000,
+            in_chans=3,
+            output_stride=32,
+            drop_rate=0.,
+            global_pool='avg',
+            norm_layer='batchnorm2d',
+            norm_eps=1e-3,
+            act_layer='relu',
+    ):
         super(InceptionV4, self).__init__()
         assert output_stride == 32
-        self.drop_rate = drop_rate
         self.num_classes = num_classes
         self.num_features = 1536
-
-        self.features = nn.Sequential(
-            BasicConv2d(in_chans, 32, kernel_size=3, stride=2),
-            BasicConv2d(32, 32, kernel_size=3, stride=1),
-            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),
-            Mixed3a(),
-            Mixed4a(),
-            Mixed5a(),
-            InceptionA(),
-            InceptionA(),
-            InceptionA(),
-            InceptionA(),
-            ReductionA(),  # Mixed6a
-            InceptionB(),
-            InceptionB(),
-            InceptionB(),
-            InceptionB(),
-            InceptionB(),
-            InceptionB(),
-            InceptionB(),
-            ReductionB(),  # Mixed7a
-            InceptionC(),
-            InceptionC(),
-            InceptionC(),
-        )
+        conv_block = partial(
+            ConvNormAct,
+            padding=0,
+            norm_layer=norm_layer,
+            act_layer=act_layer,
+            norm_kwargs=dict(eps=norm_eps),
+            act_kwargs=dict(inplace=True),
+        )
+
+        features = [
+            conv_block(in_chans, 32, kernel_size=3, stride=2),
+            conv_block(32, 32, kernel_size=3, stride=1),
+            conv_block(32, 64, kernel_size=3, stride=1, padding=1),
+            Mixed3a(conv_block),
+            Mixed4a(conv_block),
+            Mixed5a(conv_block),
+        ]
+        features += [InceptionA(conv_block) for _ in range(4)]
+        features += [ReductionA(conv_block)]  # Mixed6a
+        features += [InceptionB(conv_block) for _ in range(7)]
+        features += [ReductionB(conv_block)]  # Mixed7a
+        features += [InceptionC(conv_block) for _ in range(3)]
+        self.features = nn.Sequential(*features)
         self.feature_info = [
             dict(num_chs=64, reduction=2, module='features.2'),
             dict(num_chs=160, reduction=4, module='features.3'),
             dict(num_chs=384, reduction=8, module='features.9'),
             dict(num_chs=1024, reduction=16, module='features.17'),
             dict(num_chs=1536, reduction=32, module='features.21'),
         ]
-        self.global_pool, self.last_linear = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool, self.head_drop, self.last_linear = create_classifier(
+            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^features\.[012]\.',
             blocks=r'^features\.(\d+)'
         )
@@ -304,27 +286,40 @@
             self.num_features, self.num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
         return self.features(x)
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
-        if self.drop_rate > 0:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         return x if pre_logits else self.last_linear(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _create_inception_v4(variant, pretrained=False, **kwargs):
+def _create_inception_v4(variant, pretrained=False, **kwargs) -> InceptionV4:
     return build_model_with_cfg(
-        InceptionV4, variant, pretrained,
+        InceptionV4,
+        variant,
+        pretrained,
         feature_cfg=dict(flatten_sequential=True),
-        **kwargs)
+        **kwargs,
+    )
+
+
+default_cfgs = generate_default_cfgs({
+    'inception_v4.tf_in1k': {
+        'hf_hub_id': 'timm/',
+        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'features.0.conv', 'classifier': 'last_linear',
+    }
+})
 
 
 @register_model
 def inception_v4(pretrained=False, **kwargs):
     return _create_inception_v4('inception_v4', pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/layers/__init__.py` & `timm-0.9.0/timm/models/layers/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -42,8 +42,8 @@
 from timm.layers.split_batchnorm import SplitBatchNorm2d, convert_splitbn_model
 from timm.layers.std_conv import StdConv2d, StdConv2dSame, ScaledStdConv2d, ScaledStdConv2dSame
 from timm.layers.test_time_pool import TestTimePoolHead, apply_test_time_pool
 from timm.layers.trace_utils import _assert, _float_to_int
 from timm.layers.weight_init import trunc_normal_, trunc_normal_tf_, variance_scaling_, lecun_normal_
 
 import warnings
-warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", DeprecationWarning)
+warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", DeprecationWarning)
```

### Comparing `timm-0.8.6.dev0/timm/models/levit.py` & `timm-0.9.0/timm/models/vision_transformer_relpos.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,592 +1,597 @@
-""" LeViT
+""" Relative Position Vision Transformer (ViT) in PyTorch
 
-Paper: `LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference`
-    - https://arxiv.org/abs/2104.01136
+NOTE: these models are experimental / WIP, expect changes
 
-@article{graham2021levit,
-  title={LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},
-  author={Benjamin Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\'e J\'egou and Matthijs Douze},
-  journal={arXiv preprint arXiv:22104.01136},
-  year={2021}
-}
-
-Adapted from official impl at https://github.com/facebookresearch/LeViT, original copyright bellow.
-
-This version combines both conv/linear models and fixes torchscript compatibility.
-
-Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman
+Hacked together by / Copyright 2022, Ross Wightman
 """
-
-# Copyright (c) 2015-present, Facebook, Inc.
-# All rights reserved.
-
-# Modified from
-# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
-# Copyright 2020 Ross Wightman, Apache-2.0 License
+import logging
+import math
 from functools import partial
-from typing import Dict
+from typing import Optional, Tuple
 
 import torch
 import torch.nn as nn
+from torch.jit import Final
+from torch.utils.checkpoint import checkpoint
 
-from timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN
-from timm.layers import to_ntuple, get_act_layer, trunc_normal_
+from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
+from timm.layers import PatchEmbed, Mlp, DropPath, RelPosMlp, RelPosBias, use_fused_attn
 from ._builder import build_model_with_cfg
-from ._manipulate import checkpoint_seq
-from ._registry import register_model
-
-__all__ = ['LevitDistilled']  # model_registry will add each entrypoint fn to this
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.0.c', 'classifier': ('head.l', 'head_dist.l'),
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    levit_128s=_cfg(
-        url='https://dl.fbaipublicfiles.com/LeViT/LeViT-128S-96703c44.pth'
-    ),
-    levit_128=_cfg(
-        url='https://dl.fbaipublicfiles.com/LeViT/LeViT-128-b88c2750.pth'
-    ),
-    levit_192=_cfg(
-        url='https://dl.fbaipublicfiles.com/LeViT/LeViT-192-92712e41.pth'
-    ),
-    levit_256=_cfg(
-        url='https://dl.fbaipublicfiles.com/LeViT/LeViT-256-13b5763e.pth'
-    ),
-    levit_384=_cfg(
-        url='https://dl.fbaipublicfiles.com/LeViT/LeViT-384-9bdaf2e2.pth'
-    ),
+from ._registry import generate_default_cfgs, register_model
 
-    levit_256d=_cfg(url='', classifier='head.l'),
-)
+__all__ = ['VisionTransformerRelPos']  # model_registry will add each entrypoint fn to this
 
-model_cfgs = dict(
-    levit_128s=dict(
-        embed_dim=(128, 256, 384), key_dim=16, num_heads=(4, 6, 8), depth=(2, 3, 4)),
-    levit_128=dict(
-        embed_dim=(128, 256, 384), key_dim=16, num_heads=(4, 8, 12), depth=(4, 4, 4)),
-    levit_192=dict(
-        embed_dim=(192, 288, 384), key_dim=32, num_heads=(3, 5, 6), depth=(4, 4, 4)),
-    levit_256=dict(
-        embed_dim=(256, 384, 512), key_dim=32, num_heads=(4, 6, 8), depth=(4, 4, 4)),
-    levit_384=dict(
-        embed_dim=(384, 512, 768), key_dim=32, num_heads=(6, 9, 12), depth=(4, 4, 4)),
+_logger = logging.getLogger(__name__)
 
-    levit_256d=dict(
-        embed_dim=(256, 384, 512), key_dim=32, num_heads=(4, 6, 8), depth=(4, 8, 6)),
-)
 
-__all__ = ['Levit']
+class RelPosAttention(nn.Module):
+    fused_attn: Final[bool]
 
-
-@register_model
-def levit_128s(pretrained=False, use_conv=False, **kwargs):
-    return create_levit(
-        'levit_128s', pretrained=pretrained, use_conv=use_conv, **kwargs)
-
-
-@register_model
-def levit_128(pretrained=False, use_conv=False, **kwargs):
-    return create_levit(
-        'levit_128', pretrained=pretrained, use_conv=use_conv, **kwargs)
-
-
-@register_model
-def levit_192(pretrained=False, use_conv=False, **kwargs):
-    return create_levit(
-        'levit_192', pretrained=pretrained, use_conv=use_conv, **kwargs)
-
-
-@register_model
-def levit_256(pretrained=False, use_conv=False, **kwargs):
-    return create_levit(
-        'levit_256', pretrained=pretrained, use_conv=use_conv, **kwargs)
-
-
-@register_model
-def levit_384(pretrained=False, use_conv=False, **kwargs):
-    return create_levit(
-        'levit_384', pretrained=pretrained, use_conv=use_conv, **kwargs)
-
-
-@register_model
-def levit_256d(pretrained=False, use_conv=False, **kwargs):
-    return create_levit(
-        'levit_256d', pretrained=pretrained, use_conv=use_conv, distilled=False, **kwargs)
-
-
-class ConvNorm(nn.Sequential):
     def __init__(
-            self, in_chs, out_chs, kernel_size=1, stride=1, pad=0, dilation=1,
-            groups=1, bn_weight_init=1, resolution=-10000):
-        super().__init__()
-        self.add_module('c', nn.Conv2d(in_chs, out_chs, kernel_size, stride, pad, dilation, groups, bias=False))
-        self.add_module('bn', nn.BatchNorm2d(out_chs))
-
-        nn.init.constant_(self.bn.weight, bn_weight_init)
-
-    @torch.no_grad()
-    def fuse(self):
-        c, bn = self._modules.values()
-        w = bn.weight / (bn.running_var + bn.eps) ** 0.5
-        w = c.weight * w[:, None, None, None]
-        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5
-        m = nn.Conv2d(
-            w.size(1), w.size(0), w.shape[2:], stride=self.c.stride,
-            padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)
-        m.weight.data.copy_(w)
-        m.bias.data.copy_(b)
-        return m
-
-
-class LinearNorm(nn.Sequential):
-    def __init__(self, in_features, out_features, bn_weight_init=1, resolution=-100000):
-        super().__init__()
-        self.add_module('c', nn.Linear(in_features, out_features, bias=False))
-        self.add_module('bn', nn.BatchNorm1d(out_features))
-
-        nn.init.constant_(self.bn.weight, bn_weight_init)
-
-    @torch.no_grad()
-    def fuse(self):
-        l, bn = self._modules.values()
-        w = bn.weight / (bn.running_var + bn.eps) ** 0.5
-        w = l.weight * w[:, None]
-        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5
-        m = nn.Linear(w.size(1), w.size(0))
-        m.weight.data.copy_(w)
-        m.bias.data.copy_(b)
-        return m
-
-    def forward(self, x):
-        x = self.c(x)
-        return self.bn(x.flatten(0, 1)).reshape_as(x)
-
-
-class NormLinear(nn.Sequential):
-    def __init__(self, in_features, out_features, bias=True, std=0.02):
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            qk_norm=False,
+            rel_pos_cls=None,
+            attn_drop=0.,
+            proj_drop=0.,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
-        self.add_module('bn', nn.BatchNorm1d(in_features))
-        self.add_module('l', nn.Linear(in_features, out_features, bias=bias))
+        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
+        self.num_heads = num_heads
+        self.head_dim = dim // num_heads
+        self.scale = self.head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
+
+        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
+        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
+        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
+        self.rel_pos = rel_pos_cls(num_heads=num_heads) if rel_pos_cls else None
+        self.attn_drop = nn.Dropout(attn_drop)
+        self.proj = nn.Linear(dim, dim)
+        self.proj_drop = nn.Dropout(proj_drop)
 
-        trunc_normal_(self.l.weight, std=std)
-        if self.l.bias is not None:
-            nn.init.constant_(self.l.bias, 0)
-
-    @torch.no_grad()
-    def fuse(self):
-        bn, l = self._modules.values()
-        w = bn.weight / (bn.running_var + bn.eps) ** 0.5
-        b = bn.bias - self.bn.running_mean * self.bn.weight / (bn.running_var + bn.eps) ** 0.5
-        w = l.weight * w[None, :]
-        if l.bias is None:
-            b = b @ self.l.weight.T
+    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):
+        B, N, C = x.shape
+        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
+        q, k, v = qkv.unbind(0)
+        q = self.q_norm(q)
+        k = self.k_norm(k)
+
+        if self.fused_attn:
+            if self.rel_pos is not None:
+                attn_bias = self.rel_pos.get_bias()
+            elif shared_rel_pos is not None:
+                attn_bias = shared_rel_pos
+            else:
+                attn_bias = None
+
+            x = torch.nn.functional.scaled_dot_product_attention(
+                q, k, v,
+                attn_mask=attn_bias,
+                dropout_p=self.attn_drop.p,
+            )
         else:
-            b = (l.weight @ b[:, None]).view(-1) + self.l.bias
-        m = nn.Linear(w.size(1), w.size(0))
-        m.weight.data.copy_(w)
-        m.bias.data.copy_(b)
-        return m
-
-
-def stem_b16(in_chs, out_chs, activation, resolution=224):
-    return nn.Sequential(
-        ConvNorm(in_chs, out_chs // 8, 3, 2, 1, resolution=resolution),
-        activation(),
-        ConvNorm(out_chs // 8, out_chs // 4, 3, 2, 1, resolution=resolution // 2),
-        activation(),
-        ConvNorm(out_chs // 4, out_chs // 2, 3, 2, 1, resolution=resolution // 4),
-        activation(),
-        ConvNorm(out_chs // 2, out_chs, 3, 2, 1, resolution=resolution // 8))
-
-
-class Residual(nn.Module):
-    def __init__(self, m, drop):
-        super().__init__()
-        self.m = m
-        self.drop = drop
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            if self.rel_pos is not None:
+                attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)
+            elif shared_rel_pos is not None:
+                attn = attn + shared_rel_pos
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-    def forward(self, x):
-        if self.training and self.drop > 0:
-            return x + self.m(x) * torch.rand(
-                x.size(0), 1, 1, device=x.device).ge_(self.drop).div(1 - self.drop).detach()
-        else:
-            return x + self.m(x)
+        x = x.transpose(1, 2).reshape(B, N, C)
+        x = self.proj(x)
+        x = self.proj_drop(x)
+        return x
 
 
-class Subsample(nn.Module):
-    def __init__(self, stride, resolution):
+class LayerScale(nn.Module):
+    def __init__(self, dim, init_values=1e-5, inplace=False):
         super().__init__()
-        self.stride = stride
-        self.resolution = resolution
+        self.inplace = inplace
+        self.gamma = nn.Parameter(init_values * torch.ones(dim))
 
     def forward(self, x):
-        B, N, C = x.shape
-        x = x.view(B, self.resolution, self.resolution, C)[:, ::self.stride, ::self.stride]
-        return x.reshape(B, -1, C)
+        return x.mul_(self.gamma) if self.inplace else x * self.gamma
 
 
-class Attention(nn.Module):
-    ab: Dict[str, torch.Tensor]
+class RelPosBlock(nn.Module):
 
     def __init__(
-            self, dim, key_dim, num_heads=8, attn_ratio=4, act_layer=None, resolution=14, use_conv=False):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            qk_norm=False,
+            rel_pos_cls=None,
+            init_values=None,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
-        ln_layer = ConvNorm if use_conv else LinearNorm
-        self.use_conv = use_conv
-        self.num_heads = num_heads
-        self.scale = key_dim ** -0.5
-        self.key_dim = key_dim
-        self.key_attn_dim = key_dim * num_heads
-        self.val_dim = int(attn_ratio * key_dim)
-        self.val_attn_dim = int(attn_ratio * key_dim) * num_heads
-
-        self.qkv = ln_layer(dim, self.val_attn_dim + self.key_attn_dim * 2, resolution=resolution)
-        self.proj = nn.Sequential(
-            act_layer(),
-            ln_layer(self.val_attn_dim, dim, bn_weight_init=0, resolution=resolution)
+        self.norm1 = norm_layer(dim)
+        self.attn = RelPosAttention(
+            dim,
+            num_heads,
+            qkv_bias=qkv_bias,
+            qk_norm=qk_norm,
+            rel_pos_cls=rel_pos_cls,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
         )
+        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
+        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+
+        self.norm2 = norm_layer(dim)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
+        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-        self.attention_biases = nn.Parameter(torch.zeros(num_heads, resolution ** 2))
-        pos = torch.stack(torch.meshgrid(torch.arange(resolution), torch.arange(resolution))).flatten(1)
-        rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()
-        rel_pos = (rel_pos[0] * resolution) + rel_pos[1]
-        self.register_buffer('attention_bias_idxs', rel_pos)
-        self.ab = {}
-
-    @torch.no_grad()
-    def train(self, mode=True):
-        super().train(mode)
-        if mode and self.ab:
-            self.ab = {}  # clear ab cache
-
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
-        if self.training:
-            return self.attention_biases[:, self.attention_bias_idxs]
-        else:
-            device_key = str(device)
-            if device_key not in self.ab:
-                self.ab[device_key] = self.attention_biases[:, self.attention_bias_idxs]
-            return self.ab[device_key]
-
-    def forward(self, x):  # x (B,C,H,W)
-        if self.use_conv:
-            B, C, H, W = x.shape
-            q, k, v = self.qkv(x).view(
-                B, self.num_heads, -1, H * W).split([self.key_dim, self.key_dim, self.val_dim], dim=2)
-
-            attn = (q.transpose(-2, -1) @ k) * self.scale + self.get_attention_biases(x.device)
-            attn = attn.softmax(dim=-1)
-
-            x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W)
-        else:
-            B, N, C = x.shape
-            q, k, v = self.qkv(x).view(
-                B, N, self.num_heads, -1).split([self.key_dim, self.key_dim, self.val_dim], dim=3)
-            q = q.permute(0, 2, 1, 3)
-            k = k.permute(0, 2, 3, 1)
-            v = v.permute(0, 2, 1, 3)
-
-            attn = q @ k * self.scale + self.get_attention_biases(x.device)
-            attn = attn.softmax(dim=-1)
-
-            x = (attn @ v).transpose(1, 2).reshape(B, N, self.val_attn_dim)
-        x = self.proj(x)
+    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):
+        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))
+        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class AttentionSubsample(nn.Module):
-    ab: Dict[str, torch.Tensor]
+class ResPostRelPosBlock(nn.Module):
 
     def __init__(
-            self, in_dim, out_dim, key_dim, num_heads=8, attn_ratio=2,
-            act_layer=None, stride=2, resolution=14, resolution_out=7, use_conv=False):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            qk_norm=False,
+            rel_pos_cls=None,
+            init_values=None,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
-        self.stride = stride
-        self.num_heads = num_heads
-        self.scale = key_dim ** -0.5
-        self.key_dim = key_dim
-        self.key_attn_dim = key_dim * num_heads
-        self.val_dim = int(attn_ratio * key_dim)
-        self.val_attn_dim = self.val_dim * self.num_heads
-        self.resolution = resolution
-        self.resolution_out_area = resolution_out ** 2
-
-        self.use_conv = use_conv
-        if self.use_conv:
-            ln_layer = ConvNorm
-            sub_layer = partial(nn.AvgPool2d, kernel_size=1, padding=0)
-        else:
-            ln_layer = LinearNorm
-            sub_layer = partial(Subsample, resolution=resolution)
+        self.init_values = init_values
 
-        self.kv = ln_layer(in_dim, self.val_attn_dim + self.key_attn_dim, resolution=resolution)
-        self.q = nn.Sequential(
-            sub_layer(stride=stride),
-            ln_layer(in_dim, self.key_attn_dim, resolution=resolution_out)
+        self.attn = RelPosAttention(
+            dim,
+            num_heads,
+            qkv_bias=qkv_bias,
+            qk_norm=qk_norm,
+            rel_pos_cls=rel_pos_cls,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
         )
-        self.proj = nn.Sequential(
-            act_layer(),
-            ln_layer(self.val_attn_dim, out_dim, resolution=resolution_out)
-        )
-
-        self.attention_biases = nn.Parameter(torch.zeros(num_heads, self.resolution ** 2))
-        k_pos = torch.stack(torch.meshgrid(torch.arange(resolution), torch.arange(resolution))).flatten(1)
-        q_pos = torch.stack(torch.meshgrid(
-            torch.arange(0, resolution, step=stride),
-            torch.arange(0, resolution, step=stride))).flatten(1)
-        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()
-        rel_pos = (rel_pos[0] * resolution) + rel_pos[1]
-        self.register_buffer('attention_bias_idxs', rel_pos)
-
-        self.ab = {}  # per-device attention_biases cache
-
-    @torch.no_grad()
-    def train(self, mode=True):
-        super().train(mode)
-        if mode and self.ab:
-            self.ab = {}  # clear ab cache
-
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
-        if self.training:
-            return self.attention_biases[:, self.attention_bias_idxs]
-        else:
-            device_key = str(device)
-            if device_key not in self.ab:
-                self.ab[device_key] = self.attention_biases[:, self.attention_bias_idxs]
-            return self.ab[device_key]
-
-    def forward(self, x):
-        if self.use_conv:
-            B, C, H, W = x.shape
-            k, v = self.kv(x).view(B, self.num_heads, -1, H * W).split([self.key_dim, self.val_dim], dim=2)
-            q = self.q(x).view(B, self.num_heads, self.key_dim, self.resolution_out_area)
+        self.norm1 = norm_layer(dim)
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-            attn = (q.transpose(-2, -1) @ k) * self.scale + self.get_attention_biases(x.device)
-            attn = attn.softmax(dim=-1)
-
-            x = (v @ attn.transpose(-2, -1)).reshape(B, -1, self.resolution, self.resolution)
-        else:
-            B, N, C = x.shape
-            k, v = self.kv(x).view(B, N, self.num_heads, -1).split([self.key_dim, self.val_dim], dim=3)
-            k = k.permute(0, 2, 3, 1)  # BHCN
-            v = v.permute(0, 2, 1, 3)  # BHNC
-            q = self.q(x).view(B, self.resolution_out_area, self.num_heads, self.key_dim).permute(0, 2, 1, 3)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
+        self.norm2 = norm_layer(dim)
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-            attn = q @ k * self.scale + self.get_attention_biases(x.device)
-            attn = attn.softmax(dim=-1)
+        self.init_weights()
 
-            x = (attn @ v).transpose(1, 2).reshape(B, -1, self.val_attn_dim)
-        x = self.proj(x)
+    def init_weights(self):
+        # NOTE this init overrides that base model init with specific changes for the block type
+        if self.init_values is not None:
+            nn.init.constant_(self.norm1.weight, self.init_values)
+            nn.init.constant_(self.norm2.weight, self.init_values)
+
+    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):
+        x = x + self.drop_path1(self.norm1(self.attn(x, shared_rel_pos=shared_rel_pos)))
+        x = x + self.drop_path2(self.norm2(self.mlp(x)))
         return x
 
 
-class Levit(nn.Module):
-    """ Vision Transformer with support for patch or hybrid CNN input stage
+class VisionTransformerRelPos(nn.Module):
+    """ Vision Transformer w/ Relative Position Bias
 
-    NOTE: distillation is defaulted to True since pretrained weights use it, will cause problems
-    w/ train scripts that don't take tuple outputs,
+    Differing from classic vit, this impl
+      * uses relative position index (swin v1 / beit) or relative log coord + mlp (swin v2) pos embed
+      * defaults to no class token (can be enabled)
+      * defaults to global avg pool for head (can be changed)
+      * layer-scale (residual branch gain) enabled
     """
 
     def __init__(
             self,
             img_size=224,
             patch_size=16,
             in_chans=3,
             num_classes=1000,
-            embed_dim=(192,),
-            key_dim=64,
-            depth=(12,),
-            num_heads=(3,),
-            attn_ratio=2,
-            mlp_ratio=2,
-            hybrid_backbone=None,
-            down_ops=None,
-            act_layer='hard_swish',
-            attn_act_layer='hard_swish',
-            use_conv=False,
             global_pool='avg',
+            embed_dim=768,
+            depth=12,
+            num_heads=12,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            qk_norm=False,
+            init_values=1e-6,
+            class_token=False,
+            fc_norm=False,
+            rel_pos_type='mlp',
+            rel_pos_dim=None,
+            shared_rel_pos=False,
             drop_rate=0.,
-            drop_path_rate=0.):
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            weight_init='skip',
+            embed_layer=PatchEmbed,
+            norm_layer=None,
+            act_layer=None,
+            block_fn=RelPosBlock
+    ):
+        """
+        Args:
+            img_size (int, tuple): input image size
+            patch_size (int, tuple): patch size
+            in_chans (int): number of input channels
+            num_classes (int): number of classes for classification head
+            global_pool (str): type of global pooling for final sequence (default: 'avg')
+            embed_dim (int): embedding dimension
+            depth (int): depth of transformer
+            num_heads (int): number of attention heads
+            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
+            qkv_bias (bool): enable bias for qkv if True
+            qk_norm (bool): Enable normalization of query and key in attention
+            init_values: (float): layer-scale init values
+            class_token (bool): use class token (default: False)
+            fc_norm (bool): use pre classifier norm instead of pre-pool
+            rel_pos_ty pe (str): type of relative position
+            shared_rel_pos (bool): share relative pos across all blocks
+            drop_rate (float): dropout rate
+            proj_drop_rate (float): projection dropout rate
+            attn_drop_rate (float): attention dropout rate
+            drop_path_rate (float): stochastic depth rate
+            weight_init (str): weight init scheme
+            embed_layer (nn.Module): patch embedding layer
+            norm_layer: (nn.Module): normalization layer
+            act_layer: (nn.Module): MLP activation layer
+        """
         super().__init__()
-        act_layer = get_act_layer(act_layer)
-        attn_act_layer = get_act_layer(attn_act_layer)
-        ln_layer = ConvNorm if use_conv else LinearNorm
-        self.use_conv = use_conv
-        if isinstance(img_size, tuple):
-            # FIXME origin impl passes single img/res dim through whole hierarchy,
-            # not sure this model will be used enough to spend time fixing it.
-            assert img_size[0] == img_size[1]
-            img_size = img_size[0]
+        assert global_pool in ('', 'avg', 'token')
+        assert class_token or global_pool != 'token'
+        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
+        act_layer = act_layer or nn.GELU
+
         self.num_classes = num_classes
         self.global_pool = global_pool
-        self.num_features = embed_dim[-1]
-        self.embed_dim = embed_dim
+        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
+        self.num_prefix_tokens = 1 if class_token else 0
         self.grad_checkpointing = False
 
-        num_stages = len(embed_dim)
-        assert len(depth) == len(num_heads) == num_stages
-        key_dim = to_ntuple(num_stages)(key_dim)
-        attn_ratio = to_ntuple(num_stages)(attn_ratio)
-        mlp_ratio = to_ntuple(num_stages)(mlp_ratio)
-        down_ops = down_ops or (
-            # ('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)
-            ('Subsample', key_dim[0], embed_dim[0] // key_dim[0], 4, 2, 2),
-            ('Subsample', key_dim[0], embed_dim[1] // key_dim[1], 4, 2, 2),
-            ('',)
+        self.patch_embed = embed_layer(
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
         )
+        feat_size = self.patch_embed.grid_size
 
-        self.patch_embed = hybrid_backbone or stem_b16(in_chans, embed_dim[0], activation=act_layer)
-
-        self.blocks = []
-        resolution = img_size // patch_size
-        for i, (ed, kd, dpth, nh, ar, mr, do) in enumerate(
-                zip(embed_dim, key_dim, depth, num_heads, attn_ratio, mlp_ratio, down_ops)):
-            for _ in range(dpth):
-                self.blocks.append(
-                    Residual(
-                        Attention(
-                            ed, kd, nh, attn_ratio=ar, act_layer=attn_act_layer,
-                            resolution=resolution, use_conv=use_conv),
-                        drop_path_rate))
-                if mr > 0:
-                    h = int(ed * mr)
-                    self.blocks.append(
-                        Residual(nn.Sequential(
-                            ln_layer(ed, h, resolution=resolution),
-                            act_layer(),
-                            ln_layer(h, ed, bn_weight_init=0, resolution=resolution),
-                        ), drop_path_rate))
-            if do[0] == 'Subsample':
-                # ('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)
-                resolution_out = (resolution - 1) // do[5] + 1
-                self.blocks.append(
-                    AttentionSubsample(
-                        *embed_dim[i:i + 2], key_dim=do[1], num_heads=do[2],
-                        attn_ratio=do[3], act_layer=attn_act_layer, stride=do[5],
-                        resolution=resolution, resolution_out=resolution_out, use_conv=use_conv))
-                resolution = resolution_out
-                if do[4] > 0:  # mlp_ratio
-                    h = int(embed_dim[i + 1] * do[4])
-                    self.blocks.append(
-                        Residual(nn.Sequential(
-                            ln_layer(embed_dim[i + 1], h, resolution=resolution),
-                            act_layer(),
-                            ln_layer(h, embed_dim[i + 1], bn_weight_init=0, resolution=resolution),
-                        ), drop_path_rate))
-        self.blocks = nn.Sequential(*self.blocks)
-
-        # Classifier head
-        self.head = NormLinear(embed_dim[-1], num_classes) if num_classes > 0 else nn.Identity()
+        rel_pos_args = dict(window_size=feat_size, prefix_tokens=self.num_prefix_tokens)
+        if rel_pos_type.startswith('mlp'):
+            if rel_pos_dim:
+                rel_pos_args['hidden_dim'] = rel_pos_dim
+            if 'swin' in rel_pos_type:
+                rel_pos_args['mode'] = 'swin'
+            rel_pos_cls = partial(RelPosMlp, **rel_pos_args)
+        else:
+            rel_pos_cls = partial(RelPosBias, **rel_pos_args)
+        self.shared_rel_pos = None
+        if shared_rel_pos:
+            self.shared_rel_pos = rel_pos_cls(num_heads=num_heads)
+            # NOTE shared rel pos currently mutually exclusive w/ per-block, but could support both...
+            rel_pos_cls = None
+
+        self.cls_token = nn.Parameter(torch.zeros(1, self.num_prefix_tokens, embed_dim)) if class_token else None
+
+        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
+        self.blocks = nn.ModuleList([
+            block_fn(
+                dim=embed_dim,
+                num_heads=num_heads,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                qk_norm=qk_norm,
+                rel_pos_cls=rel_pos_cls,
+                init_values=init_values,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+                act_layer=act_layer,
+            )
+            for i in range(depth)])
+        self.norm = norm_layer(embed_dim) if not fc_norm else nn.Identity()
+
+        # Classifier Head
+        self.fc_norm = norm_layer(embed_dim) if fc_norm else nn.Identity()
+        self.head_drop = nn.Dropout(drop_rate)
+        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+
+        if weight_init != 'skip':
+            self.init_weights(weight_init)
+
+    def init_weights(self, mode=''):
+        assert mode in ('jax', 'moco', '')
+        if self.cls_token is not None:
+            nn.init.normal_(self.cls_token, std=1e-6)
+        # FIXME weight init scheme using PyTorch defaults curently
+        #named_apply(get_init_weights_vit(mode, head_bias), self)
 
     @torch.jit.ignore
     def no_weight_decay(self):
-        return {x for x in self.state_dict().keys() if 'attention_biases' in x}
+        return {'cls_token'}
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
-        matcher = dict(
-            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed
+        return dict(
+            stem=r'^cls_token|patch_embed',  # stem and embed
             blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))]
         )
-        return matcher
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head
 
-    def reset_classifier(self, num_classes, global_pool=None, distillation=None):
+    def reset_classifier(self, num_classes: int, global_pool=None):
         self.num_classes = num_classes
         if global_pool is not None:
+            assert global_pool in ('', 'avg', 'token')
             self.global_pool = global_pool
-        self.head = NormLinear(self.embed_dim[-1], num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
     def forward_features(self, x):
         x = self.patch_embed(x)
-        if not self.use_conv:
-            x = x.flatten(2).transpose(1, 2)
-        if self.grad_checkpointing and not torch.jit.is_scripting():
-            x = checkpoint_seq(self.blocks, x)
-        else:
-            x = self.blocks(x)
+        if self.cls_token is not None:
+            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
+
+        shared_rel_pos = self.shared_rel_pos.get_bias() if self.shared_rel_pos is not None else None
+        for blk in self.blocks:
+            if self.grad_checkpointing and not torch.jit.is_scripting():
+                x = checkpoint(blk, x, shared_rel_pos=shared_rel_pos)
+            else:
+                x = blk(x, shared_rel_pos=shared_rel_pos)
+        x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool == 'avg':
-            x = x.mean(dim=(-2, -1)) if self.use_conv else x.mean(dim=1)
+        if self.global_pool:
+            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
+        x = self.fc_norm(x)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-class LevitDistilled(Levit):
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.head_dist = NormLinear(self.num_features, self.num_classes) if self.num_classes > 0 else nn.Identity()
-        self.distilled_training = False  # must set this True to train w/ distillation token
+def _create_vision_transformer_relpos(variant, pretrained=False, **kwargs):
+    if kwargs.get('features_only', None):
+        raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
-    @torch.jit.ignore
-    def get_classifier(self):
-        return self.head, self.head_dist
+    model = build_model_with_cfg(VisionTransformerRelPos, variant, pretrained, **kwargs)
+    return model
 
-    def reset_classifier(self, num_classes, global_pool=None, distillation=None):
-        self.num_classes = num_classes
-        if global_pool is not None:
-            self.global_pool = global_pool
-        self.head = NormLinear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-        self.head_dist = NormLinear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
-    @torch.jit.ignore
-    def set_distilled_training(self, enable=True):
-        self.distilled_training = enable
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head',
+        **kwargs
+    }
 
-    def forward_head(self, x):
-        if self.global_pool == 'avg':
-            x = x.mean(dim=(-2, -1)) if self.use_conv else x.mean(dim=1)
-        x, x_dist = self.head(x), self.head_dist(x)
-        if self.distilled_training and self.training and not torch.jit.is_scripting():
-            # only return separate classification predictions when training in distilled mode
-            return x, x_dist
-        else:
-            # during standard train/finetune, inference average the classifier predictions
-            return (x + x_dist) / 2
 
+default_cfgs = generate_default_cfgs({
+    'vit_relpos_base_patch32_plus_rpn_256.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_replos_base_patch32_plus_rpn_256-sw-dd486f51.pth',
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256)),
+    'vit_relpos_base_patch16_plus_240.untrained': _cfg(url='', input_size=(3, 240, 240)),
+
+    'vit_relpos_small_patch16_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_small_patch16_224-sw-ec2778b4.pth',
+        hf_hub_id='timm/'),
+    'vit_relpos_medium_patch16_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_224-sw-11c174af.pth',
+        hf_hub_id='timm/'),
+    'vit_relpos_base_patch16_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_base_patch16_224-sw-49049aed.pth',
+        hf_hub_id='timm/'),
+
+    'vit_srelpos_small_patch16_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_srelpos_small_patch16_224-sw-6cdb8849.pth',
+        hf_hub_id='timm/'),
+    'vit_srelpos_medium_patch16_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_srelpos_medium_patch16_224-sw-ad702b8c.pth',
+        hf_hub_id='timm/'),
+
+    'vit_relpos_medium_patch16_cls_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_cls_224-sw-cfe8e259.pth',
+        hf_hub_id='timm/'),
+    'vit_relpos_base_patch16_cls_224.untrained': _cfg(),
+    'vit_relpos_base_patch16_clsgap_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_base_patch16_gapcls_224-sw-1a341d6c.pth',
+        hf_hub_id='timm/'),
+
+    'vit_relpos_small_patch16_rpn_224.untrained': _cfg(),
+    'vit_relpos_medium_patch16_rpn_224.sw_in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_rpn_224-sw-5d2befd8.pth',
+        hf_hub_id='timm/'),
+    'vit_relpos_base_patch16_rpn_224.untrained': _cfg(),
+})
 
-def checkpoint_filter_fn(state_dict, model):
-    if 'model' in state_dict:
-        # For deit models
-        state_dict = state_dict['model']
-    D = model.state_dict()
-    for k in state_dict.keys():
-        if k in D and D[k].ndim == 4 and state_dict[k].ndim == 2:
-            state_dict[k] = state_dict[k][:, :, None, None]
-    return state_dict
 
+@register_model
+def vit_relpos_base_patch32_plus_rpn_256(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/32+) w/ relative log-coord position and residual post-norm, no class token
+    """
+    model_args = dict(patch_size=32, embed_dim=896, depth=12, num_heads=14, block_fn=ResPostRelPosBlock)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_base_patch32_plus_rpn_256', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
 
-def create_levit(variant, pretrained=False, distilled=True, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
-    model_cfg = dict(**model_cfgs[variant], **kwargs)
-    model = build_model_with_cfg(
-        LevitDistilled if distilled else Levit, variant, pretrained,
-        pretrained_filter_fn=checkpoint_filter_fn,
-        **model_cfg)
+@register_model
+def vit_relpos_base_patch16_plus_240(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16+) w/ relative log-coord position, no class token
+    """
+    model_args = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_base_patch16_plus_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
+
+@register_model
+def vit_relpos_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position, no class token
+    """
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, fc_norm=True)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_medium_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=True)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_medium_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, fc_norm=True)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_srelpos_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ shared relative log-coord position, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, fc_norm=False,
+        rel_pos_dim=384, shared_rel_pos=True)
+    model = _create_vision_transformer_relpos(
+        'vit_srelpos_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_srelpos_medium_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ shared relative log-coord position, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=False,
+        rel_pos_dim=512, shared_rel_pos=True)
+    model = _create_vision_transformer_relpos(
+        'vit_srelpos_medium_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_medium_patch16_cls_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-M/16) w/ relative log-coord position, class token present
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=False,
+        rel_pos_dim=256, class_token=True, global_pool='token')
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_medium_patch16_cls_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_base_patch16_cls_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position, class token present
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, class_token=True, global_pool='token')
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_base_patch16_cls_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_base_patch16_clsgap_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position, class token present
+    NOTE this config is a bit of a mistake, class token was enabled but global avg-pool w/ fc-norm was not disabled
+    Leaving here for comparisons w/ a future re-train as it performs quite well.
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, fc_norm=True, class_token=True)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_base_patch16_clsgap_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_small_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, block_fn=ResPostRelPosBlock)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_small_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_medium_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, block_fn=ResPostRelPosBlock)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_medium_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_relpos_base_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:
+    """ ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, block_fn=ResPostRelPosBlock)
+    model = _create_vision_transformer_relpos(
+        'vit_relpos_base_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
```

### Comparing `timm-0.8.6.dev0/timm/models/maxxvit.py` & `timm-0.9.0/timm/models/maxxvit.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,17 +8,14 @@
 There are multiple sets of models defined for both architectures. Typically, names with a
  `_rw` suffix are my own original configs prior to referencing https://github.com/google-research/maxvit.
 These configs work well and appear to be a bit faster / lower resource than the paper.
 
 The models without extra prefix / suffix' (coatnet_0_224, maxvit_tiny_224, etc), are intended to
 match paper, BUT, without any official pretrained weights it's difficult to confirm a 100% match.
 
-# FIXME / WARNING
-This impl remains a WIP, some configs and models may vanish or change...
-
 Papers:
 
 MaxViT: Multi-Axis Vision Transformer - https://arxiv.org/abs/2204.01697
 @article{tu2022maxvit,
   title={MaxViT: Multi-Axis Vision Transformer},
   author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
   journal={ECCV},
@@ -35,31 +32,31 @@
 }
 
 Hacked together by / Copyright 2022, Ross Wightman
 """
 
 import math
 from collections import OrderedDict
-from dataclasses import dataclass, replace
+from dataclasses import dataclass, replace, field
 from functools import partial
 from typing import Callable, Optional, Union, Tuple, List
 
 import torch
 from torch import nn
+from torch.jit import Final
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import Mlp, ConvMlp, DropPath, ClassifierHead, LayerNorm, SelectAdaptivePool2d
+from timm.layers import Mlp, ConvMlp, DropPath, LayerNorm, ClassifierHead, NormMlpClassifierHead
 from timm.layers import create_attn, get_act_layer, get_norm_layer, get_norm_act_layer, create_conv2d, create_pool2d
 from timm.layers import trunc_normal_tf_, to_2tuple, extend_tuple, make_divisible, _assert
-from timm.layers import RelPosMlp, RelPosBias, RelPosBiasTf
+from timm.layers import RelPosMlp, RelPosBias, RelPosBiasTf, use_fused_attn
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
 from ._manipulate import named_apply, checkpoint_seq
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model
 
 __all__ = ['MaxxVitCfg', 'MaxxVitConvCfg', 'MaxxVitTransformerCfg', 'MaxxVit']
 
 
 @dataclass
 class MaxxVitTransformerCfg:
     dim_head: int = 32
@@ -72,14 +69,16 @@
     proj_drop: float = 0.
     pool_type: str = 'avg2'
     rel_pos_type: str = 'bias'
     rel_pos_dim: int = 512  # for relative position types w/ MLP
     partition_ratio: int = 32
     window_size: Optional[Tuple[int, int]] = None
     grid_size: Optional[Tuple[int, int]] = None
+    no_block_attn: bool = False  # disable window block attention for maxvit (ie only grid)
+    use_nchw_attn: bool = False  # for MaxViT variants (not used for CoAt), keep tensors in NCHW order
     init_values: Optional[float] = None
     act_layer: str = 'gelu'
     norm_layer: str = 'layernorm2d'
     norm_layer_cl: str = 'layernorm'
     norm_eps: float = 1e-6
 
     def __post_init__(self):
@@ -130,21 +129,23 @@
 @dataclass
 class MaxxVitCfg:
     embed_dim: Tuple[int, ...] = (96, 192, 384, 768)
     depths: Tuple[int, ...] = (2, 3, 5, 2)
     block_type: Tuple[Union[str, Tuple[str, ...]], ...] = ('C', 'C', 'T', 'T')
     stem_width: Union[int, Tuple[int, int]] = 64
     stem_bias: bool = False
-    conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg()
-    transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg()
+    conv_cfg: MaxxVitConvCfg = field(default_factory=MaxxVitConvCfg)
+    transformer_cfg: MaxxVitTransformerCfg = field(default_factory=MaxxVitTransformerCfg)
     head_hidden_size: int = None
     weight_init: str = 'vit_eff'
 
 
 class Attention2d(nn.Module):
+    fused_attn: Final[bool]
+
     """ multi-head attention for 2D NCHW tensors"""
     def __init__(
             self,
             dim: int,
             dim_out: Optional[int] = None,
             dim_head: int = 32,
             bias: bool = True,
@@ -157,14 +158,15 @@
         super().__init__()
         dim_out = dim_out or dim
         dim_attn = dim_out if expand_first else dim
         self.num_heads = dim_attn // dim_head
         self.dim_head = dim_head
         self.head_first = head_first
         self.scale = dim_head ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias)
         self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias)
         self.proj_drop = nn.Dropout(proj_drop)
 
@@ -172,30 +174,48 @@
         B, C, H, W = x.shape
 
         if self.head_first:
             q, k, v = self.qkv(x).view(B, self.num_heads, self.dim_head * 3, -1).chunk(3, dim=2)
         else:
             q, k, v = self.qkv(x).reshape(B, 3, self.num_heads, self.dim_head, -1).unbind(1)
 
-        attn = (q.transpose(-2, -1) @ k) * self.scale
-        if self.rel_pos is not None:
-            attn = self.rel_pos(attn)
-        elif shared_rel_pos is not None:
-            attn = attn + shared_rel_pos
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        if self.fused_attn:
+            attn_bias = None
+            if self.rel_pos is not None:
+                attn_bias = self.rel_pos.get_bias()
+            elif shared_rel_pos is not None:
+                attn_bias = shared_rel_pos
+
+            x = torch.nn.functional.scaled_dot_product_attention(
+                q.transpose(-1, -2),
+                k.transpose(-1, -2),
+                v.transpose(-1, -2),
+                attn_mask=attn_bias,
+                dropout_p=self.attn_drop.p,
+            ).transpose(-1, -2).reshape(B, -1, H, W)
+        else:
+            q = q * self.scale
+            attn = q.transpose(-2, -1) @ k
+            if self.rel_pos is not None:
+                attn = self.rel_pos(attn)
+            elif shared_rel_pos is not None:
+                attn = attn + shared_rel_pos
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W)
 
-        x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class AttentionCl(nn.Module):
     """ Channels-last multi-head attention (B, ..., C) """
+    fused_attn: Final[bool]
+
     def __init__(
             self,
             dim: int,
             dim_out: Optional[int] = None,
             dim_head: int = 32,
             bias: bool = True,
             expand_first: bool = True,
@@ -208,14 +228,15 @@
         dim_out = dim_out or dim
         dim_attn = dim_out if expand_first and dim_out > dim else dim
         assert dim_attn % dim_head == 0, 'attn dim should be divisible by head_dim'
         self.num_heads = dim_attn // dim_head
         self.dim_head = dim_head
         self.head_first = head_first
         self.scale = dim_head ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.qkv = nn.Linear(dim, dim_attn * 3, bias=bias)
         self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim_attn, dim_out, bias=bias)
         self.proj_drop = nn.Dropout(proj_drop)
 
@@ -224,23 +245,38 @@
         restore_shape = x.shape[:-1]
 
         if self.head_first:
             q, k, v = self.qkv(x).view(B, -1, self.num_heads, self.dim_head * 3).transpose(1, 2).chunk(3, dim=3)
         else:
             q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)
 
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        if self.rel_pos is not None:
-            attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)
-        elif shared_rel_pos is not None:
-            attn = attn + shared_rel_pos
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        if self.fused_attn:
+            attn_bias = None
+            if self.rel_pos is not None:
+                attn_bias = self.rel_pos.get_bias()
+            elif shared_rel_pos is not None:
+                attn_bias = shared_rel_pos
+
+            x = torch.nn.functional.scaled_dot_product_attention(
+                q, k, v,
+                attn_mask=attn_bias,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            if self.rel_pos is not None:
+                attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)
+            elif shared_rel_pos is not None:
+                attn = attn + shared_rel_pos
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-        x = (attn @ v).transpose(1, 2).reshape(restore_shape + (-1,))
+        x = x.transpose(1, 2).reshape(restore_shape + (-1,))
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class LayerScale(nn.Module):
     def __init__(self, dim, init_values=1e-5, inplace=False):
@@ -885,27 +921,25 @@
     def __init__(
             self,
             dim: int,
             dim_out: int,
             stride: int = 1,
             conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),
             transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),
-            use_nchw_attn: bool = False,  # FIXME move to cfg? True is ~20-30% faster on TPU, 5-10% slower on GPU
-            use_block_attn: bool = True,  # FIXME for testing ConvNeXt conv w/o block attention
             drop_path: float = 0.,
     ):
         super().__init__()
+        self.nchw_attn = transformer_cfg.use_nchw_attn
 
         conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock
         self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)
 
         attn_kwargs = dict(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path)
-        partition_layer = PartitionAttention2d if use_nchw_attn else PartitionAttentionCl
-        self.nchw_attn = use_nchw_attn
-        self.attn_block = partition_layer(**attn_kwargs) if use_block_attn else None
+        partition_layer = PartitionAttention2d if self.nchw_attn else PartitionAttentionCl
+        self.attn_block = None if transformer_cfg.no_block_attn else partition_layer(**attn_kwargs)
         self.attn_grid = partition_layer(partition_type='grid', **attn_kwargs)
 
     def init_weights(self, scheme=''):
         if self.attn_block is not None:
             named_apply(partial(_init_transformer, scheme=scheme), self.attn_block)
         named_apply(partial(_init_transformer, scheme=scheme), self.attn_grid)
         named_apply(partial(_init_conv, scheme=scheme), self.conv)
@@ -1071,55 +1105,14 @@
         assert cfg.grid_size
         return cfg
     partition_size = img_size[0] // cfg.partition_ratio, img_size[1] // cfg.partition_ratio
     cfg = replace(cfg, window_size=partition_size, grid_size=partition_size)
     return cfg
 
 
-class NormMlpHead(nn.Module):
-
-    def __init__(
-            self,
-            in_features,
-            num_classes,
-            hidden_size=None,
-            pool_type='avg',
-            drop_rate=0.,
-            norm_layer=nn.LayerNorm,
-            act_layer=nn.Tanh,
-    ):
-        super().__init__()
-        self.drop_rate = drop_rate
-        self.num_features = in_features
-
-        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)
-        self.norm = norm_layer(in_features)
-        self.flatten = nn.Flatten(1) if pool_type else nn.Identity()
-        if hidden_size:
-            self.pre_logits = nn.Sequential(OrderedDict([
-                ('fc', nn.Linear(in_features, hidden_size)),
-                ('act', act_layer()),
-            ]))
-            self.num_features = hidden_size
-        else:
-            self.pre_logits = nn.Identity()
-        self.drop = nn.Dropout(self.drop_rate)
-        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
-        x = self.global_pool(x)
-        x = self.norm(x)
-        x = self.flatten(x)
-        x = self.pre_logits(x)
-        if pre_logits:
-            return x
-        x = self.fc(x)
-        return x
-
-
 def _overlay_kwargs(cfg: MaxxVitCfg, **kwargs):
     transformer_kwargs = {}
     conv_kwargs = {}
     base_kwargs = {}
     for k, v in kwargs.items():
         if k.startswith('transformer_'):
             transformer_kwargs[k.replace('transformer_', '')] = v
@@ -1159,26 +1152,27 @@
             cfg = _overlay_kwargs(cfg, **kwargs)
         transformer_cfg = cfg_window_size(cfg.transformer_cfg, img_size)
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_features = self.embed_dim = cfg.embed_dim[-1]
         self.drop_rate = drop_rate
         self.grad_checkpointing = False
+        self.feature_info = []
 
         self.stem = Stem(
             in_chs=in_chans,
             out_chs=cfg.stem_width,
             padding=cfg.conv_cfg.padding,
             bias=cfg.stem_bias,
             act_layer=cfg.conv_cfg.act_layer,
             norm_layer=cfg.conv_cfg.norm_layer,
             norm_eps=cfg.conv_cfg.norm_eps,
         )
-
         stride = self.stem.stride
+        self.feature_info += [dict(num_chs=self.stem.out_chs, reduction=2, module='stem')]
         feat_size = tuple([i // s for i, s in zip(img_size, to_2tuple(stride))])
 
         num_stages = len(cfg.embed_dim)
         assert len(cfg.depths) == num_stages
         dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]
         in_chs = self.stem.out_chs
         stages = []
@@ -1194,23 +1188,25 @@
                 conv_cfg=cfg.conv_cfg,
                 transformer_cfg=transformer_cfg,
                 feat_size=feat_size,
                 drop_path=dpr[i],
             )]
             stride *= stage_stride
             in_chs = out_chs
+            self.feature_info += [dict(num_chs=out_chs, reduction=stride, module=f'stages.{i}')]
         self.stages = nn.Sequential(*stages)
 
         final_norm_layer = partial(get_norm_layer(cfg.transformer_cfg.norm_layer), eps=cfg.transformer_cfg.norm_eps)
-        if cfg.head_hidden_size:
+        self.head_hidden_size = cfg.head_hidden_size
+        if self.head_hidden_size:
             self.norm = nn.Identity()
-            self.head = NormMlpHead(
+            self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
-                hidden_size=cfg.head_hidden_size,
+                hidden_size=self.head_hidden_size,
                 pool_type=global_pool,
                 drop_rate=drop_rate,
                 norm_layer=final_norm_layer,
             )
         else:
             # standard classifier head w/ norm, pooling, fc classifier
             self.norm = final_norm_layer(self.num_features)
@@ -1249,17 +1245,15 @@
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool=None):
         self.num_classes = num_classes
-        if global_pool is None:
-            global_pool = self.head.global_pool.pool_type
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm(x)
         return x
 
@@ -1372,14 +1366,15 @@
         stride_mode='dw',
         pool_type='avg2',
         conv_norm_layer='layernorm2d',
         conv_norm_layer_cl='layernorm',
         transformer_norm_layer='layernorm2d',
         transformer_norm_layer_cl='layernorm',
         window_size=None,
+        no_block_attn=False,
         init_values=1e-6,
         rel_pos_type='mlp',  # MLP by default for maxxvit
         rel_pos_dim=512,
 ):
     # For experimental models with convnext instead of mbconv
     init_values = to_2tuple(init_values)
     return dict(
@@ -1392,14 +1387,15 @@
             norm_layer=conv_norm_layer,
             norm_layer_cl=conv_norm_layer_cl,
         ),
         transformer_cfg=MaxxVitTransformerCfg(
             expand_first=False,
             pool_type=pool_type,
             window_size=window_size,
+            no_block_attn=no_block_attn,  # enabled for MaxxViT-V2
             init_values=init_values[1],
             norm_layer=transformer_norm_layer,
             norm_layer_cl=transformer_norm_layer_cl,
             rel_pos_type=rel_pos_type,
             rel_pos_dim=rel_pos_dim,
         ),
     )
@@ -1418,336 +1414,331 @@
             head_first=False,  # heads are interleaved (q_nh, q_hdim, k_nh, q_hdim, ....)
             rel_pos_type='bias_tf',
         ),
     )
 
 
 model_cfgs = dict(
-    # Fiddling with configs / defaults / still pretraining
-    coatnet_pico_rw_224=MaxxVitCfg(
+    # timm specific CoAtNet configs
+    coatnet_pico_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(2, 3, 5, 2),
         stem_width=(32, 64),
         **_rw_max_cfg(  # using newer max defaults here
             conv_output_bias=True,
             conv_attn_ratio=0.25,
         ),
     ),
-    coatnet_nano_rw_224=MaxxVitCfg(
+    coatnet_nano_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(3, 4, 6, 3),
         stem_width=(32, 64),
         **_rw_max_cfg(  # using newer max defaults here
             stride_mode='pool',
             conv_output_bias=True,
             conv_attn_ratio=0.25,
         ),
     ),
-    coatnet_0_rw_224=MaxxVitCfg(
+    coatnet_0_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 3, 7, 2),  # deeper than paper '0' model
         stem_width=(32, 64),
         **_rw_coat_cfg(
             conv_attn_early=True,
             transformer_shortcut_bias=False,
         ),
     ),
-    coatnet_1_rw_224=MaxxVitCfg(
+    coatnet_1_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 6, 14, 2),
         stem_width=(32, 64),
         **_rw_coat_cfg(
             stride_mode='dw',
             conv_attn_early=True,
             transformer_shortcut_bias=False,
         )
     ),
-    coatnet_2_rw_224=MaxxVitCfg(
+    coatnet_2_rw=MaxxVitCfg(
         embed_dim=(128, 256, 512, 1024),
         depths=(2, 6, 14, 2),
         stem_width=(64, 128),
         **_rw_coat_cfg(
             stride_mode='dw',
             conv_attn_act_layer='silu',
             #init_values=1e-6,
         ),
     ),
-    coatnet_3_rw_224=MaxxVitCfg(
+    coatnet_3_rw=MaxxVitCfg(
         embed_dim=(192, 384, 768, 1536),
         depths=(2, 6, 14, 2),
         stem_width=(96, 192),
         **_rw_coat_cfg(
             stride_mode='dw',
             conv_attn_act_layer='silu',
             init_values=1e-6,
         ),
     ),
 
-    # Highly experimental configs
-    coatnet_bn_0_rw_224=MaxxVitCfg(
+    # Experimental CoAtNet configs w/ ImageNet-1k train (different norm layers, MLP rel-pos)
+    coatnet_bn_0_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 3, 7, 2),  # deeper than paper '0' model
         stem_width=(32, 64),
         **_rw_coat_cfg(
             stride_mode='dw',
             conv_attn_early=True,
             transformer_shortcut_bias=False,
             transformer_norm_layer='batchnorm2d',
         )
     ),
-    coatnet_rmlp_nano_rw_224=MaxxVitCfg(
+    coatnet_rmlp_nano_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(3, 4, 6, 3),
         stem_width=(32, 64),
         **_rw_max_cfg(
             conv_output_bias=True,
             conv_attn_ratio=0.25,
             rel_pos_type='mlp',
             rel_pos_dim=384,
         ),
     ),
-    coatnet_rmlp_0_rw_224=MaxxVitCfg(
+    coatnet_rmlp_0_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 3, 7, 2),  # deeper than paper '0' model
         stem_width=(32, 64),
         **_rw_coat_cfg(
             stride_mode='dw',
             rel_pos_type='mlp',
         ),
     ),
-    coatnet_rmlp_1_rw_224=MaxxVitCfg(
+    coatnet_rmlp_1_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 6, 14, 2),
         stem_width=(32, 64),
         **_rw_coat_cfg(
             pool_type='max',
             conv_attn_early=True,
             transformer_shortcut_bias=False,
             rel_pos_type='mlp',
             rel_pos_dim=384,  # was supposed to be 512, woops
         ),
     ),
-    coatnet_rmlp_1_rw2_224=MaxxVitCfg(
+    coatnet_rmlp_1_rw2=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 6, 14, 2),
         stem_width=(32, 64),
         **_rw_coat_cfg(
             stride_mode='dw',
             rel_pos_type='mlp',
             rel_pos_dim=512,  # was supposed to be 512, woops
         ),
     ),
-    coatnet_rmlp_2_rw_224=MaxxVitCfg(
+    coatnet_rmlp_2_rw=MaxxVitCfg(
         embed_dim=(128, 256, 512, 1024),
         depths=(2, 6, 14, 2),
         stem_width=(64, 128),
         **_rw_coat_cfg(
             stride_mode='dw',
             conv_attn_act_layer='silu',
             init_values=1e-6,
             rel_pos_type='mlp'
         ),
     ),
-    coatnet_rmlp_3_rw_224=MaxxVitCfg(
+    coatnet_rmlp_3_rw=MaxxVitCfg(
         embed_dim=(192, 384, 768, 1536),
         depths=(2, 6, 14, 2),
         stem_width=(96, 192),
         **_rw_coat_cfg(
             stride_mode='dw',
             conv_attn_act_layer='silu',
             init_values=1e-6,
             rel_pos_type='mlp'
         ),
     ),
 
-    coatnet_nano_cc_224=MaxxVitCfg(
+    coatnet_nano_cc=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(3, 4, 6, 3),
         stem_width=(32, 64),
         block_type=('C', 'C', ('C', 'T'), ('C', 'T')),
         **_rw_coat_cfg(),
     ),
-    coatnext_nano_rw_224=MaxxVitCfg(
+    coatnext_nano_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(3, 4, 6, 3),
         stem_width=(32, 64),
         weight_init='normal',
         **_next_cfg(
             rel_pos_type='bias',
             init_values=(1e-5, None)
         ),
     ),
 
     # Trying to be like the CoAtNet paper configs
-    coatnet_0_224=MaxxVitCfg(
+    coatnet_0=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 3, 5, 2),
         stem_width=64,
+        head_hidden_size=768,
     ),
-    coatnet_1_224=MaxxVitCfg(
+    coatnet_1=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 6, 14, 2),
         stem_width=64,
+        head_hidden_size=768,
     ),
-    coatnet_2_224=MaxxVitCfg(
+    coatnet_2=MaxxVitCfg(
         embed_dim=(128, 256, 512, 1024),
         depths=(2, 6, 14, 2),
         stem_width=128,
+        head_hidden_size=1024,
     ),
-    coatnet_3_224=MaxxVitCfg(
+    coatnet_3=MaxxVitCfg(
         embed_dim=(192, 384, 768, 1536),
         depths=(2, 6, 14, 2),
         stem_width=192,
+        head_hidden_size=1536,
     ),
-    coatnet_4_224=MaxxVitCfg(
+    coatnet_4=MaxxVitCfg(
         embed_dim=(192, 384, 768, 1536),
         depths=(2, 12, 28, 2),
         stem_width=192,
+        head_hidden_size=1536,
     ),
-    coatnet_5_224=MaxxVitCfg(
+    coatnet_5=MaxxVitCfg(
         embed_dim=(256, 512, 1280, 2048),
         depths=(2, 12, 28, 2),
         stem_width=192,
+        head_hidden_size=2048,
     ),
 
     # Experimental MaxVit configs
-    maxvit_pico_rw_256=MaxxVitCfg(
+    maxvit_pico_rw=MaxxVitCfg(
         embed_dim=(32, 64, 128, 256),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(24, 32),
         **_rw_max_cfg(),
     ),
-    maxvit_nano_rw_256=MaxxVitCfg(
+    maxvit_nano_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(1, 2, 3, 1),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         **_rw_max_cfg(),
     ),
-    maxvit_tiny_rw_224=MaxxVitCfg(
+    maxvit_tiny_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         **_rw_max_cfg(),
     ),
-    maxvit_tiny_rw_256=MaxxVitCfg(
+    maxvit_tiny_pm=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(2, 2, 5, 2),
-        block_type=('M',) * 4,
+        block_type=('PM',) * 4,
         stem_width=(32, 64),
         **_rw_max_cfg(),
     ),
 
-    maxvit_rmlp_pico_rw_256=MaxxVitCfg(
+    maxvit_rmlp_pico_rw=MaxxVitCfg(
         embed_dim=(32, 64, 128, 256),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(24, 32),
         **_rw_max_cfg(rel_pos_type='mlp'),
     ),
-    maxvit_rmlp_nano_rw_256=MaxxVitCfg(
+    maxvit_rmlp_nano_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(1, 2, 3, 1),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         **_rw_max_cfg(rel_pos_type='mlp'),
     ),
-    maxvit_rmlp_tiny_rw_256=MaxxVitCfg(
+    maxvit_rmlp_tiny_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         **_rw_max_cfg(rel_pos_type='mlp'),
     ),
-    maxvit_rmlp_small_rw_224=MaxxVitCfg(
+    maxvit_rmlp_small_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         **_rw_max_cfg(
             rel_pos_type='mlp',
             init_values=1e-6,
         ),
     ),
-    maxvit_rmlp_small_rw_256=MaxxVitCfg(
-        embed_dim=(96, 192, 384, 768),
-        depths=(2, 2, 5, 2),
-        block_type=('M',) * 4,
-        stem_width=(32, 64),
-        **_rw_max_cfg(
-            rel_pos_type='mlp',
-            init_values=1e-6,
-        ),
-    ),
-    maxvit_rmlp_base_rw_224=MaxxVitCfg(
+    maxvit_rmlp_base_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 6, 14, 2),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         head_hidden_size=768,
         **_rw_max_cfg(
             rel_pos_type='mlp',
         ),
     ),
-    maxvit_rmlp_base_rw_384=MaxxVitCfg(
-        embed_dim=(96, 192, 384, 768),
-        depths=(2, 6, 14, 2),
-        block_type=('M',) * 4,
-        stem_width=(32, 64),
-        head_hidden_size=768,
-        **_rw_max_cfg(
-            rel_pos_type='mlp',
-        ),
-    ),
-
-    maxvit_tiny_pm_256=MaxxVitCfg(
-        embed_dim=(64, 128, 256, 512),
-        depths=(2, 2, 5, 2),
-        block_type=('PM',) * 4,
-        stem_width=(32, 64),
-        **_rw_max_cfg(),
-    ),
 
-    maxxvit_rmlp_nano_rw_256=MaxxVitCfg(
+    maxxvit_rmlp_nano_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(1, 2, 3, 1),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         weight_init='normal',
         **_next_cfg(),
     ),
-    maxxvit_rmlp_tiny_rw_256=MaxxVitCfg(
+    maxxvit_rmlp_tiny_rw=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(32, 64),
         **_next_cfg(),
     ),
-    maxxvit_rmlp_small_rw_256=MaxxVitCfg(
+    maxxvit_rmlp_small_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
         stem_width=(48, 96),
         **_next_cfg(),
     ),
-    maxxvit_rmlp_base_rw_224=MaxxVitCfg(
+
+    maxxvitv2_nano_rw=MaxxVitCfg(
         embed_dim=(96, 192, 384, 768),
-        depths=(2, 6, 14, 2),
+        depths=(1, 2, 3, 1),
         block_type=('M',) * 4,
         stem_width=(48, 96),
-        **_next_cfg(),
+        weight_init='normal',
+        **_next_cfg(
+            no_block_attn=True,
+            rel_pos_type='bias',
+        ),
     ),
-    maxxvit_rmlp_large_rw_224=MaxxVitCfg(
+    maxxvitv2_rmlp_base_rw=MaxxVitCfg(
         embed_dim=(128, 256, 512, 1024),
         depths=(2, 6, 12, 2),
         block_type=('M',) * 4,
         stem_width=(64, 128),
-        **_next_cfg(),
+        **_next_cfg(
+            no_block_attn=True,
+        ),
+    ),
+    maxxvitv2_rmlp_large_rw=MaxxVitCfg(
+        embed_dim=(160, 320, 640, 1280),
+        depths=(2, 6, 16, 2),
+        block_type=('M',) * 4,
+        stem_width=(80, 160),
+        head_hidden_size=1280,
+        **_next_cfg(
+            no_block_attn=True,
+        ),
     ),
 
     # Trying to be like the MaxViT paper configs
     maxvit_tiny_tf=MaxxVitCfg(
         embed_dim=(64, 128, 256, 512),
         depths=(2, 2, 5, 2),
         block_type=('M',) * 4,
@@ -1791,19 +1782,37 @@
         stem_bias=True,
         head_hidden_size=1536,
         **_tf_cfg(),
     ),
 )
 
 
+def checkpoint_filter_fn(state_dict, model: nn.Module):
+    model_state_dict = model.state_dict()
+    out_dict = {}
+    for k, v in state_dict.items():
+        if k in model_state_dict and v.ndim != model_state_dict[k].ndim and v.numel() == model_state_dict[k].numel():
+            # adapt between conv2d / linear layers
+            assert v.ndim in (2, 4)
+            v = v.reshape(model_state_dict[k].shape)
+        out_dict[k] = v
+    return out_dict
+
+
 def _create_maxxvit(variant, cfg_variant=None, pretrained=False, **kwargs):
+    if cfg_variant is None:
+        if variant in model_cfgs:
+            cfg_variant = variant
+        else:
+            cfg_variant = '_'.join(variant.split('_')[:-1])
     return build_model_with_cfg(
         MaxxVit, variant, pretrained,
-        model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],
+        model_cfg=model_cfgs[cfg_variant],
         feature_cfg=dict(flatten_sequential=True),
+        pretrained_filter_fn=checkpoint_filter_fn,
         **kwargs)
 
 
 def _cfg(url='', **kwargs):
     return {
         'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
         'crop_pct': 0.95, 'interpolation': 'bicubic',
@@ -1811,422 +1820,503 @@
         'first_conv': 'stem.conv1', 'classifier': 'head.fc',
         'fixed_input_size': True,
         **kwargs
     }
 
 
 default_cfgs = generate_default_cfgs({
-    # Fiddling with configs / defaults / still pretraining
-    'coatnet_pico_rw_224': _cfg(url=''),
-    'coatnet_nano_rw_224': _cfg(
+    # timm specific CoAtNet configs, ImageNet-1k pretrain, fixed rel-pos
+    'coatnet_pico_rw_224.untrained': _cfg(url=''),
+    'coatnet_nano_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_nano_rw_224_sw-f53093b4.pth',
         crop_pct=0.9),
-    'coatnet_0_rw_224': _cfg(
+    'coatnet_0_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_0_rw_224_sw-a6439706.pth'),
-    'coatnet_1_rw_224': _cfg(
+    'coatnet_1_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_1_rw_224_sw-5cae1ea8.pth'
     ),
-    'coatnet_2_rw_224': _cfg(url=''),
-    'coatnet_3_rw_224': _cfg(url=''),
 
-    # Highly experimental configs
-    'coatnet_bn_0_rw_224': _cfg(
+    # timm specific CoAtNet configs, ImageNet-12k pretrain w/ 1k fine-tune, fixed rel-pos
+    'coatnet_2_rw_224.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/'),
+    #'coatnet_3_rw_224.untrained': _cfg(url=''),
+
+    # Experimental CoAtNet configs w/ ImageNet-12k pretrain -> 1k fine-tune (different norm layers, MLP rel-pos)
+    'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/'),
+    'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/'),
+    'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+
+    # Experimental CoAtNet configs w/ ImageNet-1k train (different norm layers, MLP rel-pos)
+    'coatnet_bn_0_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_bn_0_rw_224_sw-c228e218.pth',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,
         crop_pct=0.95),
-    'coatnet_rmlp_nano_rw_224': _cfg(
+    'coatnet_rmlp_nano_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_nano_rw_224_sw-bd1d51b3.pth',
         crop_pct=0.9),
-    'coatnet_rmlp_0_rw_224': _cfg(url=''),
-    'coatnet_rmlp_1_rw_224': _cfg(
+    'coatnet_rmlp_0_rw_224.untrained': _cfg(url=''),
+    'coatnet_rmlp_1_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_1_rw_224_sw-9051e6c3.pth'),
-    'coatnet_rmlp_1_rw2_224': _cfg(url=''),
-    'coatnet_rmlp_2_rw_224': _cfg(
+    'coatnet_rmlp_2_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_2_rw_224_sw-5ccfac55.pth'),
-    'coatnet_rmlp_3_rw_224': _cfg(url=''),
-    'coatnet_nano_cc_224': _cfg(url=''),
-    'coatnext_nano_rw_224': _cfg(
+    'coatnet_rmlp_3_rw_224.untrained': _cfg(url=''),
+    'coatnet_nano_cc_224.untrained': _cfg(url=''),
+    'coatnext_nano_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnext_nano_rw_224_ad-22cb71c2.pth',
         crop_pct=0.9),
 
-    # Trying to be like the CoAtNet paper configs
-    'coatnet_0_224': _cfg(url=''),
-    'coatnet_1_224': _cfg(url=''),
-    'coatnet_2_224': _cfg(url=''),
-    'coatnet_3_224': _cfg(url=''),
-    'coatnet_4_224': _cfg(url=''),
-    'coatnet_5_224': _cfg(url=''),
-
-    # Experimental configs
-    'maxvit_pico_rw_256': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_nano_rw_256': _cfg(
+    # ImagenNet-12k pretrain CoAtNet
+    'coatnet_2_rw_224.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
+    'coatnet_3_rw_224.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
+    'coatnet_rmlp_1_rw2_224.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
+    'coatnet_rmlp_2_rw_224.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
+
+    # Trying to be like the CoAtNet paper configs (will adapt if 'tf' weights are ever released)
+    'coatnet_0_224.untrained': _cfg(url=''),
+    'coatnet_1_224.untrained': _cfg(url=''),
+    'coatnet_2_224.untrained': _cfg(url=''),
+    'coatnet_3_224.untrained': _cfg(url=''),
+    'coatnet_4_224.untrained': _cfg(url=''),
+    'coatnet_5_224.untrained': _cfg(url=''),
+
+    # timm specific MaxVit configs, ImageNet-1k pretrain or untrained
+    'maxvit_pico_rw_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),
+    'maxvit_nano_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_nano_rw_256_sw-fb127241.pth',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_tiny_rw_224': _cfg(
+    'maxvit_tiny_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_tiny_rw_224_sw-7d0dffeb.pth'),
-    'maxvit_tiny_rw_256': _cfg(
+    'maxvit_tiny_rw_256.untrained': _cfg(
         url='',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_rmlp_pico_rw_256': _cfg(
+    'maxvit_tiny_pm_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),
+
+    # timm specific MaxVit w/ MLP rel-pos, ImageNet-1k pretrain
+    'maxvit_rmlp_pico_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_pico_rw_256_sw-8d82f2c6.pth',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_rmlp_nano_rw_256': _cfg(
+    'maxvit_rmlp_nano_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_nano_rw_256_sw-c17bb0d6.pth',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_rmlp_tiny_rw_256': _cfg(
+    'maxvit_rmlp_tiny_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_tiny_rw_256_sw-bbef0ff5.pth',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_rmlp_small_rw_224': _cfg(
+    'maxvit_rmlp_small_rw_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_small_rw_224_sw-6ef0ae4f.pth',
         crop_pct=0.9,
     ),
-    'maxvit_rmlp_small_rw_256': _cfg(
+    'maxvit_rmlp_small_rw_256.untrained': _cfg(
         url='',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxvit_rmlp_base_rw_224': _cfg(
-        url='',
-    ),
-    'maxvit_rmlp_base_rw_384': _cfg(
-        url='',
-        input_size=(3, 384, 384), pool_size=(12, 12)),
 
-    'maxvit_tiny_pm_256': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),
-
-    'maxxvit_rmlp_nano_rw_256': _cfg(
+    # timm specific MaxVit w/ ImageNet-12k pretrain and 1k fine-tune
+    'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+    ),
+    'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+
+    # timm specific MaxVit w/ ImageNet-12k pretrain
+    'maxvit_rmlp_base_rw_224.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821,
+    ),
+
+    # timm MaxxViT configs (ConvNeXt conv blocks mixed with MaxVit transformer blocks)
+    'maxxvit_rmlp_nano_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxxvit_rmlp_nano_rw_256_sw-0325d459.pth',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxxvit_rmlp_tiny_rw_256': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxxvit_rmlp_small_rw_256': _cfg(
+    'maxxvit_rmlp_tiny_rw_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),
+    'maxxvit_rmlp_small_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxxvit_rmlp_small_rw_256_sw-37e217ff.pth',
         input_size=(3, 256, 256), pool_size=(8, 8)),
-    'maxxvit_rmlp_base_rw_224': _cfg(url=''),
-    'maxxvit_rmlp_large_rw_224': _cfg(url=''),
 
+    # timm MaxxViT-V2 configs (ConvNeXt conv blocks mixed with MaxVit transformer blocks, more width, no block attn)
+    'maxxvitv2_nano_rw_256.sw_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256), pool_size=(8, 8)),
+    'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/'),
+    'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
+    'maxxvitv2_rmlp_large_rw_224.untrained': _cfg(url=''),
+
+    'maxxvitv2_rmlp_base_rw_224.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
 
     # MaxViT models ported from official Tensorflow impl
     'maxvit_tiny_tf_224.in1k': _cfg(
-        hf_hub_id='timm/maxvit_tiny_tf_224.in1k',
+        hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
     'maxvit_tiny_tf_384.in1k': _cfg(
-        hf_hub_id='timm/maxvit_tiny_tf_384.in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_tiny_tf_512.in1k': _cfg(
-        hf_hub_id='timm/maxvit_tiny_tf_512.in1k',
-        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),
     'maxvit_small_tf_224.in1k': _cfg(
-        hf_hub_id='timm/maxvit_small_tf_224.in1k',
+        hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
     'maxvit_small_tf_384.in1k': _cfg(
-        hf_hub_id='timm/maxvit_small_tf_384.in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_small_tf_512.in1k': _cfg(
-        hf_hub_id='timm/maxvit_small_tf_512.in1k',
-        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),
     'maxvit_base_tf_224.in1k': _cfg(
-        hf_hub_id='timm/maxvit_base_tf_224.in1k',
+        hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
     'maxvit_base_tf_384.in1k': _cfg(
-        hf_hub_id='timm/maxvit_base_tf_384.in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_base_tf_512.in1k': _cfg(
-        hf_hub_id='timm/maxvit_base_tf_512.in1k',
-        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),
     'maxvit_large_tf_224.in1k': _cfg(
-        hf_hub_id='timm/maxvit_large_tf_224.in1k',
+        hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
     'maxvit_large_tf_384.in1k': _cfg(
-        hf_hub_id='timm/maxvit_large_tf_384.in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_large_tf_512.in1k': _cfg(
-        hf_hub_id='timm/maxvit_large_tf_512.in1k',
-        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),
 
     'maxvit_base_tf_224.in21k': _cfg(
-        url=''),
+        hf_hub_id='timm/',
+        num_classes=21843),
     'maxvit_base_tf_384.in21k_ft_in1k': _cfg(
-        hf_hub_id='timm/maxvit_base_tf_384.in21k_ft_in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_base_tf_512.in21k_ft_in1k': _cfg(
-        hf_hub_id='timm/maxvit_base_tf_512.in21k_ft_in1k',
-        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),
     'maxvit_large_tf_224.in21k': _cfg(
-        url=''),
+        hf_hub_id='timm/',
+        num_classes=21843),
     'maxvit_large_tf_384.in21k_ft_in1k': _cfg(
-        hf_hub_id='timm/maxvit_large_tf_384.in21k_ft_in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_large_tf_512.in21k_ft_in1k': _cfg(
-        hf_hub_id='timm/maxvit_large_tf_512.in21k_ft_in1k',
+        hf_hub_id='timm/',
         input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
     'maxvit_xlarge_tf_224.in21k': _cfg(
-        url=''),
+        hf_hub_id='timm/',
+        num_classes=21843),
     'maxvit_xlarge_tf_384.in21k_ft_in1k': _cfg(
-        hf_hub_id='timm/maxvit_xlarge_tf_384.in21k_ft_in1k',
-        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),
     'maxvit_xlarge_tf_512.in21k_ft_in1k': _cfg(
-        hf_hub_id='timm/maxvit_xlarge_tf_512.in21k_ft_in1k',
-        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),
+        hf_hub_id='timm/',
+        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),
 })
 
 
 @register_model
-def coatnet_pico_rw_224(pretrained=False, **kwargs):
+def coatnet_pico_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_pico_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_nano_rw_224(pretrained=False, **kwargs):
+def coatnet_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_nano_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_0_rw_224(pretrained=False, **kwargs):
+def coatnet_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_0_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_1_rw_224(pretrained=False, **kwargs):
+def coatnet_1_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_1_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_2_rw_224(pretrained=False, **kwargs):
+def coatnet_2_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_2_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_3_rw_224(pretrained=False, **kwargs):
+def coatnet_3_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_3_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_bn_0_rw_224(pretrained=False, **kwargs):
+def coatnet_bn_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_bn_0_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_rmlp_nano_rw_224(pretrained=False, **kwargs):
+def coatnet_rmlp_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_rmlp_nano_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_rmlp_0_rw_224(pretrained=False, **kwargs):
+def coatnet_rmlp_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_rmlp_0_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_rmlp_1_rw_224(pretrained=False, **kwargs):
+def coatnet_rmlp_1_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_rmlp_1_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_rmlp_1_rw2_224(pretrained=False, **kwargs):
+def coatnet_rmlp_1_rw2_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_rmlp_1_rw2_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_rmlp_2_rw_224(pretrained=False, **kwargs):
+def coatnet_rmlp_2_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_rmlp_2_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_rmlp_3_rw_224(pretrained=False, **kwargs):
+def coatnet_rmlp_2_rw_384(pretrained=False, **kwargs) -> MaxxVit:
+    return _create_maxxvit('coatnet_rmlp_2_rw_384', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def coatnet_rmlp_3_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_rmlp_3_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_nano_cc_224(pretrained=False, **kwargs):
+def coatnet_nano_cc_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_nano_cc_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnext_nano_rw_224(pretrained=False, **kwargs):
+def coatnext_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnext_nano_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_0_224(pretrained=False, **kwargs):
+def coatnet_0_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_0_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_1_224(pretrained=False, **kwargs):
+def coatnet_1_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_1_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_2_224(pretrained=False, **kwargs):
+def coatnet_2_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_2_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_3_224(pretrained=False, **kwargs):
+def coatnet_3_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_3_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_4_224(pretrained=False, **kwargs):
+def coatnet_4_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_4_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def coatnet_5_224(pretrained=False, **kwargs):
+def coatnet_5_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('coatnet_5_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_pico_rw_256(pretrained=False, **kwargs):
+def maxvit_pico_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_pico_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_nano_rw_256(pretrained=False, **kwargs):
+def maxvit_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_nano_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_tiny_rw_224(pretrained=False, **kwargs):
+def maxvit_tiny_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_tiny_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_tiny_rw_256(pretrained=False, **kwargs):
+def maxvit_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_tiny_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_pico_rw_256(pretrained=False, **kwargs):
+def maxvit_rmlp_pico_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_pico_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_nano_rw_256(pretrained=False, **kwargs):
+def maxvit_rmlp_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs):
+def maxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_small_rw_224(pretrained=False, **kwargs):
+def maxvit_rmlp_small_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_small_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_small_rw_256(pretrained=False, **kwargs):
+def maxvit_rmlp_small_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_base_rw_224(pretrained=False, **kwargs):
+def maxvit_rmlp_base_rw_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_base_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_rmlp_base_rw_384(pretrained=False, **kwargs):
+def maxvit_rmlp_base_rw_384(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_rmlp_base_rw_384', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_tiny_pm_256(pretrained=False, **kwargs):
+def maxvit_tiny_pm_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_tiny_pm_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxxvit_rmlp_nano_rw_256(pretrained=False, **kwargs):
+def maxxvit_rmlp_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs):
+def maxxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxxvit_rmlp_small_rw_256(pretrained=False, **kwargs):
+def maxxvit_rmlp_small_rw_256(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxxvit_rmlp_base_rw_224(pretrained=False, **kwargs):
-    return _create_maxxvit('maxxvit_rmlp_base_rw_224', pretrained=pretrained, **kwargs)
+def maxxvitv2_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:
+    return _create_maxxvit('maxxvitv2_nano_rw_256', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def maxxvitv2_rmlp_base_rw_224(pretrained=False, **kwargs) -> MaxxVit:
+    return _create_maxxvit('maxxvitv2_rmlp_base_rw_224', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def maxxvitv2_rmlp_base_rw_384(pretrained=False, **kwargs) -> MaxxVit:
+    return _create_maxxvit('maxxvitv2_rmlp_base_rw_384', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxxvit_rmlp_large_rw_224(pretrained=False, **kwargs):
-    return _create_maxxvit('maxxvit_rmlp_large_rw_224', pretrained=pretrained, **kwargs)
+def maxxvitv2_rmlp_large_rw_224(pretrained=False, **kwargs) -> MaxxVit:
+    return _create_maxxvit('maxxvitv2_rmlp_large_rw_224', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_tiny_tf_224(pretrained=False, **kwargs):
+def maxvit_tiny_tf_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_tiny_tf_224', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_tiny_tf_384(pretrained=False, **kwargs):
+def maxvit_tiny_tf_384(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_tiny_tf_384', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_tiny_tf_512(pretrained=False, **kwargs):
+def maxvit_tiny_tf_512(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_tiny_tf_512', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_small_tf_224(pretrained=False, **kwargs):
+def maxvit_small_tf_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_small_tf_224', 'maxvit_small_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_small_tf_384(pretrained=False, **kwargs):
+def maxvit_small_tf_384(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_small_tf_384', 'maxvit_small_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_small_tf_512(pretrained=False, **kwargs):
+def maxvit_small_tf_512(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_small_tf_512', 'maxvit_small_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_base_tf_224(pretrained=False, **kwargs):
+def maxvit_base_tf_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_base_tf_224', 'maxvit_base_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_base_tf_384(pretrained=False, **kwargs):
+def maxvit_base_tf_384(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_base_tf_384', 'maxvit_base_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_base_tf_512(pretrained=False, **kwargs):
+def maxvit_base_tf_512(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_base_tf_512', 'maxvit_base_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_large_tf_224(pretrained=False, **kwargs):
+def maxvit_large_tf_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_large_tf_224', 'maxvit_large_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_large_tf_384(pretrained=False, **kwargs):
+def maxvit_large_tf_384(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_large_tf_384', 'maxvit_large_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_large_tf_512(pretrained=False, **kwargs):
+def maxvit_large_tf_512(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_large_tf_512', 'maxvit_large_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_xlarge_tf_224(pretrained=False, **kwargs):
+def maxvit_xlarge_tf_224(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_xlarge_tf_224', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_xlarge_tf_384(pretrained=False, **kwargs):
+def maxvit_xlarge_tf_384(pretrained=False, **kwargs) -> MaxxVit:
     return _create_maxxvit('maxvit_xlarge_tf_384', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def maxvit_xlarge_tf_512(pretrained=False, **kwargs):
-    return _create_maxxvit('maxvit_xlarge_tf_512', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
+def maxvit_xlarge_tf_512(pretrained=False, **kwargs) -> MaxxVit:
+    return _create_maxxvit('maxvit_xlarge_tf_512', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/mlp_mixer.py` & `timm-0.9.0/timm/models/mlp_mixer.py`

 * *Files 14% similar despite different names*

```diff
@@ -44,118 +44,34 @@
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import PatchEmbed, Mlp, GluMlp, GatedMlp, DropPath, lecun_normal_, to_2tuple
 from ._builder import build_model_with_cfg
 from ._manipulate import named_apply, checkpoint_seq
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
-__all__ = ['MixerBlock']  # model_registry will add each entrypoint fn to this
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': 0.875, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),
-        'first_conv': 'stem.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    mixer_s32_224=_cfg(),
-    mixer_s16_224=_cfg(),
-    mixer_b32_224=_cfg(),
-    mixer_b16_224=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_b16_224-76587d61.pth',
-    ),
-    mixer_b16_224_in21k=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_b16_224_in21k-617b3de2.pth',
-        num_classes=21843
-    ),
-    mixer_l32_224=_cfg(),
-    mixer_l16_224=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_l16_224-92f9adc4.pth',
-    ),
-    mixer_l16_224_in21k=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_l16_224_in21k-846aa33c.pth',
-        num_classes=21843
-    ),
-
-    # Mixer ImageNet-21K-P pretraining
-    mixer_b16_224_miil_in21k=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mixer_b16_224_miil_in21k-2a558a71.pth',
-        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear', num_classes=11221,
-    ),
-    mixer_b16_224_miil=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mixer_b16_224_miil-9229a591.pth',
-        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear',
-    ),
-
-    gmixer_12_224=_cfg(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    gmixer_24_224=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gmixer_24_224_raa-7daf7ae6.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-
-    resmlp_12_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_no_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_24_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_no_dist.pth',
-        #url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resmlp_24_224_raa-a8256759.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_36_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_36_no_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_big_24_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_no_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-
-    resmlp_12_distilled_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_24_distilled_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_36_distilled_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_36_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_big_24_distilled_224=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_dist.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-
-    resmlp_big_24_224_in22ft1k=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_22k.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-
-    resmlp_12_224_dino=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_dino.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-    resmlp_24_224_dino=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_dino.pth',
-        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
-
-    gmlp_ti16_224=_cfg(),
-    gmlp_s16_224=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gmlp_s16_224_raa-10536d42.pth',
-    ),
-    gmlp_b16_224=_cfg(),
-)
+__all__ = ['MixerBlock', 'MlpMixer']  # model_registry will add each entrypoint fn to this
 
 
 class MixerBlock(nn.Module):
     """ Residual Block w/ token mixing and channel MLPs
     Based on: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     def __init__(
-            self, dim, seq_len, mlp_ratio=(0.5, 4.0), mlp_layer=Mlp,
-            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=nn.GELU, drop=0., drop_path=0.):
+            self,
+            dim,
+            seq_len,
+            mlp_ratio=(0.5, 4.0),
+            mlp_layer=Mlp,
+            norm_layer=partial(nn.LayerNorm, eps=1e-6),
+            act_layer=nn.GELU,
+            drop=0.,
+            drop_path=0.,
+    ):
         super().__init__()
         tokens_dim, channels_dim = [int(x * dim) for x in to_2tuple(mlp_ratio)]
         self.norm1 = norm_layer(dim)
         self.mlp_tokens = mlp_layer(seq_len, tokens_dim, act_layer=act_layer, drop=drop)
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         self.mlp_channels = mlp_layer(dim, channels_dim, act_layer=act_layer, drop=drop)
@@ -178,16 +94,25 @@
 
 class ResBlock(nn.Module):
     """ Residual MLP block w/ LayerScale and Affine 'norm'
 
     Based on: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
     """
     def __init__(
-            self, dim, seq_len, mlp_ratio=4, mlp_layer=Mlp, norm_layer=Affine,
-            act_layer=nn.GELU, init_values=1e-4, drop=0., drop_path=0.):
+            self,
+            dim,
+            seq_len,
+            mlp_ratio=4,
+            mlp_layer=Mlp,
+            norm_layer=Affine,
+            act_layer=nn.GELU,
+            init_values=1e-4,
+            drop=0.,
+            drop_path=0.,
+    ):
         super().__init__()
         channel_dim = int(dim * mlp_ratio)
         self.norm1 = norm_layer(dim)
         self.linear_tokens = nn.Linear(seq_len, seq_len)
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         self.mlp_channels = mlp_layer(dim, channel_dim, act_layer=act_layer, drop=drop)
@@ -225,16 +150,24 @@
 
 class SpatialGatingBlock(nn.Module):
     """ Residual Block w/ Spatial Gating
 
     Based on: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050
     """
     def __init__(
-            self, dim, seq_len, mlp_ratio=4, mlp_layer=GatedMlp,
-            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=nn.GELU, drop=0., drop_path=0.):
+            self,
+            dim,
+            seq_len,
+            mlp_ratio=4,
+            mlp_layer=GatedMlp,
+            norm_layer=partial(nn.LayerNorm, eps=1e-6),
+            act_layer=nn.GELU,
+            drop=0.,
+            drop_path=0.,
+    ):
         super().__init__()
         channel_dim = int(dim * mlp_ratio)
         self.norm = norm_layer(dim)
         sgu = partial(SpatialGatingUnit, seq_len=seq_len)
         self.mlp_channels = mlp_layer(dim, channel_dim, act_layer=act_layer, gate_layer=sgu, drop=drop)
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
@@ -255,35 +188,48 @@
             embed_dim=512,
             mlp_ratio=(0.5, 4.0),
             block_layer=MixerBlock,
             mlp_layer=Mlp,
             norm_layer=partial(nn.LayerNorm, eps=1e-6),
             act_layer=nn.GELU,
             drop_rate=0.,
+            proj_drop_rate=0.,
             drop_path_rate=0.,
             nlhb=False,
             stem_norm=False,
             global_pool='avg',
     ):
         super().__init__()
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
         self.grad_checkpointing = False
 
         self.stem = PatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans,
-            embed_dim=embed_dim, norm_layer=norm_layer if stem_norm else None)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+            norm_layer=norm_layer if stem_norm else None,
+        )
         # FIXME drop_path (stochastic depth scaling rule or all the same?)
         self.blocks = nn.Sequential(*[
             block_layer(
-                embed_dim, self.stem.num_patches, mlp_ratio, mlp_layer=mlp_layer, norm_layer=norm_layer,
-                act_layer=act_layer, drop=drop_rate, drop_path=drop_path_rate)
+                embed_dim,
+                self.stem.num_patches,
+                mlp_ratio,
+                mlp_layer=mlp_layer,
+                norm_layer=norm_layer,
+                act_layer=act_layer,
+                drop=proj_drop_rate,
+                drop_path=drop_path_rate,
+            )
             for _ in range(num_blocks)])
         self.norm = norm_layer(embed_dim)
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()
 
         self.init_weights(nlhb=nlhb)
 
     @torch.jit.ignore
     def init_weights(self, nlhb=False):
         head_bias = -math.log(self.num_classes) if nlhb else 0.
@@ -316,19 +262,23 @@
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
             x = self.blocks(x)
         x = self.norm(x)
         return x
 
-    def forward(self, x):
-        x = self.forward_features(x)
+    def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool == 'avg':
             x = x.mean(dim=1)
-        x = self.head(x)
+        x = self.head_drop(x)
+        return x if pre_logits else self.head(x)
+
+    def forward(self, x):
+        x = self.forward_features(x)
+        x = self.forward_head(x)
         return x
 
 
 def _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):
     """ Mixer weight initialization (trying to match Flax defaults)
     """
     if isinstance(module, nn.Linear):
@@ -380,304 +330,307 @@
 
 
 def _create_mixer(variant, pretrained=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for MLP-Mixer models.')
 
     model = build_model_with_cfg(
-        MlpMixer, variant, pretrained,
+        MlpMixer,
+        variant,
+        pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
-        **kwargs)
+        **kwargs,
+    )
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': 0.875, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),
+        'first_conv': 'stem.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'mixer_s32_224.untrained': _cfg(),
+    'mixer_s16_224.untrained': _cfg(),
+    'mixer_b32_224.untrained': _cfg(),
+    'mixer_b16_224.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_b16_224-76587d61.pth',
+    ),
+    'mixer_b16_224.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_b16_224_in21k-617b3de2.pth',
+        num_classes=21843
+    ),
+    'mixer_l32_224.untrained': _cfg(),
+    'mixer_l16_224.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_l16_224-92f9adc4.pth',
+    ),
+    'mixer_l16_224.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_l16_224_in21k-846aa33c.pth',
+        num_classes=21843
+    ),
+
+    # Mixer ImageNet-21K-P pretraining
+    'mixer_b16_224.miil_in21k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mixer_b16_224_miil_in21k-2a558a71.pth',
+        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear', num_classes=11221,
+    ),
+    'mixer_b16_224.miil_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mixer_b16_224_miil-9229a591.pth',
+        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear',
+    ),
+
+    'gmixer_12_224.untrained': _cfg(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'gmixer_24_224.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gmixer_24_224_raa-7daf7ae6.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+
+    'resmlp_12_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_no_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_24_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_no_dist.pth',
+        #url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resmlp_24_224_raa-a8256759.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_36_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_36_no_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_big_24_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_no_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+
+    'resmlp_12_224.fb_distilled_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_24_224.fb_distilled_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_36_224.fb_distilled_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_36_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_big_24_224.fb_distilled_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_dist.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+
+    'resmlp_big_24_224.fb_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_22k.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+
+    'resmlp_12_224.fb_dino': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_dino.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+    'resmlp_24_224.fb_dino': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_dino.pth',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),
+
+    'gmlp_ti16_224.untrained': _cfg(),
+    'gmlp_s16_224.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gmlp_s16_224_raa-10536d42.pth',
+    ),
+    'gmlp_b16_224.untrained': _cfg(),
+})
+
+
 @register_model
-def mixer_s32_224(pretrained=False, **kwargs):
+def mixer_s32_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Mixer-S/32 224x224
     Paper: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     model_args = dict(patch_size=32, num_blocks=8, embed_dim=512, **kwargs)
     model = _create_mixer('mixer_s32_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def mixer_s16_224(pretrained=False, **kwargs):
+def mixer_s16_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Mixer-S/16 224x224
     Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     model_args = dict(patch_size=16, num_blocks=8, embed_dim=512, **kwargs)
     model = _create_mixer('mixer_s16_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def mixer_b32_224(pretrained=False, **kwargs):
+def mixer_b32_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Mixer-B/32 224x224
     Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     model_args = dict(patch_size=32, num_blocks=12, embed_dim=768, **kwargs)
     model = _create_mixer('mixer_b32_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def mixer_b16_224(pretrained=False, **kwargs):
+def mixer_b16_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Mixer-B/16 224x224. ImageNet-1k pretrained weights.
     Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     model_args = dict(patch_size=16, num_blocks=12, embed_dim=768, **kwargs)
     model = _create_mixer('mixer_b16_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def mixer_b16_224_in21k(pretrained=False, **kwargs):
-    """ Mixer-B/16 224x224. ImageNet-21k pretrained weights.
-    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
-    """
-    model_args = dict(patch_size=16, num_blocks=12, embed_dim=768, **kwargs)
-    model = _create_mixer('mixer_b16_224_in21k', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def mixer_l32_224(pretrained=False, **kwargs):
+def mixer_l32_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Mixer-L/32 224x224.
     Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     model_args = dict(patch_size=32, num_blocks=24, embed_dim=1024, **kwargs)
     model = _create_mixer('mixer_l32_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def mixer_l16_224(pretrained=False, **kwargs):
+def mixer_l16_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Mixer-L/16 224x224. ImageNet-1k pretrained weights.
     Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
     """
     model_args = dict(patch_size=16, num_blocks=24, embed_dim=1024, **kwargs)
     model = _create_mixer('mixer_l16_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def mixer_l16_224_in21k(pretrained=False, **kwargs):
-    """ Mixer-L/16 224x224. ImageNet-21k pretrained weights.
-    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
-    """
-    model_args = dict(patch_size=16, num_blocks=24, embed_dim=1024, **kwargs)
-    model = _create_mixer('mixer_l16_224_in21k', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def mixer_b16_224_miil(pretrained=False, **kwargs):
-    """ Mixer-B/16 224x224. ImageNet-21k pretrained weights.
-    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K
-    """
-    model_args = dict(patch_size=16, num_blocks=12, embed_dim=768, **kwargs)
-    model = _create_mixer('mixer_b16_224_miil', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def mixer_b16_224_miil_in21k(pretrained=False, **kwargs):
-    """ Mixer-B/16 224x224. ImageNet-1k pretrained weights.
-    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K
-    """
-    model_args = dict(patch_size=16, num_blocks=12, embed_dim=768, **kwargs)
-    model = _create_mixer('mixer_b16_224_miil_in21k', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def gmixer_12_224(pretrained=False, **kwargs):
+def gmixer_12_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Glu-Mixer-12 224x224
-    Experiment by Ross Wightman, adding (Si)GLU to MLP-Mixer
+    Experiment by Ross Wightman, adding SwiGLU to MLP-Mixer
     """
     model_args = dict(
         patch_size=16, num_blocks=12, embed_dim=384, mlp_ratio=(1.0, 4.0),
         mlp_layer=GluMlp, act_layer=nn.SiLU, **kwargs)
     model = _create_mixer('gmixer_12_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def gmixer_24_224(pretrained=False, **kwargs):
+def gmixer_24_224(pretrained=False, **kwargs) -> MlpMixer:
     """ Glu-Mixer-24 224x224
-    Experiment by Ross Wightman, adding (Si)GLU to MLP-Mixer
+    Experiment by Ross Wightman, adding SwiGLU to MLP-Mixer
     """
     model_args = dict(
         patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=(1.0, 4.0),
         mlp_layer=GluMlp, act_layer=nn.SiLU, **kwargs)
     model = _create_mixer('gmixer_24_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def resmlp_12_224(pretrained=False, **kwargs):
+def resmlp_12_224(pretrained=False, **kwargs) -> MlpMixer:
     """ ResMLP-12
     Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
     """
     model_args = dict(
         patch_size=16, num_blocks=12, embed_dim=384, mlp_ratio=4, block_layer=ResBlock, norm_layer=Affine, **kwargs)
     model = _create_mixer('resmlp_12_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def resmlp_24_224(pretrained=False, **kwargs):
+def resmlp_24_224(pretrained=False, **kwargs) -> MlpMixer:
     """ ResMLP-24
     Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
     """
     model_args = dict(
         patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=4,
         block_layer=partial(ResBlock, init_values=1e-5), norm_layer=Affine, **kwargs)
     model = _create_mixer('resmlp_24_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def resmlp_36_224(pretrained=False, **kwargs):
+def resmlp_36_224(pretrained=False, **kwargs) -> MlpMixer:
     """ ResMLP-36
     Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
     """
     model_args = dict(
         patch_size=16, num_blocks=36, embed_dim=384, mlp_ratio=4,
         block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)
     model = _create_mixer('resmlp_36_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def resmlp_big_24_224(pretrained=False, **kwargs):
+def resmlp_big_24_224(pretrained=False, **kwargs) -> MlpMixer:
     """ ResMLP-B-24
     Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
     """
     model_args = dict(
         patch_size=8, num_blocks=24, embed_dim=768, mlp_ratio=4,
         block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)
     model = _create_mixer('resmlp_big_24_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def resmlp_12_distilled_224(pretrained=False, **kwargs):
-    """ ResMLP-12
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-    """
-    model_args = dict(
-        patch_size=16, num_blocks=12, embed_dim=384, mlp_ratio=4, block_layer=ResBlock, norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_12_distilled_224', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def resmlp_24_distilled_224(pretrained=False, **kwargs):
-    """ ResMLP-24
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-    """
-    model_args = dict(
-        patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=4,
-        block_layer=partial(ResBlock, init_values=1e-5), norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_24_distilled_224', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def resmlp_36_distilled_224(pretrained=False, **kwargs):
-    """ ResMLP-36
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-    """
-    model_args = dict(
-        patch_size=16, num_blocks=36, embed_dim=384, mlp_ratio=4,
-        block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_36_distilled_224', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def resmlp_big_24_distilled_224(pretrained=False, **kwargs):
-    """ ResMLP-B-24
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-    """
-    model_args = dict(
-        patch_size=8, num_blocks=24, embed_dim=768, mlp_ratio=4,
-        block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_big_24_distilled_224', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def resmlp_big_24_224_in22ft1k(pretrained=False, **kwargs):
-    """ ResMLP-B-24
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-    """
-    model_args = dict(
-        patch_size=8, num_blocks=24, embed_dim=768, mlp_ratio=4,
-        block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_big_24_224_in22ft1k', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def resmlp_12_224_dino(pretrained=False, **kwargs):
-    """ ResMLP-12
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-
-    Model pretrained via DINO (self-supervised) - https://arxiv.org/abs/2104.14294
-    """
-    model_args = dict(
-        patch_size=16, num_blocks=12, embed_dim=384, mlp_ratio=4, block_layer=ResBlock, norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_12_224_dino', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def resmlp_24_224_dino(pretrained=False, **kwargs):
-    """ ResMLP-24
-    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
-
-    Model pretrained via DINO (self-supervised) - https://arxiv.org/abs/2104.14294
-    """
-    model_args = dict(
-        patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=4,
-        block_layer=partial(ResBlock, init_values=1e-5), norm_layer=Affine, **kwargs)
-    model = _create_mixer('resmlp_24_224_dino', pretrained=pretrained, **model_args)
-    return model
-
-
-@register_model
-def gmlp_ti16_224(pretrained=False, **kwargs):
+def gmlp_ti16_224(pretrained=False, **kwargs) -> MlpMixer:
     """ gMLP-Tiny
     Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050
     """
     model_args = dict(
         patch_size=16, num_blocks=30, embed_dim=128, mlp_ratio=6, block_layer=SpatialGatingBlock,
         mlp_layer=GatedMlp, **kwargs)
     model = _create_mixer('gmlp_ti16_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def gmlp_s16_224(pretrained=False, **kwargs):
+def gmlp_s16_224(pretrained=False, **kwargs) -> MlpMixer:
     """ gMLP-Small
     Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050
     """
     model_args = dict(
         patch_size=16, num_blocks=30, embed_dim=256, mlp_ratio=6, block_layer=SpatialGatingBlock,
         mlp_layer=GatedMlp, **kwargs)
     model = _create_mixer('gmlp_s16_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def gmlp_b16_224(pretrained=False, **kwargs):
+def gmlp_b16_224(pretrained=False, **kwargs) -> MlpMixer:
     """ gMLP-Base
     Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050
     """
     model_args = dict(
         patch_size=16, num_blocks=30, embed_dim=512, mlp_ratio=6, block_layer=SpatialGatingBlock,
         mlp_layer=GatedMlp, **kwargs)
     model = _create_mixer('gmlp_b16_224', pretrained=pretrained, **model_args)
     return model
+
+
+register_model_deprecations(__name__, {
+    'mixer_b16_224_in21k': 'mixer_b16_224.goog_in21k_ft_in1k',
+    'mixer_l16_224_in21k': 'mixer_l16_224.goog_in21k_ft_in1k',
+    'mixer_b16_224_miil': 'mixer_b16_224.miil_in21k_ft_in1k',
+    'mixer_b16_224_miil_in21k': 'mixer_b16_224.miil_in21k',
+    'resmlp_12_distilled_224': 'resmlp_12_224.fb_distilled_in1k',
+    'resmlp_24_distilled_224': 'resmlp_24_224.fb_distilled_in1k',
+    'resmlp_36_distilled_224': 'resmlp_36_224.fb_distilled_in1k',
+    'resmlp_big_24_distilled_224': 'resmlp_big_24_224.fb_distilled_in1k',
+    'resmlp_big_24_224_in22ft1k': 'resmlp_big_24_224.fb_in22k_ft_in1k',
+    'resmlp_12_224_dino': 'resmlp_12_224',
+    'resmlp_24_224_dino': 'resmlp_24_224',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/mobilenetv3.py` & `timm-0.9.0/timm/models/mobilenetv3.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,25 +8,25 @@
 """
 from functools import partial
 from typing import List
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+from torch.utils.checkpoint import checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import SelectAdaptivePool2d, Linear, create_conv2d, get_norm_act_layer
 from ._builder import build_model_with_cfg, pretrained_cfg_for_features
 from ._efficientnet_blocks import SqueezeExcite
 from ._efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights, \
     round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT
 from ._features import FeatureInfo, FeatureHooks
 from ._manipulate import checkpoint_seq
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['MobileNetV3', 'MobileNetV3Features']
 
 
 class MobileNetV3(nn.Module):
     """ MobiletNet-V3
 
@@ -140,21 +140,20 @@
             x = self.blocks(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.conv_head(x)
         x = self.act2(x)
+        x = self.flatten(x)
         if pre_logits:
-            return x.flatten(1)
-        else:
-            x = self.flatten(x)
-            if self.drop_rate > 0.:
-                x = F.dropout(x, p=self.drop_rate, training=self.training)
-            return self.classifier(x)
+            return x
+        if self.drop_rate > 0.:
+            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        return self.classifier(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
@@ -184,14 +183,15 @@
             drop_path_rate=0.,
     ):
         super(MobileNetV3Features, self).__init__()
         act_layer = act_layer or nn.ReLU
         norm_layer = norm_layer or nn.BatchNorm2d
         se_layer = se_layer or SqueezeExcite
         self.drop_rate = drop_rate
+        self.grad_checkpointing = False
 
         # Stem
         if not fix_stem:
             stem_size = round_chs_fn(stem_size)
         self.conv_stem = create_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)
         self.bn1 = norm_layer(stem_size)
         self.act1 = act_layer(inplace=True)
@@ -216,24 +216,31 @@
 
         # Register feature extraction hooks with FeatureHooks helper
         self.feature_hooks = None
         if feature_location != 'bottleneck':
             hooks = self.feature_info.get_dicts(keys=('module', 'hook_type'))
             self.feature_hooks = FeatureHooks(hooks, self.named_modules())
 
+    @torch.jit.ignore
+    def set_grad_checkpointing(self, enable=True):
+        self.grad_checkpointing = enable
+
     def forward(self, x) -> List[torch.Tensor]:
         x = self.conv_stem(x)
         x = self.bn1(x)
         x = self.act1(x)
         if self.feature_hooks is None:
             features = []
             if 0 in self._stage_out_idx:
                 features.append(x)  # add stem out
             for i, b in enumerate(self.blocks):
-                x = b(x)
+                if self.grad_checkpointing and not torch.jit.is_scripting():
+                    x = checkpoint(b, x)
+                else:
+                    x = b(x)
                 if i + 1 in self._stage_out_idx:
                     features.append(x)
             return features
         else:
             self.blocks(x)
             out = self.feature_hooks.get_output(x.device)
             return list(out.values())
@@ -632,159 +639,165 @@
         interpolation='bicubic',
     ),
     "lcnet_150.untrained": _cfg(),
 })
 
 
 @register_model
-def mobilenetv3_large_075(pretrained=False, **kwargs):
+def mobilenetv3_large_075(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     model = _gen_mobilenet_v3('mobilenetv3_large_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv3_large_100(pretrained=False, **kwargs):
+def mobilenetv3_large_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     model = _gen_mobilenet_v3('mobilenetv3_large_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv3_small_050(pretrained=False, **kwargs):
+def mobilenetv3_small_050(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     model = _gen_mobilenet_v3('mobilenetv3_small_050', 0.50, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv3_small_075(pretrained=False, **kwargs):
+def mobilenetv3_small_075(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     model = _gen_mobilenet_v3('mobilenetv3_small_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv3_small_100(pretrained=False, **kwargs):
+def mobilenetv3_small_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     model = _gen_mobilenet_v3('mobilenetv3_small_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def mobilenetv3_rw(pretrained=False, **kwargs):
+def mobilenetv3_rw(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     if pretrained:
         # pretrained model trained with non-default BN epsilon
         kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     model = _gen_mobilenet_v3_rw('mobilenetv3_rw', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mobilenetv3_large_075(pretrained=False, **kwargs):
+def tf_mobilenetv3_large_075(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mobilenet_v3('tf_mobilenetv3_large_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mobilenetv3_large_100(pretrained=False, **kwargs):
+def tf_mobilenetv3_large_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mobilenet_v3('tf_mobilenetv3_large_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mobilenetv3_large_minimal_100(pretrained=False, **kwargs):
+def tf_mobilenetv3_large_minimal_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mobilenet_v3('tf_mobilenetv3_large_minimal_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mobilenetv3_small_075(pretrained=False, **kwargs):
+def tf_mobilenetv3_small_075(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mobilenet_v3('tf_mobilenetv3_small_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mobilenetv3_small_100(pretrained=False, **kwargs):
+def tf_mobilenetv3_small_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mobilenet_v3('tf_mobilenetv3_small_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def tf_mobilenetv3_small_minimal_100(pretrained=False, **kwargs):
+def tf_mobilenetv3_small_minimal_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ MobileNet V3 """
     kwargs['bn_eps'] = BN_EPS_TF_DEFAULT
     kwargs['pad_type'] = 'same'
     model = _gen_mobilenet_v3('tf_mobilenetv3_small_minimal_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def fbnetv3_b(pretrained=False, **kwargs):
+def fbnetv3_b(pretrained=False, **kwargs) -> MobileNetV3:
     """ FBNetV3-B """
     model = _gen_fbnetv3('fbnetv3_b', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def fbnetv3_d(pretrained=False, **kwargs):
+def fbnetv3_d(pretrained=False, **kwargs) -> MobileNetV3:
     """ FBNetV3-D """
     model = _gen_fbnetv3('fbnetv3_d', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def fbnetv3_g(pretrained=False, **kwargs):
+def fbnetv3_g(pretrained=False, **kwargs) -> MobileNetV3:
     """ FBNetV3-G """
     model = _gen_fbnetv3('fbnetv3_g', pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def lcnet_035(pretrained=False, **kwargs):
+def lcnet_035(pretrained=False, **kwargs) -> MobileNetV3:
     """ PP-LCNet 0.35"""
     model = _gen_lcnet('lcnet_035', 0.35, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def lcnet_050(pretrained=False, **kwargs):
+def lcnet_050(pretrained=False, **kwargs) -> MobileNetV3:
     """ PP-LCNet 0.5"""
     model = _gen_lcnet('lcnet_050', 0.5, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def lcnet_075(pretrained=False, **kwargs):
+def lcnet_075(pretrained=False, **kwargs) -> MobileNetV3:
     """ PP-LCNet 1.0"""
     model = _gen_lcnet('lcnet_075', 0.75, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def lcnet_100(pretrained=False, **kwargs):
+def lcnet_100(pretrained=False, **kwargs) -> MobileNetV3:
     """ PP-LCNet 1.0"""
     model = _gen_lcnet('lcnet_100', 1.0, pretrained=pretrained, **kwargs)
     return model
 
 
 @register_model
-def lcnet_150(pretrained=False, **kwargs):
+def lcnet_150(pretrained=False, **kwargs) -> MobileNetV3:
     """ PP-LCNet 1.5"""
     model = _gen_lcnet('lcnet_150', 1.5, pretrained=pretrained, **kwargs)
     return model
+
+
+register_model_deprecations(__name__, {
+    'mobilenetv3_large_100_miil': 'mobilenetv3_large_100.miil_in21k_ft_in1k',
+    'mobilenetv3_large_100_miil_in21k': 'mobilenetv3_large_100.miil_in21k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/mobilevit.py` & `timm-0.9.0/timm/models/mobilevit.py`

 * *Files 5% similar despite different names*

```diff
@@ -16,88 +16,24 @@
 import math
 from typing import Callable, Tuple, Optional
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 
-from timm.layers import to_2tuple, make_divisible, GroupNorm1, ConvMlp, DropPath
+from timm.layers import to_2tuple, make_divisible, GroupNorm1, ConvMlp, DropPath, is_exportable
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_module
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 from .byobnet import register_block, ByoBlockCfg, ByoModelCfg, ByobNet, LayerFn, num_groups
 from .vision_transformer import Block as TransformerBlock
 
 __all__ = []
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
-        'crop_pct': 0.9, 'interpolation': 'bicubic',
-        'mean': (0., 0., 0.), 'std': (1., 1., 1.),
-        'first_conv': 'stem.conv', 'classifier': 'head.fc',
-        'fixed_input_size': False,
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'mobilevit_xxs': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevit_xxs-ad385b40.pth'),
-    'mobilevit_xs': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevit_xs-8fbd6366.pth'),
-    'mobilevit_s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevit_s-38a5a959.pth'),
-    'semobilevit_s': _cfg(),
-
-    'mobilevitv2_050': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_050-49951ee2.pth',
-        crop_pct=0.888),
-    'mobilevitv2_075': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_075-b5556ef6.pth',
-        crop_pct=0.888),
-    'mobilevitv2_100': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_100-e464ef3b.pth',
-        crop_pct=0.888),
-    'mobilevitv2_125': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_125-0ae35027.pth',
-        crop_pct=0.888),
-    'mobilevitv2_150': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_150-737c5019.pth',
-        crop_pct=0.888),
-    'mobilevitv2_175': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_175-16462ee2.pth',
-        crop_pct=0.888),
-    'mobilevitv2_200': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_200-b3422f67.pth',
-        crop_pct=0.888),
-
-    'mobilevitv2_150_in22ft1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_150_in22ft1k-0b555d7b.pth',
-        crop_pct=0.888),
-    'mobilevitv2_175_in22ft1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_175_in22ft1k-4117fa1f.pth',
-        crop_pct=0.888),
-    'mobilevitv2_200_in22ft1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_200_in22ft1k-1d7c8927.pth',
-        crop_pct=0.888),
-
-    'mobilevitv2_150_384_in22ft1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_150_384_in22ft1k-9e142854.pth',
-        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
-    'mobilevitv2_175_384_in22ft1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_175_384_in22ft1k-059cbe56.pth',
-        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
-    'mobilevitv2_200_384_in22ft1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-mvit-weights/mobilevitv2_200_384_in22ft1k-32c87503.pth',
-        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
-}
-
-
 def _inverted_residual_block(d, c, s, br=4.0):
     # inverted residual is a bottleneck block with bottle_ratio > 1 applied to in_chs, linear output, gs=1 (depthwise)
     return ByoBlockCfg(
         type='bottle', d=d, c=c, s=s, gs=1, br=br,
         block_kwargs=dict(bottle_in=True, linear_out=True))
 
 
@@ -267,15 +203,15 @@
         self.transformer = nn.Sequential(*[
             TransformerBlock(
                 transformer_dim,
                 mlp_ratio=mlp_ratio,
                 num_heads=num_heads,
                 qkv_bias=True,
                 attn_drop=attn_drop,
-                drop=drop,
+                proj_drop=drop,
                 drop_path=drop_path_rate,
                 act_layer=layers.act,
                 norm_layer=transformer_norm_layer,
             )
             for _ in range(transformer_depth)
         ])
         self.norm = transformer_norm_layer(transformer_dim)
@@ -560,14 +496,15 @@
         ])
         self.norm = transformer_norm_layer(transformer_dim)
 
         self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, apply_act=False)
 
         self.patch_size = to_2tuple(patch_size)
         self.patch_area = self.patch_size[0] * self.patch_size[1]
+        self.coreml_exportable = is_exportable()
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         B, C, H, W = x.shape
         patch_h, patch_w = self.patch_size
         new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w
         num_patch_h, num_patch_w = new_h // patch_h, new_w // patch_w  # n_h, n_w
         num_patches = num_patch_h * num_patch_w  # N
@@ -576,24 +513,32 @@
 
         # Local representation
         x = self.conv_kxk(x)
         x = self.conv_1x1(x)
 
         # Unfold (feature map -> patches), [B, C, H, W] -> [B, C, P, N]
         C = x.shape[1]
-        x = x.reshape(B, C, num_patch_h, patch_h, num_patch_w, patch_w).permute(0, 1, 3, 5, 2, 4)
+        if self.coreml_exportable:
+            x = F.unfold(x, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))
+        else:
+            x = x.reshape(B, C, num_patch_h, patch_h, num_patch_w, patch_w).permute(0, 1, 3, 5, 2, 4)
         x = x.reshape(B, C, -1, num_patches)
 
         # Global representations
         x = self.transformer(x)
         x = self.norm(x)
 
         # Fold (patches -> feature map), [B, C, P, N] --> [B, C, H, W]
-        x = x.reshape(B, C, patch_h, patch_w, num_patch_h, num_patch_w).permute(0, 1, 4, 2, 5, 3)
-        x = x.reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)
+        if self.coreml_exportable:
+            # adopted from https://github.com/apple/ml-cvnets/blob/main/cvnets/modules/mobilevit_block.py#L609-L624
+            x = x.reshape(B, C * patch_h * patch_w, num_patch_h, num_patch_w)
+            x = F.pixel_shuffle(x, upscale_factor=patch_h)
+        else:
+            x = x.reshape(B, C, patch_h, patch_w, num_patch_h, num_patch_w).permute(0, 1, 4, 2, 5, 3)
+            x = x.reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)
 
         x = self.conv_proj(x)
         return x
 
 
 register_block('mobilevit', MobileVitBlock)
 register_block('mobilevit2', MobileVitV2Block)
@@ -611,96 +556,126 @@
     return build_model_with_cfg(
         ByobNet, variant, pretrained,
         model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],
         feature_cfg=dict(flatten_sequential=True),
         **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
+        'crop_pct': 0.9, 'interpolation': 'bicubic',
+        'mean': (0., 0., 0.), 'std': (1., 1., 1.),
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        'fixed_input_size': False,
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'mobilevit_xxs.cvnets_in1k': _cfg(hf_hub_id='timm/'),
+    'mobilevit_xs.cvnets_in1k': _cfg(hf_hub_id='timm/'),
+    'mobilevit_s.cvnets_in1k': _cfg(hf_hub_id='timm/'),
+
+    'mobilevitv2_050.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_075.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_100.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_125.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_150.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_175.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_200.cvnets_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+
+    'mobilevitv2_150.cvnets_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_175.cvnets_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+    'mobilevitv2_200.cvnets_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.888),
+
+    'mobilevitv2_150.cvnets_in22k_ft_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'mobilevitv2_175.cvnets_in22k_ft_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'mobilevitv2_200.cvnets_in22k_ft_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+})
+
+
 @register_model
-def mobilevit_xxs(pretrained=False, **kwargs):
+def mobilevit_xxs(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevit_xxs', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevit_xs(pretrained=False, **kwargs):
+def mobilevit_xs(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevit_xs', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevit_s(pretrained=False, **kwargs):
+def mobilevit_s(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevit_s', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def semobilevit_s(pretrained=False, **kwargs):
-    return _create_mobilevit('semobilevit_s', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def mobilevitv2_050(pretrained=False, **kwargs):
+def mobilevitv2_050(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_050', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevitv2_075(pretrained=False, **kwargs):
+def mobilevitv2_075(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_075', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevitv2_100(pretrained=False, **kwargs):
+def mobilevitv2_100(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_100', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevitv2_125(pretrained=False, **kwargs):
+def mobilevitv2_125(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_125', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevitv2_150(pretrained=False, **kwargs):
+def mobilevitv2_150(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_150', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevitv2_175(pretrained=False, **kwargs):
+def mobilevitv2_175(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_175', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mobilevitv2_200(pretrained=False, **kwargs):
+def mobilevitv2_200(pretrained=False, **kwargs) -> ByobNet:
     return _create_mobilevit('mobilevitv2_200', pretrained=pretrained, **kwargs)
 
 
-@register_model
-def mobilevitv2_150_in22ft1k(pretrained=False, **kwargs):
-    return _create_mobilevit(
-        'mobilevitv2_150_in22ft1k', cfg_variant='mobilevitv2_150', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def mobilevitv2_175_in22ft1k(pretrained=False, **kwargs):
-    return _create_mobilevit(
-        'mobilevitv2_175_in22ft1k', cfg_variant='mobilevitv2_175', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def mobilevitv2_200_in22ft1k(pretrained=False, **kwargs):
-    return _create_mobilevit(
-        'mobilevitv2_200_in22ft1k', cfg_variant='mobilevitv2_200', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def mobilevitv2_150_384_in22ft1k(pretrained=False, **kwargs):
-    return _create_mobilevit(
-        'mobilevitv2_150_384_in22ft1k', cfg_variant='mobilevitv2_150', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def mobilevitv2_175_384_in22ft1k(pretrained=False, **kwargs):
-    return _create_mobilevit(
-        'mobilevitv2_175_384_in22ft1k', cfg_variant='mobilevitv2_175', pretrained=pretrained, **kwargs)
-
-
-@register_model
-def mobilevitv2_200_384_in22ft1k(pretrained=False, **kwargs):
-    return _create_mobilevit(
-        'mobilevitv2_200_384_in22ft1k', cfg_variant='mobilevitv2_200', pretrained=pretrained, **kwargs)
+register_model_deprecations(__name__, {
+    'mobilevitv2_150_in22ft1k': 'mobilevitv2_150.cvnets_in22k_ft_in1k',
+    'mobilevitv2_175_in22ft1k': 'mobilevitv2_175.cvnets_in22k_ft_in1k',
+    'mobilevitv2_200_in22ft1k': 'mobilevitv2_200.cvnets_in22k_ft_in1k',
+
+    'mobilevitv2_150_384_in22ft1k': 'mobilevitv2_150.cvnets_in22k_ft_in1k_384',
+    'mobilevitv2_175_384_in22ft1k': 'mobilevitv2_175.cvnets_in22k_ft_in1k_384',
+    'mobilevitv2_200_384_in22ft1k': 'mobilevitv2_200.cvnets_in22k_ft_in1k_384',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/mvitv2.py` & `timm-0.9.0/timm/models/mvitv2.py`

 * *Files 3% similar despite different names*

```diff
@@ -23,51 +23,19 @@
 import torch.utils.checkpoint as checkpoint
 from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import Mlp, DropPath, trunc_normal_tf_, get_norm_layer, to_2tuple
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
-from ._registry import register_model
+from ._registry import register_model, register_model_deprecations, generate_default_cfgs
 
 __all__ = ['MultiScaleVit', 'MultiScaleVitCfg']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',
-        'fixed_input_size': True,
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    mvitv2_tiny=_cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_T_in1k.pyth'),
-    mvitv2_small=_cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_S_in1k.pyth'),
-    mvitv2_base=_cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_B_in1k.pyth'),
-    mvitv2_large=_cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_L_in1k.pyth'),
-
-    mvitv2_base_in21k=_cfg(
-        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_B_in21k.pyth',
-        num_classes=19168),
-    mvitv2_large_in21k=_cfg(
-        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_L_in21k.pyth',
-        num_classes=19168),
-    mvitv2_huge_in21k=_cfg(
-        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_H_in21k.pyth',
-        num_classes=19168),
-
-    mvitv2_small_cls=_cfg(url=''),
-)
-
-
 @dataclass
 class MultiScaleVitCfg:
     depths: Tuple[int, ...] = (2, 3, 16, 3)
     embed_dim: Union[int, Tuple[int, ...]] = 96
     num_heads: Union[int, Tuple[int, ...]] = 1
     mlp_ratio: float = 4.
     pool_first: bool = False
@@ -109,48 +77,14 @@
                         max(_stride_kv[d] // self.stride_q[i][d], 1)
                         for d in range(len(_stride_kv))
                     ]
                 pool_kv_stride.append(tuple(_stride_kv))
             self.stride_kv = tuple(pool_kv_stride)
 
 
-model_cfgs = dict(
-    mvitv2_tiny=MultiScaleVitCfg(
-        depths=(1, 2, 5, 2),
-    ),
-    mvitv2_small=MultiScaleVitCfg(
-        depths=(1, 2, 11, 2),
-    ),
-    mvitv2_base=MultiScaleVitCfg(
-        depths=(2, 3, 16, 3),
-    ),
-    mvitv2_large=MultiScaleVitCfg(
-        depths=(2, 6, 36, 4),
-        embed_dim=144,
-        num_heads=2,
-        expand_attn=False,
-    ),
-
-    mvitv2_base_in21k=MultiScaleVitCfg(
-        depths=(2, 3, 16, 3),
-    ),
-    mvitv2_large_in21k=MultiScaleVitCfg(
-        depths=(2, 6, 36, 4),
-        embed_dim=144,
-        num_heads=2,
-        expand_attn=False,
-    ),
-
-    mvitv2_small_cls=MultiScaleVitCfg(
-        depths=(1, 2, 11, 2),
-        use_cls_token=True,
-    ),
-)
-
-
 def prod(iterable):
     return reduce(operator.mul, iterable, 1)
 
 
 class PatchEmbed(nn.Module):
     """
     PatchEmbed.
@@ -225,34 +159,40 @@
     sp_idx = 1 if has_cls_token else 0
     q_h, q_w = q_size
     k_h, k_w = k_size
 
     # Scale up rel pos if shapes for q and k are different.
     q_h_ratio = max(k_h / q_h, 1.0)
     k_h_ratio = max(q_h / k_h, 1.0)
-    dist_h = torch.arange(q_h)[:, None] * q_h_ratio - torch.arange(k_h)[None, :] * k_h_ratio
+    dist_h = (
+            torch.arange(q_h, device=q.device).unsqueeze(-1) * q_h_ratio -
+            torch.arange(k_h, device=q.device).unsqueeze(0) * k_h_ratio
+    )
     dist_h += (k_h - 1) * k_h_ratio
     q_w_ratio = max(k_w / q_w, 1.0)
     k_w_ratio = max(q_w / k_w, 1.0)
-    dist_w = torch.arange(q_w)[:, None] * q_w_ratio - torch.arange(k_w)[None, :] * k_w_ratio
+    dist_w = (
+            torch.arange(q_w, device=q.device).unsqueeze(-1) * q_w_ratio -
+            torch.arange(k_w, device=q.device).unsqueeze(0) * k_w_ratio
+    )
     dist_w += (k_w - 1) * k_w_ratio
 
-    Rh = rel_pos_h[dist_h.long()]
-    Rw = rel_pos_w[dist_w.long()]
+    rel_h = rel_pos_h[dist_h.long()]
+    rel_w = rel_pos_w[dist_w.long()]
 
     B, n_head, q_N, dim = q.shape
 
     r_q = q[:, :, sp_idx:].reshape(B, n_head, q_h, q_w, dim)
-    rel_h = torch.einsum("byhwc,hkc->byhwk", r_q, Rh)
-    rel_w = torch.einsum("byhwc,wkc->byhwk", r_q, Rw)
+    rel_h = torch.einsum("byhwc,hkc->byhwk", r_q, rel_h)
+    rel_w = torch.einsum("byhwc,wkc->byhwk", r_q, rel_w)
 
     attn[:, :, sp_idx:, sp_idx:] = (
         attn[:, :, sp_idx:, sp_idx:].view(B, -1, q_h, q_w, k_h, k_w)
-        + rel_h[:, :, :, :, :, None]
-        + rel_w[:, :, :, :, None, :]
+        + rel_h.unsqueeze(-1)
+        + rel_w.unsqueeze(-2)
     ).view(B, -1, q_h * q_w, k_h * k_w)
 
     return attn
 
 
 class MultiScaleAttentionPoolFirst(nn.Module):
     def __init__(
@@ -386,26 +326,26 @@
             v, v_size = reshape_post_pool(v, self.num_heads, v_tok)
         else:
             v_size = feat_size
         if self.norm_v is not None:
             v = self.norm_v(v)
 
         q_N = q_size[0] * q_size[1] + int(self.has_cls_token)
-        q = q.permute(0, 2, 1, 3).reshape(B, q_N, -1)
-        q = self.q(q).reshape(B, q_N, self.num_heads, -1).permute(0, 2, 1, 3)
+        q = q.transpose(1, 2).reshape(B, q_N, -1)
+        q = self.q(q).reshape(B, q_N, self.num_heads, -1).transpose(1, 2)
 
         k_N = k_size[0] * k_size[1] + int(self.has_cls_token)
-        k = k.permute(0, 2, 1, 3).reshape(B, k_N, -1)
-        k = self.k(k).reshape(B, k_N, self.num_heads, -1).permute(0, 2, 1, 3)
+        k = k.transpose(1, 2).reshape(B, k_N, -1)
+        k = self.k(k).reshape(B, k_N, self.num_heads, -1)
 
         v_N = v_size[0] * v_size[1] + int(self.has_cls_token)
-        v = v.permute(0, 2, 1, 3).reshape(B, v_N, -1)
-        v = self.v(v).reshape(B, v_N, self.num_heads, -1).permute(0, 2, 1, 3)
+        v = v.transpose(1, 2).reshape(B, v_N, -1)
+        v = self.v(v).reshape(B, v_N, self.num_heads, -1).transpose(1, 2)
 
-        attn = (q * self.scale) @ k.transpose(-2, -1)
+        attn = (q * self.scale) @ k
         if self.rel_pos_type == 'spatial':
             attn = cal_rel_pos_type(
                 attn,
                 q,
                 self.has_cls_token,
                 q_size,
                 k_size,
@@ -760,24 +700,26 @@
     """
 
     def __init__(
             self,
             cfg: MultiScaleVitCfg,
             img_size: Tuple[int, int] = (224, 224),
             in_chans: int = 3,
-            global_pool: str = 'avg',
+            global_pool: Optional[str] = None,
             num_classes: int = 1000,
             drop_path_rate: float = 0.,
             drop_rate: float = 0.,
     ):
         super().__init__()
         img_size = to_2tuple(img_size)
         norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)
         self.num_classes = num_classes
         self.drop_rate = drop_rate
+        if global_pool is None:
+            global_pool = 'token' if cfg.use_cls_token else 'avg'
         self.global_pool = global_pool
         self.depths = tuple(cfg.depths)
         self.expand_attn = cfg.expand_attn
 
         embed_dim = cfg.embed_dim[0]
         self.patch_embed = PatchEmbed(
             dim_in=in_chans,
@@ -959,54 +901,130 @@
     #             0 if getattr(model, 'no_embed_class') else getattr(model, 'num_prefix_tokens', 1),
     #             model.patch_embed.grid_size
     #         )
 
     return out_dict
 
 
+model_cfgs = dict(
+    mvitv2_tiny=MultiScaleVitCfg(
+        depths=(1, 2, 5, 2),
+    ),
+    mvitv2_small=MultiScaleVitCfg(
+        depths=(1, 2, 11, 2),
+    ),
+    mvitv2_base=MultiScaleVitCfg(
+        depths=(2, 3, 16, 3),
+    ),
+    mvitv2_large=MultiScaleVitCfg(
+        depths=(2, 6, 36, 4),
+        embed_dim=144,
+        num_heads=2,
+        expand_attn=False,
+    ),
+
+    mvitv2_small_cls=MultiScaleVitCfg(
+        depths=(1, 2, 11, 2),
+        use_cls_token=True,
+    ),
+    mvitv2_base_cls=MultiScaleVitCfg(
+        depths=(2, 3, 16, 3),
+        use_cls_token=True,
+    ),
+    mvitv2_large_cls=MultiScaleVitCfg(
+        depths=(2, 6, 36, 4),
+        embed_dim=144,
+        num_heads=2,
+        use_cls_token=True,
+        expand_attn=True,
+    ),
+    mvitv2_huge_cls=MultiScaleVitCfg(
+        depths=(4, 8, 60, 8),
+        embed_dim=192,
+        num_heads=3,
+        use_cls_token=True,
+        expand_attn=True,
+    ),
+)
+
+
 def _create_mvitv2(variant, cfg_variant=None, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        MultiScaleVit, variant, pretrained,
+        MultiScaleVit,
+        variant,
+        pretrained,
         model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],
         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(flatten_sequential=True),
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .9, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',
+        'fixed_input_size': True,
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'mvitv2_tiny.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_T_in1k.pyth'),
+    'mvitv2_small.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_S_in1k.pyth'),
+    'mvitv2_base.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_B_in1k.pyth'),
+    'mvitv2_large.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_L_in1k.pyth'),
+
+    'mvitv2_small_cls': _cfg(url=''),
+    'mvitv2_base_cls.fb_inw21k': _cfg(
+        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_B_in21k.pyth',
+        num_classes=19168),
+    'mvitv2_large_cls.fb_inw21k': _cfg(
+        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_L_in21k.pyth',
+        num_classes=19168),
+    'mvitv2_huge_cls.fb_inw21k': _cfg(
+        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_H_in21k.pyth',
+        num_classes=19168),
+})
 
 
 @register_model
-def mvitv2_tiny(pretrained=False, **kwargs):
+def mvitv2_tiny(pretrained=False, **kwargs) -> MultiScaleVit:
     return _create_mvitv2('mvitv2_tiny', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mvitv2_small(pretrained=False, **kwargs):
+def mvitv2_small(pretrained=False, **kwargs) -> MultiScaleVit:
     return _create_mvitv2('mvitv2_small', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mvitv2_base(pretrained=False, **kwargs):
+def mvitv2_base(pretrained=False, **kwargs) -> MultiScaleVit:
     return _create_mvitv2('mvitv2_base', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mvitv2_large(pretrained=False, **kwargs):
+def mvitv2_large(pretrained=False, **kwargs) -> MultiScaleVit:
     return _create_mvitv2('mvitv2_large', pretrained=pretrained, **kwargs)
 
 
-# @register_model
-# def mvitv2_base_in21k(pretrained=False, **kwargs):
-#     return _create_mvitv2('mvitv2_base_in21k', pretrained=pretrained, **kwargs)
-#
-#
-# @register_model
-# def mvitv2_large_in21k(pretrained=False, **kwargs):
-#     return _create_mvitv2('mvitv2_large_in21k', pretrained=pretrained, **kwargs)
-#
-#
-# @register_model
-# def mvitv2_huge_in21k(pretrained=False, **kwargs):
-#     return _create_mvitv2('mvitv2_huge_in21k', pretrained=pretrained, **kwargs)
+@register_model
+def mvitv2_small_cls(pretrained=False, **kwargs) -> MultiScaleVit:
+    return _create_mvitv2('mvitv2_small_cls', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def mvitv2_small_cls(pretrained=False, **kwargs):
-    return _create_mvitv2('mvitv2_small_cls', pretrained=pretrained, **kwargs)
+def mvitv2_base_cls(pretrained=False, **kwargs) -> MultiScaleVit:
+    return _create_mvitv2('mvitv2_base_cls', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def mvitv2_large_cls(pretrained=False, **kwargs) -> MultiScaleVit:
+    return _create_mvitv2('mvitv2_large_cls', pretrained=pretrained, **kwargs)
+
+
+@register_model
+def mvitv2_huge_cls(pretrained=False, **kwargs) -> MultiScaleVit:
+    return _create_mvitv2('mvitv2_huge_cls', pretrained=pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/nasnet.py` & `timm-0.9.0/timm/models/nasnet.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,33 +6,18 @@
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['NASNetALarge']
 
-default_cfgs = {
-    'nasnetalarge': {
-        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nasnetalarge-dc4a7b8b.pth',
-        'input_size': (3, 331, 331),
-        'pool_size': (11, 11),
-        'crop_pct': 0.911,
-        'interpolation': 'bicubic',
-        'mean': (0.5, 0.5, 0.5),
-        'std': (0.5, 0.5, 0.5),
-        'num_classes': 1000,
-        'first_conv': 'conv0.conv',
-        'classifier': 'last_linear',
-        'label_offset': 1,  # 1001 classes in pretrained weights
-    },
-}
 
 
 class ActConvBn(nn.Module):
 
     def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=''):
         super(ActConvBn, self).__init__()
         self.act = nn.ReLU()
@@ -404,22 +389,30 @@
         return x_out
 
 
 class NASNetALarge(nn.Module):
     """NASNetALarge (6 @ 4032) """
 
     def __init__(
-            self, num_classes=1000, in_chans=3, stem_size=96, channel_multiplier=2,
-            num_features=4032, output_stride=32, drop_rate=0., global_pool='avg', pad_type='same'):
+            self,
+            num_classes=1000,
+            in_chans=3,
+            stem_size=96,
+            channel_multiplier=2,
+            num_features=4032,
+            output_stride=32,
+            drop_rate=0.,
+            global_pool='avg',
+            pad_type='same',
+    ):
         super(NASNetALarge, self).__init__()
         self.num_classes = num_classes
         self.stem_size = stem_size
         self.num_features = num_features
         self.channel_multiplier = channel_multiplier
-        self.drop_rate = drop_rate
         assert output_stride == 32
 
         channels = self.num_features // 24
         # 24 is default value for the architecture
 
         self.conv0 = ConvNormAct(
             in_channels=in_chans, out_channels=self.stem_size, kernel_size=3, padding=0, stride=2,
@@ -497,16 +490,16 @@
             dict(num_chs=96, reduction=2, module='conv0'),
             dict(num_chs=168, reduction=4, module='cell_stem_1.conv_1x1.act'),
             dict(num_chs=1008, reduction=8, module='reduction_cell_0.conv_1x1.act'),
             dict(num_chs=2016, reduction=16, module='reduction_cell_1.conv_1x1.act'),
             dict(num_chs=4032, reduction=32, module='act'),
         ]
 
-        self.global_pool, self.last_linear = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool, self.head_drop, self.last_linear = create_classifier(
+            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^conv0|cell_stem_[01]',
             blocks=[
                 (r'^cell_(\d+)', None),
@@ -558,31 +551,50 @@
         x_cell_16 = self.cell_16(x_cell_15, x_cell_14)
         x_cell_17 = self.cell_17(x_cell_16, x_cell_15)
         x = self.act(x_cell_17)
         return x
 
     def forward_head(self, x):
         x = self.global_pool(x)
-        if self.drop_rate > 0:
-            x = F.dropout(x, self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         x = self.last_linear(x)
         return x
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_nasnet(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        NASNetALarge, variant, pretrained,
+        NASNetALarge,
+        variant,
+        pretrained,
         feature_cfg=dict(feature_cls='hook', no_rewrite=True),  # not possible to re-write this model
-        **kwargs)
+        **kwargs,
+    )
+
+
+default_cfgs = generate_default_cfgs({
+    'nasnetalarge.tf_in1k': {
+        'hf_hub_id': 'timm/',
+        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nasnetalarge-dc4a7b8b.pth',
+        'input_size': (3, 331, 331),
+        'pool_size': (11, 11),
+        'crop_pct': 0.911,
+        'interpolation': 'bicubic',
+        'mean': (0.5, 0.5, 0.5),
+        'std': (0.5, 0.5, 0.5),
+        'num_classes': 1000,
+        'first_conv': 'conv0.conv',
+        'classifier': 'last_linear',
+    },
+})
 
 
 @register_model
-def nasnetalarge(pretrained=False, **kwargs):
+def nasnetalarge(pretrained=False, **kwargs) -> NASNetALarge:
     """NASNet-A large model architecture.
     """
     model_kwargs = dict(pad_type='same', **kwargs)
     return _create_nasnet('nasnetalarge', pretrained, **model_kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/nest.py` & `timm-0.9.0/timm/models/nest.py`

 * *Files 10% similar despite different names*

```diff
@@ -22,60 +22,38 @@
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import PatchEmbed, Mlp, DropPath, create_classifier, trunc_normal_, _assert
-from timm.layers import create_conv2d, create_pool2d, to_ntuple
+from timm.layers import create_conv2d, create_pool2d, to_ntuple, use_fused_attn, LayerNorm
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
 from ._manipulate import checkpoint_seq, named_apply
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 
 __all__ = ['Nest']  # model_registry will add each entrypoint fn to this
 
 _logger = logging.getLogger(__name__)
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': [14, 14],
-        'crop_pct': .875, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # (weights from official Google JAX impl)
-    'nest_base': _cfg(),
-    'nest_small': _cfg(),
-    'nest_tiny': _cfg(),
-    'jx_nest_base': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/jx_nest_base-8bc41011.pth'),
-    'jx_nest_small': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/jx_nest_small-422eaded.pth'),
-    'jx_nest_tiny': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/jx_nest_tiny-e3428fb9.pth'),
-}
-
-
 class Attention(nn.Module):
     """
     This is much like `.vision_transformer.Attention` but uses *localised* self attention by accepting an input with
      an extra "image block" dim
     """
+    fused_attn: torch.jit.Final[bool]
+
     def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.qkv = nn.Linear(dim, 3*dim, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
 
     def forward(self, x):
@@ -83,40 +61,66 @@
         x is shape: B (batch_size), T (image blocks), N (seq length per image block), C (embed dim)
         """ 
         B, T, N, C = x.shape
         # result of next line is (qkv, B, num (H)eads, T, N, (C')hannels per head)
         qkv = self.qkv(x).reshape(B, T, N, 3, self.num_heads, C // self.num_heads).permute(3, 0, 4, 1, 2, 5)
         q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
 
-        attn = (q @ k.transpose(-2, -1)) * self.scale # (B, H, T, N, N)
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        if self.fused_attn:
+            x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.attn_drop.p)
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1) # (B, H, T, N, N)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
         # (B, H, T, N, C'), permute -> (B, T, N, C', H)
-        x = (attn @ v).permute(0, 2, 3, 4, 1).reshape(B, T, N, C)
+        x = x.permute(0, 2, 3, 4, 1).reshape(B, T, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x  # (B, T, N, C)
 
 
 class TransformerLayer(nn.Module):
     """
     This is much like `.vision_transformer.Block` but:
         - Called TransformerLayer here to allow for "block" as defined in the paper ("non-overlapping image blocks")
         - Uses modified Attention layer that handles the "block" dimension
     """
-    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0.,
-                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+    def __init__(
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
-        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+        self.attn = Attention(
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
     def forward(self, x):
         y = self.norm1(x)
         x = x + self.drop_path(self.attn(y))
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
@@ -172,36 +176,57 @@
     return x  # (B, H, W, C)
 
 
 class NestLevel(nn.Module):
     """ Single hierarchical level of a Nested Transformer
     """
     def __init__(
-            self, num_blocks, block_size, seq_length, num_heads, depth, embed_dim, prev_embed_dim=None,
-            mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rates=[],
-            norm_layer=None, act_layer=None, pad_type=''):
+            self,
+            num_blocks,
+            block_size,
+            seq_length,
+            num_heads,
+            depth,
+            embed_dim,
+            prev_embed_dim=None,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=[],
+            norm_layer=None,
+            act_layer=None,
+            pad_type='',
+    ):
         super().__init__()
         self.block_size = block_size
         self.grad_checkpointing = False
 
         self.pos_embed = nn.Parameter(torch.zeros(1, num_blocks, seq_length, embed_dim))
 
         if prev_embed_dim is not None:
             self.pool = ConvPool(prev_embed_dim, embed_dim, norm_layer=norm_layer, pad_type=pad_type)
         else:
             self.pool = nn.Identity()
 
         # Transformer encoder
-        if len(drop_path_rates):
-            assert len(drop_path_rates) == depth, 'Must provide as many drop path rates as there are transformer layers'
+        if len(drop_path):
+            assert len(drop_path) == depth, 'Must provide as many drop path rates as there are transformer layers'
         self.transformer_encoder = nn.Sequential(*[
             TransformerLayer(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rates[i],
-                norm_layer=norm_layer, act_layer=act_layer)
+                dim=embed_dim,
+                num_heads=num_heads,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop,
+                attn_drop=attn_drop,
+                drop_path=drop_path[i],
+                norm_layer=norm_layer,
+                act_layer=act_layer,
+            )
             for i in range(depth)])
 
     def forward(self, x):
         """
         expects x as (B, C, H, W)
         """
         x = self.pool(x)
@@ -221,18 +246,34 @@
     """ Nested Transformer (NesT)
 
     A PyTorch impl of : `Aggregating Nested Transformers`
         - https://arxiv.org/abs/2105.12723
     """
 
     def __init__(
-            self, img_size=224, in_chans=3, patch_size=4, num_levels=3, embed_dims=(128, 256, 512),
-            num_heads=(4, 8, 16), depths=(2, 2, 20), num_classes=1000, mlp_ratio=4., qkv_bias=True,
-            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.5, norm_layer=None, act_layer=None,
-            pad_type='', weight_init='', global_pool='avg'
+            self,
+            img_size=224,
+            in_chans=3,
+            patch_size=4,
+            num_levels=3,
+            embed_dims=(128, 256, 512),
+            num_heads=(4, 8, 16),
+            depths=(2, 2, 20),
+            num_classes=1000,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.5,
+            norm_layer=None,
+            act_layer=None,
+            pad_type='',
+            weight_init='',
+            global_pool='avg',
     ):
         """
         Args:
             img_size (int, tuple): input image size
             in_chans (int): number of input channels
             patch_size (int): patch size
             num_levels (int): number of block hierarchies (T_d in the paper)
@@ -266,15 +307,15 @@
 
         embed_dims = to_ntuple(num_levels)(embed_dims)
         num_heads = to_ntuple(num_levels)(num_heads)
         depths = to_ntuple(num_levels)(depths)
         self.num_classes = num_classes
         self.num_features = embed_dims[-1]
         self.feature_info = []
-        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
+        norm_layer = norm_layer or LayerNorm
         act_layer = act_layer or nn.GELU
         self.drop_rate = drop_rate
         self.num_levels = num_levels
         if isinstance(img_size, collections.abc.Sequence):
             assert img_size[0] == img_size[1], 'Model only handles square inputs'
             img_size = img_size[0]
         assert img_size % patch_size == 0, '`patch_size` must divide `img_size` evenly'
@@ -288,38 +329,60 @@
         # Block edge size in units of patches
         # Hint: (img_size // patch_size) gives number of patches along edge of image. sqrt(self.num_blocks[0]) is the
         #  number of blocks along edge of image
         self.block_size = int((img_size // patch_size) // math.sqrt(self.num_blocks[0]))
         
         # Patch embedding
         self.patch_embed = PatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dims[0], flatten=False)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dims[0],
+            flatten=False,
+        )
         self.num_patches = self.patch_embed.num_patches
         self.seq_length = self.num_patches // self.num_blocks[0]
 
         # Build up each hierarchical level
         levels = []
         dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]
         prev_dim = None
         curr_stride = 4
         for i in range(len(self.num_blocks)):
             dim = embed_dims[i]
             levels.append(NestLevel(
-                self.num_blocks[i], self.block_size, self.seq_length, num_heads[i], depths[i], dim, prev_dim,
-                mlp_ratio, qkv_bias, drop_rate, attn_drop_rate, dp_rates[i], norm_layer, act_layer, pad_type=pad_type))
+                self.num_blocks[i],
+                self.block_size,
+                self.seq_length,
+                num_heads[i],
+                depths[i],
+                dim,
+                prev_dim,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dp_rates[i],
+                norm_layer=norm_layer,
+                act_layer=act_layer,
+                pad_type=pad_type,
+            ))
             self.feature_info += [dict(num_chs=dim, reduction=curr_stride, module=f'levels.{i}')]
             prev_dim = dim
             curr_stride *= 2
         self.levels = nn.Sequential(*levels)
 
         # Final normalization layer
         self.norm = norm_layer(embed_dims[-1])
 
         # Classifier
-        self.global_pool, self.head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+        global_pool, head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool = global_pool
+        self.head_drop = nn.Dropout(drop_rate)
+        self.head = head
 
         self.init_weights(weight_init)
 
     @torch.jit.ignore
     def init_weights(self, mode=''):
         assert mode in ('nlhb', '')
         head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.
@@ -362,16 +425,15 @@
         x = self.levels(x)
         # Layer norm done over channel dim only (to NHWC and back)
         x = self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
-        if self.drop_rate > 0.:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -418,71 +480,104 @@
         if state_dict[k].shape != getattr(model, k).shape:
             state_dict[k] = resize_pos_embed(state_dict[k], getattr(model, k))
     return state_dict
 
 
 def _create_nest(variant, pretrained=False, **kwargs):
     model = build_model_with_cfg(
-        Nest, variant, pretrained,
+        Nest,
+        variant,
+        pretrained,
         feature_cfg=dict(out_indices=(0, 1, 2), flatten_sequential=True),
         pretrained_filter_fn=checkpoint_filter_fn,
-        **kwargs)
+        **kwargs,
+    )
 
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': [14, 14],
+        'crop_pct': .875, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'nest_base.untrained': _cfg(),
+    'nest_small.untrained': _cfg(),
+    'nest_tiny.untrained': _cfg(),
+    # (weights from official Google JAX impl, require 'SAME' padding)
+    'nest_base_jx.goog_in1k': _cfg(hf_hub_id='timm/'),
+    'nest_small_jx.goog_in1k': _cfg(hf_hub_id='timm/'),
+    'nest_tiny_jx.goog_in1k': _cfg(hf_hub_id='timm/'),
+})
+
+
 @register_model
-def nest_base(pretrained=False, **kwargs):
+def nest_base(pretrained=False, **kwargs) -> Nest:
     """ Nest-B @ 224x224
     """
     model_kwargs = dict(
         embed_dims=(128, 256, 512), num_heads=(4, 8, 16), depths=(2, 2, 20), **kwargs)
     model = _create_nest('nest_base', pretrained=pretrained, **model_kwargs)
     return model
 
 
 @register_model
-def nest_small(pretrained=False, **kwargs):
+def nest_small(pretrained=False, **kwargs) -> Nest:
     """ Nest-S @ 224x224
     """
     model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 20), **kwargs)
     model = _create_nest('nest_small', pretrained=pretrained, **model_kwargs)
     return model
 
 
 @register_model
-def nest_tiny(pretrained=False, **kwargs):
+def nest_tiny(pretrained=False, **kwargs) -> Nest:
     """ Nest-T @ 224x224
     """
     model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 8), **kwargs)
     model = _create_nest('nest_tiny', pretrained=pretrained, **model_kwargs)
     return model
 
 
 @register_model
-def jx_nest_base(pretrained=False, **kwargs):
-    """ Nest-B @ 224x224, Pretrained weights converted from official Jax impl.
+def nest_base_jx(pretrained=False, **kwargs) -> Nest:
+    """ Nest-B @ 224x224
     """
-    kwargs['pad_type'] = 'same'
-    model_kwargs = dict(embed_dims=(128, 256, 512), num_heads=(4, 8, 16), depths=(2, 2, 20), **kwargs)
-    model = _create_nest('jx_nest_base', pretrained=pretrained, **model_kwargs)
+    kwargs.setdefault('pad_type', 'same')
+    model_kwargs = dict(
+        embed_dims=(128, 256, 512), num_heads=(4, 8, 16), depths=(2, 2, 20), **kwargs)
+    model = _create_nest('nest_base_jx', pretrained=pretrained, **model_kwargs)
     return model
 
 
 @register_model
-def jx_nest_small(pretrained=False, **kwargs):
-    """ Nest-S @ 224x224, Pretrained weights converted from official Jax impl.
+def nest_small_jx(pretrained=False, **kwargs) -> Nest:
+    """ Nest-S @ 224x224
     """
-    kwargs['pad_type'] = 'same'
+    kwargs.setdefault('pad_type', 'same')
     model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 20), **kwargs)
-    model = _create_nest('jx_nest_small', pretrained=pretrained, **model_kwargs)
+    model = _create_nest('nest_small_jx', pretrained=pretrained, **model_kwargs)
     return model
 
 
 @register_model
-def jx_nest_tiny(pretrained=False, **kwargs):
-    """ Nest-T @ 224x224, Pretrained weights converted from official Jax impl.
+def nest_tiny_jx(pretrained=False, **kwargs) -> Nest:
+    """ Nest-T @ 224x224
     """
-    kwargs['pad_type'] = 'same'
+    kwargs.setdefault('pad_type', 'same')
     model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 8), **kwargs)
-    model = _create_nest('jx_nest_tiny', pretrained=pretrained, **model_kwargs)
+    model = _create_nest('nest_tiny_jx', pretrained=pretrained, **model_kwargs)
     return model
+
+
+register_model_deprecations(__name__, {
+    'jx_nest_base': 'nest_base_jx',
+    'jx_nest_small': 'nest_small_jx',
+    'jx_nest_tiny': 'nest_tiny_jx',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/nfnet.py` & `timm-0.9.0/timm/models/nfnet.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,128 +15,30 @@
 * NF-ResNet, NF-RegNet-B, and NFNet-F models supported
 
 Hacked together by / copyright Ross Wightman, 2021.
 """
 from collections import OrderedDict
 from dataclasses import dataclass, replace
 from functools import partial
-from typing import Tuple, Optional
+from typing import Callable, Tuple, Optional
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, DropPath, AvgPool2dSame, ScaledStdConv2d, ScaledStdConv2dSame, \
     get_act_layer, get_act_fn, get_attn, make_divisible
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_module
 from ._manipulate import checkpoint_seq
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model
 
 __all__ = ['NormFreeNet', 'NfCfg']  # model_registry will add each entrypoint fn to this
 
 
-def _dcfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.9, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv1', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    dm_nfnet_f0=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f0-604f9c3a.pth',
-        pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256), crop_pct=.9),
-    dm_nfnet_f1=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f1-fc540f82.pth',
-        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 320, 320), crop_pct=0.91),
-    dm_nfnet_f2=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f2-89875923.pth',
-        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 352, 352), crop_pct=0.92),
-    dm_nfnet_f3=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f3-d74ab3aa.pth',
-        pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 416, 416), crop_pct=0.94),
-    dm_nfnet_f4=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f4-0ac5b10b.pth',
-        pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 512, 512), crop_pct=0.951),
-    dm_nfnet_f5=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f5-ecb20ab1.pth',
-        pool_size=(13, 13), input_size=(3, 416, 416), test_input_size=(3, 544, 544), crop_pct=0.954),
-    dm_nfnet_f6=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f6-e0f12116.pth',
-        pool_size=(14, 14), input_size=(3, 448, 448), test_input_size=(3, 576, 576), crop_pct=0.956),
-
-    nfnet_f0=_dcfg(
-        url='', pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256)),
-    nfnet_f1=_dcfg(
-        url='', pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 320, 320)),
-    nfnet_f2=_dcfg(
-        url='', pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 352, 352)),
-    nfnet_f3=_dcfg(
-        url='', pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 416, 416)),
-    nfnet_f4=_dcfg(
-        url='', pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 512, 512)),
-    nfnet_f5=_dcfg(
-        url='', pool_size=(13, 13), input_size=(3, 416, 416), test_input_size=(3, 544, 544)),
-    nfnet_f6=_dcfg(
-        url='', pool_size=(14, 14), input_size=(3, 448, 448), test_input_size=(3, 576, 576)),
-    nfnet_f7=_dcfg(
-        url='', pool_size=(15, 15), input_size=(3, 480, 480), test_input_size=(3, 608, 608)),
-
-    nfnet_l0=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nfnet_l0_ra2-45c6688d.pth',
-        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 288, 288), crop_pct=1.0),
-    eca_nfnet_l0=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l0_ra2-e3e9ac50.pth',
-        hf_hub_id='timm/eca_nfnet_l0',
-        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 288, 288), crop_pct=1.0),
-    eca_nfnet_l1=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l1_ra2-7dce93cd.pth',
-        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 320, 320), crop_pct=1.0),
-    eca_nfnet_l2=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l2_ra3-da781a61.pth',
-        pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 384, 384), crop_pct=1.0),
-    eca_nfnet_l3=_dcfg(
-        url='',
-        pool_size=(11, 11), input_size=(3, 352, 352), test_input_size=(3, 448, 448), crop_pct=1.0),
-
-    nf_regnet_b0=_dcfg(
-        url='', pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256), first_conv='stem.conv'),
-    nf_regnet_b1=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nf_regnet_b1_256_ra2-ad85cfef.pth',
-        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 288, 288), first_conv='stem.conv'),  # NOT to paper spec
-    nf_regnet_b2=_dcfg(
-        url='', pool_size=(8, 8), input_size=(3, 240, 240), test_input_size=(3, 272, 272), first_conv='stem.conv'),
-    nf_regnet_b3=_dcfg(
-        url='', pool_size=(9, 9), input_size=(3, 288, 288), test_input_size=(3, 320, 320), first_conv='stem.conv'),
-    nf_regnet_b4=_dcfg(
-        url='', pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 384, 384), first_conv='stem.conv'),
-    nf_regnet_b5=_dcfg(
-        url='', pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 456, 456), first_conv='stem.conv'),
-
-    nf_resnet26=_dcfg(url='', first_conv='stem.conv'),
-    nf_resnet50=_dcfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nf_resnet50_ra2-9f236009.pth',
-        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 288, 288), crop_pct=0.94, first_conv='stem.conv'),
-    nf_resnet101=_dcfg(url='', first_conv='stem.conv'),
-
-    nf_seresnet26=_dcfg(url='', first_conv='stem.conv'),
-    nf_seresnet50=_dcfg(url='', first_conv='stem.conv'),
-    nf_seresnet101=_dcfg(url='', first_conv='stem.conv'),
-
-    nf_ecaresnet26=_dcfg(url='', first_conv='stem.conv'),
-    nf_ecaresnet50=_dcfg(url='', first_conv='stem.conv'),
-    nf_ecaresnet101=_dcfg(url='', first_conv='stem.conv'),
-)
-
-
 @dataclass
 class NfCfg:
     depths: Tuple[int, int, int, int]
     channels: Tuple[int, int, int, int]
     alpha: float = 0.2
     stem_type: str = '3x3'
     stem_chs: Optional[int] = None
@@ -154,171 +56,14 @@
     same_padding: bool = False
     std_conv_eps: float = 1e-5
     skipinit: bool = False  # disabled by default, non-trivial performance impact
     zero_init_fc: bool = False
     act_layer: str = 'silu'
 
 
-def _nfres_cfg(
-        depths,
-        channels=(256, 512, 1024, 2048),
-        group_size=None,
-        act_layer='relu',
-        attn_layer=None,
-        attn_kwargs=None,
-):
-    attn_kwargs = attn_kwargs or {}
-    cfg = NfCfg(
-        depths=depths,
-        channels=channels,
-        stem_type='7x7_pool',
-        stem_chs=64,
-        bottle_ratio=0.25,
-        group_size=group_size,
-        act_layer=act_layer,
-        attn_layer=attn_layer,
-        attn_kwargs=attn_kwargs,
-    )
-    return cfg
-
-
-def _nfreg_cfg(depths, channels=(48, 104, 208, 440)):
-    num_features = 1280 * channels[-1] // 440
-    attn_kwargs = dict(rd_ratio=0.5)
-    cfg = NfCfg(
-        depths=depths,
-        channels=channels,
-        stem_type='3x3',
-        group_size=8,
-        width_factor=0.75,
-        bottle_ratio=2.25,
-        num_features=num_features,
-        reg=True,
-        attn_layer='se',
-        attn_kwargs=attn_kwargs,
-    )
-    return cfg
-
-
-def _nfnet_cfg(
-        depths,
-        channels=(256, 512, 1536, 1536),
-        group_size=128,
-        bottle_ratio=0.5,
-        feat_mult=2.,
-        act_layer='gelu',
-        attn_layer='se',
-        attn_kwargs=None,
-):
-    num_features = int(channels[-1] * feat_mult)
-    attn_kwargs = attn_kwargs if attn_kwargs is not None else dict(rd_ratio=0.5)
-    cfg = NfCfg(
-        depths=depths,
-        channels=channels,
-        stem_type='deep_quad',
-        stem_chs=128,
-        group_size=group_size,
-        bottle_ratio=bottle_ratio,
-        extra_conv=True,
-        num_features=num_features,
-        act_layer=act_layer,
-        attn_layer=attn_layer,
-        attn_kwargs=attn_kwargs,
-    )
-    return cfg
-
-
-def _dm_nfnet_cfg(
-        depths,
-        channels=(256, 512, 1536, 1536),
-        act_layer='gelu',
-        skipinit=True,
-):
-    cfg = NfCfg(
-        depths=depths,
-        channels=channels,
-        stem_type='deep_quad',
-        stem_chs=128,
-        group_size=128,
-        bottle_ratio=0.5,
-        extra_conv=True,
-        gamma_in_act=True,
-        same_padding=True,
-        skipinit=skipinit,
-        num_features=int(channels[-1] * 2.0),
-        act_layer=act_layer,
-        attn_layer='se',
-        attn_kwargs=dict(rd_ratio=0.5),
-    )
-    return cfg
-
-
-model_cfgs = dict(
-    # NFNet-F models w/ GELU compatible with DeepMind weights
-    dm_nfnet_f0=_dm_nfnet_cfg(depths=(1, 2, 6, 3)),
-    dm_nfnet_f1=_dm_nfnet_cfg(depths=(2, 4, 12, 6)),
-    dm_nfnet_f2=_dm_nfnet_cfg(depths=(3, 6, 18, 9)),
-    dm_nfnet_f3=_dm_nfnet_cfg(depths=(4, 8, 24, 12)),
-    dm_nfnet_f4=_dm_nfnet_cfg(depths=(5, 10, 30, 15)),
-    dm_nfnet_f5=_dm_nfnet_cfg(depths=(6, 12, 36, 18)),
-    dm_nfnet_f6=_dm_nfnet_cfg(depths=(7, 14, 42, 21)),
-
-    # NFNet-F models w/ GELU
-    nfnet_f0=_nfnet_cfg(depths=(1, 2, 6, 3)),
-    nfnet_f1=_nfnet_cfg(depths=(2, 4, 12, 6)),
-    nfnet_f2=_nfnet_cfg(depths=(3, 6, 18, 9)),
-    nfnet_f3=_nfnet_cfg(depths=(4, 8, 24, 12)),
-    nfnet_f4=_nfnet_cfg(depths=(5, 10, 30, 15)),
-    nfnet_f5=_nfnet_cfg(depths=(6, 12, 36, 18)),
-    nfnet_f6=_nfnet_cfg(depths=(7, 14, 42, 21)),
-    nfnet_f7=_nfnet_cfg(depths=(8, 16, 48, 24)),
-
-    # Experimental 'light' versions of NFNet-F that are little leaner
-    nfnet_l0=_nfnet_cfg(
-        depths=(1, 2, 6, 3), feat_mult=1.5, group_size=64, bottle_ratio=0.25,
-        attn_kwargs=dict(rd_ratio=0.25, rd_divisor=8), act_layer='silu'),
-    eca_nfnet_l0=_nfnet_cfg(
-        depths=(1, 2, 6, 3), feat_mult=1.5, group_size=64, bottle_ratio=0.25,
-        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
-    eca_nfnet_l1=_nfnet_cfg(
-        depths=(2, 4, 12, 6), feat_mult=2, group_size=64, bottle_ratio=0.25,
-        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
-    eca_nfnet_l2=_nfnet_cfg(
-        depths=(3, 6, 18, 9), feat_mult=2, group_size=64, bottle_ratio=0.25,
-        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
-    eca_nfnet_l3=_nfnet_cfg(
-        depths=(4, 8, 24, 12), feat_mult=2, group_size=64, bottle_ratio=0.25,
-        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
-
-    # EffNet influenced RegNet defs.
-    # NOTE: These aren't quite the official ver, ch_div=1 must be set for exact ch counts. I round to ch_div=8.
-    nf_regnet_b0=_nfreg_cfg(depths=(1, 3, 6, 6)),
-    nf_regnet_b1=_nfreg_cfg(depths=(2, 4, 7, 7)),
-    nf_regnet_b2=_nfreg_cfg(depths=(2, 4, 8, 8), channels=(56, 112, 232, 488)),
-    nf_regnet_b3=_nfreg_cfg(depths=(2, 5, 9, 9), channels=(56, 128, 248, 528)),
-    nf_regnet_b4=_nfreg_cfg(depths=(2, 6, 11, 11), channels=(64, 144, 288, 616)),
-    nf_regnet_b5=_nfreg_cfg(depths=(3, 7, 14, 14), channels=(80, 168, 336, 704)),
-    # FIXME add B6-B8
-
-    # ResNet (preact, D style deep stem/avg down) defs
-    nf_resnet26=_nfres_cfg(depths=(2, 2, 2, 2)),
-    nf_resnet50=_nfres_cfg(depths=(3, 4, 6, 3)),
-    nf_resnet101=_nfres_cfg(depths=(3, 4, 23, 3)),
-
-    nf_seresnet26=_nfres_cfg(depths=(2, 2, 2, 2), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),
-    nf_seresnet50=_nfres_cfg(depths=(3, 4, 6, 3), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),
-    nf_seresnet101=_nfres_cfg(depths=(3, 4, 23, 3), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),
-
-    nf_ecaresnet26=_nfres_cfg(depths=(2, 2, 2, 2), attn_layer='eca', attn_kwargs=dict()),
-    nf_ecaresnet50=_nfres_cfg(depths=(3, 4, 6, 3), attn_layer='eca', attn_kwargs=dict()),
-    nf_ecaresnet101=_nfres_cfg(depths=(3, 4, 23, 3), attn_layer='eca', attn_kwargs=dict()),
-
-)
-
-
 class GammaAct(nn.Module):
     def __init__(self, act_type='relu', gamma: float = 1.0, inplace=False):
         super().__init__()
         self.act_fn = get_act_fn(act_type)
         self.gamma = gamma
         self.inplace = inplace
 
@@ -331,20 +76,20 @@
         return GammaAct(act_type, gamma=gamma, inplace=inplace)
     return _create
 
 
 class DownsampleAvg(nn.Module):
     def __init__(
             self,
-            in_chs,
-            out_chs,
-            stride=1,
-            dilation=1,
-            first_dilation=None,
-            conv_layer=ScaledStdConv2d,
+            in_chs: int,
+            out_chs: int,
+            stride: int = 1,
+            dilation: int = 1,
+            first_dilation: Optional[int] = None,
+            conv_layer: Callable = ScaledStdConv2d,
     ):
         """ AvgPool Downsampling as in 'D' ResNet variants. Support for dilation."""
         super(DownsampleAvg, self).__init__()
         avg_stride = stride if dilation == 1 else 1
         if stride > 1 or dilation > 1:
             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
@@ -359,32 +104,32 @@
 @register_notrace_module  # reason: mul_ causes FX to drop a relevant node. https://github.com/pytorch/pytorch/issues/68301
 class NormFreeBlock(nn.Module):
     """Normalization-Free pre-activation block.
     """
 
     def __init__(
             self,
-            in_chs,
-            out_chs=None,
-            stride=1,
-            dilation=1,
-            first_dilation=None,
-            alpha=1.0,
-            beta=1.0,
-            bottle_ratio=0.25,
-            group_size=None,
-            ch_div=1,
-            reg=True,
-            extra_conv=False,
-            skipinit=False,
-            attn_layer=None,
-            attn_gain=2.0,
-            act_layer=None,
-            conv_layer=None,
-            drop_path_rate=0.,
+            in_chs: int,
+            out_chs: Optional[int] = None,
+            stride: int = 1,
+            dilation: int = 1,
+            first_dilation: Optional[int] = None,
+            alpha: float = 1.0,
+            beta: float = 1.0,
+            bottle_ratio: float = 0.25,
+            group_size: Optional[int] = None,
+            ch_div: int = 1,
+            reg: bool = True,
+            extra_conv: bool = False,
+            skipinit: bool = False,
+            attn_layer: Optional[Callable] = None,
+            attn_gain: bool = 2.0,
+            act_layer: Optional[Callable] = None,
+            conv_layer: Callable = ScaledStdConv2d,
+            drop_path_rate: float = 0.,
     ):
         super().__init__()
         first_dilation = first_dilation or dilation
         out_chs = out_chs or in_chs
         # RegNet variants scale bottleneck from in_chs, otherwise scale from out_chs like ResNet
         mid_chs = make_divisible(in_chs * bottle_ratio if reg else out_chs * bottle_ratio, ch_div)
         groups = 1 if not group_size else mid_chs // group_size
@@ -446,20 +191,27 @@
             out = self.attn_gain * self.attn(out)
         out = self.conv3(self.act3(out))
         if self.attn_last is not None:
             out = self.attn_gain * self.attn_last(out)
         out = self.drop_path(out)
 
         if self.skipinit_gain is not None:
-            out.mul_(self.skipinit_gain)  # this slows things down more than expected, TBD
+            out.mul_(self.skipinit_gain)
         out = out * self.alpha + shortcut
         return out
 
 
-def create_stem(in_chs, out_chs, stem_type='', conv_layer=None, act_layer=None, preact_feature=True):
+def create_stem(
+        in_chs: int,
+        out_chs: int,
+        stem_type: str = '',
+        conv_layer: Optional[Callable] = None,
+        act_layer: Optional[Callable] = None,
+        preact_feature: bool = True,
+):
     stem_stride = 2
     stem_feature = dict(num_chs=out_chs, reduction=2, module='stem.conv')
     stem = OrderedDict()
     assert stem_type in ('', 'deep', 'deep_tiered', 'deep_quad', '3x3', '7x7', 'deep_pool', '3x3_pool', '7x7_pool')
     if 'deep' in stem_type:
         if 'quad' in stem_type:
             # 4 deep conv stack as in NFNet-F models
@@ -536,32 +288,32 @@
             apply it in each activation. This is slightly slower, numerically different, but matches official impl.
         * skipinit is disabled by default, it seems to have a rather drastic impact on GPU memory use and throughput
             for what it is/does. Approx 8-10% throughput loss.
     """
     def __init__(
             self,
             cfg: NfCfg,
-            num_classes=1000,
-            in_chans=3,
-            global_pool='avg',
-            output_stride=32,
-            drop_rate=0.,
-            drop_path_rate=0.,
+            num_classes: int = 1000,
+            in_chans: int = 3,
+            global_pool: str = 'avg',
+            output_stride: int = 32,
+            drop_rate: float = 0.,
+            drop_path_rate: float = 0.,
             **kwargs,
     ):
         """
         Args:
-            cfg (NfCfg): Model architecture configuration
-            num_classes (int): Number of classifier classes (default: 1000)
-            in_chans (int): Number of input channels (default: 3)
-            global_pool (str): Global pooling type (default: 'avg')
-            output_stride (int): Output stride of network, one of (8, 16, 32) (default: 32)
-            drop_rate (float): Dropout rate (default: 0.)
-            drop_path_rate (float): Stochastic depth drop-path rate (default: 0.)
-            kwargs (dict): Extra kwargs overlayed onto cfg
+            cfg: Model architecture configuration.
+            num_classes: Number of classifier classes.
+            in_chans: Number of input channels.
+            global_pool: Global pooling type.
+            output_stride: Output stride of network, one of (8, 16, 32).
+            drop_rate: Dropout rate.
+            drop_path_rate: Stochastic depth drop-path rate.
+            **kwargs: Extra kwargs overlayed onto cfg.
         """
         super().__init__()
         self.num_classes = num_classes
         self.drop_rate = drop_rate
         self.grad_checkpointing = False
 
         cfg = replace(cfg, **kwargs)
@@ -637,15 +389,20 @@
             self.final_conv = conv_layer(prev_chs, self.num_features, 1)
             self.feature_info[-1] = dict(num_chs=self.num_features, reduction=net_stride, module=f'final_conv')
         else:
             self.num_features = prev_chs
             self.final_conv = nn.Identity()
         self.final_act = act_layer(inplace=cfg.num_features > 0)
 
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=self.drop_rate,
+        )
 
         for n, m in self.named_modules():
             if 'fc' in n and isinstance(m, nn.Linear):
                 if cfg.zero_init_fc:
                     nn.init.zeros_(m.weight)
                 else:
                     nn.init.normal_(m.weight, 0., .01)
@@ -672,334 +429,602 @@
         self.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool='avg'):
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x)
         else:
             x = self.stages(x)
         x = self.final_conv(x)
         x = self.final_act(x)
         return x
 
-    def forward_head(self, x):
-        return self.head(x)
+    def forward_head(self, x, pre_logits: bool = False):
+        return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
+def _nfres_cfg(
+        depths,
+        channels=(256, 512, 1024, 2048),
+        group_size=None,
+        act_layer='relu',
+        attn_layer=None,
+        attn_kwargs=None,
+):
+    attn_kwargs = attn_kwargs or {}
+    cfg = NfCfg(
+        depths=depths,
+        channels=channels,
+        stem_type='7x7_pool',
+        stem_chs=64,
+        bottle_ratio=0.25,
+        group_size=group_size,
+        act_layer=act_layer,
+        attn_layer=attn_layer,
+        attn_kwargs=attn_kwargs,
+    )
+    return cfg
+
+
+def _nfreg_cfg(depths, channels=(48, 104, 208, 440)):
+    num_features = 1280 * channels[-1] // 440
+    attn_kwargs = dict(rd_ratio=0.5)
+    cfg = NfCfg(
+        depths=depths,
+        channels=channels,
+        stem_type='3x3',
+        group_size=8,
+        width_factor=0.75,
+        bottle_ratio=2.25,
+        num_features=num_features,
+        reg=True,
+        attn_layer='se',
+        attn_kwargs=attn_kwargs,
+    )
+    return cfg
+
+
+def _nfnet_cfg(
+        depths,
+        channels=(256, 512, 1536, 1536),
+        group_size=128,
+        bottle_ratio=0.5,
+        feat_mult=2.,
+        act_layer='gelu',
+        attn_layer='se',
+        attn_kwargs=None,
+):
+    num_features = int(channels[-1] * feat_mult)
+    attn_kwargs = attn_kwargs if attn_kwargs is not None else dict(rd_ratio=0.5)
+    cfg = NfCfg(
+        depths=depths,
+        channels=channels,
+        stem_type='deep_quad',
+        stem_chs=128,
+        group_size=group_size,
+        bottle_ratio=bottle_ratio,
+        extra_conv=True,
+        num_features=num_features,
+        act_layer=act_layer,
+        attn_layer=attn_layer,
+        attn_kwargs=attn_kwargs,
+    )
+    return cfg
+
+
+def _dm_nfnet_cfg(
+        depths,
+        channels=(256, 512, 1536, 1536),
+        act_layer='gelu',
+        skipinit=True,
+):
+    cfg = NfCfg(
+        depths=depths,
+        channels=channels,
+        stem_type='deep_quad',
+        stem_chs=128,
+        group_size=128,
+        bottle_ratio=0.5,
+        extra_conv=True,
+        gamma_in_act=True,
+        same_padding=True,
+        skipinit=skipinit,
+        num_features=int(channels[-1] * 2.0),
+        act_layer=act_layer,
+        attn_layer='se',
+        attn_kwargs=dict(rd_ratio=0.5),
+    )
+    return cfg
+
+
+model_cfgs = dict(
+    # NFNet-F models w/ GELU compatible with DeepMind weights
+    dm_nfnet_f0=_dm_nfnet_cfg(depths=(1, 2, 6, 3)),
+    dm_nfnet_f1=_dm_nfnet_cfg(depths=(2, 4, 12, 6)),
+    dm_nfnet_f2=_dm_nfnet_cfg(depths=(3, 6, 18, 9)),
+    dm_nfnet_f3=_dm_nfnet_cfg(depths=(4, 8, 24, 12)),
+    dm_nfnet_f4=_dm_nfnet_cfg(depths=(5, 10, 30, 15)),
+    dm_nfnet_f5=_dm_nfnet_cfg(depths=(6, 12, 36, 18)),
+    dm_nfnet_f6=_dm_nfnet_cfg(depths=(7, 14, 42, 21)),
+
+    # NFNet-F models w/ GELU
+    nfnet_f0=_nfnet_cfg(depths=(1, 2, 6, 3)),
+    nfnet_f1=_nfnet_cfg(depths=(2, 4, 12, 6)),
+    nfnet_f2=_nfnet_cfg(depths=(3, 6, 18, 9)),
+    nfnet_f3=_nfnet_cfg(depths=(4, 8, 24, 12)),
+    nfnet_f4=_nfnet_cfg(depths=(5, 10, 30, 15)),
+    nfnet_f5=_nfnet_cfg(depths=(6, 12, 36, 18)),
+    nfnet_f6=_nfnet_cfg(depths=(7, 14, 42, 21)),
+    nfnet_f7=_nfnet_cfg(depths=(8, 16, 48, 24)),
+
+    # Experimental 'light' versions of NFNet-F that are little leaner, w/ SiLU act
+    nfnet_l0=_nfnet_cfg(
+        depths=(1, 2, 6, 3), feat_mult=1.5, group_size=64, bottle_ratio=0.25,
+        attn_kwargs=dict(rd_ratio=0.25, rd_divisor=8), act_layer='silu'),
+    eca_nfnet_l0=_nfnet_cfg(
+        depths=(1, 2, 6, 3), feat_mult=1.5, group_size=64, bottle_ratio=0.25,
+        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
+    eca_nfnet_l1=_nfnet_cfg(
+        depths=(2, 4, 12, 6), feat_mult=2, group_size=64, bottle_ratio=0.25,
+        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
+    eca_nfnet_l2=_nfnet_cfg(
+        depths=(3, 6, 18, 9), feat_mult=2, group_size=64, bottle_ratio=0.25,
+        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
+    eca_nfnet_l3=_nfnet_cfg(
+        depths=(4, 8, 24, 12), feat_mult=2, group_size=64, bottle_ratio=0.25,
+        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),
+
+    # EffNet influenced RegNet defs.
+    # NOTE: These aren't quite the official ver, ch_div=1 must be set for exact ch counts. I round to ch_div=8.
+    nf_regnet_b0=_nfreg_cfg(depths=(1, 3, 6, 6)),
+    nf_regnet_b1=_nfreg_cfg(depths=(2, 4, 7, 7)),
+    nf_regnet_b2=_nfreg_cfg(depths=(2, 4, 8, 8), channels=(56, 112, 232, 488)),
+    nf_regnet_b3=_nfreg_cfg(depths=(2, 5, 9, 9), channels=(56, 128, 248, 528)),
+    nf_regnet_b4=_nfreg_cfg(depths=(2, 6, 11, 11), channels=(64, 144, 288, 616)),
+    nf_regnet_b5=_nfreg_cfg(depths=(3, 7, 14, 14), channels=(80, 168, 336, 704)),
+
+    # ResNet (preact, D style deep stem/avg down) defs
+    nf_resnet26=_nfres_cfg(depths=(2, 2, 2, 2)),
+    nf_resnet50=_nfres_cfg(depths=(3, 4, 6, 3)),
+    nf_resnet101=_nfres_cfg(depths=(3, 4, 23, 3)),
+
+    nf_seresnet26=_nfres_cfg(depths=(2, 2, 2, 2), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),
+    nf_seresnet50=_nfres_cfg(depths=(3, 4, 6, 3), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),
+    nf_seresnet101=_nfres_cfg(depths=(3, 4, 23, 3), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),
+
+    nf_ecaresnet26=_nfres_cfg(depths=(2, 2, 2, 2), attn_layer='eca', attn_kwargs=dict()),
+    nf_ecaresnet50=_nfres_cfg(depths=(3, 4, 6, 3), attn_layer='eca', attn_kwargs=dict()),
+    nf_ecaresnet101=_nfres_cfg(depths=(3, 4, 23, 3), attn_layer='eca', attn_kwargs=dict()),
+)
+
+
 def _create_normfreenet(variant, pretrained=False, **kwargs):
     model_cfg = model_cfgs[variant]
     feature_cfg = dict(flatten_sequential=True)
     return build_model_with_cfg(
-        NormFreeNet, variant, pretrained,
+        NormFreeNet,
+        variant,
+        pretrained,
         model_cfg=model_cfg,
         feature_cfg=feature_cfg,
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _dcfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.9, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv1', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'dm_nfnet_f0.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f0-604f9c3a.pth',
+        pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256), crop_pct=.9, crop_mode='squash'),
+    'dm_nfnet_f1.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f1-fc540f82.pth',
+        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 320, 320), crop_pct=0.91, crop_mode='squash'),
+    'dm_nfnet_f2.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f2-89875923.pth',
+        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 352, 352), crop_pct=0.92, crop_mode='squash'),
+    'dm_nfnet_f3.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f3-d74ab3aa.pth',
+        pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 416, 416), crop_pct=0.94, crop_mode='squash'),
+    'dm_nfnet_f4.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f4-0ac5b10b.pth',
+        pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 512, 512), crop_pct=0.951, crop_mode='squash'),
+    'dm_nfnet_f5.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f5-ecb20ab1.pth',
+        pool_size=(13, 13), input_size=(3, 416, 416), test_input_size=(3, 544, 544), crop_pct=0.954, crop_mode='squash'),
+    'dm_nfnet_f6.dm_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f6-e0f12116.pth',
+        pool_size=(14, 14), input_size=(3, 448, 448), test_input_size=(3, 576, 576), crop_pct=0.956, crop_mode='squash'),
+
+    'nfnet_f0': _dcfg(
+        url='', pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256)),
+    'nfnet_f1': _dcfg(
+        url='', pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 320, 320)),
+    'nfnet_f2': _dcfg(
+        url='', pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 352, 352)),
+    'nfnet_f3': _dcfg(
+        url='', pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 416, 416)),
+    'nfnet_f4': _dcfg(
+        url='', pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 512, 512)),
+    'nfnet_f5': _dcfg(
+        url='', pool_size=(13, 13), input_size=(3, 416, 416), test_input_size=(3, 544, 544)),
+    'nfnet_f6': _dcfg(
+        url='', pool_size=(14, 14), input_size=(3, 448, 448), test_input_size=(3, 576, 576)),
+    'nfnet_f7': _dcfg(
+        url='', pool_size=(15, 15), input_size=(3, 480, 480), test_input_size=(3, 608, 608)),
+
+    'nfnet_l0.ra2_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nfnet_l0_ra2-45c6688d.pth',
+        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'eca_nfnet_l0.ra2_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l0_ra2-e3e9ac50.pth',
+        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'eca_nfnet_l1.ra2_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l1_ra2-7dce93cd.pth',
+        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 320, 320), test_crop_pct=1.0),
+    'eca_nfnet_l2.ra3_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l2_ra3-da781a61.pth',
+        pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 384, 384), test_crop_pct=1.0),
+    'eca_nfnet_l3': _dcfg(
+        url='',
+        pool_size=(11, 11), input_size=(3, 352, 352), test_input_size=(3, 448, 448), test_crop_pct=1.0),
+
+    'nf_regnet_b0': _dcfg(
+        url='', pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256), first_conv='stem.conv'),
+    'nf_regnet_b1.ra2_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nf_regnet_b1_256_ra2-ad85cfef.pth',
+        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 288, 288), first_conv='stem.conv'),  # NOT to paper spec
+    'nf_regnet_b2': _dcfg(
+        url='', pool_size=(8, 8), input_size=(3, 240, 240), test_input_size=(3, 272, 272), first_conv='stem.conv'),
+    'nf_regnet_b3': _dcfg(
+        url='', pool_size=(9, 9), input_size=(3, 288, 288), test_input_size=(3, 320, 320), first_conv='stem.conv'),
+    'nf_regnet_b4': _dcfg(
+        url='', pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 384, 384), first_conv='stem.conv'),
+    'nf_regnet_b5': _dcfg(
+        url='', pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 456, 456), first_conv='stem.conv'),
+
+    'nf_resnet26': _dcfg(url='', first_conv='stem.conv'),
+    'nf_resnet50.ra2_in1k': _dcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nf_resnet50_ra2-9f236009.pth',
+        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 288, 288), crop_pct=0.94, first_conv='stem.conv'),
+    'nf_resnet101': _dcfg(url='', first_conv='stem.conv'),
+
+    'nf_seresnet26': _dcfg(url='', first_conv='stem.conv'),
+    'nf_seresnet50': _dcfg(url='', first_conv='stem.conv'),
+    'nf_seresnet101': _dcfg(url='', first_conv='stem.conv'),
+
+    'nf_ecaresnet26': _dcfg(url='', first_conv='stem.conv'),
+    'nf_ecaresnet50': _dcfg(url='', first_conv='stem.conv'),
+    'nf_ecaresnet101': _dcfg(url='', first_conv='stem.conv'),
+})
 
 
 @register_model
-def dm_nfnet_f0(pretrained=False, **kwargs):
+def dm_nfnet_f0(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F0 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f0', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def dm_nfnet_f1(pretrained=False, **kwargs):
+def dm_nfnet_f1(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F1 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f1', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def dm_nfnet_f2(pretrained=False, **kwargs):
+def dm_nfnet_f2(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F2 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f2', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def dm_nfnet_f3(pretrained=False, **kwargs):
+def dm_nfnet_f3(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F3 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f3', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def dm_nfnet_f4(pretrained=False, **kwargs):
+def dm_nfnet_f4(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F4 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f4', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def dm_nfnet_f5(pretrained=False, **kwargs):
+def dm_nfnet_f5(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F5 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f5', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def dm_nfnet_f6(pretrained=False, **kwargs):
+def dm_nfnet_f6(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F6 (DeepMind weight compatible)
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('dm_nfnet_f6', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f0(pretrained=False, **kwargs):
+def nfnet_f0(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F0
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f0', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f1(pretrained=False, **kwargs):
+def nfnet_f1(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F1
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f1', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f2(pretrained=False, **kwargs):
+def nfnet_f2(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F2
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f2', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f3(pretrained=False, **kwargs):
+def nfnet_f3(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F3
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f3', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f4(pretrained=False, **kwargs):
+def nfnet_f4(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F4
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f4', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f5(pretrained=False, **kwargs):
+def nfnet_f5(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F5
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f5', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f6(pretrained=False, **kwargs):
+def nfnet_f6(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F6
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f6', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_f7(pretrained=False, **kwargs):
+def nfnet_f7(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-F7
     `High-Performance Large-Scale Image Recognition Without Normalization`
         - https://arxiv.org/abs/2102.06171
     """
     return _create_normfreenet('nfnet_f7', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nfnet_l0(pretrained=False, **kwargs):
+def nfnet_l0(pretrained=False, **kwargs) -> NormFreeNet:
     """ NFNet-L0b w/ SiLU
     My experimental 'light' model w/ F0 repeats, 1.5x final_conv mult, 64 group_size, .25 bottleneck & SE ratio
     """
     return _create_normfreenet('nfnet_l0', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_nfnet_l0(pretrained=False, **kwargs):
+def eca_nfnet_l0(pretrained=False, **kwargs) -> NormFreeNet:
     """ ECA-NFNet-L0 w/ SiLU
     My experimental 'light' model w/ F0 repeats, 1.5x final_conv mult, 64 group_size, .25 bottleneck & ECA attn
     """
     return _create_normfreenet('eca_nfnet_l0', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_nfnet_l1(pretrained=False, **kwargs):
+def eca_nfnet_l1(pretrained=False, **kwargs) -> NormFreeNet:
     """ ECA-NFNet-L1 w/ SiLU
     My experimental 'light' model w/ F1 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn
     """
     return _create_normfreenet('eca_nfnet_l1', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_nfnet_l2(pretrained=False, **kwargs):
+def eca_nfnet_l2(pretrained=False, **kwargs) -> NormFreeNet:
     """ ECA-NFNet-L2 w/ SiLU
     My experimental 'light' model w/ F2 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn
     """
     return _create_normfreenet('eca_nfnet_l2', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_nfnet_l3(pretrained=False, **kwargs):
+def eca_nfnet_l3(pretrained=False, **kwargs) -> NormFreeNet:
     """ ECA-NFNet-L3 w/ SiLU
     My experimental 'light' model w/ F3 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn
     """
     return _create_normfreenet('eca_nfnet_l3', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_regnet_b0(pretrained=False, **kwargs):
+def nf_regnet_b0(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free RegNet-B0
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_regnet_b0', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_regnet_b1(pretrained=False, **kwargs):
+def nf_regnet_b1(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free RegNet-B1
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_regnet_b1', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_regnet_b2(pretrained=False, **kwargs):
+def nf_regnet_b2(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free RegNet-B2
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_regnet_b2', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_regnet_b3(pretrained=False, **kwargs):
+def nf_regnet_b3(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free RegNet-B3
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_regnet_b3', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_regnet_b4(pretrained=False, **kwargs):
+def nf_regnet_b4(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free RegNet-B4
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_regnet_b4', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_regnet_b5(pretrained=False, **kwargs):
+def nf_regnet_b5(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free RegNet-B5
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_regnet_b5', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_resnet26(pretrained=False, **kwargs):
+def nf_resnet26(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free ResNet-26
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_resnet26', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_resnet50(pretrained=False, **kwargs):
+def nf_resnet50(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free ResNet-50
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_resnet50', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_resnet101(pretrained=False, **kwargs):
+def nf_resnet101(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free ResNet-101
     `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
         - https://arxiv.org/abs/2101.08692
     """
     return _create_normfreenet('nf_resnet101', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_seresnet26(pretrained=False, **kwargs):
+def nf_seresnet26(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free SE-ResNet26
     """
     return _create_normfreenet('nf_seresnet26', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_seresnet50(pretrained=False, **kwargs):
+def nf_seresnet50(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free SE-ResNet50
     """
     return _create_normfreenet('nf_seresnet50', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_seresnet101(pretrained=False, **kwargs):
+def nf_seresnet101(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free SE-ResNet101
     """
     return _create_normfreenet('nf_seresnet101', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_ecaresnet26(pretrained=False, **kwargs):
+def nf_ecaresnet26(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free ECA-ResNet26
     """
     return _create_normfreenet('nf_ecaresnet26', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_ecaresnet50(pretrained=False, **kwargs):
+def nf_ecaresnet50(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free ECA-ResNet50
     """
     return _create_normfreenet('nf_ecaresnet50', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def nf_ecaresnet101(pretrained=False, **kwargs):
+def nf_ecaresnet101(pretrained=False, **kwargs) -> NormFreeNet:
     """ Normalization-Free ECA-ResNet101
     """
     return _create_normfreenet('nf_ecaresnet101', pretrained=pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/pit.py` & `timm-0.9.0/timm/models/pit.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,192 +10,213 @@
 # PiT
 # Copyright 2021-present NAVER Corp.
 # Apache License v2.0
 
 import math
 import re
 from functools import partial
-from typing import Tuple
+from typing import Sequence, Tuple
 
 import torch
 from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import trunc_normal_, to_2tuple
+from timm.layers import trunc_normal_, to_2tuple, LayerNorm
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .vision_transformer import Block
 
 
 __all__ = ['PoolingVisionTransformer']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.conv', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # deit models (FB weights)
-    'pit_ti_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_ti_730.pth'),
-    'pit_xs_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_xs_781.pth'),
-    'pit_s_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_s_809.pth'),
-    'pit_b_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_b_820.pth'),
-    'pit_ti_distilled_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_ti_distill_746.pth',
-        classifier=('head', 'head_dist')),
-    'pit_xs_distilled_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_xs_distill_791.pth',
-        classifier=('head', 'head_dist')),
-    'pit_s_distilled_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_s_distill_819.pth',
-        classifier=('head', 'head_dist')),
-    'pit_b_distilled_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-pit-weights/pit_b_distill_840.pth',
-        classifier=('head', 'head_dist')),
-}
-
-
 class SequentialTuple(nn.Sequential):
     """ This module exists to work around torchscript typing issues list -> list"""
     def __init__(self, *args):
         super(SequentialTuple, self).__init__(*args)
 
     def forward(self, x: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
         for module in self:
             x = module(x)
         return x
 
 
 class Transformer(nn.Module):
     def __init__(
-            self, base_dim, depth, heads, mlp_ratio, pool=None, drop_rate=.0, attn_drop_rate=.0, drop_path_prob=None):
+            self,
+            base_dim,
+            depth,
+            heads,
+            mlp_ratio,
+            pool=None,
+            proj_drop=.0,
+            attn_drop=.0,
+            drop_path_prob=None,
+            norm_layer=None,
+    ):
         super(Transformer, self).__init__()
-        self.layers = nn.ModuleList([])
         embed_dim = base_dim * heads
 
+        self.pool = pool
+        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()
         self.blocks = nn.Sequential(*[
             Block(
                 dim=embed_dim,
                 num_heads=heads,
                 mlp_ratio=mlp_ratio,
                 qkv_bias=True,
-                drop=drop_rate,
-                attn_drop=attn_drop_rate,
+                proj_drop=proj_drop,
+                attn_drop=attn_drop,
                 drop_path=drop_path_prob[i],
                 norm_layer=partial(nn.LayerNorm, eps=1e-6)
             )
             for i in range(depth)])
 
-        self.pool = pool
-
     def forward(self, x: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
         x, cls_tokens = x
-        B, C, H, W = x.shape
         token_length = cls_tokens.shape[1]
+        if self.pool is not None:
+            x, cls_tokens = self.pool(x, cls_tokens)
 
+        B, C, H, W = x.shape
         x = x.flatten(2).transpose(1, 2)
         x = torch.cat((cls_tokens, x), dim=1)
 
+        x = self.norm(x)
         x = self.blocks(x)
 
         cls_tokens = x[:, :token_length]
         x = x[:, token_length:]
         x = x.transpose(1, 2).reshape(B, C, H, W)
 
-        if self.pool is not None:
-            x, cls_tokens = self.pool(x, cls_tokens)
         return x, cls_tokens
 
 
-class ConvHeadPooling(nn.Module):
+class Pooling(nn.Module):
     def __init__(self, in_feature, out_feature, stride, padding_mode='zeros'):
-        super(ConvHeadPooling, self).__init__()
+        super(Pooling, self).__init__()
 
         self.conv = nn.Conv2d(
-            in_feature, out_feature, kernel_size=stride + 1, padding=stride // 2, stride=stride,
-            padding_mode=padding_mode, groups=in_feature)
+            in_feature,
+            out_feature,
+            kernel_size=stride + 1,
+            padding=stride // 2,
+            stride=stride,
+            padding_mode=padding_mode,
+            groups=in_feature,
+        )
         self.fc = nn.Linear(in_feature, out_feature)
 
     def forward(self, x, cls_token) -> Tuple[torch.Tensor, torch.Tensor]:
         x = self.conv(x)
         cls_token = self.fc(cls_token)
         return x, cls_token
 
 
 class ConvEmbedding(nn.Module):
-    def __init__(self, in_channels, out_channels, patch_size, stride, padding):
+    def __init__(
+            self,
+            in_channels,
+            out_channels,
+            img_size: int = 224,
+            patch_size: int = 16,
+            stride: int = 8,
+            padding: int = 0,
+    ):
         super(ConvEmbedding, self).__init__()
+        padding = padding
+        self.img_size = to_2tuple(img_size)
+        self.patch_size = to_2tuple(patch_size)
+        self.height = math.floor((self.img_size[0] + 2 * padding - self.patch_size[0]) / stride + 1)
+        self.width = math.floor((self.img_size[1] + 2 * padding - self.patch_size[1]) / stride + 1)
+        self.grid_size = (self.height, self.width)
+
         self.conv = nn.Conv2d(
-            in_channels, out_channels, kernel_size=patch_size, stride=stride, padding=padding, bias=True)
+            in_channels, out_channels, kernel_size=patch_size,
+            stride=stride, padding=padding, bias=True)
 
     def forward(self, x):
         x = self.conv(x)
         return x
 
 
 class PoolingVisionTransformer(nn.Module):
     """ Pooling-based Vision Transformer
 
     A PyTorch implement of 'Rethinking Spatial Dimensions of Vision Transformers'
         - https://arxiv.org/abs/2103.16302
     """
     def __init__(
-            self, img_size, patch_size, stride, base_dims, depth, heads,
-            mlp_ratio, num_classes=1000, in_chans=3, global_pool='token',
-            distilled=False, attn_drop_rate=.0, drop_rate=.0, drop_path_rate=.0):
+            self,
+            img_size: int = 224,
+            patch_size: int = 16,
+            stride: int = 8,
+            stem_type: str = 'overlap',
+            base_dims: Sequence[int] = (48, 48, 48),
+            depth: Sequence[int] = (2, 6, 4),
+            heads: Sequence[int] = (2, 4, 8),
+            mlp_ratio: float = 4,
+            num_classes=1000,
+            in_chans=3,
+            global_pool='token',
+            distilled=False,
+            drop_rate=0.,
+            pos_drop_drate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+    ):
         super(PoolingVisionTransformer, self).__init__()
         assert global_pool in ('token',)
 
-        padding = 0
-        img_size = to_2tuple(img_size)
-        patch_size = to_2tuple(patch_size)
-        height = math.floor((img_size[0] + 2 * padding - patch_size[0]) / stride + 1)
-        width = math.floor((img_size[1] + 2 * padding - patch_size[1]) / stride + 1)
-
         self.base_dims = base_dims
         self.heads = heads
+        embed_dim = base_dims[0] * heads[0]
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_tokens = 2 if distilled else 1
+        self.feature_info = []
 
-        self.patch_size = patch_size
-        self.pos_embed = nn.Parameter(torch.randn(1, base_dims[0] * heads[0], height, width))
-        self.patch_embed = ConvEmbedding(in_chans, base_dims[0] * heads[0], patch_size, stride, padding)
-
-        self.cls_token = nn.Parameter(torch.randn(1, self.num_tokens, base_dims[0] * heads[0]))
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.patch_embed = ConvEmbedding(in_chans, embed_dim, img_size, patch_size, stride)
+        self.pos_embed = nn.Parameter(torch.randn(1, embed_dim, self.patch_embed.height, self.patch_embed.width))
+        self.cls_token = nn.Parameter(torch.randn(1, self.num_tokens, embed_dim))
+        self.pos_drop = nn.Dropout(p=pos_drop_drate)
 
         transformers = []
         # stochastic depth decay rule
         dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depth)).split(depth)]
-        for stage in range(len(depth)):
+        prev_dim = embed_dim
+        for i in range(len(depth)):
             pool = None
-            if stage < len(heads) - 1:
-                pool = ConvHeadPooling(
-                    base_dims[stage] * heads[stage], base_dims[stage + 1] * heads[stage + 1], stride=2)
+            embed_dim = base_dims[i] * heads[i]
+            if i > 0:
+                pool = Pooling(
+                    prev_dim,
+                    embed_dim,
+                    stride=2,
+                )
             transformers += [Transformer(
-                base_dims[stage], depth[stage], heads[stage], mlp_ratio, pool=pool,
-                drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_prob=dpr[stage])
-            ]
+                base_dims[i],
+                depth[i],
+                heads[i],
+                mlp_ratio,
+                pool=pool,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path_prob=dpr[i],
+            )]
+            prev_dim = embed_dim
+            self.feature_info += [dict(num_chs=prev_dim, reduction=(stride - 1) * 2**i, module=f'transformers.{i}')]
+
         self.transformers = SequentialTuple(*transformers)
         self.norm = nn.LayerNorm(base_dims[-1] * heads[-1], eps=1e-6)
-        self.num_features = self.embed_dim = base_dims[-1] * heads[-1]
+        self.num_features = self.embed_dim = embed_dim
 
         # Classifier head
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
         self.head_dist = None
         if distilled:
             self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()
         self.distilled_training = False  # must set this True to train w/ distillation token
 
         trunc_normal_(self.pos_embed, std=.02)
@@ -239,26 +260,29 @@
         cls_tokens = self.norm(cls_tokens)
         return cls_tokens
 
     def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:
         if self.head_dist is not None:
             assert self.global_pool == 'token'
             x, x_dist = x[:, 0], x[:, 1]
+            x = self.head_drop(x)
+            x_dist = self.head_drop(x)
             if not pre_logits:
                 x = self.head(x)
                 x_dist = self.head_dist(x_dist)
             if self.distilled_training and self.training and not torch.jit.is_scripting():
                 # only return separate classification predictions when training in distilled mode
                 return x, x_dist
             else:
                 # during standard train / finetune, inference average the classifier predictions
                 return (x + x_dist) / 2
         else:
             if self.global_pool == 'token':
                 x = x[:, 0]
+            x = self.head_drop(x)
             if not pre_logits:
                 x = self.head(x)
             return x
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
@@ -270,137 +294,165 @@
     out_dict = {}
     p_blocks = re.compile(r'pools\.(\d)\.')
     for k, v in state_dict.items():
         # FIXME need to update resize for PiT impl
         # if k == 'pos_embed' and v.shape != model.pos_embed.shape:
         #     # To resize pos embedding when using model at different size from pretrained weights
         #     v = resize_pos_embed(v, model.pos_embed)
-        k = p_blocks.sub(lambda exp: f'transformers.{int(exp.group(1))}.pool.', k)
+        k = p_blocks.sub(lambda exp: f'transformers.{int(exp.group(1)) + 1}.pool.', k)
         out_dict[k] = v
     return out_dict
 
 
 def _create_pit(variant, pretrained=False, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Vision Transformer models.')
+    default_out_indices = tuple(range(3))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
 
     model = build_model_with_cfg(
-        PoolingVisionTransformer, variant, pretrained,
+        PoolingVisionTransformer,
+        variant,
+        pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
-        **kwargs)
+        feature_cfg=dict(feature_cls='hook', no_rewrite=True, out_indices=out_indices),
+        **kwargs,
+    )
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.conv', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    # deit models (FB weights)
+    'pit_ti_224.in1k': _cfg(hf_hub_id='timm/'),
+    'pit_xs_224.in1k': _cfg(hf_hub_id='timm/'),
+    'pit_s_224.in1k': _cfg(hf_hub_id='timm/'),
+    'pit_b_224.in1k': _cfg(hf_hub_id='timm/'),
+    'pit_ti_distilled_224.in1k': _cfg(
+        hf_hub_id='timm/',
+        classifier=('head', 'head_dist')),
+    'pit_xs_distilled_224.in1k': _cfg(
+        hf_hub_id='timm/',
+        classifier=('head', 'head_dist')),
+    'pit_s_distilled_224.in1k': _cfg(
+        hf_hub_id='timm/',
+        classifier=('head', 'head_dist')),
+    'pit_b_distilled_224.in1k': _cfg(
+        hf_hub_id='timm/',
+        classifier=('head', 'head_dist')),
+})
+
+
 @register_model
-def pit_b_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_b_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=14,
         stride=7,
         base_dims=[64, 64, 64],
         depth=[3, 6, 4],
         heads=[4, 8, 16],
         mlp_ratio=4,
-        **kwargs
     )
-    return _create_pit('pit_b_224', pretrained, **model_kwargs)
+    return _create_pit('pit_b_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_s_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_s_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=16,
         stride=8,
         base_dims=[48, 48, 48],
         depth=[2, 6, 4],
         heads=[3, 6, 12],
         mlp_ratio=4,
-        **kwargs
     )
-    return _create_pit('pit_s_224', pretrained, **model_kwargs)
+    return _create_pit('pit_s_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_xs_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_xs_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=16,
         stride=8,
         base_dims=[48, 48, 48],
         depth=[2, 6, 4],
         heads=[2, 4, 8],
         mlp_ratio=4,
-        **kwargs
     )
-    return _create_pit('pit_xs_224', pretrained, **model_kwargs)
+    return _create_pit('pit_xs_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_ti_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_ti_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=16,
         stride=8,
         base_dims=[32, 32, 32],
         depth=[2, 6, 4],
         heads=[2, 4, 8],
         mlp_ratio=4,
-        **kwargs
     )
-    return _create_pit('pit_ti_224', pretrained, **model_kwargs)
+    return _create_pit('pit_ti_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_b_distilled_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_b_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=14,
         stride=7,
         base_dims=[64, 64, 64],
         depth=[3, 6, 4],
         heads=[4, 8, 16],
         mlp_ratio=4,
         distilled=True,
-        **kwargs
     )
-    return _create_pit('pit_b_distilled_224', pretrained, **model_kwargs)
+    return _create_pit('pit_b_distilled_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_s_distilled_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_s_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=16,
         stride=8,
         base_dims=[48, 48, 48],
         depth=[2, 6, 4],
         heads=[3, 6, 12],
         mlp_ratio=4,
         distilled=True,
-        **kwargs
     )
-    return _create_pit('pit_s_distilled_224', pretrained, **model_kwargs)
+    return _create_pit('pit_s_distilled_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_xs_distilled_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_xs_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=16,
         stride=8,
         base_dims=[48, 48, 48],
         depth=[2, 6, 4],
         heads=[2, 4, 8],
         mlp_ratio=4,
         distilled=True,
-        **kwargs
     )
-    return _create_pit('pit_xs_distilled_224', pretrained, **model_kwargs)
+    return _create_pit('pit_xs_distilled_224', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pit_ti_distilled_224(pretrained, **kwargs):
-    model_kwargs = dict(
+def pit_ti_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:
+    model_args = dict(
         patch_size=16,
         stride=8,
         base_dims=[32, 32, 32],
         depth=[2, 6, 4],
         heads=[2, 4, 8],
         mlp_ratio=4,
         distilled=True,
-        **kwargs
     )
-    return _create_pit('pit_ti_distilled_224', pretrained, **model_kwargs)
+    return _create_pit('pit_ti_distilled_224', pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/pnasnet.py` & `timm-0.9.0/timm/models/pnasnet.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,34 +10,18 @@
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['PNASNet5Large']
 
-default_cfgs = {
-    'pnasnet5large': {
-        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/pnasnet5large-bf079911.pth',
-        'input_size': (3, 331, 331),
-        'pool_size': (11, 11),
-        'crop_pct': 0.911,
-        'interpolation': 'bicubic',
-        'mean': (0.5, 0.5, 0.5),
-        'std': (0.5, 0.5, 0.5),
-        'num_classes': 1000,
-        'first_conv': 'conv_0.conv',
-        'classifier': 'last_linear',
-        'label_offset': 1,  # 1001 classes in pretrained weights
-    },
-}
-
 
 class SeparableConv2d(nn.Module):
 
     def __init__(self, in_channels, out_channels, kernel_size, stride, padding=''):
         super(SeparableConv2d, self).__init__()
         self.depthwise_conv2d = create_conv2d(
             in_channels, in_channels, kernel_size=kernel_size,
@@ -181,16 +165,24 @@
         x_right = self.conv_1x1(x_left)
         x_out = self.cell_forward(x_left, x_right)
         return x_out
 
 
 class Cell(CellBase):
 
-    def __init__(self, in_chs_left, out_chs_left, in_chs_right, out_chs_right, pad_type='',
-                 is_reduction=False, match_prev_layer_dims=False):
+    def __init__(
+            self,
+            in_chs_left,
+            out_chs_left,
+            in_chs_right,
+            out_chs_right,
+            pad_type='',
+            is_reduction=False,
+            match_prev_layer_dims=False,
+    ):
         super(Cell, self).__init__()
 
         # If `is_reduction` is set to `True` stride 2 is used for
         # convolution and pooling layers to reduce the spatial size of
         # the output of a cell approximately by a factor of 2.
         stride = 2 if is_reduction else 1
 
@@ -232,18 +224,25 @@
         x_left = self.conv_prev_1x1(x_left)
         x_right = self.conv_1x1(x_right)
         x_out = self.cell_forward(x_left, x_right)
         return x_out
 
 
 class PNASNet5Large(nn.Module):
-    def __init__(self, num_classes=1000, in_chans=3, output_stride=32, drop_rate=0., global_pool='avg', pad_type=''):
+    def __init__(
+            self,
+            num_classes=1000,
+            in_chans=3,
+            output_stride=32,
+            drop_rate=0.,
+            global_pool='avg',
+            pad_type='',
+    ):
         super(PNASNet5Large, self).__init__()
         self.num_classes = num_classes
-        self.drop_rate = drop_rate
         self.num_features = 4320
         assert output_stride == 32
 
         self.conv_0 = ConvNormAct(
             in_chans, 96, kernel_size=3, stride=2, padding=0,
             norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), apply_act=False)
 
@@ -289,16 +288,16 @@
             dict(num_chs=96, reduction=2, module='conv_0'),
             dict(num_chs=270, reduction=4, module='cell_stem_1.conv_1x1.act'),
             dict(num_chs=1080, reduction=8, module='cell_4.conv_1x1.act'),
             dict(num_chs=2160, reduction=16, module='cell_8.conv_1x1.act'),
             dict(num_chs=4320, reduction=32, module='act'),
         ]
 
-        self.global_pool, self.last_linear = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool, self.head_drop, self.last_linear = create_classifier(
+            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(stem=r'^conv_0|cell_stem_[01]', blocks=r'^cell_(\d+)')
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
@@ -330,32 +329,50 @@
         x_cell_10 = self.cell_10(x_cell_8, x_cell_9)
         x_cell_11 = self.cell_11(x_cell_9, x_cell_10)
         x = self.act(x_cell_11)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
-        if self.drop_rate > 0:
-            x = F.dropout(x, self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         return x if pre_logits else self.last_linear(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_pnasnet(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        PNASNet5Large, variant, pretrained,
+        PNASNet5Large,
+        variant,
+        pretrained,
         feature_cfg=dict(feature_cls='hook', no_rewrite=True),  # not possible to re-write this model
-        **kwargs)
+        **kwargs,
+    )
+
+
+default_cfgs = generate_default_cfgs({
+    'pnasnet5large.tf_in1k': {
+        'hf_hub_id': 'timm/',
+        'input_size': (3, 331, 331),
+        'pool_size': (11, 11),
+        'crop_pct': 0.911,
+        'interpolation': 'bicubic',
+        'mean': (0.5, 0.5, 0.5),
+        'std': (0.5, 0.5, 0.5),
+        'num_classes': 1000,
+        'first_conv': 'conv_0.conv',
+        'classifier': 'last_linear',
+    },
+})
 
 
 @register_model
-def pnasnet5large(pretrained=False, **kwargs):
+def pnasnet5large(pretrained=False, **kwargs) -> PNASNet5Large:
     r"""PNASNet-5 model architecture from the
     `"Progressive Neural Architecture Search"
     <https://arxiv.org/abs/1712.00559>`_ paper.
     """
     model_kwargs = dict(pad_type='same', **kwargs)
     return _create_pnasnet('pnasnet5large', pretrained, **model_kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/poolformer.py` & `timm-0.9.0/timm/models/rexnet.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,313 +1,356 @@
-""" PoolFormer implementation
+""" ReXNet
 
-Paper: `PoolFormer: MetaFormer is Actually What You Need for Vision` - https://arxiv.org/abs/2111.11418
+A PyTorch impl of `ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network` -
+https://arxiv.org/abs/2007.00992
 
-Code adapted from official impl at https://github.com/sail-sg/poolformer, original copyright in comment below
+Adapted from original impl at https://github.com/clovaai/rexnet
+Copyright (c) 2020-present NAVER Corp. MIT license
 
-Modifications and additions for timm by / Copyright 2022, Ross Wightman
+Changes for timm, feature extraction, and rounded channel variant hacked together by Ross Wightman
+Copyright 2020 Ross Wightman
 """
-# Copyright 2021 Garena Online Private Limited
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+
+from functools import partial
+from math import ceil
+
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import DropPath, trunc_normal_, to_2tuple, ConvMlp, GroupNorm1
+from timm.layers import ClassifierHead, create_act_layer, ConvNormAct, DropPath, make_divisible, SEModule
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._efficientnet_builder import efficientnet_init_weights
+from ._manipulate import checkpoint_seq
+from ._registry import generate_default_cfgs, register_model
 
-__all__ = ['PoolFormer']  # model_registry will add each entrypoint fn to this
+__all__ = ['RexNet']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .95, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
+SEWithNorm = partial(SEModule, norm_layer=nn.BatchNorm2d)
 
-default_cfgs = dict(
-    poolformer_s12=_cfg(
-        url='https://github.com/sail-sg/poolformer/releases/download/v1.0/poolformer_s12.pth.tar',
-        crop_pct=0.9),
-    poolformer_s24=_cfg(
-        url='https://github.com/sail-sg/poolformer/releases/download/v1.0/poolformer_s24.pth.tar',
-        crop_pct=0.9),
-    poolformer_s36=_cfg(
-        url='https://github.com/sail-sg/poolformer/releases/download/v1.0/poolformer_s36.pth.tar',
-        crop_pct=0.9),
-    poolformer_m36=_cfg(
-        url='https://github.com/sail-sg/poolformer/releases/download/v1.0/poolformer_m36.pth.tar',
-        crop_pct=0.95),
-    poolformer_m48=_cfg(
-        url='https://github.com/sail-sg/poolformer/releases/download/v1.0/poolformer_m48.pth.tar',
-        crop_pct=0.95),
-)
-
-
-class PatchEmbed(nn.Module):
-    """ Patch Embedding that is implemented by a layer of conv.
-    Input: tensor in shape [B, C, H, W]
-    Output: tensor in shape [B, C, H/stride, W/stride]
-    """
-
-    def __init__(self, in_chs=3, embed_dim=768, patch_size=16, stride=16, padding=0, norm_layer=None):
-        super().__init__()
-        patch_size = to_2tuple(patch_size)
-        stride = to_2tuple(stride)
-        padding = to_2tuple(padding)
-        self.proj = nn.Conv2d(in_chs, embed_dim, kernel_size=patch_size, stride=stride, padding=padding)
-        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()
 
-    def forward(self, x):
-        x = self.proj(x)
-        x = self.norm(x)
-        return x
-
-
-class Pooling(nn.Module):
-    def __init__(self, pool_size=3):
-        super().__init__()
-        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)
-
-    def forward(self, x):
-        return self.pool(x) - x
+class LinearBottleneck(nn.Module):
+    def __init__(
+            self,
+            in_chs,
+            out_chs,
+            stride,
+            dilation=(1, 1),
+            exp_ratio=1.0,
+            se_ratio=0.,
+            ch_div=1,
+            act_layer='swish',
+            dw_act_layer='relu6',
+            drop_path=None,
+    ):
+        super(LinearBottleneck, self).__init__()
+        self.use_shortcut = stride == 1 and dilation[0] == dilation[1] and in_chs <= out_chs
+        self.in_channels = in_chs
+        self.out_channels = out_chs
+
+        if exp_ratio != 1.:
+            dw_chs = make_divisible(round(in_chs * exp_ratio), divisor=ch_div)
+            self.conv_exp = ConvNormAct(in_chs, dw_chs, act_layer=act_layer)
+        else:
+            dw_chs = in_chs
+            self.conv_exp = None
 
+        self.conv_dw = ConvNormAct(
+            dw_chs,
+            dw_chs,
+            kernel_size=3,
+            stride=stride,
+            dilation=dilation[0],
+            groups=dw_chs,
+            apply_act=False,
+        )
+        if se_ratio > 0:
+            self.se = SEWithNorm(dw_chs, rd_channels=make_divisible(int(dw_chs * se_ratio), ch_div))
+        else:
+            self.se = None
+        self.act_dw = create_act_layer(dw_act_layer)
 
-class PoolFormerBlock(nn.Module):
-    """
-    Args:
-        dim: embedding dim
-        pool_size: pooling size
-        mlp_ratio: mlp expansion ratio
-        act_layer: activation
-        norm_layer: normalization
-        drop: dropout rate
-        drop path: Stochastic Depth, refer to https://arxiv.org/abs/1603.09382
-        use_layer_scale, --layer_scale_init_value: LayerScale, refer to https://arxiv.org/abs/2103.17239
-    """
+        self.conv_pwl = ConvNormAct(dw_chs, out_chs, 1, apply_act=False)
+        self.drop_path = drop_path
 
-    def __init__(
-            self, dim, pool_size=3, mlp_ratio=4.,
-            act_layer=nn.GELU, norm_layer=GroupNorm1,
-            drop=0., drop_path=0., layer_scale_init_value=1e-5):
-
-        super().__init__()
-
-        self.norm1 = norm_layer(dim)
-        self.token_mixer = Pooling(pool_size=pool_size)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.norm2 = norm_layer(dim)
-        self.mlp = ConvMlp(dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        if layer_scale_init_value:
-            self.layer_scale_1 = nn.Parameter(layer_scale_init_value * torch.ones(dim))
-            self.layer_scale_2 = nn.Parameter(layer_scale_init_value * torch.ones(dim))
-        else:
-            self.layer_scale_1 = None
-            self.layer_scale_2 = None
+    def feat_channels(self, exp=False):
+        return self.conv_dw.out_channels if exp else self.out_channels
 
     def forward(self, x):
-        if self.layer_scale_1 is not None:
-            x = x + self.drop_path1(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.token_mixer(self.norm1(x)))
-            x = x + self.drop_path2(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x)))
-        else:
-            x = x + self.drop_path1(self.token_mixer(self.norm1(x)))
-            x = x + self.drop_path2(self.mlp(self.norm2(x)))
+        shortcut = x
+        if self.conv_exp is not None:
+            x = self.conv_exp(x)
+        x = self.conv_dw(x)
+        if self.se is not None:
+            x = self.se(x)
+        x = self.act_dw(x)
+        x = self.conv_pwl(x)
+        if self.use_shortcut:
+            if self.drop_path is not None:
+                x = self.drop_path(x)
+            x = torch.cat([x[:, 0:self.in_channels] + shortcut, x[:, self.in_channels:]], dim=1)
         return x
 
 
-def basic_blocks(
-        dim, index, layers,
-        pool_size=3, mlp_ratio=4.,
-        act_layer=nn.GELU, norm_layer=GroupNorm1,
-        drop_rate=.0, drop_path_rate=0.,
-        layer_scale_init_value=1e-5,
+def _block_cfg(
+        width_mult=1.0,
+        depth_mult=1.0,
+        initial_chs=16,
+        final_chs=180,
+        se_ratio=0.,
+        ch_div=1,
+):
+    layers = [1, 2, 2, 3, 3, 5]
+    strides = [1, 2, 2, 2, 1, 2]
+    layers = [ceil(element * depth_mult) for element in layers]
+    strides = sum([[element] + [1] * (layers[idx] - 1) for idx, element in enumerate(strides)], [])
+    exp_ratios = [1] * layers[0] + [6] * sum(layers[1:])
+    depth = sum(layers[:]) * 3
+    base_chs = initial_chs / width_mult if width_mult < 1.0 else initial_chs
+
+    # The following channel configuration is a simple instance to make each layer become an expand layer.
+    out_chs_list = []
+    for i in range(depth // 3):
+        out_chs_list.append(make_divisible(round(base_chs * width_mult), divisor=ch_div))
+        base_chs += final_chs / (depth // 3 * 1.0)
+
+    se_ratios = [0.] * (layers[0] + layers[1]) + [se_ratio] * sum(layers[2:])
+
+    return list(zip(out_chs_list, exp_ratios, strides, se_ratios))
+
+
+def _build_blocks(
+        block_cfg,
+        prev_chs,
+        width_mult,
+        ch_div=1,
+        output_stride=32,
+        act_layer='swish',
+        dw_act_layer='relu6',
+        drop_path_rate=0.,
 ):
-    """ generate PoolFormer blocks for a stage """
-    blocks = []
-    for block_idx in range(layers[index]):
-        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)
-        blocks.append(PoolFormerBlock(
-            dim, pool_size=pool_size, mlp_ratio=mlp_ratio,
-            act_layer=act_layer, norm_layer=norm_layer,
-            drop=drop_rate, drop_path=block_dpr,
-            layer_scale_init_value=layer_scale_init_value,
+    feat_chs = [prev_chs]
+    feature_info = []
+    curr_stride = 2
+    dilation = 1
+    features = []
+    num_blocks = len(block_cfg)
+    for block_idx, (chs, exp_ratio, stride, se_ratio) in enumerate(block_cfg):
+        next_dilation = dilation
+        if stride > 1:
+            fname = 'stem' if block_idx == 0 else f'features.{block_idx - 1}'
+            feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=fname)]
+            if curr_stride >= output_stride:
+                next_dilation = dilation * stride
+                stride = 1
+        block_dpr = drop_path_rate * block_idx / (num_blocks - 1)  # stochastic depth linear decay rule
+        drop_path = DropPath(block_dpr) if block_dpr > 0. else None
+        features.append(LinearBottleneck(
+            in_chs=prev_chs,
+            out_chs=chs,
+            exp_ratio=exp_ratio,
+            stride=stride,
+            dilation=(dilation, next_dilation),
+            se_ratio=se_ratio,
+            ch_div=ch_div,
+            act_layer=act_layer,
+            dw_act_layer=dw_act_layer,
+            drop_path=drop_path,
         ))
-    blocks = nn.Sequential(*blocks)
-    return blocks
-
+        curr_stride *= stride
+        dilation = next_dilation
+        prev_chs = chs
+        feat_chs += [features[-1].feat_channels()]
+    pen_chs = make_divisible(1280 * width_mult, divisor=ch_div)
+    feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=f'features.{len(features) - 1}')]
+    features.append(ConvNormAct(prev_chs, pen_chs, act_layer=act_layer))
+    return features, feature_info
 
-class PoolFormer(nn.Module):
-    """ PoolFormer
-    """
 
+class RexNet(nn.Module):
     def __init__(
             self,
-            layers,
-            embed_dims=(64, 128, 320, 512),
-            mlp_ratios=(4, 4, 4, 4),
-            downsamples=(True, True, True, True),
-            pool_size=3,
             in_chans=3,
             num_classes=1000,
             global_pool='avg',
-            norm_layer=GroupNorm1,
-            act_layer=nn.GELU,
-            in_patch_size=7,
-            in_stride=4,
-            in_pad=2,
-            down_patch_size=3,
-            down_stride=2,
-            down_pad=1,
-            drop_rate=0., drop_path_rate=0.,
-            layer_scale_init_value=1e-5,
-            **kwargs):
-
-        super().__init__()
+            output_stride=32,
+            initial_chs=16,
+            final_chs=180,
+            width_mult=1.0,
+            depth_mult=1.0,
+            se_ratio=1/12.,
+            ch_div=1,
+            act_layer='swish',
+            dw_act_layer='relu6',
+            drop_rate=0.2,
+            drop_path_rate=0.,
+    ):
+        super(RexNet, self).__init__()
         self.num_classes = num_classes
-        self.global_pool = global_pool
-        self.num_features = embed_dims[-1]
+        self.drop_rate = drop_rate
         self.grad_checkpointing = False
 
-        self.patch_embed = PatchEmbed(
-            patch_size=in_patch_size, stride=in_stride, padding=in_pad,
-            in_chs=in_chans, embed_dim=embed_dims[0])
-
-        # set the main block in network
-        network = []
-        for i in range(len(layers)):
-            network.append(basic_blocks(
-                embed_dims[i], i, layers,
-                pool_size=pool_size, mlp_ratio=mlp_ratios[i],
-                act_layer=act_layer, norm_layer=norm_layer,
-                drop_rate=drop_rate, drop_path_rate=drop_path_rate,
-                layer_scale_init_value=layer_scale_init_value)
-            )
-            if i < len(layers) - 1 and (downsamples[i] or embed_dims[i] != embed_dims[i + 1]):
-                # downsampling between stages
-                network.append(PatchEmbed(
-                    in_chs=embed_dims[i], embed_dim=embed_dims[i + 1],
-                    patch_size=down_patch_size, stride=down_stride, padding=down_pad)
-                )
-
-        self.network = nn.Sequential(*network)
-        self.norm = norm_layer(self.num_features)
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-        self.apply(self._init_weights)
-
-    # init for classification
-    def _init_weights(self, m):
-        if isinstance(m, nn.Linear):
-            trunc_normal_(m.weight, std=.02)
-            if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+        assert output_stride in (32, 16, 8)
+        stem_base_chs = 32 / width_mult if width_mult < 1.0 else 32
+        stem_chs = make_divisible(round(stem_base_chs * width_mult), divisor=ch_div)
+        self.stem = ConvNormAct(in_chans, stem_chs, 3, stride=2, act_layer=act_layer)
+
+        block_cfg = _block_cfg(width_mult, depth_mult, initial_chs, final_chs, se_ratio, ch_div)
+        features, self.feature_info = _build_blocks(
+            block_cfg,
+            stem_chs,
+            width_mult,
+            ch_div,
+            output_stride,
+            act_layer,
+            dw_act_layer,
+            drop_path_rate,
+        )
+        self.num_features = features[-1].out_channels
+        self.features = nn.Sequential(*features)
+
+        self.head = ClassifierHead(self.num_features, num_classes, global_pool, drop_rate)
+
+        efficientnet_init_weights(self)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
-        return dict(
-            stem=r'^patch_embed',  # stem and embed
-            blocks=[
-                (r'^network\.(\d+).*\.proj', (99999,)),
-                (r'^network\.(\d+)', None) if coarse else (r'^network\.(\d+)\.(\d+)', None),
-                (r'^norm', (99999,))
-            ],
+        matcher = dict(
+            stem=r'^stem',
+            blocks=r'^features\.(\d+)',
         )
+        return matcher
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
-        return self.head
+        return self.head.fc
 
-    def reset_classifier(self, num_classes, global_pool=None):
-        self.num_classes = num_classes
-        if global_pool is not None:
-            self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+    def reset_classifier(self, num_classes, global_pool='avg'):
+        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
 
     def forward_features(self, x):
-        x = self.patch_embed(x)
-        x = self.network(x)
-        x = self.norm(x)
+        x = self.stem(x)
+        if self.grad_checkpointing and not torch.jit.is_scripting():
+            x = checkpoint_seq(self.features, x, flatten=True)
+        else:
+            x = self.features(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool == 'avg':
-            x = x.mean([-2, -1])
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _create_poolformer(variant, pretrained=False, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Vision Transformer models.')
-    model = build_model_with_cfg(PoolFormer, variant, pretrained, **kwargs)
-    return model
+def _create_rexnet(variant, pretrained, **kwargs):
+    feature_cfg = dict(flatten_sequential=True)
+    return build_model_with_cfg(
+        RexNet,
+        variant,
+        pretrained,
+        feature_cfg=feature_cfg,
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        'license': 'mit', **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'rexnet_100.nav_in1k': _cfg(hf_hub_id='timm/'),
+    'rexnet_130.nav_in1k': _cfg(hf_hub_id='timm/'),
+    'rexnet_150.nav_in1k': _cfg(hf_hub_id='timm/'),
+    'rexnet_200.nav_in1k': _cfg(hf_hub_id='timm/'),
+    'rexnet_300.nav_in1k': _cfg(hf_hub_id='timm/'),
+    'rexnetr_100.untrained': _cfg(),
+    'rexnetr_130.untrained': _cfg(),
+    'rexnetr_150.untrained': _cfg(),
+    'rexnetr_200.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),
+    'rexnetr_300.sw_in12k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),
+    'rexnetr_200.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821,
+        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),
+    'rexnetr_300.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821,
+        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),
+})
+
+
+@register_model
+def rexnet_100(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 1.0x"""
+    return _create_rexnet('rexnet_100', pretrained, **kwargs)
+
+
+@register_model
+def rexnet_130(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 1.3x"""
+    return _create_rexnet('rexnet_130', pretrained, width_mult=1.3, **kwargs)
+
+
+@register_model
+def rexnet_150(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 1.5x"""
+    return _create_rexnet('rexnet_150', pretrained, width_mult=1.5, **kwargs)
+
+
+@register_model
+def rexnet_200(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 2.0x"""
+    return _create_rexnet('rexnet_200', pretrained, width_mult=2.0, **kwargs)
+
+
+@register_model
+def rexnet_300(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 3.0x"""
+    return _create_rexnet('rexnet_300', pretrained, width_mult=3.0, **kwargs)
 
 
 @register_model
-def poolformer_s12(pretrained=False, **kwargs):
-    """ PoolFormer-S12 model, Params: 12M """
-    model = _create_poolformer('poolformer_s12', pretrained=pretrained, layers=(2, 2, 6, 2), **kwargs)
-    return model
+def rexnetr_100(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 1.0x w/ rounded (mod 8) channels"""
+    return _create_rexnet('rexnetr_100', pretrained, ch_div=8, **kwargs)
 
 
 @register_model
-def poolformer_s24(pretrained=False, **kwargs):
-    """ PoolFormer-S24 model, Params: 21M """
-    model = _create_poolformer('poolformer_s24', pretrained=pretrained, layers=(4, 4, 12, 4), **kwargs)
-    return model
+def rexnetr_130(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 1.3x w/ rounded (mod 8) channels"""
+    return _create_rexnet('rexnetr_130', pretrained, width_mult=1.3, ch_div=8, **kwargs)
 
 
 @register_model
-def poolformer_s36(pretrained=False, **kwargs):
-    """ PoolFormer-S36 model, Params: 31M """
-    model = _create_poolformer(
-        'poolformer_s36', pretrained=pretrained, layers=(6, 6, 18, 6), layer_scale_init_value=1e-6, **kwargs)
-    return model
+def rexnetr_150(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 1.5x w/ rounded (mod 8) channels"""
+    return _create_rexnet('rexnetr_150', pretrained, width_mult=1.5, ch_div=8, **kwargs)
 
 
 @register_model
-def poolformer_m36(pretrained=False, **kwargs):
-    """ PoolFormer-M36 model, Params: 56M """
-    layers = (6, 6, 18, 6)
-    embed_dims = (96, 192, 384, 768)
-    model = _create_poolformer(
-        'poolformer_m36', pretrained=pretrained, layers=layers, embed_dims=embed_dims,
-        layer_scale_init_value=1e-6, **kwargs)
-    return model
+def rexnetr_200(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 2.0x w/ rounded (mod 8) channels"""
+    return _create_rexnet('rexnetr_200', pretrained, width_mult=2.0, ch_div=8, **kwargs)
 
 
 @register_model
-def poolformer_m48(pretrained=False, **kwargs):
-    """ PoolFormer-M48 model, Params: 73M """
-    layers = (8, 8, 24, 8)
-    embed_dims = (96, 192, 384, 768)
-    model = _create_poolformer(
-        'poolformer_m48', pretrained=pretrained, layers=layers, embed_dims=embed_dims,
-        layer_scale_init_value=1e-6, **kwargs)
-    return model
+def rexnetr_300(pretrained=False, **kwargs) -> RexNet:
+    """ReXNet V1 3.0x w/ rounded (mod 16) channels"""
+    return _create_rexnet('rexnetr_300', pretrained, width_mult=3.0, ch_div=16, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/pvt_v2.py` & `timm-0.9.0/timm/models/pvt_v2.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,54 +12,39 @@
 
 Based on Apache 2.0 licensed code at https://github.com/whai362/PVT
 
 Modifications and timm support by / Copyright 2022, Ross Wightman
 """
 
 import math
-from functools import partial
 from typing import Tuple, List, Callable, Union
 
 import torch
 import torch.nn as nn
+import torch.nn.functional as F
 import torch.utils.checkpoint as checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import DropPath, to_2tuple, to_ntuple, trunc_normal_
+from timm.layers import DropPath, to_2tuple, to_ntuple, trunc_normal_, LayerNorm, use_fused_attn
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['PyramidVisionTransformerV2']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.9, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head', 'fixed_input_size': False,
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'pvt_v2_b0': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b0.pth'),
-    'pvt_v2_b1': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b1.pth'),
-    'pvt_v2_b2': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b2.pth'),
-    'pvt_v2_b3': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b3.pth'),
-    'pvt_v2_b4': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b4.pth'),
-    'pvt_v2_b5': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b5.pth'),
-    'pvt_v2_b2_li': _cfg(url='https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b2_li.pth')
-}
-
-
 class MlpWithDepthwiseConv(nn.Module):
     def __init__(
-            self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,
-            drop=0., extra_relu=False):
+            self,
+            in_features,
+            hidden_features=None,
+            out_features=None,
+            act_layer=nn.GELU,
+            drop=0.,
+            extra_relu=False,
+    ):
         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         self.fc1 = nn.Linear(in_features, hidden_features)
         self.relu = nn.ReLU() if extra_relu else nn.Identity()
         self.dwconv = nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features)
         self.act = act_layer()
@@ -77,14 +62,16 @@
         x = self.drop(x)
         x = self.fc2(x)
         x = self.drop(x)
         return x
 
 
 class Attention(nn.Module):
+    fused_attn: torch.jit.Final[bool]
+
     def __init__(
             self,
             dim,
             num_heads=8,
             sr_ratio=1,
             linear_attn=False,
             qkv_bias=True,
@@ -94,14 +81,15 @@
         super().__init__()
         assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."
 
         self.dim = dim
         self.num_heads = num_heads
         self.head_dim = dim // num_heads
         self.scale = self.head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.q = nn.Linear(dim, dim, bias=qkv_bias)
         self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
 
@@ -122,212 +110,240 @@
 
     def forward(self, x, feat_size: List[int]):
         B, N, C = x.shape
         H, W = feat_size
         q = self.q(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
 
         if self.pool is not None:
-            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)
-            x_ = self.sr(self.pool(x_)).reshape(B, C, -1).permute(0, 2, 1)
-            x_ = self.norm(x_)
-            x_ = self.act(x_)
-            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
+            x = x.permute(0, 2, 1).reshape(B, C, H, W)
+            x = self.sr(self.pool(x)).reshape(B, C, -1).permute(0, 2, 1)
+            x = self.norm(x)
+            x = self.act(x)
+            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
         else:
             if self.sr is not None:
-                x_ = x.permute(0, 2, 1).reshape(B, C, H, W)
-                x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)
-                x_ = self.norm(x_)
-                kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
+                x = x.permute(0, 2, 1).reshape(B, C, H, W)
+                x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)
+                x = self.norm(x)
+                kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
             else:
                 kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
         k, v = kv.unbind(0)
 
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        if self.fused_attn:
+            x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.attn_drop.p)
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
+        x = x.transpose(1, 2).reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class Block(nn.Module):
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., sr_ratio=1, linear_attn=False, qkv_bias=False,
-            drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            sr_ratio=1,
+            linear_attn=False,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=LayerNorm,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = Attention(
             dim,
             num_heads=num_heads,
             sr_ratio=sr_ratio,
             linear_attn=linear_attn,
             qkv_bias=qkv_bias,
             attn_drop=attn_drop,
-            proj_drop=drop,
+            proj_drop=proj_drop,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+
         self.norm2 = norm_layer(dim)
         self.mlp = MlpWithDepthwiseConv(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
             act_layer=act_layer,
-            drop=drop,
-            extra_relu=linear_attn
+            drop=proj_drop,
+            extra_relu=linear_attn,
         )
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
     def forward(self, x, feat_size: List[int]):
-        x = x + self.drop_path(self.attn(self.norm1(x), feat_size))
-        x = x + self.drop_path(self.mlp(self.norm2(x), feat_size))
+        x = x + self.drop_path1(self.attn(self.norm1(x), feat_size))
+        x = x + self.drop_path2(self.mlp(self.norm2(x), feat_size))
 
         return x
 
 
 class OverlapPatchEmbed(nn.Module):
     """ Image to Patch Embedding
     """
     def __init__(self, patch_size=7, stride=4, in_chans=3, embed_dim=768):
         super().__init__()
         patch_size = to_2tuple(patch_size)
         assert max(patch_size) > stride, "Set larger patch_size than stride"
         self.patch_size = patch_size
         self.proj = nn.Conv2d(
-            in_chans, embed_dim, kernel_size=patch_size, stride=stride,
-            padding=(patch_size[0] // 2, patch_size[1] // 2))
+            in_chans, embed_dim, patch_size,
+            stride=stride, padding=(patch_size[0] // 2, patch_size[1] // 2))
         self.norm = nn.LayerNorm(embed_dim)
 
     def forward(self, x):
         x = self.proj(x)
-        feat_size = x.shape[-2:]
-        x = x.flatten(2).transpose(1, 2)
+        x = x.permute(0, 2, 3, 1)
         x = self.norm(x)
-        return x, feat_size
+        return x
 
 
 class PyramidVisionTransformerStage(nn.Module):
     def __init__(
             self,
             dim: int,
             dim_out: int,
             depth: int,
             downsample: bool = True,
             num_heads: int = 8,
             sr_ratio: int = 1,
             linear_attn: bool = False,
             mlp_ratio: float = 4.0,
             qkv_bias: bool = True,
-            drop: float = 0.,
+            proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: Union[List[float], float] = 0.0,
-            norm_layer: Callable = nn.LayerNorm,
+            norm_layer: Callable = LayerNorm,
     ):
         super().__init__()
         self.grad_checkpointing = False
 
         if downsample:
             self.downsample = OverlapPatchEmbed(
                 patch_size=3,
                 stride=2,
                 in_chans=dim,
-                embed_dim=dim_out)
+                embed_dim=dim_out,
+            )
         else:
             assert dim == dim_out
             self.downsample = None
 
         self.blocks = nn.ModuleList([Block(
             dim=dim_out,
             num_heads=num_heads,
             sr_ratio=sr_ratio,
             linear_attn=linear_attn,
             mlp_ratio=mlp_ratio,
             qkv_bias=qkv_bias,
-            drop=drop,
+            proj_drop=proj_drop,
             attn_drop=attn_drop,
             drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
             norm_layer=norm_layer,
         ) for i in range(depth)])
 
         self.norm = norm_layer(dim_out)
 
-    def forward(self, x, feat_size: List[int]) -> Tuple[torch.Tensor, List[int]]:
+    def forward(self, x):
+        # x is either B, C, H, W (if downsample) or B, H, W, C if not
         if self.downsample is not None:
-            x, feat_size = self.downsample(x)
+            # input to downsample is B, C, H, W
+            x = self.downsample(x)  # output B, H, W, C
+        B, H, W, C = x.shape
+        feat_size = (H, W)
+        x = x.reshape(B, -1, C)
         for blk in self.blocks:
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint.checkpoint(blk, x, feat_size)
             else:
                 x = blk(x, feat_size)
         x = self.norm(x)
-        x = x.reshape(x.shape[0], feat_size[0], feat_size[1], -1).permute(0, 3, 1, 2).contiguous()
-        return x, feat_size
+        x = x.reshape(B, feat_size[0], feat_size[1], -1).permute(0, 3, 1, 2).contiguous()
+        return x
 
 
 class PyramidVisionTransformerV2(nn.Module):
     def __init__(
             self,
-            img_size=None,
             in_chans=3,
             num_classes=1000,
             global_pool='avg',
             depths=(3, 4, 6, 3),
             embed_dims=(64, 128, 256, 512),
             num_heads=(1, 2, 4, 8),
             sr_ratios=(8, 4, 2, 1),
             mlp_ratios=(8., 8., 4., 4.),
             qkv_bias=True,
             linear=False,
             drop_rate=0.,
+            proj_drop_rate=0.,
             attn_drop_rate=0.,
             drop_path_rate=0.,
-            norm_layer=nn.LayerNorm,
+            norm_layer=LayerNorm,
     ):
         super().__init__()
         self.num_classes = num_classes
         assert global_pool in ('avg', '')
         self.global_pool = global_pool
         self.depths = depths
         num_stages = len(depths)
         mlp_ratios = to_ntuple(num_stages)(mlp_ratios)
         num_heads = to_ntuple(num_stages)(num_heads)
         sr_ratios = to_ntuple(num_stages)(sr_ratios)
         assert(len(embed_dims)) == num_stages
+        self.feature_info = []
 
         self.patch_embed = OverlapPatchEmbed(
             patch_size=7,
             stride=4,
             in_chans=in_chans,
-            embed_dim=embed_dims[0])
+            embed_dim=embed_dims[0],
+        )
 
         dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]
         cur = 0
         prev_dim = embed_dims[0]
-        self.stages = nn.ModuleList()
+        stages = []
         for i in range(num_stages):
-            self.stages.append(PyramidVisionTransformerStage(
+            stages += [PyramidVisionTransformerStage(
                 dim=prev_dim,
                 dim_out=embed_dims[i],
                 depth=depths[i],
                 downsample=i > 0,
                 num_heads=num_heads[i],
                 sr_ratio=sr_ratios[i],
                 mlp_ratio=mlp_ratios[i],
                 linear_attn=linear,
                 qkv_bias=qkv_bias,
-                drop=drop_rate,
+                proj_drop=proj_drop_rate,
                 attn_drop=attn_drop_rate,
                 drop_path=dpr[i],
-                norm_layer=norm_layer
-            ))
+                norm_layer=norm_layer,
+            )]
             prev_dim = embed_dims[i]
             cur += depths[i]
+            self.feature_info += [dict(num_chs=prev_dim, reduction=4 * 2**i, module=f'stages.{i}')]
+        self.stages = nn.Sequential(*stages)
 
         # classification head
         self.num_features = embed_dims[-1]
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()
 
         self.apply(self._init_weights)
 
     def _init_weights(self, m):
         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
@@ -367,22 +383,22 @@
         self.num_classes = num_classes
         if global_pool is not None:
             assert global_pool in ('avg', '')
             self.global_pool = global_pool
         self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
     def forward_features(self, x):
-        x, feat_size = self.patch_embed(x)
-        for stage in self.stages:
-            x, feat_size = stage(x, feat_size=feat_size)
+        x = self.patch_embed(x)
+        x = self.stages(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool:
             x = x.mean(dim=(-1, -2))
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -404,73 +420,84 @@
         k = re.sub(r'block(\d+).(\d+)', lambda x: f'stages.{int(x.group(1)) - 1}.blocks.{x.group(2)}', k)
         k = re.sub(r'^norm(\d+)', lambda x: f'stages.{int(x.group(1)) - 1}.norm', k)
         out_dict[k] = v
     return out_dict
 
 
 def _create_pvt2(variant, pretrained=False, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Vision Transformer models.')
+    default_out_indices = tuple(range(4))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
     model = build_model_with_cfg(
-        PyramidVisionTransformerV2, variant, pretrained,
+        PyramidVisionTransformerV2,
+        variant,
+        pretrained,
         pretrained_filter_fn=_checkpoint_filter_fn,
-        **kwargs
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
+        **kwargs,
     )
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.9, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head', 'fixed_input_size': False,
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'pvt_v2_b0.in1k': _cfg(hf_hub_id='timm/'),
+    'pvt_v2_b1.in1k': _cfg(hf_hub_id='timm/'),
+    'pvt_v2_b2.in1k': _cfg(hf_hub_id='timm/'),
+    'pvt_v2_b3.in1k': _cfg(hf_hub_id='timm/'),
+    'pvt_v2_b4.in1k': _cfg(hf_hub_id='timm/'),
+    'pvt_v2_b5.in1k': _cfg(hf_hub_id='timm/'),
+    'pvt_v2_b2_li.in1k': _cfg(hf_hub_id='timm/'),
+})
+
+
 @register_model
-def pvt_v2_b0(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(2, 2, 2, 2), embed_dims=(32, 64, 160, 256), num_heads=(1, 2, 5, 8),
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return _create_pvt2('pvt_v2_b0', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b0(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(depths=(2, 2, 2, 2), embed_dims=(32, 64, 160, 256), num_heads=(1, 2, 5, 8))
+    return _create_pvt2('pvt_v2_b0', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pvt_v2_b1(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(2, 2, 2, 2), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return _create_pvt2('pvt_v2_b1', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b1(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(depths=(2, 2, 2, 2), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))
+    return _create_pvt2('pvt_v2_b1', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pvt_v2_b2(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(3, 4, 6, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
-        norm_layer=partial(nn.LayerNorm, eps=1e-6),  **kwargs)
-    return _create_pvt2('pvt_v2_b2', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b2(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(depths=(3, 4, 6, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))
+    return _create_pvt2('pvt_v2_b2', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pvt_v2_b3(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(3, 4, 18, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return _create_pvt2('pvt_v2_b3', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b3(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(depths=(3, 4, 18, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))
+    return _create_pvt2('pvt_v2_b3', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pvt_v2_b4(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(3, 8, 27, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return _create_pvt2('pvt_v2_b4', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b4(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(depths=(3, 8, 27, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))
+    return _create_pvt2('pvt_v2_b4', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pvt_v2_b5(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(3, 6, 40, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
-        mlp_ratios=(4, 4, 4, 4), norm_layer=partial(nn.LayerNorm, eps=1e-6),
-        **kwargs)
-    return _create_pvt2('pvt_v2_b5', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b5(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(
+        depths=(3, 6, 40, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8), mlp_ratios=(4, 4, 4, 4))
+    return _create_pvt2('pvt_v2_b5', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def pvt_v2_b2_li(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        depths=(3, 4, 6, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), linear=True, **kwargs)
-    return _create_pvt2('pvt_v2_b2_li', pretrained=pretrained, **model_kwargs)
+def pvt_v2_b2_li(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:
+    model_args = dict(
+        depths=(3, 4, 6, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8), linear=True)
+    return _create_pvt2('pvt_v2_b2_li', pretrained=pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/regnet.py` & `timm-0.9.0/timm/models/regnet.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,231 +1,143 @@
-"""RegNet
+"""RegNet X, Y, Z, and more
 
 Paper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678
 Original Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py
 
+Paper: `Fast and Accurate Model Scaling` - https://arxiv.org/abs/2103.06877
+Original Impl: None
+
 Based on original PyTorch impl linked above, but re-wrote to use my own blocks (adapted from ResNet here)
 and cleaned up with more descriptive variable names.
 
-Weights from original impl have been modified
+Weights from original pycls impl have been modified:
 * first layer from BGR -> RGB as most PyTorch models are
 * removed training specific dict entries from checkpoints and keep model state_dict only
 * remap names to match the ones here
 
+Supports weight loading from torchvision and classy-vision (incl VISSL SEER)
+
+A number of custom timm model definitions additions including:
+* stochastic depth, gradient checkpointing, layer-decay, configurable dilation
+* a pre-activation 'V' variant
+* only known RegNet-Z model definitions with pretrained weights
+
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import math
 from dataclasses import dataclass, replace
 from functools import partial
 from typing import Optional, Union, Callable
 
 import numpy as np
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, AvgPool2dSame, ConvNormAct, SEModule, DropPath, GroupNormAct
-from timm.layers import get_act_layer, get_norm_act_layer, create_conv2d
+from timm.layers import get_act_layer, get_norm_act_layer, create_conv2d, make_divisible
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq, named_apply
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['RegNet', 'RegNetCfg']  # model_registry will add each entrypoint fn to this
 
 
 @dataclass
 class RegNetCfg:
     depth: int = 21
     w0: int = 80
     wa: float = 42.63
     wm: float = 2.66
     group_size: int = 24
     bottle_ratio: float = 1.
     se_ratio: float = 0.
+    group_min_ratio: float = 0.
     stem_width: int = 32
     downsample: Optional[str] = 'conv1x1'
     linear_out: bool = False
     preact: bool = False
     num_features: int = 0
     act_layer: Union[str, Callable] = 'relu'
     norm_layer: Union[str, Callable] = 'batchnorm'
 
 
-# Model FLOPS = three trailing digits * 10^8
-model_cfgs = dict(
-    # RegNet-X
-    regnetx_002=RegNetCfg(w0=24, wa=36.44, wm=2.49, group_size=8, depth=13),
-    regnetx_004=RegNetCfg(w0=24, wa=24.48, wm=2.54, group_size=16, depth=22),
-    regnetx_006=RegNetCfg(w0=48, wa=36.97, wm=2.24, group_size=24, depth=16),
-    regnetx_008=RegNetCfg(w0=56, wa=35.73, wm=2.28, group_size=16, depth=16),
-    regnetx_016=RegNetCfg(w0=80, wa=34.01, wm=2.25, group_size=24, depth=18),
-    regnetx_032=RegNetCfg(w0=88, wa=26.31, wm=2.25, group_size=48, depth=25),
-    regnetx_040=RegNetCfg(w0=96, wa=38.65, wm=2.43, group_size=40, depth=23),
-    regnetx_064=RegNetCfg(w0=184, wa=60.83, wm=2.07, group_size=56, depth=17),
-    regnetx_080=RegNetCfg(w0=80, wa=49.56, wm=2.88, group_size=120, depth=23),
-    regnetx_120=RegNetCfg(w0=168, wa=73.36, wm=2.37, group_size=112, depth=19),
-    regnetx_160=RegNetCfg(w0=216, wa=55.59, wm=2.1, group_size=128, depth=22),
-    regnetx_320=RegNetCfg(w0=320, wa=69.86, wm=2.0, group_size=168, depth=23),
-
-    # RegNet-Y
-    regnety_002=RegNetCfg(w0=24, wa=36.44, wm=2.49, group_size=8, depth=13, se_ratio=0.25),
-    regnety_004=RegNetCfg(w0=48, wa=27.89, wm=2.09, group_size=8, depth=16, se_ratio=0.25),
-    regnety_006=RegNetCfg(w0=48, wa=32.54, wm=2.32, group_size=16, depth=15, se_ratio=0.25),
-    regnety_008=RegNetCfg(w0=56, wa=38.84, wm=2.4, group_size=16, depth=14, se_ratio=0.25),
-    regnety_016=RegNetCfg(w0=48, wa=20.71, wm=2.65, group_size=24, depth=27, se_ratio=0.25),
-    regnety_032=RegNetCfg(w0=80, wa=42.63, wm=2.66, group_size=24, depth=21, se_ratio=0.25),
-    regnety_040=RegNetCfg(w0=96, wa=31.41, wm=2.24, group_size=64, depth=22, se_ratio=0.25),
-    regnety_064=RegNetCfg(w0=112, wa=33.22, wm=2.27, group_size=72, depth=25, se_ratio=0.25),
-    regnety_080=RegNetCfg(w0=192, wa=76.82, wm=2.19, group_size=56, depth=17, se_ratio=0.25),
-    regnety_120=RegNetCfg(w0=168, wa=73.36, wm=2.37, group_size=112, depth=19, se_ratio=0.25),
-    regnety_160=RegNetCfg(w0=200, wa=106.23, wm=2.48, group_size=112, depth=18, se_ratio=0.25),
-    regnety_320=RegNetCfg(w0=232, wa=115.89, wm=2.53, group_size=232, depth=20, se_ratio=0.25),
-    regnety_640=RegNetCfg(w0=352, wa=147.48, wm=2.4, group_size=328, depth=20, se_ratio=0.25),
-    regnety_1280=RegNetCfg(w0=456, wa=160.83, wm=2.52, group_size=264, depth=27, se_ratio=0.25),
-    regnety_2560=RegNetCfg(w0=640, wa=124.47, wm=2.04, group_size=848, depth=27, se_ratio=0.25),
-
-    # Experimental
-    regnety_040s_gn=RegNetCfg(
-        w0=96, wa=31.41, wm=2.24, group_size=64, depth=22, se_ratio=0.25,
-        act_layer='silu', norm_layer=partial(GroupNormAct, group_size=16)),
-
-    # regnetv = 'preact regnet y'
-    regnetv_040=RegNetCfg(
-        depth=22, w0=96, wa=31.41, wm=2.24, group_size=64, se_ratio=0.25, preact=True, act_layer='silu'),
-    regnetv_064=RegNetCfg(
-        depth=25, w0=112, wa=33.22, wm=2.27, group_size=72, se_ratio=0.25, preact=True, act_layer='silu',
-        downsample='avg'),
-
-    # RegNet-Z (unverified)
-    regnetz_005=RegNetCfg(
-        depth=21, w0=16, wa=10.7, wm=2.51, group_size=4, bottle_ratio=4.0, se_ratio=0.25,
-        downsample=None, linear_out=True, num_features=1024, act_layer='silu',
-    ),
-    regnetz_040=RegNetCfg(
-        depth=28, w0=48, wa=14.5, wm=2.226, group_size=8, bottle_ratio=4.0, se_ratio=0.25,
-        downsample=None, linear_out=True, num_features=0, act_layer='silu',
-    ),
-    regnetz_040h=RegNetCfg(
-        depth=28, w0=48, wa=14.5, wm=2.226, group_size=8, bottle_ratio=4.0, se_ratio=0.25,
-        downsample=None, linear_out=True, num_features=1536, act_layer='silu',
-    ),
-)
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    regnetx_002=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_002-e7e85e5c.pth'),
-    regnetx_004=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_004-7d0e9424.pth'),
-    regnetx_006=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_006-85ec1baa.pth'),
-    regnetx_008=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_008-d8b470eb.pth'),
-    regnetx_016=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_016-65ca972a.pth'),
-    regnetx_032=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_032-ed0c7f7e.pth'),
-    regnetx_040=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_040-73c2a654.pth'),
-    regnetx_064=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_064-29278baa.pth'),
-    regnetx_080=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_080-7c7fcab1.pth'),
-    regnetx_120=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_120-65d5521e.pth'),
-    regnetx_160=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_160-c98c4112.pth'),
-    regnetx_320=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_320-8ea38b93.pth'),
-
-    regnety_002=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_002-e68ca334.pth'),
-    regnety_004=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_004-0db870e6.pth'),
-    regnety_006=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_006-c67e57ec.pth'),
-    regnety_008=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_008-dc900dbe.pth'),
-    regnety_016=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_016-54367f74.pth'),
-    regnety_032=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/regnety_032_ra-7f2439f9.pth',
-        crop_pct=1.0, test_input_size=(3, 288, 288)),
-    regnety_040=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_040_ra3-670e1166.pth',
-        crop_pct=1.0, test_input_size=(3, 288, 288)),
-    regnety_064=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_064_ra3-aa26dc7d.pth',
-        crop_pct=1.0, test_input_size=(3, 288, 288)),
-    regnety_080=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_080_ra3-1fdc4344.pth',
-        crop_pct=1.0, test_input_size=(3, 288, 288)),
-    regnety_120=_cfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_120-721ba79a.pth'),
-    regnety_160=_cfg(
-        url='https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth',  # from Facebook DeiT GitHub repository
-        crop_pct=1.0, test_input_size=(3, 288, 288)),
-    regnety_320=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_320-ba464b29.pth'
-    ),
-    regnety_640=_cfg(url=''),
-    regnety_1280=_cfg(url=''),
-    regnety_2560=_cfg(url=''),
-
-    regnety_040s_gn=_cfg(url=''),
-    regnetv_040=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetv_040_ra3-c248f51f.pth',
-        first_conv='stem', crop_pct=1.0, test_input_size=(3, 288, 288)),
-    regnetv_064=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetv_064_ra3-530616c2.pth',
-        first_conv='stem', crop_pct=1.0, test_input_size=(3, 288, 288)),
-
-    regnetz_005=_cfg(url=''),
-    regnetz_040=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_040_ra3-9007edf5.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320)),
-    regnetz_040h=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_040h_ra3-f594343b.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320)),
-)
-
-
 def quantize_float(f, q):
-    """Converts a float to closest non-zero int divisible by q."""
+    """Converts a float to the closest non-zero int divisible by q."""
     return int(round(f / q) * q)
 
 
-def adjust_widths_groups_comp(widths, bottle_ratios, groups):
+def adjust_widths_groups_comp(widths, bottle_ratios, groups, min_ratio=0.):
     """Adjusts the compatibility of widths and groups."""
     bottleneck_widths = [int(w * b) for w, b in zip(widths, bottle_ratios)]
     groups = [min(g, w_bot) for g, w_bot in zip(groups, bottleneck_widths)]
-    bottleneck_widths = [quantize_float(w_bot, g) for w_bot, g in zip(bottleneck_widths, groups)]
+    if min_ratio:
+        # torchvision uses a different rounding scheme for ensuring bottleneck widths divisible by group widths
+        bottleneck_widths = [make_divisible(w_bot, g, min_ratio) for w_bot, g in zip(bottleneck_widths, groups)]
+    else:
+        bottleneck_widths = [quantize_float(w_bot, g) for w_bot, g in zip(bottleneck_widths, groups)]
     widths = [int(w_bot / b) for w_bot, b in zip(bottleneck_widths, bottle_ratios)]
     return widths, groups
 
 
-def generate_regnet(width_slope, width_initial, width_mult, depth, group_size, q=8):
+def generate_regnet(width_slope, width_initial, width_mult, depth, group_size, quant=8):
     """Generates per block widths from RegNet parameters."""
-    assert width_slope >= 0 and width_initial > 0 and width_mult > 1 and width_initial % q == 0
+    assert width_slope >= 0 and width_initial > 0 and width_mult > 1 and width_initial % quant == 0
     # TODO dWr scaling?
     # depth = int(depth * (scale ** 0.1))
     # width_scale = scale ** 0.4  # dWr scale, exp 0.8 / 2, applied to both group and layer widths
     widths_cont = np.arange(depth) * width_slope + width_initial
     width_exps = np.round(np.log(widths_cont / width_initial) / np.log(width_mult))
-    widths = width_initial * np.power(width_mult, width_exps)
-    widths = np.round(np.divide(widths, q)) * q
+    widths = np.round(np.divide(width_initial * np.power(width_mult, width_exps), quant)) * quant
     num_stages, max_stage = len(np.unique(widths)), width_exps.max() + 1
     groups = np.array([group_size for _ in range(num_stages)])
     return widths.astype(int).tolist(), num_stages, groups.astype(int).tolist()
 
 
-def downsample_conv(in_chs, out_chs, kernel_size=1, stride=1, dilation=1, norm_layer=None, preact=False):
+def downsample_conv(
+        in_chs,
+        out_chs,
+        kernel_size=1,
+        stride=1,
+        dilation=1,
+        norm_layer=None,
+        preact=False,
+):
     norm_layer = norm_layer or nn.BatchNorm2d
     kernel_size = 1 if stride == 1 and dilation == 1 else kernel_size
     dilation = dilation if kernel_size > 1 else 1
     if preact:
-        return create_conv2d(in_chs, out_chs, kernel_size, stride=stride, dilation=dilation)
+        return create_conv2d(
+            in_chs,
+            out_chs,
+            kernel_size,
+            stride=stride,
+            dilation=dilation,
+        )
     else:
         return ConvNormAct(
-            in_chs, out_chs, kernel_size, stride=stride, dilation=dilation, norm_layer=norm_layer, apply_act=False)
+            in_chs,
+            out_chs,
+            kernel_size,
+            stride=stride,
+            dilation=dilation,
+            norm_layer=norm_layer,
+            apply_act=False,
+        )
 
 
-def downsample_avg(in_chs, out_chs, kernel_size=1, stride=1, dilation=1, norm_layer=None, preact=False):
+def downsample_avg(
+        in_chs,
+        out_chs,
+        kernel_size=1,
+        stride=1,
+        dilation=1,
+        norm_layer=None,
+        preact=False,
+):
     """ AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment."""
     norm_layer = norm_layer or nn.BatchNorm2d
     avg_stride = stride if dilation == 1 else 1
     pool = nn.Identity()
     if stride > 1 or dilation > 1:
         avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
         pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
@@ -286,24 +198,39 @@
         act_layer = get_act_layer(act_layer)
         bottleneck_chs = int(round(out_chs * bottle_ratio))
         groups = bottleneck_chs // group_size
 
         cargs = dict(act_layer=act_layer, norm_layer=norm_layer)
         self.conv1 = ConvNormAct(in_chs, bottleneck_chs, kernel_size=1, **cargs)
         self.conv2 = ConvNormAct(
-            bottleneck_chs, bottleneck_chs, kernel_size=3, stride=stride, dilation=dilation[0],
-            groups=groups, drop_layer=drop_block, **cargs)
+            bottleneck_chs,
+            bottleneck_chs,
+            kernel_size=3,
+            stride=stride,
+            dilation=dilation[0],
+            groups=groups,
+            drop_layer=drop_block,
+            **cargs,
+        )
         if se_ratio:
             se_channels = int(round(in_chs * se_ratio))
             self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer)
         else:
             self.se = nn.Identity()
         self.conv3 = ConvNormAct(bottleneck_chs, out_chs, kernel_size=1, apply_act=False, **cargs)
         self.act3 = nn.Identity() if linear_out else act_layer()
-        self.downsample = create_shortcut(downsample, in_chs, out_chs, 1, stride, dilation, norm_layer=norm_layer)
+        self.downsample = create_shortcut(
+            downsample,
+            in_chs,
+            out_chs,
+            kernel_size=1,
+            stride=stride,
+            dilation=dilation,
+            norm_layer=norm_layer,
+        )
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
 
     def zero_init_last(self):
         nn.init.zeros_(self.conv3.bn.weight)
 
     def forward(self, x):
         shortcut = x
@@ -347,23 +274,37 @@
         bottleneck_chs = int(round(out_chs * bottle_ratio))
         groups = bottleneck_chs // group_size
 
         self.norm1 = norm_act_layer(in_chs)
         self.conv1 = create_conv2d(in_chs, bottleneck_chs, kernel_size=1)
         self.norm2 = norm_act_layer(bottleneck_chs)
         self.conv2 = create_conv2d(
-            bottleneck_chs, bottleneck_chs, kernel_size=3, stride=stride, dilation=dilation[0], groups=groups)
+            bottleneck_chs,
+            bottleneck_chs,
+            kernel_size=3,
+            stride=stride,
+            dilation=dilation[0],
+            groups=groups,
+        )
         if se_ratio:
             se_channels = int(round(in_chs * se_ratio))
             self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer)
         else:
             self.se = nn.Identity()
         self.norm3 = norm_act_layer(bottleneck_chs)
         self.conv3 = create_conv2d(bottleneck_chs, out_chs, kernel_size=1)
-        self.downsample = create_shortcut(downsample, in_chs, out_chs, 1, stride, dilation, preact=True)
+        self.downsample = create_shortcut(
+            downsample,
+            in_chs,
+            out_chs,
+            kernel_size=1,
+            stride=stride,
+            dilation=dilation,
+            preact=True,
+        )
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
 
     def zero_init_last(self):
         pass
 
     def forward(self, x):
         x = self.norm1(x)
@@ -402,15 +343,16 @@
         for i in range(depth):
             block_stride = stride if i == 0 else 1
             block_in_chs = in_chs if i == 0 else out_chs
             block_dilation = (first_dilation, dilation)
             dpr = drop_path_rates[i] if drop_path_rates is not None else 0.
             name = "b{}".format(i + 1)
             self.add_module(
-                name, block_fn(
+                name,
+                block_fn(
                     block_in_chs,
                     out_chs,
                     stride=block_stride,
                     dilation=block_dilation,
                     drop_path_rate=dpr,
                     **block_kwargs,
                 )
@@ -473,34 +415,49 @@
             self.stem = ConvNormAct(in_chans, stem_width, 3, stride=2, **na_args)
         self.feature_info = [dict(num_chs=stem_width, reduction=2, module='stem')]
 
         # Construct the stages
         prev_width = stem_width
         curr_stride = 2
         per_stage_args, common_args = self._get_stage_args(
-            cfg, output_stride=output_stride, drop_path_rate=drop_path_rate)
+            cfg,
+            output_stride=output_stride,
+            drop_path_rate=drop_path_rate,
+        )
         assert len(per_stage_args) == 4
         block_fn = PreBottleneck if cfg.preact else Bottleneck
         for i, stage_args in enumerate(per_stage_args):
             stage_name = "s{}".format(i + 1)
-            self.add_module(stage_name, RegStage(in_chs=prev_width, block_fn=block_fn, **stage_args, **common_args))
+            self.add_module(
+                stage_name,
+                RegStage(
+                    in_chs=prev_width,
+                    block_fn=block_fn,
+                    **stage_args,
+                    **common_args,
+                )
+            )
             prev_width = stage_args['out_chs']
             curr_stride *= stage_args['stride']
             self.feature_info += [dict(num_chs=prev_width, reduction=curr_stride, module=stage_name)]
 
         # Construct the head
         if cfg.num_features:
             self.final_conv = ConvNormAct(prev_width, cfg.num_features, kernel_size=1, **na_args)
             self.num_features = cfg.num_features
         else:
             final_act = cfg.linear_out or cfg.preact
             self.final_conv = get_act_layer(cfg.act_layer)() if final_act else nn.Identity()
             self.num_features = prev_width
         self.head = ClassifierHead(
-            in_chs=self.num_features, num_classes=num_classes, pool_type=global_pool, drop_rate=drop_rate)
+            in_features=self.num_features,
+            num_classes=num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+        )
 
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
     def _get_stage_args(self, cfg: RegNetCfg, default_stride=2, output_stride=32, drop_path_rate=0.):
         # Generate RegNet ws per block
         widths, num_stages, stage_gs = generate_regnet(cfg.wa, cfg.w0, cfg.wm, cfg.depth, cfg.group_size)
 
@@ -519,19 +476,21 @@
                 stride = default_stride
                 net_stride *= stride
             stage_strides.append(stride)
             stage_dilations.append(dilation)
         stage_dpr = np.split(np.linspace(0, drop_path_rate, sum(stage_depths)), np.cumsum(stage_depths[:-1]))
 
         # Adjust the compatibility of ws and gws
-        stage_widths, stage_gs = adjust_widths_groups_comp(stage_widths, stage_br, stage_gs)
+        stage_widths, stage_gs = adjust_widths_groups_comp(
+            stage_widths, stage_br, stage_gs, min_ratio=cfg.group_min_ratio)
         arg_names = ['out_chs', 'stride', 'dilation', 'depth', 'bottle_ratio', 'group_size', 'drop_path_rates']
         per_stage_args = [
             dict(zip(arg_names, params)) for params in
-            zip(stage_widths, stage_strides, stage_dilations, stage_depths, stage_br, stage_gs, stage_dpr)]
+            zip(stage_widths, stage_strides, stage_dilations, stage_depths, stage_br, stage_gs, stage_dpr)
+        ]
         common_args = dict(
             downsample=cfg.downsample,
             se_ratio=cfg.se_ratio,
             linear_out=cfg.linear_out,
             act_layer=cfg.act_layer,
             norm_layer=cfg.norm_layer,
         )
@@ -550,15 +509,15 @@
             s.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool='avg'):
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.head.reset(num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
         x = self.s1(x)
         x = self.s2(x)
         x = self.s3(x)
         x = self.s4(x)
@@ -586,254 +545,578 @@
         if module.bias is not None:
             nn.init.zeros_(module.bias)
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
 
 def _filter_fn(state_dict):
+    state_dict = state_dict.get('model', state_dict)
+    replaces = [
+        ('f.a.0', 'conv1.conv'),
+        ('f.a.1', 'conv1.bn'),
+        ('f.b.0', 'conv2.conv'),
+        ('f.b.1', 'conv2.bn'),
+        ('f.final_bn', 'conv3.bn'),
+        ('f.se.excitation.0', 'se.fc1'),
+        ('f.se.excitation.2', 'se.fc2'),
+        ('f.se', 'se'),
+        ('f.c.0', 'conv3.conv'),
+        ('f.c.1', 'conv3.bn'),
+        ('f.c', 'conv3.conv'),
+        ('proj.0', 'downsample.conv'),
+        ('proj.1', 'downsample.bn'),
+        ('proj', 'downsample.conv'),
+    ]
     if 'classy_state_dict' in state_dict:
+        # classy-vision & vissl (SEER) weights
         import re
         state_dict = state_dict['classy_state_dict']['base_model']['model']
         out = {}
         for k, v in state_dict['trunk'].items():
             k = k.replace('_feature_blocks.conv1.stem.0', 'stem.conv')
             k = k.replace('_feature_blocks.conv1.stem.1', 'stem.bn')
             k = re.sub(
                 r'^_feature_blocks.res\d.block(\d)-(\d+)',
                 lambda x: f's{int(x.group(1))}.b{int(x.group(2)) + 1}', k)
             k = re.sub(r's(\d)\.b(\d+)\.bn', r's\1.b\2.downsample.bn', k)
-            k = k.replace('proj', 'downsample.conv')
-            k = k.replace('f.a.0', 'conv1.conv')
-            k = k.replace('f.a.1', 'conv1.bn')
-            k = k.replace('f.b.0', 'conv2.conv')
-            k = k.replace('f.b.1', 'conv2.bn')
-            k = k.replace('f.c', 'conv3.conv')
-            k = k.replace('f.final_bn', 'conv3.bn')
-            k = k.replace('f.se.excitation.0', 'se.fc1')
-            k = k.replace('f.se.excitation.2', 'se.fc2')
+            for s, r in replaces:
+                k = k.replace(s, r)
             out[k] = v
         for k, v in state_dict['heads'].items():
             if 'projection_head' in k or 'prototypes' in k:
                 continue
             k = k.replace('0.clf.0', 'head.fc')
             out[k] = v
         return out
-
-    if 'model' in state_dict:
-        # For DeiT trained regnety_160 pretraiend model
-        state_dict = state_dict['model']
+    if 'stem.0.weight' in state_dict:
+        # torchvision weights
+        import re
+        out = {}
+        for k, v in state_dict.items():
+            k = k.replace('stem.0', 'stem.conv')
+            k = k.replace('stem.1', 'stem.bn')
+            k = re.sub(
+                r'trunk_output.block(\d)\.block(\d+)\-(\d+)',
+                lambda x: f's{int(x.group(1))}.b{int(x.group(3)) + 1}', k)
+            for s, r in replaces:
+                k = k.replace(s, r)
+            k = k.replace('fc.', 'head.fc.')
+            out[k] = v
+        return out
     return state_dict
 
 
+# Model FLOPS = three trailing digits * 10^8
+model_cfgs = dict(
+    # RegNet-X
+    regnetx_002=RegNetCfg(w0=24, wa=36.44, wm=2.49, group_size=8, depth=13),
+    regnetx_004=RegNetCfg(w0=24, wa=24.48, wm=2.54, group_size=16, depth=22),
+    regnetx_004_tv=RegNetCfg(w0=24, wa=24.48, wm=2.54, group_size=16, depth=22, group_min_ratio=0.9),
+    regnetx_006=RegNetCfg(w0=48, wa=36.97, wm=2.24, group_size=24, depth=16),
+    regnetx_008=RegNetCfg(w0=56, wa=35.73, wm=2.28, group_size=16, depth=16),
+    regnetx_016=RegNetCfg(w0=80, wa=34.01, wm=2.25, group_size=24, depth=18),
+    regnetx_032=RegNetCfg(w0=88, wa=26.31, wm=2.25, group_size=48, depth=25),
+    regnetx_040=RegNetCfg(w0=96, wa=38.65, wm=2.43, group_size=40, depth=23),
+    regnetx_064=RegNetCfg(w0=184, wa=60.83, wm=2.07, group_size=56, depth=17),
+    regnetx_080=RegNetCfg(w0=80, wa=49.56, wm=2.88, group_size=120, depth=23),
+    regnetx_120=RegNetCfg(w0=168, wa=73.36, wm=2.37, group_size=112, depth=19),
+    regnetx_160=RegNetCfg(w0=216, wa=55.59, wm=2.1, group_size=128, depth=22),
+    regnetx_320=RegNetCfg(w0=320, wa=69.86, wm=2.0, group_size=168, depth=23),
+
+    # RegNet-Y
+    regnety_002=RegNetCfg(w0=24, wa=36.44, wm=2.49, group_size=8, depth=13, se_ratio=0.25),
+    regnety_004=RegNetCfg(w0=48, wa=27.89, wm=2.09, group_size=8, depth=16, se_ratio=0.25),
+    regnety_006=RegNetCfg(w0=48, wa=32.54, wm=2.32, group_size=16, depth=15, se_ratio=0.25),
+    regnety_008=RegNetCfg(w0=56, wa=38.84, wm=2.4, group_size=16, depth=14, se_ratio=0.25),
+    regnety_008_tv=RegNetCfg(w0=56, wa=38.84, wm=2.4, group_size=16, depth=14, se_ratio=0.25, group_min_ratio=0.9),
+    regnety_016=RegNetCfg(w0=48, wa=20.71, wm=2.65, group_size=24, depth=27, se_ratio=0.25),
+    regnety_032=RegNetCfg(w0=80, wa=42.63, wm=2.66, group_size=24, depth=21, se_ratio=0.25),
+    regnety_040=RegNetCfg(w0=96, wa=31.41, wm=2.24, group_size=64, depth=22, se_ratio=0.25),
+    regnety_064=RegNetCfg(w0=112, wa=33.22, wm=2.27, group_size=72, depth=25, se_ratio=0.25),
+    regnety_080=RegNetCfg(w0=192, wa=76.82, wm=2.19, group_size=56, depth=17, se_ratio=0.25),
+    regnety_080_tv=RegNetCfg(w0=192, wa=76.82, wm=2.19, group_size=56, depth=17, se_ratio=0.25, group_min_ratio=0.9),
+    regnety_120=RegNetCfg(w0=168, wa=73.36, wm=2.37, group_size=112, depth=19, se_ratio=0.25),
+    regnety_160=RegNetCfg(w0=200, wa=106.23, wm=2.48, group_size=112, depth=18, se_ratio=0.25),
+    regnety_320=RegNetCfg(w0=232, wa=115.89, wm=2.53, group_size=232, depth=20, se_ratio=0.25),
+    regnety_640=RegNetCfg(w0=352, wa=147.48, wm=2.4, group_size=328, depth=20, se_ratio=0.25),
+    regnety_1280=RegNetCfg(w0=456, wa=160.83, wm=2.52, group_size=264, depth=27, se_ratio=0.25),
+    regnety_2560=RegNetCfg(w0=640, wa=230.83, wm=2.53, group_size=373, depth=27, se_ratio=0.25),
+    #regnety_2560=RegNetCfg(w0=640, wa=124.47, wm=2.04, group_size=848, depth=27, se_ratio=0.25),
+
+    # Experimental
+    regnety_040_sgn=RegNetCfg(
+        w0=96, wa=31.41, wm=2.24, group_size=64, depth=22, se_ratio=0.25,
+        act_layer='silu', norm_layer=partial(GroupNormAct, group_size=16)),
+
+    # regnetv = 'preact regnet y'
+    regnetv_040=RegNetCfg(
+        depth=22, w0=96, wa=31.41, wm=2.24, group_size=64, se_ratio=0.25, preact=True, act_layer='silu'),
+    regnetv_064=RegNetCfg(
+        depth=25, w0=112, wa=33.22, wm=2.27, group_size=72, se_ratio=0.25, preact=True, act_layer='silu',
+        downsample='avg'),
+
+    # RegNet-Z (unverified)
+    regnetz_005=RegNetCfg(
+        depth=21, w0=16, wa=10.7, wm=2.51, group_size=4, bottle_ratio=4.0, se_ratio=0.25,
+        downsample=None, linear_out=True, num_features=1024, act_layer='silu',
+    ),
+    regnetz_040=RegNetCfg(
+        depth=28, w0=48, wa=14.5, wm=2.226, group_size=8, bottle_ratio=4.0, se_ratio=0.25,
+        downsample=None, linear_out=True, num_features=0, act_layer='silu',
+    ),
+    regnetz_040_h=RegNetCfg(
+        depth=28, w0=48, wa=14.5, wm=2.226, group_size=8, bottle_ratio=4.0, se_ratio=0.25,
+        downsample=None, linear_out=True, num_features=1536, act_layer='silu',
+    ),
+)
+
+
 def _create_regnet(variant, pretrained, **kwargs):
     return build_model_with_cfg(
         RegNet, variant, pretrained,
         model_cfg=model_cfgs[variant],
         pretrained_filter_fn=_filter_fn,
         **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'test_input_size': (3, 288, 288), 'crop_pct': 0.95, 'test_crop_pct': 1.0,
+        'interpolation': 'bicubic', 'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+def _cfgpyc(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        'license': 'mit', 'origin_url': 'https://github.com/facebookresearch/pycls', **kwargs
+    }
+
+
+def _cfgtv2(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.965, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        'license': 'bsd-3-clause', 'origin_url': 'https://github.com/pytorch/vision', **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    # timm trained models
+    'regnety_032.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/regnety_032_ra-7f2439f9.pth'),
+    'regnety_040.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_040_ra3-670e1166.pth'),
+    'regnety_064.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_064_ra3-aa26dc7d.pth'),
+    'regnety_080.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_080_ra3-1fdc4344.pth'),
+    'regnety_120.sw_in12k_ft_in1k': _cfg(hf_hub_id='timm/'),
+    'regnety_160.sw_in12k_ft_in1k': _cfg(hf_hub_id='timm/'),
+    'regnety_160.lion_in12k_ft_in1k': _cfg(hf_hub_id='timm/'),
+
+    # timm in12k pretrain
+    'regnety_120.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
+    'regnety_160.sw_in12k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=11821),
+
+    # timm custom arch (v and z guess) + trained models
+    'regnety_040_sgn.untrained': _cfg(url=''),
+    'regnetv_040.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetv_040_ra3-c248f51f.pth',
+        first_conv='stem'),
+    'regnetv_064.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetv_064_ra3-530616c2.pth',
+        first_conv='stem'),
+
+    'regnetz_005.untrained': _cfg(url=''),
+    'regnetz_040.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_040_ra3-9007edf5.pth',
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320)),
+    'regnetz_040_h.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_040h_ra3-f594343b.pth',
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320)),
+
+    # used in DeiT for distillation (from Facebook DeiT GitHub repository)
+    'regnety_160.deit_in1k': _cfg(
+        hf_hub_id='timm/', url='https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth'),
+
+    'regnetx_004_tv.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_400mf-62229a5f.pth'),
+    'regnetx_008.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_800mf-94a99ebd.pth'),
+    'regnetx_016.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_1_6gf-a12f2b72.pth'),
+    'regnetx_032.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_3_2gf-7071aa85.pth'),
+    'regnetx_080.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_8gf-2b70d774.pth'),
+    'regnetx_160.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_16gf-ba3796d7.pth'),
+    'regnetx_320.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_x_32gf-6eb8fdc6.pth'),
+
+    'regnety_004.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_400mf-e6988f5f.pth'),
+    'regnety_008_tv.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_800mf-58fc7688.pth'),
+    'regnety_016.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_1_6gf-0d7bc02a.pth'),
+    'regnety_032.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_3_2gf-9180c971.pth'),
+    'regnety_080_tv.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_8gf-dc2b1b54.pth'),
+    'regnety_160.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_16gf-3e4a00f9.pth'),
+    'regnety_320.tv2_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_32gf-8db6d4b5.pth'),
+
+    'regnety_160.swag_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_16gf_swag-43afe44d.pth', license='cc-by-nc-4.0',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'regnety_320.swag_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_32gf_swag-04fdfa75.pth', license='cc-by-nc-4.0',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'regnety_1280.swag_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_128gf_swag-c8ce3e52.pth', license='cc-by-nc-4.0',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+
+    'regnety_160.swag_lc_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_16gf_lc_swag-f3ec0043.pth', license='cc-by-nc-4.0'),
+    'regnety_320.swag_lc_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_32gf_lc_swag-e1583746.pth', license='cc-by-nc-4.0'),
+    'regnety_1280.swag_lc_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/regnet_y_128gf_lc_swag-cbe8ce12.pth', license='cc-by-nc-4.0'),
+
+    'regnety_320.seer_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        license='other', origin_url='https://github.com/facebookresearch/vissl',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet32_finetuned_in1k_model_final_checkpoint_phase78.torch',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'regnety_640.seer_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        license='other', origin_url='https://github.com/facebookresearch/vissl',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet64_finetuned_in1k_model_final_checkpoint_phase78.torch',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'regnety_1280.seer_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        license='other', origin_url='https://github.com/facebookresearch/vissl',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet128_finetuned_in1k_model_final_checkpoint_phase78.torch',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'regnety_2560.seer_ft_in1k': _cfgtv2(
+        hf_hub_id='timm/',
+        license='other', origin_url='https://github.com/facebookresearch/vissl',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet256_finetuned_in1k_model_final_checkpoint_phase38.torch',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+
+    'regnety_320.seer': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_regnet32d/seer_regnet32gf_model_iteration244000.torch',
+        num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),
+    'regnety_640.seer': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_regnet64/seer_regnet64gf_model_final_checkpoint_phase0.torch',
+        num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),
+    'regnety_1280.seer': _cfgtv2(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_ig1b_regnet128Gf_cnstant_bs32_node16_sinkhorn10_proto16k_syncBN64_warmup8k/model_final_checkpoint_phase0.torch',
+        num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),
+    # FIXME invalid weight <-> model match, mistake on their end
+    #'regnety_2560.seer': _cfgtv2(
+    #    url='https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_ig1b_cosine_rg256gf_noBNhead_wd1e5_fairstore_bs16_node64_sinkhorn10_proto16k_apex_syncBN64_warmup8k/model_final_checkpoint_phase0.torch',
+    #    num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),
+
+    'regnetx_002.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_004.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_006.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_008.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_016.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_032.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_040.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_064.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_080.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_120.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_160.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnetx_320.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+
+    'regnety_002.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_004.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_006.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_008.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_016.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_032.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_040.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_064.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_080.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_120.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_160.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+    'regnety_320.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),
+})
+
+
 @register_model
-def regnetx_002(pretrained=False, **kwargs):
+def regnetx_002(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-200MF"""
     return _create_regnet('regnetx_002', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_004(pretrained=False, **kwargs):
+def regnetx_004(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-400MF"""
     return _create_regnet('regnetx_004', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_006(pretrained=False, **kwargs):
+def regnetx_004_tv(pretrained=False, **kwargs) -> RegNet:
+    """RegNetX-400MF w/ torchvision group rounding"""
+    return _create_regnet('regnetx_004_tv', pretrained, **kwargs)
+
+
+@register_model
+def regnetx_006(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-600MF"""
     return _create_regnet('regnetx_006', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_008(pretrained=False, **kwargs):
+def regnetx_008(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-800MF"""
     return _create_regnet('regnetx_008', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_016(pretrained=False, **kwargs):
+def regnetx_016(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-1.6GF"""
     return _create_regnet('regnetx_016', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_032(pretrained=False, **kwargs):
+def regnetx_032(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-3.2GF"""
     return _create_regnet('regnetx_032', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_040(pretrained=False, **kwargs):
+def regnetx_040(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-4.0GF"""
     return _create_regnet('regnetx_040', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_064(pretrained=False, **kwargs):
+def regnetx_064(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-6.4GF"""
     return _create_regnet('regnetx_064', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_080(pretrained=False, **kwargs):
+def regnetx_080(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-8.0GF"""
     return _create_regnet('regnetx_080', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_120(pretrained=False, **kwargs):
+def regnetx_120(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-12GF"""
     return _create_regnet('regnetx_120', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_160(pretrained=False, **kwargs):
+def regnetx_160(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-16GF"""
     return _create_regnet('regnetx_160', pretrained, **kwargs)
 
 
 @register_model
-def regnetx_320(pretrained=False, **kwargs):
+def regnetx_320(pretrained=False, **kwargs) -> RegNet:
     """RegNetX-32GF"""
     return _create_regnet('regnetx_320', pretrained, **kwargs)
 
 
 @register_model
-def regnety_002(pretrained=False, **kwargs):
+def regnety_002(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-200MF"""
     return _create_regnet('regnety_002', pretrained, **kwargs)
 
 
 @register_model
-def regnety_004(pretrained=False, **kwargs):
+def regnety_004(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-400MF"""
     return _create_regnet('regnety_004', pretrained, **kwargs)
 
 
 @register_model
-def regnety_006(pretrained=False, **kwargs):
+def regnety_006(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-600MF"""
     return _create_regnet('regnety_006', pretrained, **kwargs)
 
 
 @register_model
-def regnety_008(pretrained=False, **kwargs):
+def regnety_008(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-800MF"""
     return _create_regnet('regnety_008', pretrained, **kwargs)
 
 
 @register_model
-def regnety_016(pretrained=False, **kwargs):
+def regnety_008_tv(pretrained=False, **kwargs) -> RegNet:
+    """RegNetY-800MF w/ torchvision group rounding"""
+    return _create_regnet('regnety_008_tv', pretrained, **kwargs)
+
+
+@register_model
+def regnety_016(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-1.6GF"""
     return _create_regnet('regnety_016', pretrained, **kwargs)
 
 
 @register_model
-def regnety_032(pretrained=False, **kwargs):
+def regnety_032(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-3.2GF"""
     return _create_regnet('regnety_032', pretrained, **kwargs)
 
 
 @register_model
-def regnety_040(pretrained=False, **kwargs):
+def regnety_040(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-4.0GF"""
     return _create_regnet('regnety_040', pretrained, **kwargs)
 
 
 @register_model
-def regnety_064(pretrained=False, **kwargs):
+def regnety_064(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-6.4GF"""
     return _create_regnet('regnety_064', pretrained, **kwargs)
 
 
 @register_model
-def regnety_080(pretrained=False, **kwargs):
+def regnety_080(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-8.0GF"""
     return _create_regnet('regnety_080', pretrained, **kwargs)
 
 
 @register_model
-def regnety_120(pretrained=False, **kwargs):
+def regnety_080_tv(pretrained=False, **kwargs) -> RegNet:
+    """RegNetY-8.0GF w/ torchvision group rounding"""
+    return _create_regnet('regnety_080_tv', pretrained, **kwargs)
+
+
+@register_model
+def regnety_120(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-12GF"""
     return _create_regnet('regnety_120', pretrained, **kwargs)
 
 
 @register_model
-def regnety_160(pretrained=False, **kwargs):
+def regnety_160(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-16GF"""
     return _create_regnet('regnety_160', pretrained, **kwargs)
 
 
 @register_model
-def regnety_320(pretrained=False, **kwargs):
+def regnety_320(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-32GF"""
     return _create_regnet('regnety_320', pretrained, **kwargs)
 
 
 @register_model
-def regnety_640(pretrained=False, **kwargs):
+def regnety_640(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-64GF"""
     return _create_regnet('regnety_640', pretrained, **kwargs)
 
 
 @register_model
-def regnety_1280(pretrained=False, **kwargs):
+def regnety_1280(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-128GF"""
     return _create_regnet('regnety_1280', pretrained, **kwargs)
 
 
 @register_model
-def regnety_2560(pretrained=False, **kwargs):
+def regnety_2560(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-256GF"""
     return _create_regnet('regnety_2560', pretrained, **kwargs)
 
 
 @register_model
-def regnety_040s_gn(pretrained=False, **kwargs):
+def regnety_040_sgn(pretrained=False, **kwargs) -> RegNet:
     """RegNetY-4.0GF w/ GroupNorm """
-    return _create_regnet('regnety_040s_gn', pretrained, **kwargs)
+    return _create_regnet('regnety_040_sgn', pretrained, **kwargs)
 
 
 @register_model
-def regnetv_040(pretrained=False, **kwargs):
-    """"""
+def regnetv_040(pretrained=False, **kwargs) -> RegNet:
+    """RegNetV-4.0GF (pre-activation)"""
     return _create_regnet('regnetv_040', pretrained, **kwargs)
 
 
 @register_model
-def regnetv_064(pretrained=False, **kwargs):
-    """"""
+def regnetv_064(pretrained=False, **kwargs) -> RegNet:
+    """RegNetV-6.4GF (pre-activation)"""
     return _create_regnet('regnetv_064', pretrained, **kwargs)
 
 
 @register_model
-def regnetz_005(pretrained=False, **kwargs):
+def regnetz_005(pretrained=False, **kwargs) -> RegNet:
     """RegNetZ-500MF
     NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py
     but it's not clear it is equivalent to paper model as not detailed in the paper.
     """
     return _create_regnet('regnetz_005', pretrained, zero_init_last=False, **kwargs)
 
 
 @register_model
-def regnetz_040(pretrained=False, **kwargs):
+def regnetz_040(pretrained=False, **kwargs) -> RegNet:
     """RegNetZ-4.0GF
     NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py
     but it's not clear it is equivalent to paper model as not detailed in the paper.
     """
     return _create_regnet('regnetz_040', pretrained, zero_init_last=False, **kwargs)
 
 
 @register_model
-def regnetz_040h(pretrained=False, **kwargs):
+def regnetz_040_h(pretrained=False, **kwargs) -> RegNet:
     """RegNetZ-4.0GF
     NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py
     but it's not clear it is equivalent to paper model as not detailed in the paper.
     """
-    return _create_regnet('regnetz_040h', pretrained, zero_init_last=False, **kwargs)
+    return _create_regnet('regnetz_040_h', pretrained, zero_init_last=False, **kwargs)
+
+
+register_model_deprecations(__name__, {
+    'regnetz_040h': 'regnetz_040_h',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/res2net.py` & `timm-0.9.0/timm/models/res2net.py`

 * *Files 15% similar despite different names*

```diff
@@ -5,49 +5,20 @@
 import math
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .resnet import ResNet
 
 __all__ = []
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv1', 'classifier': 'fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'res2net50_26w_4s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_4s-06e79181.pth'),
-    'res2net50_48w_2s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_48w_2s-afed724a.pth'),
-    'res2net50_14w_8s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_14w_8s-6527dddc.pth'),
-    'res2net50_26w_6s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_6s-19041792.pth'),
-    'res2net50_26w_8s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_8s-2c7c9f12.pth'),
-    'res2net101_26w_4s': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101_26w_4s-02a759a1.pth'),
-    'res2next50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2next50_4s-6ef7e7bf.pth'),
-}
-
-
 class Bottle2neck(nn.Module):
     """ Res2Net/Res2NeXT Bottleneck
     Adapted from https://github.com/gasvn/Res2Net/blob/master/res2net.py
     """
     expansion = 4
 
     def __init__(
@@ -145,82 +116,112 @@
         return out
 
 
 def _create_res2net(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv1', 'classifier': 'fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'res2net50_26w_4s.in1k': _cfg(hf_hub_id='timm/'),
+    'res2net50_48w_2s.in1k': _cfg(hf_hub_id='timm/'),
+    'res2net50_14w_8s.in1k': _cfg(hf_hub_id='timm/'),
+    'res2net50_26w_6s.in1k': _cfg(hf_hub_id='timm/'),
+    'res2net50_26w_8s.in1k': _cfg(hf_hub_id='timm/'),
+    'res2net101_26w_4s.in1k': _cfg(hf_hub_id='timm/'),
+    'res2next50.in1k': _cfg(hf_hub_id='timm/'),
+    'res2net50d.in1k': _cfg(hf_hub_id='timm/', first_conv='conv1.0'),
+    'res2net101d.in1k': _cfg(hf_hub_id='timm/', first_conv='conv1.0'),
+})
+
+
 @register_model
-def res2net50_26w_4s(pretrained=False, **kwargs):
+def res2net50_26w_4s(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Res2Net-50 26w4s model.
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, block_args=dict(scale=4))
     return _create_res2net('res2net50_26w_4s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def res2net101_26w_4s(pretrained=False, **kwargs):
+def res2net101_26w_4s(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Res2Net-101 26w4s model.
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 23, 3], base_width=26, block_args=dict(scale=4))
     return _create_res2net('res2net101_26w_4s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def res2net50_26w_6s(pretrained=False, **kwargs):
+def res2net50_26w_6s(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Res2Net-50 26w6s model.
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, block_args=dict(scale=6))
     return _create_res2net('res2net50_26w_6s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def res2net50_26w_8s(pretrained=False, **kwargs):
+def res2net50_26w_8s(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Res2Net-50 26w8s model.
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, block_args=dict(scale=8))
     return _create_res2net('res2net50_26w_8s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def res2net50_48w_2s(pretrained=False, **kwargs):
+def res2net50_48w_2s(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Res2Net-50 48w2s model.
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 6, 3], base_width=48, block_args=dict(scale=2))
     return _create_res2net('res2net50_48w_2s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def res2net50_14w_8s(pretrained=False, **kwargs):
+def res2net50_14w_8s(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Res2Net-50 14w8s model.
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 6, 3], base_width=14, block_args=dict(scale=8))
     return _create_res2net('res2net50_14w_8s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def res2next50(pretrained=False, **kwargs):
+def res2next50(pretrained=False, **kwargs) -> ResNet:
     """Construct Res2NeXt-50 4s
-    Args:
-        pretrained (bool): If True, returns a model pre-trained on ImageNet
     """
     model_args = dict(
         block=Bottle2neck, layers=[3, 4, 6, 3], base_width=4, cardinality=8, block_args=dict(scale=4))
     return _create_res2net('res2next50', pretrained, **dict(model_args, **kwargs))
+
+
+@register_model
+def res2net50d(pretrained=False, **kwargs) -> ResNet:
+    """Construct Res2Net-50
+    """
+    model_args = dict(
+        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, stem_type='deep',
+        avg_down=True, stem_width=32, block_args=dict(scale=4))
+    return _create_res2net('res2net50d', pretrained, **dict(model_args, **kwargs))
+
+
+@register_model
+def res2net101d(pretrained=False, **kwargs) -> ResNet:
+    """Construct Res2Net-50
+    """
+    model_args = dict(
+        block=Bottle2neck, layers=[3, 4, 23, 3], base_width=26, stem_type='deep',
+        avg_down=True, stem_width=32, block_args=dict(scale=4))
+    return _create_res2net('res2net101d', pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/resnest.py` & `timm-0.9.0/timm/models/resnest.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,53 +7,18 @@
 Modified for torchscript compat, and consistency with timm by Ross Wightman
 """
 from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SplitAttn
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .resnet import ResNet
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv1.0', 'classifier': 'fc',
-        **kwargs
-    }
-
-default_cfgs = {
-    'resnest14d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest14-9c8fe254.pth'),
-    'resnest26d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest26-50eb607c.pth'),
-    'resnest50d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50-528c19ca.pth'),
-    'resnest101e': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest101-22405ba7.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8)),
-    'resnest200e': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest200-75117900.pth',
-        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=0.909, interpolation='bicubic'),
-    'resnest269e': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest269-0cc87c48.pth',
-        input_size=(3, 416, 416), pool_size=(13, 13), crop_pct=0.928, interpolation='bicubic'),
-    'resnest50d_4s2x40d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_4s2x40d-41d14ed0.pth',
-        interpolation='bicubic'),
-    'resnest50d_1s4x24d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_1s4x24d-d4a4f76f.pth',
-        interpolation='bicubic')
-}
-
-
 class ResNestBottleneck(nn.Module):
     """ResNet Bottleneck
     """
     # pylint: disable=unused-argument
     expansion = 4
 
     def __init__(
@@ -149,100 +114,138 @@
 
         out += shortcut
         out = self.act3(out)
         return out
 
 
 def _create_resnest(variant, pretrained=False, **kwargs):
-    return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)
+    return build_model_with_cfg(
+        ResNet,
+        variant,
+        pretrained,
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv1.0', 'classifier': 'fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'resnest14d.gluon_in1k': _cfg(hf_hub_id='timm/'),
+    'resnest26d.gluon_in1k': _cfg(hf_hub_id='timm/'),
+    'resnest50d.in1k': _cfg(hf_hub_id='timm/'),
+    'resnest101e.in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 256, 256), pool_size=(8, 8)),
+    'resnest200e.in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=0.909, interpolation='bicubic'),
+    'resnest269e.in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 416, 416), pool_size=(13, 13), crop_pct=0.928, interpolation='bicubic'),
+    'resnest50d_4s2x40d.in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic'),
+    'resnest50d_1s4x24d.in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic')
+})
 
 
 @register_model
-def resnest14d(pretrained=False, **kwargs):
+def resnest14d(pretrained=False, **kwargs) -> ResNet:
     """ ResNeSt-14d model. Weights ported from GluonCV.
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[1, 1, 1, 1],
         stem_type='deep', stem_width=32, avg_down=True, base_width=64, cardinality=1,
         block_args=dict(radix=2, avd=True, avd_first=False))
     return _create_resnest('resnest14d', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest26d(pretrained=False, **kwargs):
+def resnest26d(pretrained=False, **kwargs) -> ResNet:
     """ ResNeSt-26d model. Weights ported from GluonCV.
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[2, 2, 2, 2],
         stem_type='deep', stem_width=32, avg_down=True, base_width=64, cardinality=1,
         block_args=dict(radix=2, avd=True, avd_first=False))
     return _create_resnest('resnest26d', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest50d(pretrained=False, **kwargs):
+def resnest50d(pretrained=False, **kwargs) -> ResNet:
     """ ResNeSt-50d model. Matches paper ResNeSt-50 model, https://arxiv.org/abs/2004.08955
     Since this codebase supports all possible variations, 'd' for deep stem, stem_width 32, avg in downsample.
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[3, 4, 6, 3],
         stem_type='deep', stem_width=32, avg_down=True, base_width=64, cardinality=1,
         block_args=dict(radix=2, avd=True, avd_first=False))
     return _create_resnest('resnest50d', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest101e(pretrained=False, **kwargs):
+def resnest101e(pretrained=False, **kwargs) -> ResNet:
     """ ResNeSt-101e model. Matches paper ResNeSt-101 model, https://arxiv.org/abs/2004.08955
      Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[3, 4, 23, 3],
         stem_type='deep', stem_width=64, avg_down=True, base_width=64, cardinality=1,
         block_args=dict(radix=2, avd=True, avd_first=False))
     return _create_resnest('resnest101e', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest200e(pretrained=False, **kwargs):
+def resnest200e(pretrained=False, **kwargs) -> ResNet:
     """ ResNeSt-200e model. Matches paper ResNeSt-200 model, https://arxiv.org/abs/2004.08955
     Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[3, 24, 36, 3],
         stem_type='deep', stem_width=64, avg_down=True, base_width=64, cardinality=1,
         block_args=dict(radix=2, avd=True, avd_first=False))
     return _create_resnest('resnest200e', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest269e(pretrained=False, **kwargs):
+def resnest269e(pretrained=False, **kwargs) -> ResNet:
     """ ResNeSt-269e model. Matches paper ResNeSt-269 model, https://arxiv.org/abs/2004.08955
     Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[3, 30, 48, 8],
         stem_type='deep', stem_width=64, avg_down=True, base_width=64, cardinality=1,
         block_args=dict(radix=2, avd=True, avd_first=False))
     return _create_resnest('resnest269e', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest50d_4s2x40d(pretrained=False, **kwargs):
+def resnest50d_4s2x40d(pretrained=False, **kwargs) -> ResNet:
     """ResNeSt-50 4s2x40d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[3, 4, 6, 3],
         stem_type='deep', stem_width=32, avg_down=True, base_width=40, cardinality=2,
         block_args=dict(radix=4, avd=True, avd_first=True))
     return _create_resnest('resnest50d_4s2x40d', pretrained=pretrained, **dict(model_kwargs, **kwargs))
 
 
 @register_model
-def resnest50d_1s4x24d(pretrained=False, **kwargs):
+def resnest50d_1s4x24d(pretrained=False, **kwargs) -> ResNet:
     """ResNeSt-50 1s4x24d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md
     """
     model_kwargs = dict(
         block=ResNestBottleneck, layers=[3, 4, 6, 3],
         stem_type='deep', stem_width=32, avg_down=True, base_width=24, cardinality=4,
         block_args=dict(radix=1, avd=True, avd_first=True))
     return _create_resnest('resnest50d_1s4x24d', pretrained=pretrained, **dict(model_kwargs, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/resnet.py` & `timm-0.9.0/timm/models/resnet.py`

 * *Files 17% similar despite different names*

```diff
@@ -15,326 +15,31 @@
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropBlock2d, DropPath, AvgPool2dSame, BlurPool2d, GroupNorm, create_attn, get_attn, \
     get_act_layer, get_norm_layer, create_classifier
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq
-from ._registry import register_model, model_entrypoint
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 
 __all__ = ['ResNet', 'BasicBlock', 'Bottleneck']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv1', 'classifier': 'fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # ResNet and Wide ResNet
-    'resnet10t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet10t_176_c3-f3215ab1.pth',
-        input_size=(3, 176, 176), pool_size=(6, 6),
-        test_crop_pct=0.95, test_input_size=(3, 224, 224),
-        first_conv='conv1.0'),
-    'resnet14t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet14t_176_c3-c4ed2c37.pth',
-        input_size=(3, 176, 176), pool_size=(6, 6),
-        test_crop_pct=0.95, test_input_size=(3, 224, 224),
-        first_conv='conv1.0'),
-    'resnet18': _cfg(url='https://download.pytorch.org/models/resnet18-5c106cde.pth'),
-    'resnet18d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet18d_ra2-48a79e06.pth',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnet34': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth'),
-    'resnet34d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34d_ra2-f8dcfcaf.pth',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnet26': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26-9aa10e23.pth',
-        interpolation='bicubic'),
-    'resnet26d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26d-69e92c46.pth',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnet26t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet26t_256_ra2-6f6fa748.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),
-    'resnet50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth',
-        interpolation='bicubic', crop_pct=0.95),
-    'resnet50d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnet50t': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnet101': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a1h-36d3f2aa.pth',
-        interpolation='bicubic', crop_pct=0.95),
-    'resnet101d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet101d_ra2-2803ffab.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
-        crop_pct=1.0, test_input_size=(3, 320, 320)),
-    'resnet152': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a1h-dc400468.pth',
-        interpolation='bicubic', crop_pct=0.95),
-    'resnet152d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet152d_ra2-5cac0439.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
-        crop_pct=1.0, test_input_size=(3, 320, 320)),
-    'resnet200': _cfg(url='', interpolation='bicubic'),
-    'resnet200d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet200d_ra2-bdba9bf9.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
-        crop_pct=1.0, test_input_size=(3, 320, 320)),
-    'tv_resnet34': _cfg(url='https://download.pytorch.org/models/resnet34-333f7ec4.pth'),
-    'tv_resnet50': _cfg(url='https://download.pytorch.org/models/resnet50-19c8e357.pth'),
-    'tv_resnet101': _cfg(url='https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'),
-    'tv_resnet152': _cfg(url='https://download.pytorch.org/models/resnet152-b121ed2d.pth'),
-    'wide_resnet50_2': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth',
-        interpolation='bicubic'),
-    'wide_resnet101_2': _cfg(url='https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth'),
-
-    # ResNets w/ alternative norm layers
-    'resnet50_gn': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_gn_a1h2-8fe6c4d0.pth',
-        crop_pct=0.94, interpolation='bicubic'),
-
-    # ResNeXt
-    'resnext50_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a1h-0146ab0a.pth',
-        interpolation='bicubic', crop_pct=0.95),
-    'resnext50d_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50d_32x4d-103e99f8.pth',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'resnext101_32x4d': _cfg(url=''),
-    'resnext101_32x8d': _cfg(url='https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth'),
-    'resnext101_64x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnext101_64x4d_c-0d0e0cc0.pth',
-        interpolation='bicubic', crop_pct=1.0,  test_input_size=(3, 288, 288)),
-    'tv_resnext50_32x4d': _cfg(url='https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth'),
-
-    #  ResNeXt models - Weakly Supervised Pretraining on Instagram Hashtags
-    #  from https://github.com/facebookresearch/WSL-Images
-    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.
-    'ig_resnext101_32x8d': _cfg(url='https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth'),
-    'ig_resnext101_32x16d': _cfg(url='https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth'),
-    'ig_resnext101_32x32d': _cfg(url='https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth'),
-    'ig_resnext101_32x48d': _cfg(url='https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth'),
-
-    #  Semi-Supervised ResNe*t models from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models
-    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.
-    'ssl_resnet18':  _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92f0530.pth'),
-    'ssl_resnet50':  _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth'),
-    'ssl_resnext50_32x4d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth'),
-    'ssl_resnext101_32x4d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth'),
-    'ssl_resnext101_32x8d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth'),
-    'ssl_resnext101_32x16d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x16-15fffa57.pth'),
-
-    #  Semi-Weakly Supervised ResNe*t models from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models
-    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.
-    'swsl_resnet18': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth'),
-    'swsl_resnet50': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet50-16a12f1b.pth'),
-    'swsl_resnext50_32x4d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext50_32x4-72679e44.pth'),
-    'swsl_resnext101_32x4d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x4-3f87e46b.pth'),
-    'swsl_resnext101_32x8d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x8-b4712904.pth'),
-    'swsl_resnext101_32x16d': _cfg(
-        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x16-f3559a9c.pth'),
-
-    #  Efficient Channel Attention ResNets
-    'ecaresnet26t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet26t_ra2-46609757.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
-        crop_pct=0.95, test_input_size=(3, 320, 320)),
-    'ecaresnetlight': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnetlight-75a9c627.pth',
-        interpolation='bicubic'),
-    'ecaresnet50d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet50d-93c81e3b.pth',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'ecaresnet50d_pruned': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet50d_p-e4fa23c2.pth',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'ecaresnet50t': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet50t_ra2-f7ac63c4.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
-        crop_pct=0.95, test_input_size=(3, 320, 320)),
-    'ecaresnet101d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet101d-153dad65.pth',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'ecaresnet101d_pruned': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet101d_p-9e74cb91.pth',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'ecaresnet200d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), crop_pct=0.94, pool_size=(8, 8)),
-    'ecaresnet269d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet269d_320_ra2-7baa55cb.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 320, 320), pool_size=(10, 10),
-        crop_pct=1.0, test_input_size=(3, 352, 352)),
-
-    #  Efficient Channel Attention ResNeXts
-    'ecaresnext26t_32x4d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'ecaresnext50t_32x4d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-
-    #  Squeeze-Excitation ResNets, to eventually replace the models in senet.py
-    'seresnet18': _cfg(
-        url='',
-        interpolation='bicubic'),
-    'seresnet34': _cfg(
-        url='',
-        interpolation='bicubic'),
-    'seresnet50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth',
-        interpolation='bicubic'),
-    'seresnet50t': _cfg(
-        url='',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'seresnet101': _cfg(
-        url='',
-        interpolation='bicubic'),
-    'seresnet152': _cfg(
-        url='',
-        interpolation='bicubic'),
-    'seresnet152d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
-        crop_pct=1.0, test_input_size=(3, 320, 320)
-    ),
-    'seresnet200d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), crop_pct=0.94, pool_size=(8, 8)),
-    'seresnet269d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0', input_size=(3, 256, 256), crop_pct=0.94, pool_size=(8, 8)),
-
-    #  Squeeze-Excitation ResNeXts, to eventually replace the models in senet.py
-    'seresnext26d_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26d_32x4d-80fa48a3.pth',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'seresnext26t_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26tn_32x4d-569cb627.pth',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-    'seresnext50_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext50_32x4d_racm-a304a460.pth',
-        interpolation='bicubic'),
-    'seresnext101_32x4d': _cfg(
-        url='',
-        interpolation='bicubic'),
-    'seresnext101_32x8d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnext101_32x8d_ah-e6bc4c0a.pth',
-        interpolation='bicubic', test_input_size=(3, 288, 288), crop_pct=1.0),
-    'seresnext101d_32x8d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnext101d_32x8d_ah-191d7b94.pth',
-        interpolation='bicubic', first_conv='conv1.0', test_input_size=(3, 288, 288), crop_pct=1.0),
-
-    'senet154': _cfg(
-        url='',
-        interpolation='bicubic',
-        first_conv='conv1.0'),
-
-    # ResNets with anti-aliasing / blur pool
-    'resnetblur18': _cfg(
-        interpolation='bicubic'),
-    'resnetblur50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnetblur50-84f4748f.pth',
-        interpolation='bicubic'),
-    'resnetblur50d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetblur101d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetaa50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnetaa50_a1h-4cf422b3.pth',
-        test_input_size=(3, 288, 288), test_crop_pct=1.0, interpolation='bicubic'),
-    'resnetaa50d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetaa101d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'seresnetaa50d': _cfg(
-        url='',
-        interpolation='bicubic', first_conv='conv1.0'),
-    'seresnextaa101d_32x8d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnextaa101d_32x8d_ah-83c8ae12.pth',
-        interpolation='bicubic', first_conv='conv1.0', test_input_size=(3, 288, 288), crop_pct=1.0),
-
-    # ResNet-RS models
-    'resnetrs50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs50_ema-6b53758b.pth',
-        input_size=(3, 160, 160), pool_size=(5, 5), crop_pct=0.91, test_input_size=(3, 224, 224),
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetrs101': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs101_i192_ema-1509bbf6.pth',
-        input_size=(3, 192, 192), pool_size=(6, 6), crop_pct=0.94, test_input_size=(3, 288, 288),
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetrs152': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs152_i256_ema-a9aff7f9.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320),
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetrs200': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnetrs200_c-6b698b88.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320),
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetrs270': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs270_ema-b40e674c.pth',
-        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 352, 352),
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetrs350': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs350_i256_ema-5a1aa8f1.pth',
-        input_size=(3, 288, 288), pool_size=(9, 9), crop_pct=1.0, test_input_size=(3, 384, 384),
-        interpolation='bicubic', first_conv='conv1.0'),
-    'resnetrs420': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs420_ema-972dee69.pth',
-        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, test_input_size=(3, 416, 416),
-        interpolation='bicubic', first_conv='conv1.0'),
-}
-
-
 def get_padding(kernel_size, stride, dilation=1):
     padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2
     return padding
 
 
 def create_aa(aa_layer, channels, stride=2, enable=True):
     if not aa_layer or not enable:
         return nn.Identity()
-    return aa_layer(stride) if issubclass(aa_layer, nn.AvgPool2d) else aa_layer(channels=channels, stride=stride)
+    if issubclass(aa_layer, nn.AvgPool2d):
+        return aa_layer(stride)
+    else:
+        return aa_layer(channels=channels, stride=stride)
 
 
 class BasicBlock(nn.Module):
     expansion = 1
 
     def __init__(
             self,
@@ -595,16 +300,22 @@
         block_kwargs = dict(reduce_first=reduce_first, dilation=dilation, drop_block=db, **kwargs)
         blocks = []
         for block_idx in range(num_blocks):
             downsample = downsample if block_idx == 0 else None
             stride = stride if block_idx == 0 else 1
             block_dpr = drop_path_rate * net_block_idx / (net_num_blocks - 1)  # stochastic depth linear decay rule
             blocks.append(block_fn(
-                inplanes, planes, stride, downsample, first_dilation=prev_dilation,
-                drop_path=DropPath(block_dpr) if block_dpr > 0. else None, **block_kwargs))
+                inplanes,
+                planes,
+                stride,
+                downsample,
+                first_dilation=prev_dilation,
+                drop_path=DropPath(block_dpr) if block_dpr > 0. else None,
+                **block_kwargs,
+            ))
             prev_dilation = dilation
             inplanes = planes * block_fn.expansion
             net_block_idx += 1
 
         stages.append((stage_name, nn.Sequential(*blocks)))
         feature_info.append(dict(num_chs=inplanes, reduction=net_stride, module=stage_name))
 
@@ -731,15 +442,15 @@
 
         # Stem pooling. The name 'maxpool' remains for weight compatibility.
         if replace_stem_pool:
             self.maxpool = nn.Sequential(*filter(None, [
                 nn.Conv2d(inplanes, inplanes, 3, stride=1 if aa_layer else 2, padding=1, bias=False),
                 create_aa(aa_layer, channels=inplanes, stride=2) if aa_layer is not None else None,
                 norm_layer(inplanes),
-                act_layer(inplace=True)
+                act_layer(inplace=True),
             ]))
         else:
             if aa_layer is not None:
                 if issubclass(aa_layer, nn.AvgPool2d):
                     self.maxpool = aa_layer(2)
                 else:
                     self.maxpool = nn.Sequential(*[
@@ -774,19 +485,14 @@
 
         # Head (Pooling and Classifier)
         self.num_features = 512 * block.expansion
         self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
 
         self.init_weights(zero_init_last=zero_init_last)
 
-    @staticmethod
-    def from_pretrained(model_name: str, load_weights=True, **kwargs) -> 'ResNet':
-        entry_fn = model_entrypoint(model_name, 'resnet')
-        return entry_fn(pretrained=not load_weights, **kwargs)
-
     @torch.jit.ignore
     def init_weights(self, zero_init_last=True):
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
         if zero_init_last:
             for m in self.modules():
@@ -837,877 +543,1434 @@
         return x
 
 
 def _create_resnet(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv1', 'classifier': 'fc',
+        **kwargs
+    }
+
+
+def _tcfg(url='', **kwargs):
+    return _cfg(url=url, **dict({'interpolation': 'bicubic'}, **kwargs))
+
+
+def _ttcfg(url='', **kwargs):
+    return _cfg(url=url, **dict({
+        'interpolation': 'bicubic', 'test_input_size': (3, 288, 288), 'test_crop_pct': 0.95,
+        'origin_url': 'https://github.com/huggingface/pytorch-image-models',
+    }, **kwargs))
+
+
+def _rcfg(url='', **kwargs):
+    return _cfg(url=url, **dict({
+        'interpolation': 'bicubic', 'crop_pct': 0.95, 'test_input_size': (3, 288, 288), 'test_crop_pct': 1.0,
+        'origin_url': 'https://github.com/huggingface/pytorch-image-models', 'paper_ids': 'arXiv:2110.00476'
+    }, **kwargs))
+
+
+def _r3cfg(url='', **kwargs):
+    return _cfg(url=url, **dict({
+        'interpolation': 'bicubic', 'input_size': (3, 160, 160), 'pool_size': (5, 5),
+        'crop_pct': 0.95, 'test_input_size': (3, 224, 224), 'test_crop_pct': 0.95,
+        'origin_url': 'https://github.com/huggingface/pytorch-image-models', 'paper_ids': 'arXiv:2110.00476',
+    }, **kwargs))
+
+
+def _gcfg(url='', **kwargs):
+    return _cfg(url=url, **dict({
+        'interpolation': 'bicubic',
+        'origin_url': 'https://cv.gluon.ai/model_zoo/classification.html',
+    }, **kwargs))
+
+
+default_cfgs = generate_default_cfgs({
+    # ResNet and Wide ResNet trained w/ timm (RSB paper and others)
+    'resnet10t.c3_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet10t_176_c3-f3215ab1.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_crop_pct=0.95, test_input_size=(3, 224, 224),
+        first_conv='conv1.0'),
+    'resnet14t.c3_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet14t_176_c3-c4ed2c37.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_crop_pct=0.95, test_input_size=(3, 224, 224),
+        first_conv='conv1.0'),
+    'resnet18.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a1_0-d63eafa0.pth'),
+    'resnet18.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a2_0-b61bd467.pth'),
+    'resnet18.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a3_0-40c531c8.pth'),
+    'resnet18d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet18d_ra2-48a79e06.pth',
+        first_conv='conv1.0'),
+    'resnet34.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet34_a1_0-46f8f793.pth'),
+    'resnet34.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet34_a2_0-82d47d71.pth'),
+    'resnet34.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet34_a3_0-a20cabb6.pth',
+        crop_pct=0.95),
+    'resnet34.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth'),
+    'resnet34d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34d_ra2-f8dcfcaf.pth',
+        first_conv='conv1.0'),
+    'resnet26.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26-9aa10e23.pth'),
+    'resnet26d.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26d-69e92c46.pth',
+        first_conv='conv1.0'),
+    'resnet26t.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet26t_256_ra2-6f6fa748.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
+        crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),
+    'resnet50.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth'),
+    'resnet50.a1h_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1h2_176-001a1197.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), crop_pct=0.9, test_input_size=(3, 224, 224), test_crop_pct=1.0),
+    'resnet50.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a2_0-a2746f79.pth'),
+    'resnet50.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a3_0-59cae1ef.pth'),
+    'resnet50.b1k_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_b1k-532a802a.pth'),
+    'resnet50.b2k_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_b2k-1ba180c1.pth'),
+    'resnet50.c1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_c1-5ba5e060.pth'),
+    'resnet50.c2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_c2-d01e05b2.pth'),
+    'resnet50.d_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_d-f39db8af.pth'),
+    'resnet50.ram_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnet50_ram-a26f946b.pth'),
+    'resnet50.am_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnet50_am-6c502b37.pth'),
+    'resnet50.ra_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnet50_ra-85ebb6e5.pth'),
+    'resnet50.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/rw_resnet50-86acaeed.pth'),
+    'resnet50d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth',
+        first_conv='conv1.0'),
+    'resnet50d.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50d_a1_0-e20cff14.pth',
+        first_conv='conv1.0'),
+    'resnet50d.a2_in1k': _rcfg(
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50d_a2_0-a3adc64d.pth',
+        first_conv='conv1.0'),
+    'resnet50d.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50d_a3_0-403fdfad.pth',
+        first_conv='conv1.0'),
+    'resnet50t.untrained': _ttcfg(first_conv='conv1.0'),
+    'resnet101.a1h_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a1h-36d3f2aa.pth'),
+    'resnet101.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a1_0-cdcb52a9.pth'),
+    'resnet101.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a2_0-6edb36c7.pth'),
+    'resnet101.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a3_0-1db14157.pth'),
+    'resnet101d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet101d_ra2-2803ffab.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,
+        test_crop_pct=1.0, test_input_size=(3, 320, 320)),
+    'resnet152.a1h_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a1h-dc400468.pth'),
+    'resnet152.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a1_0-2eee8a7a.pth'),
+    'resnet152.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a2_0-b4c6978f.pth'),
+    'resnet152.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a3_0-134d4688.pth'),
+    'resnet152d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet152d_ra2-5cac0439.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,
+        test_crop_pct=1.0, test_input_size=(3, 320, 320)),
+    'resnet200.untrained': _ttcfg(),
+    'resnet200d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet200d_ra2-bdba9bf9.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,
+        test_crop_pct=1.0, test_input_size=(3, 320, 320)),
+    'wide_resnet50_2.racm_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth'),
+
+    # torchvision resnet weights
+    'resnet18.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet18-5c106cde.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet34.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet34-333f7ec4.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet50.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet50-19c8e357.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet50.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet101.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet101.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet101-cd907fc2.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet152.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet152-b121ed2d.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnet152.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnet152-f82ba261.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'wide_resnet50_2.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'wide_resnet50_2.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'wide_resnet101_2.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'wide_resnet101_2.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/wide_resnet101_2-d733dc28.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+
+    # ResNets w/ alternative norm layers
+    'resnet50_gn.a1h_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_gn_a1h2-8fe6c4d0.pth',
+        crop_pct=0.94),
+
+    # ResNeXt trained in timm (RSB paper and others)
+    'resnext50_32x4d.a1h_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a1h-0146ab0a.pth'),
+    'resnext50_32x4d.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a1_0-b5a91a1d.pth'),
+    'resnext50_32x4d.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a2_0-efc76add.pth'),
+    'resnext50_32x4d.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a3_0-3e450271.pth'),
+    'resnext50_32x4d.ra_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnext50_32x4d_ra-d733960d.pth'),
+    'resnext50d_32x4d.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50d_32x4d-103e99f8.pth',
+        first_conv='conv1.0'),
+    'resnext101_32x4d.untrained': _ttcfg(),
+    'resnext101_64x4d.c1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnext101_64x4d_c-0d0e0cc0.pth'),
+
+    # torchvision ResNeXt weights
+    'resnext50_32x4d.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnext101_32x8d.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnext101_64x4d.tv_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnext101_64x4d-173b62eb.pth',
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnext50_32x4d.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+    'resnext101_32x8d.tv2_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/resnext101_32x8d-110c445d.pth',
+        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,
+        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),
+
+    #  ResNeXt models - Weakly Supervised Pretraining on Instagram Hashtags
+    #  from https://github.com/facebookresearch/WSL-Images
+    #  Please note the CC-BY-NC 4.0 license on these weights, non-commercial use only.
+    'resnext101_32x8d.fb_wsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),
+    'resnext101_32x16d.fb_wsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),
+    'resnext101_32x32d.fb_wsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),
+    'resnext101_32x48d.fb_wsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),
+
+    #  Semi-Supervised ResNe*t models from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models
+    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.
+    'resnet18.fb_ssl_yfcc100m_ft_in1k':  _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92f0530.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnet50.fb_ssl_yfcc100m_ft_in1k':  _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x16-15fffa57.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+
+    #  Semi-Weakly Supervised ResNe*t models from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models
+    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.
+    'resnet18.fb_swsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnet50.fb_swsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet50-16a12f1b.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext50_32x4d.fb_swsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext50_32x4-72679e44.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext101_32x4d.fb_swsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x4-3f87e46b.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext101_32x8d.fb_swsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x8-b4712904.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+    'resnext101_32x16d.fb_swsl_ig1b_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x16-f3559a9c.pth',
+        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),
+
+    #  Efficient Channel Attention ResNets
+    'ecaresnet26t.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet26t_ra2-46609757.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
+        test_crop_pct=0.95, test_input_size=(3, 320, 320)),
+    'ecaresnetlight.miil_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnetlight-75a9c627.pth',
+        test_crop_pct=0.95, test_input_size=(3, 288, 288)),
+    'ecaresnet50d.miil_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet50d-93c81e3b.pth',
+        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),
+    'ecaresnet50d_pruned.miil_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet50d_p-e4fa23c2.pth',
+        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),
+    'ecaresnet50t.ra2_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet50t_ra2-f7ac63c4.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),
+        test_crop_pct=0.95, test_input_size=(3, 320, 320)),
+    'ecaresnet50t.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/ecaresnet50t_a1_0-99bd76a8.pth',
+        first_conv='conv1.0'),
+    'ecaresnet50t.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/ecaresnet50t_a2_0-b1c7b745.pth',
+        first_conv='conv1.0'),
+    'ecaresnet50t.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/ecaresnet50t_a3_0-8cc311f1.pth',
+        first_conv='conv1.0'),
+    'ecaresnet101d.miil_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet101d-153dad65.pth',
+        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),
+    'ecaresnet101d_pruned.miil_in1k': _tcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet101d_p-9e74cb91.pth',
+        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),
+    'ecaresnet200d.untrained': _ttcfg(
+        first_conv='conv1.0', input_size=(3, 256, 256), crop_pct=0.95, pool_size=(8, 8)),
+    'ecaresnet269d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet269d_320_ra2-7baa55cb.pth',
+        first_conv='conv1.0', input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=0.95,
+        test_crop_pct=1.0, test_input_size=(3, 352, 352)),
+
+    #  Efficient Channel Attention ResNeXts
+    'ecaresnext26t_32x4d.untrained': _tcfg(first_conv='conv1.0'),
+    'ecaresnext50t_32x4d.untrained': _tcfg(first_conv='conv1.0'),
+
+    #  Squeeze-Excitation ResNets, to eventually replace the models in senet.py
+    'seresnet18.untrained': _ttcfg(),
+    'seresnet34.untrained': _ttcfg(),
+    'seresnet50.a1_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/seresnet50_a1_0-ffa00869.pth',
+        crop_pct=0.95),
+    'seresnet50.a2_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/seresnet50_a2_0-850de0d9.pth',
+        crop_pct=0.95),
+    'seresnet50.a3_in1k': _r3cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/seresnet50_a3_0-317ecd56.pth',
+        crop_pct=0.95),
+    'seresnet50.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth'),
+    'seresnet50t.untrained': _ttcfg(
+        first_conv='conv1.0'),
+    'seresnet101.untrained': _ttcfg(),
+    'seresnet152.untrained': _ttcfg(),
+    'seresnet152d.ra2_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth',
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,
+        test_crop_pct=1.0, test_input_size=(3, 320, 320)
+    ),
+    'seresnet200d.untrained': _ttcfg(
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8)),
+    'seresnet269d.untrained': _ttcfg(
+        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8)),
+
+    #  Squeeze-Excitation ResNeXts, to eventually replace the models in senet.py
+    'seresnext26d_32x4d.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26d_32x4d-80fa48a3.pth',
+        first_conv='conv1.0'),
+    'seresnext26t_32x4d.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26tn_32x4d-569cb627.pth',
+        first_conv='conv1.0'),
+    'seresnext50_32x4d.racm_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext50_32x4d_racm-a304a460.pth'),
+    'seresnext101_32x4d.untrained': _ttcfg(),
+    'seresnext101_32x8d.ah_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnext101_32x8d_ah-e6bc4c0a.pth'),
+    'seresnext101d_32x8d.ah_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnext101d_32x8d_ah-191d7b94.pth',
+        first_conv='conv1.0'),
+
+    # ResNets with anti-aliasing / blur pool
+    'resnetaa50d.sw_in12k_ft_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),
+    'resnetaa101d.sw_in12k_ft_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),
+    'seresnextaa101d_32x8d.sw_in12k_ft_in1k_288': _ttcfg(
+        hf_hub_id='timm/',
+        crop_pct=0.95, input_size=(3, 288, 288), pool_size=(9, 9), test_input_size=(3, 320, 320), test_crop_pct=1.0,
+        first_conv='conv1.0'),
+    'seresnextaa101d_32x8d.sw_in12k_ft_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        first_conv='conv1.0', test_crop_pct=1.0),
+
+    'resnetaa50d.sw_in12k': _ttcfg(
+        hf_hub_id='timm/',
+        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),
+    'resnetaa50d.d_in12k': _ttcfg(
+        hf_hub_id='timm/',
+        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),
+    'resnetaa101d.sw_in12k': _ttcfg(
+        hf_hub_id='timm/',
+        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),
+    'seresnextaa101d_32x8d.sw_in12k': _ttcfg(
+        hf_hub_id='timm/',
+        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),
+
+    'resnetblur18.untrained': _ttcfg(),
+    'resnetblur50.bt_in1k': _ttcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnetblur50-84f4748f.pth'),
+    'resnetblur50d.untrained': _ttcfg(first_conv='conv1.0'),
+    'resnetblur101d.untrained': _ttcfg(first_conv='conv1.0'),
+    'resnetaa50.a1h_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnetaa50_a1h-4cf422b3.pth'),
+
+    'seresnetaa50d.untrained': _ttcfg(first_conv='conv1.0'),
+    'seresnextaa101d_32x8d.ah_in1k': _rcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnextaa101d_32x8d_ah-83c8ae12.pth',
+        first_conv='conv1.0'),
+
+    # ResNet-RS models
+    'resnetrs50.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs50_ema-6b53758b.pth',
+        input_size=(3, 160, 160), pool_size=(5, 5), crop_pct=0.91, test_input_size=(3, 224, 224),
+        interpolation='bicubic', first_conv='conv1.0'),
+    'resnetrs101.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs101_i192_ema-1509bbf6.pth',
+        input_size=(3, 192, 192), pool_size=(6, 6), crop_pct=0.94, test_input_size=(3, 288, 288),
+        interpolation='bicubic', first_conv='conv1.0'),
+    'resnetrs152.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs152_i256_ema-a9aff7f9.pth',
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320),
+        interpolation='bicubic', first_conv='conv1.0'),
+    'resnetrs200.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnetrs200_c-6b698b88.pth',
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320),
+        interpolation='bicubic', first_conv='conv1.0'),
+    'resnetrs270.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs270_ema-b40e674c.pth',
+        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 352, 352),
+        interpolation='bicubic', first_conv='conv1.0'),
+    'resnetrs350.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs350_i256_ema-5a1aa8f1.pth',
+        input_size=(3, 288, 288), pool_size=(9, 9), crop_pct=1.0, test_input_size=(3, 384, 384),
+        interpolation='bicubic', first_conv='conv1.0'),
+    'resnetrs420.tf_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs420_ema-972dee69.pth',
+        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, test_input_size=(3, 416, 416),
+        interpolation='bicubic', first_conv='conv1.0'),
+
+    # gluon resnet weights
+    'resnet18.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet18_v1b-0757602b.pth'),
+    'resnet34.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet34_v1b-c6d82d59.pth'),
+    'resnet50.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1b-0ebe02e2.pth'),
+    'resnet101.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1b-3b017079.pth'),
+    'resnet152.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1b-c1edb0dd.pth'),
+    'resnet50c.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1c-48092f55.pth',
+        first_conv='conv1.0'),
+    'resnet101c.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1c-1f26822a.pth',
+        first_conv='conv1.0'),
+    'resnet152c.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1c-a3bb0b98.pth',
+        first_conv='conv1.0'),
+    'resnet50d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1d-818a1b1b.pth',
+        first_conv='conv1.0'),
+    'resnet101d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1d-0f9c8644.pth',
+        first_conv='conv1.0'),
+    'resnet152d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1d-bd354e12.pth',
+        first_conv='conv1.0'),
+    'resnet50s.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1s-1762acc0.pth',
+        first_conv='conv1.0'),
+    'resnet101s.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1s-60fe0cc1.pth',
+        first_conv='conv1.0'),
+    'resnet152s.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1s-dcc41b81.pth',
+        first_conv='conv1.0'),
+    'resnext50_32x4d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext50_32x4d-e6a097c1.pth'),
+    'resnext101_32x4d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_32x4d-b253c8c4.pth'),
+    'resnext101_64x4d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_64x4d-f9a8e184.pth'),
+    'seresnext50_32x4d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext50_32x4d-90cf2d6e.pth'),
+    'seresnext101_32x4d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext101_32x4d-cf52900d.pth'),
+    'seresnext101_64x4d.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext101_64x4d-f9926f93.pth'),
+    'senet154.gluon_in1k': _gcfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_senet154-70a1a3c0.pth',
+        first_conv='conv1.0'),
+})
+
+
 @register_model
-def resnet10t(pretrained=False, **kwargs):
+def resnet10t(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-10-T model.
     """
     model_args = dict(block=BasicBlock, layers=[1, 1, 1, 1], stem_width=32, stem_type='deep_tiered', avg_down=True)
     return _create_resnet('resnet10t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet14t(pretrained=False, **kwargs):
+def resnet14t(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-14-T model.
     """
     model_args = dict(block=Bottleneck, layers=[1, 1, 1, 1], stem_width=32, stem_type='deep_tiered', avg_down=True)
     return _create_resnet('resnet14t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet18(pretrained=False, **kwargs):
+def resnet18(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-18 model.
     """
     model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2])
     return _create_resnet('resnet18', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet18d(pretrained=False, **kwargs):
+def resnet18d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-18-D model.
     """
     model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnet18d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet34(pretrained=False, **kwargs):
+def resnet34(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-34 model.
     """
     model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3])
     return _create_resnet('resnet34', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet34d(pretrained=False, **kwargs):
+def resnet34d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-34-D model.
     """
     model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnet34d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet26(pretrained=False, **kwargs):
+def resnet26(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-26 model.
     """
     model_args = dict(block=Bottleneck, layers=[2, 2, 2, 2])
     return _create_resnet('resnet26', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet26t(pretrained=False, **kwargs):
+def resnet26t(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-26-T model.
     """
     model_args = dict(block=Bottleneck, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep_tiered', avg_down=True)
     return _create_resnet('resnet26t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet26d(pretrained=False, **kwargs):
+def resnet26d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-26-D model.
     """
     model_args = dict(block=Bottleneck, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnet26d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet50(pretrained=False, **kwargs):
+def resnet50(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50 model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)
     return _create_resnet('resnet50', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
+def resnet50c(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-50-C model.
+    """
+    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep')
+    return _create_resnet('resnet50c', pretrained, **dict(model_args, **kwargs))
+
+
+@register_model
 def resnet50d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-D model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnet50d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet50t(pretrained=False, **kwargs):
+def resnet50s(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-50-S model.
+    """
+    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=64, stem_type='deep')
+    return _create_resnet('resnet50s', pretrained, **dict(model_args, **kwargs))
+
+
+@register_model
+def resnet50t(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-T model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep_tiered', avg_down=True)
     return _create_resnet('resnet50t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet101(pretrained=False, **kwargs):
+def resnet101(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-101 model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3])
     return _create_resnet('resnet101', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet101d(pretrained=False, **kwargs):
-    """Constructs a ResNet-101-D model.
+def resnet101c(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-101-C model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True)
-    return _create_resnet('resnet101d', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep')
+    return _create_resnet('resnet101c', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet152(pretrained=False, **kwargs):
-    """Constructs a ResNet-152 model.
+def resnet101d(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-101-D model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3])
-    return _create_resnet('resnet152', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True)
+    return _create_resnet('resnet101d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet152d(pretrained=False, **kwargs):
-    """Constructs a ResNet-152-D model.
+def resnet101s(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-101-S model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep', avg_down=True)
-    return _create_resnet('resnet152d', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=64, stem_type='deep')
+    return _create_resnet('resnet101s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet200(pretrained=False, **kwargs):
-    """Constructs a ResNet-200 model.
+def resnet152(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-152 model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 24, 36, 3])
-    return _create_resnet('resnet200', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3])
+    return _create_resnet('resnet152', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet200d(pretrained=False, **kwargs):
-    """Constructs a ResNet-200-D model.
+def resnet152c(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-152-C model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', avg_down=True)
-    return _create_resnet('resnet200d', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep')
+    return _create_resnet('resnet152c', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def tv_resnet34(pretrained=False, **kwargs):
-    """Constructs a ResNet-34 model with original Torchvision weights.
+def resnet152d(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-152-D model.
     """
-    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3])
-    return _create_resnet('tv_resnet34', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep', avg_down=True)
+    return _create_resnet('resnet152d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def tv_resnet50(pretrained=False, **kwargs):
-    """Constructs a ResNet-50 model with original Torchvision weights.
+def resnet152s(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-152-S model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)
-    return _create_resnet('tv_resnet50', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=64, stem_type='deep')
+    return _create_resnet('resnet152s', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def tv_resnet101(pretrained=False, **kwargs):
-    """Constructs a ResNet-101 model w/ Torchvision pretrained weights.
+def resnet200(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-200 model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3])
-    return _create_resnet('tv_resnet101', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 24, 36, 3])
+    return _create_resnet('resnet200', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def tv_resnet152(pretrained=False, **kwargs):
-    """Constructs a ResNet-152 model w/ Torchvision pretrained weights.
+def resnet200d(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNet-200-D model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3])
-    return _create_resnet('tv_resnet152', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', avg_down=True)
+    return _create_resnet('resnet200d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def wide_resnet50_2(pretrained=False, **kwargs):
+def wide_resnet50_2(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Wide ResNet-50-2 model.
     The model is the same as ResNet except for the bottleneck number of channels
     which is twice larger in every block. The number of channels in outer 1x1
     convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
     channels, and in Wide ResNet-50-2 has 2048-1024-2048.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], base_width=128)
     return _create_resnet('wide_resnet50_2', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def wide_resnet101_2(pretrained=False, **kwargs):
+def wide_resnet101_2(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Wide ResNet-101-2 model.
     The model is the same as ResNet except for the bottleneck number of channels
     which is twice larger in every block. The number of channels in outer 1x1
     convolutions is the same.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], base_width=128)
     return _create_resnet('wide_resnet101_2', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnet50_gn(pretrained=False, **kwargs):
+def resnet50_gn(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50 model w/ GroupNorm
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)
     return _create_resnet('resnet50_gn', pretrained, norm_layer=GroupNorm, **model_args)
 
 
 @register_model
-def resnext50_32x4d(pretrained=False, **kwargs):
+def resnext50_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNeXt50-32x4d model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4)
     return _create_resnet('resnext50_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnext50d_32x4d(pretrained=False, **kwargs):
+def resnext50d_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3],  cardinality=32, base_width=4,
         stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnext50d_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnext101_32x4d(pretrained=False, **kwargs):
+def resnext101_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNeXt-101 32x4d model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=4)
     return _create_resnet('resnext101_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnext101_32x8d(pretrained=False, **kwargs):
+def resnext101_32x8d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNeXt-101 32x8d model.
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8)
     return _create_resnet('resnext101_32x8d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnext101_64x4d(pretrained=False, **kwargs):
-    """Constructs a ResNeXt101-64x4d model.
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=64, base_width=4)
-    return _create_resnet('resnext101_64x4d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def tv_resnext50_32x4d(pretrained=False, **kwargs):
-    """Constructs a ResNeXt50-32x4d model with original Torchvision weights.
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4)
-    return _create_resnet('tv_resnext50_32x4d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ig_resnext101_32x8d(pretrained=False, **kwargs):
-    """Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data
-    and finetuned on ImageNet from Figure 5 in
-    `"Exploring the Limits of Weakly Supervised Pretraining" <https://arxiv.org/abs/1805.00932>`_
-    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8)
-    return _create_resnet('ig_resnext101_32x8d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ig_resnext101_32x16d(pretrained=False, **kwargs):
-    """Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data
-    and finetuned on ImageNet from Figure 5 in
-    `"Exploring the Limits of Weakly Supervised Pretraining" <https://arxiv.org/abs/1805.00932>`_
-    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/
+def resnext101_32x16d(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNeXt-101 32x16d model
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=16)
-    return _create_resnet('ig_resnext101_32x16d', pretrained, **dict(model_args, **kwargs))
+    return _create_resnet('resnext101_32x16d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ig_resnext101_32x32d(pretrained=False, **kwargs):
-    """Constructs a ResNeXt-101 32x32 model pre-trained on weakly-supervised data
-    and finetuned on ImageNet from Figure 5 in
-    `"Exploring the Limits of Weakly Supervised Pretraining" <https://arxiv.org/abs/1805.00932>`_
-    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/
+def resnext101_32x32d(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNeXt-101 32x32d model
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=32)
-    return _create_resnet('ig_resnext101_32x32d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ig_resnext101_32x48d(pretrained=False, **kwargs):
-    """Constructs a ResNeXt-101 32x48 model pre-trained on weakly-supervised data
-    and finetuned on ImageNet from Figure 5 in
-    `"Exploring the Limits of Weakly Supervised Pretraining" <https://arxiv.org/abs/1805.00932>`_
-    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=48)
-    return _create_resnet('ig_resnext101_32x48d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ssl_resnet18(pretrained=False, **kwargs):
-    """Constructs a semi-supervised ResNet-18 model pre-trained on YFCC100M dataset and finetuned on ImageNet
-    `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2])
-    return _create_resnet('ssl_resnet18', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ssl_resnet50(pretrained=False, **kwargs):
-    """Constructs a semi-supervised ResNet-50 model pre-trained on YFCC100M dataset and finetuned on ImageNet
-    `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)
-    return _create_resnet('ssl_resnet50', pretrained, **dict(model_args, **kwargs))
+    return _create_resnet('resnext101_32x32d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ssl_resnext50_32x4d(pretrained=False, **kwargs):
-    """Constructs a semi-supervised ResNeXt-50 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet
-    `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4)
-    return _create_resnet('ssl_resnext50_32x4d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ssl_resnext101_32x4d(pretrained=False, **kwargs):
-    """Constructs a semi-supervised ResNeXt-101 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet
-    `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=4)
-    return _create_resnet('ssl_resnext101_32x4d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ssl_resnext101_32x8d(pretrained=False, **kwargs):
-    """Constructs a semi-supervised ResNeXt-101 32x8 model pre-trained on YFCC100M dataset and finetuned on ImageNet
-    `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8)
-    return _create_resnet('ssl_resnext101_32x8d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def ssl_resnext101_32x16d(pretrained=False, **kwargs):
-    """Constructs a semi-supervised ResNeXt-101 32x16 model pre-trained on YFCC100M dataset and finetuned on ImageNet
-    `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=16)
-    return _create_resnet('ssl_resnext101_32x16d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def swsl_resnet18(pretrained=False, **kwargs):
-    """Constructs a semi-weakly supervised Resnet-18 model pre-trained on 1B weakly supervised
-       image dataset and finetuned on ImageNet.
-       `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2])
-    return _create_resnet('swsl_resnet18', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def swsl_resnet50(pretrained=False, **kwargs):
-    """Constructs a semi-weakly supervised ResNet-50 model pre-trained on 1B weakly supervised
-       image dataset and finetuned on ImageNet.
-       `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)
-    return _create_resnet('swsl_resnet50', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def swsl_resnext50_32x4d(pretrained=False, **kwargs):
-    """Constructs a semi-weakly supervised ResNeXt-50 32x4 model pre-trained on 1B weakly supervised
-       image dataset and finetuned on ImageNet.
-       `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4)
-    return _create_resnet('swsl_resnext50_32x4d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def swsl_resnext101_32x4d(pretrained=False, **kwargs):
-    """Constructs a semi-weakly supervised ResNeXt-101 32x4 model pre-trained on 1B weakly supervised
-       image dataset and finetuned on ImageNet.
-       `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=4)
-    return _create_resnet('swsl_resnext101_32x4d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def swsl_resnext101_32x8d(pretrained=False, **kwargs):
-    """Constructs a semi-weakly supervised ResNeXt-101 32x8 model pre-trained on 1B weakly supervised
-       image dataset and finetuned on ImageNet.
-       `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
-    """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8)
-    return _create_resnet('swsl_resnext101_32x8d', pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def swsl_resnext101_32x16d(pretrained=False, **kwargs):
-    """Constructs a semi-weakly supervised ResNeXt-101 32x16 model pre-trained on 1B weakly supervised
-       image dataset and finetuned on ImageNet.
-       `"Billion-scale Semi-Supervised Learning for Image Classification" <https://arxiv.org/abs/1905.00546>`_
-       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/
+def resnext101_64x4d(pretrained=False, **kwargs) -> ResNet:
+    """Constructs a ResNeXt101-64x4d model.
     """
-    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=16)
-    return _create_resnet('swsl_resnext101_32x16d', pretrained, **dict(model_args, **kwargs))
+    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=64, base_width=4)
+    return _create_resnet('resnext101_64x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet26t(pretrained=False, **kwargs):
+def ecaresnet26t(pretrained=False, **kwargs) -> ResNet:
     """Constructs an ECA-ResNeXt-26-T model.
     This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels
     in the deep stem and ECA attn.
     """
     model_args = dict(
         block=Bottleneck, layers=[2, 2, 2, 2], stem_width=32,
         stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet26t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet50d(pretrained=False, **kwargs):
+def ecaresnet50d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-D model with eca.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet50d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet50d_pruned(pretrained=False, **kwargs):
+def ecaresnet50d_pruned(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-D model pruned with eca.
         The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet50d_pruned', pretrained, pruned=True, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet50t(pretrained=False, **kwargs):
+def ecaresnet50t(pretrained=False, **kwargs) -> ResNet:
     """Constructs an ECA-ResNet-50-T model.
     Like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32,
         stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet50t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnetlight(pretrained=False, **kwargs):
+def ecaresnetlight(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-D light model with eca.
     """
     model_args = dict(
         block=Bottleneck, layers=[1, 1, 11, 3], stem_width=32, avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnetlight', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet101d(pretrained=False, **kwargs):
+def ecaresnet101d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-101-D model with eca.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet101d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet101d_pruned(pretrained=False, **kwargs):
+def ecaresnet101d_pruned(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-101-D model pruned with eca.
        The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet101d_pruned', pretrained, pruned=True, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet200d(pretrained=False, **kwargs):
+def ecaresnet200d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-200-D model with ECA.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet200d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnet269d(pretrained=False, **kwargs):
+def ecaresnet269d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-269-D model with ECA.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 30, 48, 8], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnet269d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnext26t_32x4d(pretrained=False, **kwargs):
+def ecaresnext26t_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs an ECA-ResNeXt-26-T model.
     This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels
     in the deep stem. This model replaces SE module with the ECA module
     """
     model_args = dict(
         block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,
         stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnext26t_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def ecaresnext50t_32x4d(pretrained=False, **kwargs):
+def ecaresnext50t_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs an ECA-ResNeXt-50-T model.
     This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels
     in the deep stem. This model replaces SE module with the ECA module
     """
     model_args = dict(
         block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,
         stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))
     return _create_resnet('ecaresnext50t_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet18(pretrained=False, **kwargs):
+def seresnet18(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet18', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet34(pretrained=False, **kwargs):
+def seresnet34(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet34', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet50(pretrained=False, **kwargs):
+def seresnet50(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet50', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet50t(pretrained=False, **kwargs):
+def seresnet50t(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3],  stem_width=32, stem_type='deep_tiered',
         avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet50t', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet101(pretrained=False, **kwargs):
+def seresnet101(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet101', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet152(pretrained=False, **kwargs):
+def seresnet152(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet152', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet152d(pretrained=False, **kwargs):
+def seresnet152d(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep',
         avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet152d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet200d(pretrained=False, **kwargs):
+def seresnet200d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-200-D model with SE attn.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep',
         avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet200d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnet269d(pretrained=False, **kwargs):
+def seresnet269d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-269-D model with SE attn.
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 30, 48, 8], stem_width=32, stem_type='deep',
         avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnet269d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnext26d_32x4d(pretrained=False, **kwargs):
+def seresnext26d_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a SE-ResNeXt-26-D model.`
     This is technically a 28 layer ResNet, using the 'D' modifier from Gluon / bag-of-tricks for
     combination of deep stem and avg_pool in downsample.
     """
     model_args = dict(
         block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,
         stem_type='deep', avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnext26d_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnext26t_32x4d(pretrained=False, **kwargs):
+def seresnext26t_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a SE-ResNet-26-T model.
     This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels
     in the deep stem.
     """
     model_args = dict(
         block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,
         stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnext26t_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnext26tn_32x4d(pretrained=False, **kwargs):
+def seresnext26tn_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a SE-ResNeXt-26-T model.
     NOTE I deprecated previous 't' model defs and replaced 't' with 'tn', this was the only tn model of note
     so keeping this def for backwards compat with any uses out there. Old 't' model is lost.
     """
     return seresnext26t_32x4d(pretrained=pretrained, **kwargs)
 
 
 @register_model
-def seresnext50_32x4d(pretrained=False, **kwargs):
+def seresnext50_32x4d(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4,
         block_args=dict(attn_layer='se'))
     return _create_resnet('seresnext50_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnext101_32x4d(pretrained=False, **kwargs):
+def seresnext101_32x4d(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=4,
         block_args=dict(attn_layer='se'))
     return _create_resnet('seresnext101_32x4d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnext101_32x8d(pretrained=False, **kwargs):
+def seresnext101_32x8d(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8,
         block_args=dict(attn_layer='se'))
     return _create_resnet('seresnext101_32x8d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnext101d_32x8d(pretrained=False, **kwargs):
+def seresnext101d_32x8d(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8,
         stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(attn_layer='se'))
     return _create_resnet('seresnext101d_32x8d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def senet154(pretrained=False, **kwargs):
+def seresnext101_64x4d(pretrained=False, **kwargs) -> ResNet:
+    model_args = dict(
+        block=Bottleneck, layers=[3, 4, 23, 3], cardinality=64, base_width=4,
+        block_args=dict(attn_layer='se'))
+    return _create_resnet('seresnext101_64x4d', pretrained, **dict(model_args, **kwargs))
+
+
+@register_model
+def senet154(pretrained=False, **kwargs) -> ResNet:
     model_args = dict(
         block=Bottleneck, layers=[3, 8, 36, 3], cardinality=64, base_width=4, stem_type='deep',
         down_kernel_size=3, block_reduce_first=2, block_args=dict(attn_layer='se'))
     return _create_resnet('senet154', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetblur18(pretrained=False, **kwargs):
+def resnetblur18(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-18 model with blur anti-aliasing
     """
     model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], aa_layer=BlurPool2d)
     return _create_resnet('resnetblur18', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetblur50(pretrained=False, **kwargs):
+def resnetblur50(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50 model with blur anti-aliasing
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=BlurPool2d)
     return _create_resnet('resnetblur50', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetblur50d(pretrained=False, **kwargs):
+def resnetblur50d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-D model with blur anti-aliasing
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=BlurPool2d,
         stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnetblur50d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetblur101d(pretrained=False, **kwargs):
+def resnetblur101d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-101-D model with blur anti-aliasing
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], aa_layer=BlurPool2d,
         stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnetblur101d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetaa34d(pretrained=False, **kwargs):
+def resnetaa34d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-34-D model w/ avgpool anti-aliasing
     """
     model_args = dict(
         block=BasicBlock, layers=[3, 4, 6, 3],  aa_layer=nn.AvgPool2d, stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnetaa34d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetaa50(pretrained=False, **kwargs):
+def resnetaa50(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50 model with avgpool anti-aliasing
     """
     model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=nn.AvgPool2d)
     return _create_resnet('resnetaa50', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetaa50d(pretrained=False, **kwargs):
+def resnetaa50d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-50-D model with avgpool anti-aliasing
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=nn.AvgPool2d,
         stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnetaa50d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetaa101d(pretrained=False, **kwargs):
+def resnetaa101d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-101-D model with avgpool anti-aliasing
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], aa_layer=nn.AvgPool2d,
         stem_width=32, stem_type='deep', avg_down=True)
     return _create_resnet('resnetaa101d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnetaa50d(pretrained=False, **kwargs):
+def seresnetaa50d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a SE=ResNet-50-D model with avgpool anti-aliasing
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=nn.AvgPool2d,
         stem_width=32, stem_type='deep', avg_down=True, block_args=dict(attn_layer='se'))
     return _create_resnet('seresnetaa50d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def seresnextaa101d_32x8d(pretrained=False, **kwargs):
+def seresnextaa101d_32x8d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a SE=ResNeXt-101-D 32x8d model with avgpool anti-aliasing
     """
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8,
         stem_width=32, stem_type='deep', avg_down=True, aa_layer=nn.AvgPool2d,
         block_args=dict(attn_layer='se'))
     return _create_resnet('seresnextaa101d_32x8d', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetrs50(pretrained=False, **kwargs):
+def resnetrs50(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-50 model.
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs50', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetrs101(pretrained=False, **kwargs):
+def resnetrs101(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-101 model.
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs101', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetrs152(pretrained=False, **kwargs):
+def resnetrs152(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-152 model.
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs152', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetrs200(pretrained=False, **kwargs):
+def resnetrs200(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-200 model.
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs200', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetrs270(pretrained=False, **kwargs):
+def resnetrs270(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-270 model.
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[4, 29, 53, 4], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs270', pretrained, **dict(model_args, **kwargs))
 
 
 
 @register_model
-def resnetrs350(pretrained=False, **kwargs):
+def resnetrs350(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-350 model.
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[4, 36, 72, 4], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs350', pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetrs420(pretrained=False, **kwargs):
+def resnetrs420(pretrained=False, **kwargs) -> ResNet:
     """Constructs a ResNet-RS-420 model
     Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579
     Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs
     """
     attn_layer = partial(get_attn('se'), rd_ratio=0.25)
     model_args = dict(
         block=Bottleneck, layers=[4, 44, 87, 4], stem_width=32, stem_type='deep', replace_stem_pool=True,
         avg_down=True,  block_args=dict(attn_layer=attn_layer))
     return _create_resnet('resnetrs420', pretrained, **dict(model_args, **kwargs))
+
+
+register_model_deprecations(__name__, {
+    'tv_resnet34': 'resnet34.tv_in1k',
+    'tv_resnet50': 'resnet50.tv_in1k',
+    'tv_resnet101': 'resnet101.tv_in1k',
+    'tv_resnet152': 'resnet152.tv_in1k',
+    'tv_resnext50_32x4d' : 'resnext50_32x4d.tv_in1k',
+    'ig_resnext101_32x8d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',
+    'ig_resnext101_32x16d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',
+    'ig_resnext101_32x32d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',
+    'ig_resnext101_32x48d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',
+    'ssl_resnet18': 'resnet18.fb_ssl_yfcc100m_ft_in1k',
+    'ssl_resnet50': 'resnet50.fb_ssl_yfcc100m_ft_in1k',
+    'ssl_resnext50_32x4d': 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k',
+    'ssl_resnext101_32x4d': 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k',
+    'ssl_resnext101_32x8d': 'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k',
+    'ssl_resnext101_32x16d': 'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k',
+    'swsl_resnet18': 'resnet18.fb_swsl_ig1b_ft_in1k',
+    'swsl_resnet50': 'resnet50.fb_swsl_ig1b_ft_in1k',
+    'swsl_resnext50_32x4d': 'resnext50_32x4d.fb_swsl_ig1b_ft_in1k',
+    'swsl_resnext101_32x4d': 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k',
+    'swsl_resnext101_32x8d': 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k',
+    'swsl_resnext101_32x16d': 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k',
+    'gluon_resnet18_v1b': 'resnet18.gluon_in1k',
+    'gluon_resnet34_v1b': 'resnet34.gluon_in1k',
+    'gluon_resnet50_v1b': 'resnet50.gluon_in1k',
+    'gluon_resnet101_v1b': 'resnet101.gluon_in1k',
+    'gluon_resnet152_v1b': 'resnet152.gluon_in1k',
+    'gluon_resnet50_v1c': 'resnet50c.gluon_in1k',
+    'gluon_resnet101_v1c': 'resnet101c.gluon_in1k',
+    'gluon_resnet152_v1c': 'resnet152c.gluon_in1k',
+    'gluon_resnet50_v1d': 'resnet50d.gluon_in1k',
+    'gluon_resnet101_v1d': 'resnet101d.gluon_in1k',
+    'gluon_resnet152_v1d': 'resnet152d.gluon_in1k',
+    'gluon_resnet50_v1s': 'resnet50s.gluon_in1k',
+    'gluon_resnet101_v1s': 'resnet101s.gluon_in1k',
+    'gluon_resnet152_v1s': 'resnet152s.gluon_in1k',
+    'gluon_resnext50_32x4d': 'resnext50_32x4d.gluon_in1k',
+    'gluon_resnext101_32x4d': 'resnext101_32x4d.gluon_in1k',
+    'gluon_resnext101_64x4d': 'resnext101_64x4d.gluon_in1k',
+    'gluon_seresnext50_32x4d': 'seresnext50_32x4d.gluon_in1k',
+    'gluon_seresnext101_32x4d': 'seresnext101_32x4d.gluon_in1k',
+    'gluon_seresnext101_64x4d': 'seresnext101_64x4d.gluon_in1k',
+    'gluon_senet154': 'senet154.gluon_in1k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/resnetv2.py` & `timm-0.9.0/timm/models/resnetv2.py`

 * *Files 9% similar despite different names*

```diff
@@ -32,122 +32,23 @@
 from collections import OrderedDict  # pylint: disable=g-importing-member
 from functools import partial
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
-from timm.layers import GroupNormAct, BatchNormAct2d, EvoNorm2dB0, EvoNorm2dS0, FilterResponseNormTlu2d, \
-    ClassifierHead, DropPath, AvgPool2dSame, create_pool2d, StdConv2d, create_conv2d, get_act_layer, get_norm_act_layer
+from timm.layers import GroupNormAct, BatchNormAct2d, EvoNorm2dS0, FilterResponseNormTlu2d, ClassifierHead, \
+    DropPath, AvgPool2dSame, create_pool2d, StdConv2d, create_conv2d, get_act_layer, get_norm_act_layer, make_divisible
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq, named_apply, adapt_input_conv
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['ResNetV2']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
-        'first_conv': 'stem.conv', 'classifier': 'head.fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # pretrained on imagenet21k, finetuned on imagenet1k
-    'resnetv2_50x1_bitm': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R50x1-ILSVRC2012.npz',
-        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
-    'resnetv2_50x3_bitm': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R50x3-ILSVRC2012.npz',
-        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
-    'resnetv2_101x1_bitm': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R101x1-ILSVRC2012.npz',
-        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
-    'resnetv2_101x3_bitm': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R101x3-ILSVRC2012.npz',
-        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
-    'resnetv2_152x2_bitm': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R152x2-ILSVRC2012.npz',
-        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
-    'resnetv2_152x4_bitm': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R152x4-ILSVRC2012.npz',
-        input_size=(3, 480, 480), pool_size=(15, 15), crop_pct=1.0, custom_load=True),  # only one at 480x480?
-
-    # trained on imagenet-21k
-    'resnetv2_50x1_bitm_in21k': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R50x1.npz',
-        num_classes=21843, custom_load=True),
-    'resnetv2_50x3_bitm_in21k': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R50x3.npz',
-        num_classes=21843, custom_load=True),
-    'resnetv2_101x1_bitm_in21k': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R101x1.npz',
-        num_classes=21843, custom_load=True),
-    'resnetv2_101x3_bitm_in21k': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R101x3.npz',
-        num_classes=21843, custom_load=True),
-    'resnetv2_152x2_bitm_in21k': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R152x2.npz',
-        num_classes=21843, custom_load=True),
-    'resnetv2_152x4_bitm_in21k': _cfg(
-        url='https://storage.googleapis.com/bit_models/BiT-M-R152x4.npz',
-        num_classes=21843, custom_load=True),
-
-    'resnetv2_50x1_bit_distilled': _cfg(
-        url='https://storage.googleapis.com/bit_models/distill/R50x1_224.npz',
-        interpolation='bicubic', custom_load=True),
-    'resnetv2_152x2_bit_teacher': _cfg(
-        url='https://storage.googleapis.com/bit_models/distill/R152x2_T_224.npz',
-        interpolation='bicubic', custom_load=True),
-    'resnetv2_152x2_bit_teacher_384': _cfg(
-        url='https://storage.googleapis.com/bit_models/distill/R152x2_T_384.npz',
-        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, interpolation='bicubic', custom_load=True),
-
-    'resnetv2_50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnetv2_50_a1h-000cdf49.pth',
-        interpolation='bicubic', crop_pct=0.95),
-    'resnetv2_50d': _cfg(
-        interpolation='bicubic', first_conv='stem.conv1'),
-    'resnetv2_50t': _cfg(
-        interpolation='bicubic', first_conv='stem.conv1'),
-    'resnetv2_101': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnetv2_101_a1h-5d01f016.pth',
-        interpolation='bicubic', crop_pct=0.95),
-    'resnetv2_101d': _cfg(
-        interpolation='bicubic', first_conv='stem.conv1'),
-    'resnetv2_152': _cfg(
-        interpolation='bicubic'),
-    'resnetv2_152d': _cfg(
-        interpolation='bicubic', first_conv='stem.conv1'),
-
-    'resnetv2_50d_gn': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnetv2_50d_gn_ah-c415c11a.pth',
-        interpolation='bicubic', first_conv='stem.conv1', test_input_size=(3, 288, 288), crop_pct=0.95),
-    'resnetv2_50d_evob': _cfg(
-        interpolation='bicubic', first_conv='stem.conv1'),
-    'resnetv2_50d_evos': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnetv2_50d_evos_ah-7c4dd548.pth',
-        interpolation='bicubic', first_conv='stem.conv1', test_input_size=(3, 288, 288), crop_pct=0.95),
-    'resnetv2_50d_frn': _cfg(
-        interpolation='bicubic', first_conv='stem.conv1'),
-}
-
-
-def make_div(v, divisor=8):
-    min_value = divisor
-    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
-    if new_v < 0.9 * v:
-        new_v += divisor
-    return new_v
-
 
 class PreActBottleneck(nn.Module):
     """Pre-activation (v2) bottleneck block.
 
     Follows the implementation of "Identity Mappings in Deep Residual Networks":
     https://github.com/KaimingHe/resnet-1k-layers/blob/master/resnet-pre-act.lua
 
@@ -170,15 +71,15 @@
             drop_path_rate=0.,
     ):
         super().__init__()
         first_dilation = first_dilation or dilation
         conv_layer = conv_layer or StdConv2d
         norm_layer = norm_layer or partial(GroupNormAct, num_groups=32)
         out_chs = out_chs or in_chs
-        mid_chs = make_div(out_chs * bottle_ratio)
+        mid_chs = make_divisible(out_chs * bottle_ratio)
 
         if proj_layer is not None:
             self.downsample = proj_layer(
                 in_chs, out_chs, stride=stride, dilation=dilation, first_dilation=first_dilation, preact=True,
                 conv_layer=conv_layer, norm_layer=norm_layer)
         else:
             self.downsample = None
@@ -230,15 +131,15 @@
     ):
         super().__init__()
         first_dilation = first_dilation or dilation
         act_layer = act_layer or nn.ReLU
         conv_layer = conv_layer or StdConv2d
         norm_layer = norm_layer or partial(GroupNormAct, num_groups=32)
         out_chs = out_chs or in_chs
-        mid_chs = make_div(out_chs * bottle_ratio)
+        mid_chs = make_divisible(out_chs * bottle_ratio)
 
         if proj_layer is not None:
             self.downsample = proj_layer(
                 in_chs, out_chs, stride=stride, dilation=dilation, preact=False,
                 conv_layer=conv_layer, norm_layer=norm_layer)
         else:
             self.downsample = None
@@ -468,15 +369,15 @@
         self.num_classes = num_classes
         self.drop_rate = drop_rate
         wf = width_factor
         norm_layer = get_norm_act_layer(norm_layer, act_layer=act_layer)
         act_layer = get_act_layer(act_layer)
 
         self.feature_info = []
-        stem_chs = make_div(stem_chs * wf)
+        stem_chs = make_divisible(stem_chs * wf)
         self.stem = create_resnetv2_stem(
             in_chans,
             stem_chs,
             stem_type,
             preact,
             conv_layer=conv_layer,
             norm_layer=norm_layer,
@@ -487,15 +388,15 @@
         prev_chs = stem_chs
         curr_stride = 4
         dilation = 1
         block_dprs = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(layers)).split(layers)]
         block_fn = PreActBottleneck if preact else Bottleneck
         self.stages = nn.Sequential()
         for stage_idx, (d, c, bdpr) in enumerate(zip(layers, channels, block_dprs)):
-            out_chs = make_div(c * wf)
+            out_chs = make_divisible(c * wf)
             stride = 1 if stage_idx == 0 else 2
             if curr_stride >= output_stride:
                 dilation *= stride
                 stride = 1
             stage = ResNetStage(
                 prev_chs,
                 out_chs,
@@ -513,15 +414,20 @@
             curr_stride *= stride
             self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{stage_idx}')]
             self.stages.add_module(str(stage_idx), stage)
 
         self.num_features = prev_chs
         self.norm = norm_layer(self.num_features) if preact else nn.Identity()
         self.head = ClassifierHead(
-            self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate, use_conv=True)
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=self.drop_rate,
+            use_conv=True,
+        )
 
         self.init_weights(zero_init_last=zero_init_last)
         self.grad_checkpointing = False
 
     @torch.jit.ignore
     def init_weights(self, zero_init_last=True):
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
@@ -547,16 +453,15 @@
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool='avg'):
         self.num_classes = num_classes
-        self.head = ClassifierHead(
-            self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate, use_conv=True)
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x, flatten=True)
         else:
             x = self.stages(x)
@@ -632,197 +537,237 @@
         feature_cfg=feature_cfg,
         **kwargs,
     )
 
 
 def _create_resnetv2_bit(variant, pretrained=False, **kwargs):
     return _create_resnetv2(
-        variant, pretrained=pretrained, stem_type='fixed',  conv_layer=partial(StdConv2d, eps=1e-8), **kwargs)
-
-
-@register_model
-def resnetv2_50x1_bitm(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_50x1_bitm', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=1, **kwargs)
-
-
-@register_model
-def resnetv2_50x3_bitm(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_50x3_bitm', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=3, **kwargs)
-
-
-@register_model
-def resnetv2_101x1_bitm(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_101x1_bitm', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=1, **kwargs)
-
-
-@register_model
-def resnetv2_101x3_bitm(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_101x3_bitm', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=3, **kwargs)
-
-
-@register_model
-def resnetv2_152x2_bitm(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_152x2_bitm', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)
+        variant,
+        pretrained=pretrained,
+        stem_type='fixed',
+        conv_layer=partial(StdConv2d, eps=1e-8),
+        **kwargs,
+    )
 
 
-@register_model
-def resnetv2_152x4_bitm(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_152x4_bitm', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=4, **kwargs)
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'stem.conv', 'classifier': 'head.fc',
+        **kwargs
+    }
 
 
-@register_model
-def resnetv2_50x1_bitm_in21k(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_50x1_bitm_in21k', pretrained=pretrained, num_classes=kwargs.pop('num_classes', 21843),
-        layers=[3, 4, 6, 3], width_factor=1, **kwargs)
+default_cfgs = generate_default_cfgs({
+    #  Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237
+    'resnetv2_50x1_bit.goog_distilled_in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic', custom_load=True),
+    'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic', custom_load=True),
+    'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, interpolation='bicubic', custom_load=True),
 
+    # pretrained on imagenet21k, finetuned on imagenet1k
+    'resnetv2_50x1_bit.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
+    'resnetv2_50x3_bit.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
+    'resnetv2_101x1_bit.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
+    'resnetv2_101x3_bit.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
+    'resnetv2_152x2_bit.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),
+    'resnetv2_152x4_bit.goog_in21k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 480, 480), pool_size=(15, 15), crop_pct=1.0, custom_load=True),  # only one at 480x480?
 
-@register_model
-def resnetv2_50x3_bitm_in21k(pretrained=False, **kwargs):
-    return _create_resnetv2_bit(
-        'resnetv2_50x3_bitm_in21k', pretrained=pretrained, num_classes=kwargs.pop('num_classes', 21843),
-        layers=[3, 4, 6, 3], width_factor=3, **kwargs)
+    # trained on imagenet-21k
+    'resnetv2_50x1_bit.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=21843, custom_load=True),
+    'resnetv2_50x3_bit.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=21843, custom_load=True),
+    'resnetv2_101x1_bit.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=21843, custom_load=True),
+    'resnetv2_101x3_bit.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=21843, custom_load=True),
+    'resnetv2_152x2_bit.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=21843, custom_load=True),
+    'resnetv2_152x4_bit.goog_in21k': _cfg(
+        hf_hub_id='timm/',
+        num_classes=21843, custom_load=True),
 
+    'resnetv2_50.a1h_in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'resnetv2_50d.untrained': _cfg(
+        interpolation='bicubic', first_conv='stem.conv1'),
+    'resnetv2_50t.untrained': _cfg(
+        interpolation='bicubic', first_conv='stem.conv1'),
+    'resnetv2_101.a1h_in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'resnetv2_101d.untrained': _cfg(
+        interpolation='bicubic', first_conv='stem.conv1'),
+    'resnetv2_152.untrained': _cfg(
+        interpolation='bicubic'),
+    'resnetv2_152d.untrained': _cfg(
+        interpolation='bicubic', first_conv='stem.conv1'),
 
-@register_model
-def resnetv2_101x1_bitm_in21k(pretrained=False, **kwargs):
-    return _create_resnetv2(
-        'resnetv2_101x1_bitm_in21k', pretrained=pretrained, num_classes=kwargs.pop('num_classes', 21843),
-        layers=[3, 4, 23, 3], width_factor=1, **kwargs)
+    'resnetv2_50d_gn.ah_in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic', first_conv='stem.conv1',
+        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'resnetv2_50d_evos.ah_in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic', first_conv='stem.conv1',
+        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),
+    'resnetv2_50d_frn.untrained': _cfg(
+        interpolation='bicubic', first_conv='stem.conv1'),
+})
 
 
 @register_model
-def resnetv2_101x3_bitm_in21k(pretrained=False, **kwargs):
+def resnetv2_50x1_bit(pretrained=False, **kwargs) -> ResNetV2:
     return _create_resnetv2_bit(
-        'resnetv2_101x3_bitm_in21k', pretrained=pretrained, num_classes=kwargs.pop('num_classes', 21843),
-        layers=[3, 4, 23, 3], width_factor=3, **kwargs)
+        'resnetv2_50x1_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=1, **kwargs)
 
 
 @register_model
-def resnetv2_152x2_bitm_in21k(pretrained=False, **kwargs):
+def resnetv2_50x3_bit(pretrained=False, **kwargs) -> ResNetV2:
     return _create_resnetv2_bit(
-        'resnetv2_152x2_bitm_in21k', pretrained=pretrained, num_classes=kwargs.pop('num_classes', 21843),
-        layers=[3, 8, 36, 3], width_factor=2, **kwargs)
+        'resnetv2_50x3_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=3, **kwargs)
 
 
 @register_model
-def resnetv2_152x4_bitm_in21k(pretrained=False, **kwargs):
+def resnetv2_101x1_bit(pretrained=False, **kwargs) -> ResNetV2:
     return _create_resnetv2_bit(
-        'resnetv2_152x4_bitm_in21k', pretrained=pretrained, num_classes=kwargs.pop('num_classes', 21843),
-        layers=[3, 8, 36, 3], width_factor=4, **kwargs)
+        'resnetv2_101x1_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=1, **kwargs)
 
 
 @register_model
-def resnetv2_50x1_bit_distilled(pretrained=False, **kwargs):
-    """ ResNetV2-50x1-BiT Distilled
-    Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237
-    """
+def resnetv2_101x3_bit(pretrained=False, **kwargs) -> ResNetV2:
     return _create_resnetv2_bit(
-        'resnetv2_50x1_bit_distilled', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=1, **kwargs)
+        'resnetv2_101x3_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=3, **kwargs)
 
 
 @register_model
-def resnetv2_152x2_bit_teacher(pretrained=False, **kwargs):
-    """ ResNetV2-152x2-BiT Teacher
-    Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237
-    """
+def resnetv2_152x2_bit(pretrained=False, **kwargs) -> ResNetV2:
     return _create_resnetv2_bit(
-        'resnetv2_152x2_bit_teacher', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)
+        'resnetv2_152x2_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)
 
 
 @register_model
-def resnetv2_152x2_bit_teacher_384(pretrained=False, **kwargs):
-    """ ResNetV2-152xx-BiT Teacher @ 384x384
-    Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237
-    """
+def resnetv2_152x4_bit(pretrained=False, **kwargs) -> ResNetV2:
     return _create_resnetv2_bit(
-        'resnetv2_152x2_bit_teacher_384', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)
+        'resnetv2_152x4_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=4, **kwargs)
 
 
 @register_model
-def resnetv2_50(pretrained=False, **kwargs):
+def resnetv2_50(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)
     return _create_resnetv2('resnetv2_50', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_50d(pretrained=False, **kwargs):
+def resnetv2_50d(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='deep', avg_down=True)
     return _create_resnetv2('resnetv2_50d', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_50t(pretrained=False, **kwargs):
+def resnetv2_50t(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='tiered', avg_down=True)
     return _create_resnetv2('resnetv2_50t', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_101(pretrained=False, **kwargs):
+def resnetv2_101(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(layers=[3, 4, 23, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)
     return _create_resnetv2('resnetv2_101', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_101d(pretrained=False, **kwargs):
+def resnetv2_101d(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 4, 23, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='deep', avg_down=True)
     return _create_resnetv2('resnetv2_101d', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_152(pretrained=False, **kwargs):
+def resnetv2_152(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(layers=[3, 8, 36, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)
     return _create_resnetv2('resnetv2_152', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_152d(pretrained=False, **kwargs):
+def resnetv2_152d(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 8, 36, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='deep', avg_down=True)
     return _create_resnetv2('resnetv2_152d', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 # Experimental configs (may change / be removed)
 
 @register_model
-def resnetv2_50d_gn(pretrained=False, **kwargs):
+def resnetv2_50d_gn(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=GroupNormAct,
         stem_type='deep', avg_down=True)
     return _create_resnetv2('resnetv2_50d_gn', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_50d_evob(pretrained=False, **kwargs):
-    model_args = dict(
-        layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=EvoNorm2dB0,
-        stem_type='deep', avg_down=True, zero_init_last=True)
-    return _create_resnetv2('resnetv2_50d_evob', pretrained=pretrained, **dict(model_args, **kwargs))
-
-
-@register_model
-def resnetv2_50d_evos(pretrained=False, **kwargs):
+def resnetv2_50d_evos(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=EvoNorm2dS0,
         stem_type='deep', avg_down=True)
     return _create_resnetv2('resnetv2_50d_evos', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def resnetv2_50d_frn(pretrained=False, **kwargs):
+def resnetv2_50d_frn(pretrained=False, **kwargs) -> ResNetV2:
     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=FilterResponseNormTlu2d,
         stem_type='deep', avg_down=True)
     return _create_resnetv2('resnetv2_50d_frn', pretrained=pretrained, **dict(model_args, **kwargs))
+
+
+register_model_deprecations(__name__, {
+    'resnetv2_50x1_bitm': 'resnetv2_50x1_bit.goog_in21k_ft_in1k',
+    'resnetv2_50x3_bitm': 'resnetv2_50x3_bit.goog_in21k_ft_in1k',
+    'resnetv2_101x1_bitm': 'resnetv2_101x1_bit.goog_in21k_ft_in1k',
+    'resnetv2_101x3_bitm': 'resnetv2_101x3_bit.goog_in21k_ft_in1k',
+    'resnetv2_152x2_bitm': 'resnetv2_152x2_bit.goog_in21k_ft_in1k',
+    'resnetv2_152x4_bitm': 'resnetv2_152x4_bit.goog_in21k_ft_in1k',
+    'resnetv2_50x1_bitm_in21k': 'resnetv2_50x1_bit.goog_in21k',
+    'resnetv2_50x3_bitm_in21k': 'resnetv2_50x3_bit.goog_in21k',
+    'resnetv2_101x1_bitm_in21k': 'resnetv2_101x1_bit.goog_in21k',
+    'resnetv2_101x3_bitm_in21k': 'resnetv2_101x3_bit.goog_in21k',
+    'resnetv2_152x2_bitm_in21k': 'resnetv2_152x2_bit.goog_in21k',
+    'resnetv2_152x4_bitm_in21k': 'resnetv2_152x4_bit.goog_in21k',
+    'resnetv2_50x1_bit_distilled': 'resnetv2_50x1_bit.goog_distilled_in1k',
+    'resnetv2_152x2_bit_teacher': 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k',
+    'resnetv2_152x2_bit_teacher_384': 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/rexnet.py` & `timm-0.9.0/timm/models/vgg.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,265 +1,301 @@
-""" ReXNet
+"""VGG
 
-A PyTorch impl of `ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network` -
-https://arxiv.org/abs/2007.00992
+Adapted from https://github.com/pytorch/vision 'vgg.py' (BSD-3-Clause) with a few changes for
+timm functionality.
 
-Adapted from original impl at https://github.com/clovaai/rexnet
-Copyright (c) 2020-present NAVER Corp. MIT license
-
-Changes for timm, feature extraction, and rounded channel variant hacked together by Ross Wightman
-Copyright 2020 Ross Wightman
+Copyright 2021 Ross Wightman
 """
-
-from functools import partial
-from math import ceil
+from typing import Union, List, Dict, Any, cast
 
 import torch
 import torch.nn as nn
+import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import ClassifierHead, create_act_layer, ConvNormAct, DropPath, make_divisible, SEModule
+from timm.layers import ClassifierHead
 from ._builder import build_model_with_cfg
-from ._efficientnet_builder import efficientnet_init_weights
-from ._manipulate import checkpoint_seq
-from ._registry import register_model
-
-__all__ = ['ReXNetV1']  # model_registry will add each entrypoint fn to this
-
+from ._features_fx import register_notrace_module
+from ._registry import register_model, generate_default_cfgs
 
-def _cfg(url=''):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.conv', 'classifier': 'head.fc',
-    }
+__all__ = ['VGG']
 
 
-default_cfgs = dict(
-    rexnet_100=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_100-1b4dddf4.pth'),
-    rexnet_130=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_130-590d768e.pth'),
-    rexnet_150=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_150-bd1a6aa8.pth'),
-    rexnet_200=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_200-8c0b7f2d.pth'),
-    rexnetr_100=_cfg(
-        url=''),
-    rexnetr_130=_cfg(
-        url=''),
-    rexnetr_150=_cfg(
-        url=''),
-    rexnetr_200=_cfg(
-        url=''),
-)
+cfgs: Dict[str, List[Union[str, int]]] = {
+    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
+    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
+    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
+    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
+}
 
-SEWithNorm = partial(SEModule, norm_layer=nn.BatchNorm2d)
 
+@register_notrace_module  # reason: FX can't symbolically trace control flow in forward method
+class ConvMlp(nn.Module):
 
-class LinearBottleneck(nn.Module):
     def __init__(
-            self, in_chs, out_chs, stride, exp_ratio=1.0, se_ratio=0., ch_div=1,
-            act_layer='swish', dw_act_layer='relu6', drop_path=None):
-        super(LinearBottleneck, self).__init__()
-        self.use_shortcut = stride == 1 and in_chs <= out_chs
-        self.in_channels = in_chs
-        self.out_channels = out_chs
-
-        if exp_ratio != 1.:
-            dw_chs = make_divisible(round(in_chs * exp_ratio), divisor=ch_div)
-            self.conv_exp = ConvNormAct(in_chs, dw_chs, act_layer=act_layer)
-        else:
-            dw_chs = in_chs
-            self.conv_exp = None
-
-        self.conv_dw = ConvNormAct(dw_chs, dw_chs, 3, stride=stride, groups=dw_chs, apply_act=False)
-        if se_ratio > 0:
-            self.se = SEWithNorm(dw_chs, rd_channels=make_divisible(int(dw_chs * se_ratio), ch_div))
-        else:
-            self.se = None
-        self.act_dw = create_act_layer(dw_act_layer)
-
-        self.conv_pwl = ConvNormAct(dw_chs, out_chs, 1, apply_act=False)
-        self.drop_path = drop_path
-
-    def feat_channels(self, exp=False):
-        return self.conv_dw.out_channels if exp else self.out_channels
+            self,
+            in_features=512,
+            out_features=4096,
+            kernel_size=7,
+            mlp_ratio=1.0,
+            drop_rate: float = 0.2,
+            act_layer: nn.Module = None,
+            conv_layer: nn.Module = None,
+    ):
+        super(ConvMlp, self).__init__()
+        self.input_kernel_size = kernel_size
+        mid_features = int(out_features * mlp_ratio)
+        self.fc1 = conv_layer(in_features, mid_features, kernel_size, bias=True)
+        self.act1 = act_layer(True)
+        self.drop = nn.Dropout(drop_rate)
+        self.fc2 = conv_layer(mid_features, out_features, 1, bias=True)
+        self.act2 = act_layer(True)
 
     def forward(self, x):
-        shortcut = x
-        if self.conv_exp is not None:
-            x = self.conv_exp(x)
-        x = self.conv_dw(x)
-        if self.se is not None:
-            x = self.se(x)
-        x = self.act_dw(x)
-        x = self.conv_pwl(x)
-        if self.use_shortcut:
-            if self.drop_path is not None:
-                x = self.drop_path(x)
-            x = torch.cat([x[:, 0:self.in_channels] + shortcut, x[:, self.in_channels:]], dim=1)
+        if x.shape[-2] < self.input_kernel_size or x.shape[-1] < self.input_kernel_size:
+            # keep the input size >= 7x7
+            output_size = (max(self.input_kernel_size, x.shape[-2]), max(self.input_kernel_size, x.shape[-1]))
+            x = F.adaptive_avg_pool2d(x, output_size)
+        x = self.fc1(x)
+        x = self.act1(x)
+        x = self.drop(x)
+        x = self.fc2(x)
+        x = self.act2(x)
         return x
 
 
-def _block_cfg(width_mult=1.0, depth_mult=1.0, initial_chs=16, final_chs=180, se_ratio=0., ch_div=1):
-    layers = [1, 2, 2, 3, 3, 5]
-    strides = [1, 2, 2, 2, 1, 2]
-    layers = [ceil(element * depth_mult) for element in layers]
-    strides = sum([[element] + [1] * (layers[idx] - 1) for idx, element in enumerate(strides)], [])
-    exp_ratios = [1] * layers[0] + [6] * sum(layers[1:])
-    depth = sum(layers[:]) * 3
-    base_chs = initial_chs / width_mult if width_mult < 1.0 else initial_chs
-
-    # The following channel configuration is a simple instance to make each layer become an expand layer.
-    out_chs_list = []
-    for i in range(depth // 3):
-        out_chs_list.append(make_divisible(round(base_chs * width_mult), divisor=ch_div))
-        base_chs += final_chs / (depth // 3 * 1.0)
-
-    se_ratios = [0.] * (layers[0] + layers[1]) + [se_ratio] * sum(layers[2:])
-
-    return list(zip(out_chs_list, exp_ratios, strides, se_ratios))
-
-
-def _build_blocks(
-        block_cfg, prev_chs, width_mult, ch_div=1, act_layer='swish', dw_act_layer='relu6', drop_path_rate=0.):
-    feat_chs = [prev_chs]
-    feature_info = []
-    curr_stride = 2
-    features = []
-    num_blocks = len(block_cfg)
-    for block_idx, (chs, exp_ratio, stride, se_ratio) in enumerate(block_cfg):
-        if stride > 1:
-            fname = 'stem' if block_idx == 0 else f'features.{block_idx - 1}'
-            feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=fname)]
-            curr_stride *= stride
-        block_dpr = drop_path_rate * block_idx / (num_blocks - 1)  # stochastic depth linear decay rule
-        drop_path = DropPath(block_dpr) if block_dpr > 0. else None
-        features.append(LinearBottleneck(
-            in_chs=prev_chs, out_chs=chs, exp_ratio=exp_ratio, stride=stride, se_ratio=se_ratio,
-            ch_div=ch_div, act_layer=act_layer, dw_act_layer=dw_act_layer, drop_path=drop_path))
-        prev_chs = chs
-        feat_chs += [features[-1].feat_channels()]
-    pen_chs = make_divisible(1280 * width_mult, divisor=ch_div)
-    feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=f'features.{len(features) - 1}')]
-    features.append(ConvNormAct(prev_chs, pen_chs, act_layer=act_layer))
-    return features, feature_info
+class VGG(nn.Module):
 
-
-class ReXNetV1(nn.Module):
     def __init__(
-            self, in_chans=3, num_classes=1000, global_pool='avg', output_stride=32,
-            initial_chs=16, final_chs=180, width_mult=1.0, depth_mult=1.0, se_ratio=1/12.,
-            ch_div=1, act_layer='swish', dw_act_layer='relu6', drop_rate=0.2, drop_path_rate=0.
-    ):
-        super(ReXNetV1, self).__init__()
+            self,
+            cfg: List[Any],
+            num_classes: int = 1000,
+            in_chans: int = 3,
+            output_stride: int = 32,
+            mlp_ratio: float = 1.0,
+            act_layer: nn.Module = nn.ReLU,
+            conv_layer: nn.Module = nn.Conv2d,
+            norm_layer: nn.Module = None,
+            global_pool: str = 'avg',
+            drop_rate: float = 0.,
+    ) -> None:
+        super(VGG, self).__init__()
+        assert output_stride == 32
         self.num_classes = num_classes
+        self.num_features = 4096
         self.drop_rate = drop_rate
         self.grad_checkpointing = False
+        self.use_norm = norm_layer is not None
+        self.feature_info = []
+        prev_chs = in_chans
+        net_stride = 1
+        pool_layer = nn.MaxPool2d
+        layers: List[nn.Module] = []
+        for v in cfg:
+            last_idx = len(layers) - 1
+            if v == 'M':
+                self.feature_info.append(dict(num_chs=prev_chs, reduction=net_stride, module=f'features.{last_idx}'))
+                layers += [pool_layer(kernel_size=2, stride=2)]
+                net_stride *= 2
+            else:
+                v = cast(int, v)
+                conv2d = conv_layer(prev_chs, v, kernel_size=3, padding=1)
+                if norm_layer is not None:
+                    layers += [conv2d, norm_layer(v), act_layer(inplace=True)]
+                else:
+                    layers += [conv2d, act_layer(inplace=True)]
+                prev_chs = v
+        self.features = nn.Sequential(*layers)
+        self.feature_info.append(dict(num_chs=prev_chs, reduction=net_stride, module=f'features.{len(layers) - 1}'))
+
+        self.pre_logits = ConvMlp(
+            prev_chs,
+            self.num_features,
+            7,
+            mlp_ratio=mlp_ratio,
+            drop_rate=drop_rate,
+            act_layer=act_layer,
+            conv_layer=conv_layer,
+        )
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+        )
 
-        assert output_stride == 32  # FIXME support dilation
-        stem_base_chs = 32 / width_mult if width_mult < 1.0 else 32
-        stem_chs = make_divisible(round(stem_base_chs * width_mult), divisor=ch_div)
-        self.stem = ConvNormAct(in_chans, stem_chs, 3, stride=2, act_layer=act_layer)
-
-        block_cfg = _block_cfg(width_mult, depth_mult, initial_chs, final_chs, se_ratio, ch_div)
-        features, self.feature_info = _build_blocks(
-            block_cfg, stem_chs, width_mult, ch_div, act_layer, dw_act_layer, drop_path_rate)
-        self.num_features = features[-1].out_channels
-        self.features = nn.Sequential(*features)
-
-        self.head = ClassifierHead(self.num_features, num_classes, global_pool, drop_rate)
-
-        efficientnet_init_weights(self)
+        self._initialize_weights()
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
-        matcher = dict(
-            stem=r'^stem',
-            blocks=r'^features\.(\d+)',
-        )
-        return matcher
+        # this treats BN layers as separate groups for bn variants, a lot of effort to fix that
+        return dict(stem=r'^features\.0', blocks=r'^features\.(\d+)')
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
-        self.grad_checkpointing = enable
+        assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool='avg'):
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+        self.num_classes = num_classes
+        self.head = ClassifierHead(
+            self.num_features,
+            self.num_classes,
+            pool_type=global_pool,
+            drop_rate=self.drop_rate,
+        )
 
-    def forward_features(self, x):
-        x = self.stem(x)
-        if self.grad_checkpointing and not torch.jit.is_scripting():
-            x = checkpoint_seq(self.features, x, flatten=True)
-        else:
-            x = self.features(x)
+    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.features(x)
         return x
 
-    def forward_head(self, x, pre_logits: bool = False):
-        return self.head(x, pre_logits=pre_logits)
+    def forward_head(self, x: torch.Tensor, pre_logits: bool = False):
+        x = self.pre_logits(x)
+        return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
+    def _initialize_weights(self) -> None:
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                if m.bias is not None:
+                    nn.init.constant_(m.bias, 0)
+            elif isinstance(m, nn.BatchNorm2d):
+                nn.init.constant_(m.weight, 1)
+                nn.init.constant_(m.bias, 0)
+            elif isinstance(m, nn.Linear):
+                nn.init.normal_(m.weight, 0, 0.01)
+                nn.init.constant_(m.bias, 0)
+
+
+def _filter_fn(state_dict):
+    """ convert patch embedding weight from manual patchify + linear proj to conv"""
+    out_dict = {}
+    for k, v in state_dict.items():
+        k_r = k
+        k_r = k_r.replace('classifier.0', 'pre_logits.fc1')
+        k_r = k_r.replace('classifier.3', 'pre_logits.fc2')
+        k_r = k_r.replace('classifier.6', 'head.fc')
+        if 'classifier.0.weight' in k:
+            v = v.reshape(-1, 512, 7, 7)
+        if 'classifier.3.weight' in k:
+            v = v.reshape(-1, 4096, 1, 1)
+        out_dict[k_r] = v
+    return out_dict
+
+
+def _create_vgg(variant: str, pretrained: bool, **kwargs: Any) -> VGG:
+    cfg = variant.split('_')[0]
+    # NOTE: VGG is one of few models with stride==1 features w/ 6 out_indices [0..5]
+    out_indices = kwargs.pop('out_indices', (0, 1, 2, 3, 4, 5))
+    model = build_model_with_cfg(
+        VGG,
+        variant,
+        pretrained,
+        model_cfg=cfgs[cfg],
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
+        pretrained_filter_fn=_filter_fn,
+        **kwargs,
+    )
+    return model
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'features.0', 'classifier': 'head.fc',
+        **kwargs
+    }
+
 
-def _create_rexnet(variant, pretrained, **kwargs):
-    feature_cfg = dict(flatten_sequential=True)
-    return build_model_with_cfg(
-        ReXNetV1, variant, pretrained,
-        feature_cfg=feature_cfg,
-        **kwargs)
+default_cfgs = generate_default_cfgs({
+    'vgg11.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg13.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg16.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg19.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg11_bn.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg13_bn.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg16_bn.tv_in1k': _cfg(hf_hub_id='timm/'),
+    'vgg19_bn.tv_in1k': _cfg(hf_hub_id='timm/'),
+})
 
 
 @register_model
-def rexnet_100(pretrained=False, **kwargs):
-    """ReXNet V1 1.0x"""
-    return _create_rexnet('rexnet_100', pretrained, **kwargs)
+def vgg11(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 11-layer model (configuration "A") from
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(**kwargs)
+    return _create_vgg('vgg11', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnet_130(pretrained=False, **kwargs):
-    """ReXNet V1 1.3x"""
-    return _create_rexnet('rexnet_130', pretrained, width_mult=1.3, **kwargs)
+def vgg11_bn(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 11-layer model (configuration "A") with batch normalization
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)
+    return _create_vgg('vgg11_bn', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnet_150(pretrained=False, **kwargs):
-    """ReXNet V1 1.5x"""
-    return _create_rexnet('rexnet_150', pretrained, width_mult=1.5, **kwargs)
+def vgg13(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 13-layer model (configuration "B")
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(**kwargs)
+    return _create_vgg('vgg13', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnet_200(pretrained=False, **kwargs):
-    """ReXNet V1 2.0x"""
-    return _create_rexnet('rexnet_200', pretrained, width_mult=2.0, **kwargs)
+def vgg13_bn(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 13-layer model (configuration "B") with batch normalization
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)
+    return _create_vgg('vgg13_bn', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnetr_100(pretrained=False, **kwargs):
-    """ReXNet V1 1.0x w/ rounded (mod 8) channels"""
-    return _create_rexnet('rexnetr_100', pretrained, ch_div=8, **kwargs)
+def vgg16(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 16-layer model (configuration "D")
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(**kwargs)
+    return _create_vgg('vgg16', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnetr_130(pretrained=False, **kwargs):
-    """ReXNet V1 1.3x w/ rounded (mod 8) channels"""
-    return _create_rexnet('rexnetr_130', pretrained, width_mult=1.3, ch_div=8, **kwargs)
+def vgg16_bn(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 16-layer model (configuration "D") with batch normalization
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)
+    return _create_vgg('vgg16_bn', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnetr_150(pretrained=False, **kwargs):
-    """ReXNet V1 1.5x w/ rounded (mod 8) channels"""
-    return _create_rexnet('rexnetr_150', pretrained, width_mult=1.5, ch_div=8, **kwargs)
+def vgg19(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 19-layer model (configuration "E")
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(**kwargs)
+    return _create_vgg('vgg19', pretrained=pretrained, **model_args)
 
 
 @register_model
-def rexnetr_200(pretrained=False, **kwargs):
-    """ReXNet V1 2.0x w/ rounded (mod 8) channels"""
-    return _create_rexnet('rexnetr_200', pretrained, width_mult=2.0, ch_div=8, **kwargs)
+def vgg19_bn(pretrained: bool = False, **kwargs: Any) -> VGG:
+    r"""VGG 19-layer model (configuration 'E') with batch normalization
+    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`._
+    """
+    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)
+    return _create_vgg('vgg19_bn', pretrained=pretrained, **model_args)
```

### Comparing `timm-0.8.6.dev0/timm/models/selecsls.py` & `timm-0.9.0/timm/models/selecsls.py`

 * *Files 14% similar despite different names*

```diff
@@ -14,47 +14,17 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
-__all__ = ['SelecSLS']  # model_registry will add each entrypoint fn to this
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (4, 4),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.0', 'classifier': 'fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'selecsls42': _cfg(
-        url='',
-        interpolation='bicubic'),
-    'selecsls42b': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-selecsls/selecsls42b-8af30141.pth',
-        interpolation='bicubic'),
-    'selecsls60': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-selecsls/selecsls60-bbf87526.pth',
-        interpolation='bicubic'),
-    'selecsls60b': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-selecsls/selecsls60b-94e619b5.pth',
-        interpolation='bicubic'),
-    'selecsls84': _cfg(
-        url='',
-        interpolation='bicubic'),
-}
+__all__ = ['SelecSls']  # model_registry will add each entrypoint fn to this
 
 
 class SequentialList(nn.Sequential):
 
     def __init__(self, *args):
         super(SequentialList, self).__init__(*args)
 
@@ -103,17 +73,17 @@
     return nn.Sequential(
         nn.Conv2d(in_chs, out_chs, k, stride, padding=padding, dilation=dilation, bias=False),
         nn.BatchNorm2d(out_chs),
         nn.ReLU(inplace=True)
     )
 
 
-class SelecSLSBlock(nn.Module):
+class SelecSlsBlock(nn.Module):
     def __init__(self, in_chs, skip_chs, mid_chs, out_chs, is_first, stride, dilation=1):
-        super(SelecSLSBlock, self).__init__()
+        super(SelecSlsBlock, self).__init__()
         self.stride = stride
         self.is_first = is_first
         assert stride in [1, 2]
 
         # Process input with 4 conv blocks with the same number of input and output channels
         self.conv1 = conv_bn(in_chs, mid_chs, 3, stride, dilation=dilation)
         self.conv2 = conv_bn(mid_chs, mid_chs, 1)
@@ -133,16 +103,16 @@
         if self.is_first:
             out = self.conv6(torch.cat([d1, d2, d3], 1))
             return [out, out]
         else:
             return [self.conv6(torch.cat([d1, d2, d3, x[1]], 1)), x[1]]
 
 
-class SelecSLS(nn.Module):
-    """SelecSLS42 / SelecSLS60 / SelecSLS84
+class SelecSls(nn.Module):
+    """SelecSls42 / SelecSls60 / SelecSls84
 
     Parameters
     ----------
     cfg : network config dictionary specifying block type, feature, and head args
     num_classes : int, default 1000
         Number of classification classes.
     in_chans : int, default 3
@@ -151,32 +121,33 @@
         Dropout probability before classifier, for training
     global_pool : str, default 'avg'
         Global pooling type. One of 'avg', 'max', 'avgmax', 'catavgmax'
     """
 
     def __init__(self, cfg, num_classes=1000, in_chans=3, drop_rate=0.0, global_pool='avg'):
         self.num_classes = num_classes
-        self.drop_rate = drop_rate
-        super(SelecSLS, self).__init__()
+        super(SelecSls, self).__init__()
 
         self.stem = conv_bn(in_chans, 32, stride=2)
         self.features = SequentialList(*[cfg['block'](*block_args) for block_args in cfg['features']])
         self.from_seq = SelectSeq()  # from List[tensor] -> Tensor in module compatible way
         self.head = nn.Sequential(*[conv_bn(*conv_args) for conv_args in cfg['head']])
         self.num_features = cfg['num_features']
         self.feature_info = cfg['feature_info']
 
-        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool, self.head_drop, self.fc = create_classifier(
+            self.num_features,
+            self.num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+        )
 
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-            elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 1.)
-                nn.init.constant_(m.bias, 0.)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
             blocks=r'^features\.(\d+)',
             blocks_head=r'^head'
@@ -198,29 +169,28 @@
         x = self.stem(x)
         x = self.features(x)
         x = self.head(self.from_seq(x))
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
-        if self.drop_rate > 0.:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+        x = self.head_drop(x)
         return x if pre_logits else self.fc(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _create_selecsls(variant, pretrained, **kwargs):
+def _create_SelecSls(variant, pretrained, **kwargs):
     cfg = {}
     feature_info = [dict(num_chs=32, reduction=2, module='stem.2')]
-    if variant.startswith('selecsls42'):
-        cfg['block'] = SelecSLSBlock
+    if variant.startswith('SelecSls42'):
+        cfg['block'] = SelecSlsBlock
         # Define configuration of the network after the initial neck
         cfg['features'] = [
             # in_chs, skip_chs, mid_chs, out_chs, is_first, stride
             (32, 0, 64, 64, True, 2),
             (64, 64, 64, 128, False, 1),
             (128, 0, 144, 144, True, 2),
             (144, 144, 144, 288, False, 1),
@@ -230,15 +200,15 @@
         feature_info.extend([
             dict(num_chs=128, reduction=4, module='features.1'),
             dict(num_chs=288, reduction=8, module='features.3'),
             dict(num_chs=480, reduction=16, module='features.5'),
         ])
         # Head can be replaced with alternative configurations depending on the problem
         feature_info.append(dict(num_chs=1024, reduction=32, module='head.1'))
-        if variant == 'selecsls42b':
+        if variant == 'SelecSls42b':
             cfg['head'] = [
                 (480, 960, 3, 2),
                 (960, 1024, 3, 1),
                 (1024, 1280, 3, 2),
                 (1280, 1024, 1, 1),
             ]
             feature_info.append(dict(num_chs=1024, reduction=64, module='head.3'))
@@ -249,16 +219,16 @@
                 (960, 1024, 3, 1),
                 (1024, 1024, 3, 2),
                 (1024, 1280, 1, 1),
             ]
             feature_info.append(dict(num_chs=1280, reduction=64, module='head.3'))
             cfg['num_features'] = 1280
 
-    elif variant.startswith('selecsls60'):
-        cfg['block'] = SelecSLSBlock
+    elif variant.startswith('SelecSls60'):
+        cfg['block'] = SelecSlsBlock
         # Define configuration of the network after the initial neck
         cfg['features'] = [
             # in_chs, skip_chs, mid_chs, out_chs, is_first, stride
             (32, 0, 64, 64, True, 2),
             (64, 64, 64, 128, False, 1),
             (128, 0, 128, 128, True, 2),
             (128, 128, 128, 128, False, 1),
@@ -271,15 +241,15 @@
         feature_info.extend([
             dict(num_chs=128, reduction=4, module='features.1'),
             dict(num_chs=288, reduction=8, module='features.4'),
             dict(num_chs=416, reduction=16, module='features.8'),
         ])
         # Head can be replaced with alternative configurations depending on the problem
         feature_info.append(dict(num_chs=1024, reduction=32, module='head.1'))
-        if variant == 'selecsls60b':
+        if variant == 'SelecSls60b':
             cfg['head'] = [
                 (416, 756, 3, 2),
                 (756, 1024, 3, 1),
                 (1024, 1280, 3, 2),
                 (1280, 1024, 1, 1),
             ]
             feature_info.append(dict(num_chs=1024, reduction=64, module='head.3'))
@@ -290,16 +260,16 @@
                 (756, 1024, 3, 1),
                 (1024, 1024, 3, 2),
                 (1024, 1280, 1, 1),
             ]
             feature_info.append(dict(num_chs=1280, reduction=64, module='head.3'))
             cfg['num_features'] = 1280
 
-    elif variant == 'selecsls84':
-        cfg['block'] = SelecSLSBlock
+    elif variant == 'SelecSls84':
+        cfg['block'] = SelecSlsBlock
         # Define configuration of the network after the initial neck
         cfg['features'] = [
             # in_chs, skip_chs, mid_chs, out_chs, is_first, stride
             (32, 0, 64, 64, True, 2),
             (64, 64, 64, 144, False, 1),
             (144, 0, 144, 144, True, 2),
             (144, 144, 144, 144, False, 1),
@@ -332,46 +302,77 @@
         ])
     else:
         raise ValueError('Invalid net configuration ' + variant + ' !!!')
     cfg['feature_info'] = feature_info
 
     # this model can do 6 feature levels by default, unlike most others, leave as 0-4 to avoid surprises?
     return build_model_with_cfg(
-        SelecSLS, variant, pretrained,
+        SelecSls,
+        variant,
+        pretrained,
         model_cfg=cfg,
         feature_cfg=dict(out_indices=(0, 1, 2, 3, 4), flatten_sequential=True),
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (4, 4),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.0', 'classifier': 'fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'SelecSls42.untrained': _cfg(
+        interpolation='bicubic'),
+    'SelecSls42b.in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic'),
+    'SelecSls60.in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic'),
+    'SelecSls60b.in1k': _cfg(
+        hf_hub_id='timm/',
+        interpolation='bicubic'),
+    'SelecSls84.untrained': _cfg(
+        interpolation='bicubic'),
+})
 
 
 @register_model
-def selecsls42(pretrained=False, **kwargs):
-    """Constructs a SelecSLS42 model.
+def SelecSls42(pretrained=False, **kwargs) -> SelecSls:
+    """Constructs a SelecSls42 model.
     """
-    return _create_selecsls('selecsls42', pretrained, **kwargs)
+    return _create_SelecSls('SelecSls42', pretrained, **kwargs)
 
 
 @register_model
-def selecsls42b(pretrained=False, **kwargs):
-    """Constructs a SelecSLS42_B model.
+def SelecSls42b(pretrained=False, **kwargs) -> SelecSls:
+    """Constructs a SelecSls42_B model.
     """
-    return _create_selecsls('selecsls42b', pretrained, **kwargs)
+    return _create_SelecSls('SelecSls42b', pretrained, **kwargs)
 
 
 @register_model
-def selecsls60(pretrained=False, **kwargs):
-    """Constructs a SelecSLS60 model.
+def SelecSls60(pretrained=False, **kwargs) -> SelecSls:
+    """Constructs a SelecSls60 model.
     """
-    return _create_selecsls('selecsls60', pretrained, **kwargs)
+    return _create_SelecSls('SelecSls60', pretrained, **kwargs)
 
 
 @register_model
-def selecsls60b(pretrained=False, **kwargs):
-    """Constructs a SelecSLS60_B model.
+def SelecSls60b(pretrained=False, **kwargs) -> SelecSls:
+    """Constructs a SelecSls60_B model.
     """
-    return _create_selecsls('selecsls60b', pretrained, **kwargs)
+    return _create_SelecSls('SelecSls60b', pretrained, **kwargs)
 
 
 @register_model
-def selecsls84(pretrained=False, **kwargs):
-    """Constructs a SelecSLS84 model.
+def SelecSls84(pretrained=False, **kwargs) -> SelecSls:
+    """Constructs a SelecSls84 model.
     """
-    return _create_selecsls('selecsls84', pretrained, **kwargs)
+    return _create_SelecSls('SelecSls84', pretrained, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/senet.py` & `timm-0.9.0/timm/models/senet.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,53 +17,19 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['SENet']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'layer0.conv1', 'classifier': 'last_linear',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'legacy_senet154': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_senet154-e9eb9fe6.pth'),
-    'legacy_seresnet18': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet18-4bb0ce65.pth',
-        interpolation='bicubic'),
-    'legacy_seresnet34': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet34-a4004e63.pth'),
-    'legacy_seresnet50': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet50-ce0d4300.pth'),
-    'legacy_seresnet101': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet101-7e38fcc6.pth'),
-    'legacy_seresnet152': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet152-d17c99b7.pth'),
-    'legacy_seresnext26_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26_32x4d-65ebdb501.pth',
-        interpolation='bicubic'),
-    'legacy_seresnext50_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_se_resnext50_32x4d-f3651bad.pth'),
-    'legacy_seresnext101_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_se_resnext101_32x4d-37725eac.pth'),
-}
-
-
 def _weight_init(m):
     if isinstance(m, nn.Conv2d):
         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
     elif isinstance(m, nn.BatchNorm2d):
         nn.init.constant_(m.weight, 1.)
         nn.init.constant_(m.bias, 0.)
 
@@ -397,69 +363,103 @@
         return x
 
 
 def _create_senet(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(SENet, variant, pretrained, **kwargs)
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bilinear',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'layer0.conv1', 'classifier': 'last_linear',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'legacy_senet154.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_senet154-e9eb9fe6.pth'),
+    'legacy_seresnet18.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet18-4bb0ce65.pth',
+        interpolation='bicubic'),
+    'legacy_seresnet34.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet34-a4004e63.pth'),
+    'legacy_seresnet50.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet50-ce0d4300.pth'),
+    'legacy_seresnet101.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet101-7e38fcc6.pth'),
+    'legacy_seresnet152.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet152-d17c99b7.pth'),
+    'legacy_seresnext26_32x4d.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26_32x4d-65ebdb501.pth',
+        interpolation='bicubic'),
+    'legacy_seresnext50_32x4d.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_se_resnext50_32x4d-f3651bad.pth'),
+    'legacy_seresnext101_32x4d.in1k': _cfg(
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_se_resnext101_32x4d-37725eac.pth'),
+})
+
+
 @register_model
-def legacy_seresnet18(pretrained=False, **kwargs):
+def legacy_seresnet18(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBlock, layers=[2, 2, 2, 2], groups=1, reduction=16, **kwargs)
     return _create_senet('legacy_seresnet18', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnet34(pretrained=False, **kwargs):
+def legacy_seresnet34(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBlock, layers=[3, 4, 6, 3], groups=1, reduction=16, **kwargs)
     return _create_senet('legacy_seresnet34', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnet50(pretrained=False, **kwargs):
+def legacy_seresnet50(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBottleneck, layers=[3, 4, 6, 3], groups=1, reduction=16, **kwargs)
     return _create_senet('legacy_seresnet50', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnet101(pretrained=False, **kwargs):
+def legacy_seresnet101(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBottleneck, layers=[3, 4, 23, 3], groups=1, reduction=16, **kwargs)
     return _create_senet('legacy_seresnet101', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnet152(pretrained=False, **kwargs):
+def legacy_seresnet152(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBottleneck, layers=[3, 8, 36, 3], groups=1, reduction=16, **kwargs)
     return _create_senet('legacy_seresnet152', pretrained, **model_args)
 
 
 @register_model
-def legacy_senet154(pretrained=False, **kwargs):
+def legacy_senet154(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEBottleneck, layers=[3, 8, 36, 3], groups=64, reduction=16,
         downsample_kernel_size=3, downsample_padding=1,  inplanes=128, input_3x3=True, **kwargs)
     return _create_senet('legacy_senet154', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnext26_32x4d(pretrained=False, **kwargs):
+def legacy_seresnext26_32x4d(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNeXtBottleneck, layers=[2, 2, 2, 2], groups=32, reduction=16, **kwargs)
     return _create_senet('legacy_seresnext26_32x4d', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnext50_32x4d(pretrained=False, **kwargs):
+def legacy_seresnext50_32x4d(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNeXtBottleneck, layers=[3, 4, 6, 3], groups=32, reduction=16, **kwargs)
     return _create_senet('legacy_seresnext50_32x4d', pretrained, **model_args)
 
 
 @register_model
-def legacy_seresnext101_32x4d(pretrained=False, **kwargs):
+def legacy_seresnext101_32x4d(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNeXtBottleneck, layers=[3, 4, 23, 3], groups=32, reduction=16, **kwargs)
     return _create_senet('legacy_seresnext101_32x4d', pretrained, **model_args)
```

### Comparing `timm-0.8.6.dev0/timm/models/sequencer.py` & `timm-0.9.0/timm/models/sequencer.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,44 +4,27 @@
 
 """
 #  Copyright (c) 2022. Yuki Tatsunami
 #  Licensed under the Apache License, Version 2.0 (the "License");
 
 import math
 from functools import partial
+from itertools import accumulate
 from typing import Tuple
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT
-from timm.layers import lecun_normal_, DropPath, Mlp, PatchEmbed as TimmPatchEmbed
+from timm.layers import lecun_normal_, DropPath, Mlp, PatchEmbed, ClassifierHead
 from ._builder import build_model_with_cfg
 from ._manipulate import named_apply
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
-__all__ = ['Sequencer2D']  # model_registry will add each entrypoint fn to this
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': DEFAULT_CROP_PCT, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    sequencer2d_s=_cfg(url="https://github.com/okojoalg/sequencer/releases/download/weights/sequencer2d_s.pth"),
-    sequencer2d_m=_cfg(url="https://github.com/okojoalg/sequencer/releases/download/weights/sequencer2d_m.pth"),
-    sequencer2d_l=_cfg(url="https://github.com/okojoalg/sequencer/releases/download/weights/sequencer2d_l.pth"),
-)
+__all__ = ['Sequencer2d']  # model_registry will add each entrypoint fn to this
 
 
 def _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):
     if isinstance(module, nn.Linear):
         if name.startswith('head'):
             nn.init.zeros_(module.weight)
             nn.init.constant_(module.bias, head_bias)
@@ -69,49 +52,34 @@
         stdv = 1.0 / math.sqrt(module.hidden_size)
         for weight in module.parameters():
             nn.init.uniform_(weight, -stdv, stdv)
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
-def get_stage(
-        index, layers, patch_sizes, embed_dims, hidden_sizes, mlp_ratios, block_layer, rnn_layer, mlp_layer,
-        norm_layer, act_layer, num_layers, bidirectional, union,
-        with_fc, drop=0., drop_path_rate=0., **kwargs):
-    assert len(layers) == len(patch_sizes) == len(embed_dims) == len(hidden_sizes) == len(mlp_ratios)
-    blocks = []
-    for block_idx in range(layers[index]):
-        drop_path = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)
-        blocks.append(block_layer(
-            embed_dims[index], hidden_sizes[index], mlp_ratio=mlp_ratios[index],
-            rnn_layer=rnn_layer, mlp_layer=mlp_layer, norm_layer=norm_layer, act_layer=act_layer,
-            num_layers=num_layers, bidirectional=bidirectional, union=union, with_fc=with_fc,
-            drop=drop, drop_path=drop_path))
-
-    if index < len(embed_dims) - 1:
-        blocks.append(Downsample2D(embed_dims[index], embed_dims[index + 1], patch_sizes[index + 1]))
-
-    blocks = nn.Sequential(*blocks)
-    return blocks
-
-
 class RNNIdentity(nn.Module):
     def __init__(self, *args, **kwargs):
         super(RNNIdentity, self).__init__()
 
     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, None]:
         return x, None
 
 
-class RNN2DBase(nn.Module):
+class RNN2dBase(nn.Module):
 
     def __init__(
-            self, input_size: int, hidden_size: int,
-            num_layers: int = 1, bias: bool = True, bidirectional: bool = True,
-            union="cat", with_fc=True):
+            self,
+            input_size: int,
+            hidden_size: int,
+            num_layers: int = 1,
+            bias: bool = True,
+            bidirectional: bool = True,
+            union="cat",
+            with_fc=True,
+    ):
         super().__init__()
 
         self.input_size = input_size
         self.hidden_size = hidden_size
         self.output_size = 2 * hidden_size if bidirectional else hidden_size
         self.union = union
 
@@ -186,97 +154,178 @@
 
         if self.fc is not None:
             x = self.fc(x)
 
         return x
 
 
-class LSTM2D(RNN2DBase):
+class LSTM2d(RNN2dBase):
 
     def __init__(
-            self, input_size: int, hidden_size: int,
-            num_layers: int = 1, bias: bool = True, bidirectional: bool = True,
-            union="cat", with_fc=True):
+            self,
+            input_size: int,
+            hidden_size: int,
+            num_layers: int = 1,
+            bias: bool = True,
+            bidirectional: bool = True,
+            union="cat",
+            with_fc=True,
+    ):
         super().__init__(input_size, hidden_size, num_layers, bias, bidirectional, union, with_fc)
         if self.with_vertical:
-            self.rnn_v = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bias=bias, bidirectional=bidirectional)
+            self.rnn_v = nn.LSTM(
+                input_size,
+                hidden_size,
+                num_layers,
+                batch_first=True,
+                bias=bias,
+                bidirectional=bidirectional,
+            )
         if self.with_horizontal:
-            self.rnn_h = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bias=bias, bidirectional=bidirectional)
+            self.rnn_h = nn.LSTM(
+                input_size,
+                hidden_size,
+                num_layers,
+                batch_first=True,
+                bias=bias,
+                bidirectional=bidirectional,
+            )
 
 
-class Sequencer2DBlock(nn.Module):
+class Sequencer2dBlock(nn.Module):
     def __init__(
-            self, dim, hidden_size, mlp_ratio=3.0, rnn_layer=LSTM2D, mlp_layer=Mlp,
-            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=nn.GELU,
-            num_layers=1, bidirectional=True, union="cat", with_fc=True, drop=0., drop_path=0.):
+            self,
+            dim,
+            hidden_size,
+            mlp_ratio=3.0,
+            rnn_layer=LSTM2d,
+            mlp_layer=Mlp,
+            norm_layer=partial(nn.LayerNorm, eps=1e-6),
+            act_layer=nn.GELU,
+            num_layers=1,
+            bidirectional=True,
+            union="cat",
+            with_fc=True,
+            drop=0.,
+            drop_path=0.,
+    ):
         super().__init__()
         channels_dim = int(mlp_ratio * dim)
         self.norm1 = norm_layer(dim)
-        self.rnn_tokens = rnn_layer(dim, hidden_size, num_layers=num_layers, bidirectional=bidirectional,
-                                    union=union, with_fc=with_fc)
+        self.rnn_tokens = rnn_layer(
+            dim,
+            hidden_size,
+            num_layers=num_layers,
+            bidirectional=bidirectional,
+            union=union,
+            with_fc=with_fc,
+        )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         self.mlp_channels = mlp_layer(dim, channels_dim, act_layer=act_layer, drop=drop)
 
     def forward(self, x):
         x = x + self.drop_path(self.rnn_tokens(self.norm1(x)))
         x = x + self.drop_path(self.mlp_channels(self.norm2(x)))
         return x
 
 
-class PatchEmbed(TimmPatchEmbed):
-    def forward(self, x):
-        x = self.proj(x)
-        if self.flatten:
-            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
-        else:
-            x = x.permute(0, 2, 3, 1)  # BCHW -> BHWC
-        x = self.norm(x)
-        return x
-
-
 class Shuffle(nn.Module):
     def __init__(self):
         super().__init__()
 
     def forward(self, x):
         if self.training:
             B, H, W, C = x.shape
             r = torch.randperm(H * W)
             x = x.reshape(B, -1, C)
             x = x[:, r, :].reshape(B, H, W, -1)
         return x
 
 
-class Downsample2D(nn.Module):
+class Downsample2d(nn.Module):
     def __init__(self, input_dim, output_dim, patch_size):
         super().__init__()
         self.down = nn.Conv2d(input_dim, output_dim, kernel_size=patch_size, stride=patch_size)
 
     def forward(self, x):
         x = x.permute(0, 3, 1, 2)
         x = self.down(x)
         x = x.permute(0, 2, 3, 1)
         return x
 
 
-class Sequencer2D(nn.Module):
+class Sequencer2dStage(nn.Module):
+    def __init__(
+            self,
+            dim,
+            dim_out,
+            depth,
+            patch_size,
+            hidden_size,
+            mlp_ratio,
+            downsample=False,
+            block_layer=Sequencer2dBlock,
+            rnn_layer=LSTM2d,
+            mlp_layer=Mlp,
+            norm_layer=partial(nn.LayerNorm, eps=1e-6),
+            act_layer=nn.GELU,
+            num_layers=1,
+            bidirectional=True,
+            union="cat",
+            with_fc=True,
+            drop=0.,
+            drop_path=0.,
+    ):
+        super().__init__()
+        if downsample:
+            self.downsample = Downsample2d(dim, dim_out, patch_size)
+        else:
+            assert dim == dim_out
+            self.downsample = nn.Identity()
+
+        blocks = []
+        for block_idx in range(depth):
+            blocks.append(block_layer(
+                dim_out,
+                hidden_size,
+                mlp_ratio=mlp_ratio,
+                rnn_layer=rnn_layer,
+                mlp_layer=mlp_layer,
+                norm_layer=norm_layer,
+                act_layer=act_layer,
+                num_layers=num_layers,
+                bidirectional=bidirectional,
+                union=union,
+                with_fc=with_fc,
+                drop=drop,
+                drop_path=drop_path[block_idx] if isinstance(drop_path, (list, tuple)) else drop_path,
+            ))
+        self.blocks = nn.Sequential(*blocks)
+
+    def forward(self, x):
+        x = self.downsample(x)
+        x = self.blocks(x)
+        return x
+
+
+class Sequencer2d(nn.Module):
     def __init__(
             self,
             num_classes=1000,
             img_size=224,
             in_chans=3,
             global_pool='avg',
-            layers=[4, 3, 8, 3],
-            patch_sizes=[7, 2, 1, 1],
-            embed_dims=[192, 384, 384, 384],
-            hidden_sizes=[48, 96, 96, 96],
-            mlp_ratios=[3.0, 3.0, 3.0, 3.0],
-            block_layer=Sequencer2DBlock,
-            rnn_layer=LSTM2D,
+            layers=(4, 3, 8, 3),
+            patch_sizes=(7, 2, 2, 1),
+            embed_dims=(192, 384, 384, 384),
+            hidden_sizes=(48, 96, 96, 96),
+            mlp_ratios=(3.0, 3.0, 3.0, 3.0),
+            block_layer=Sequencer2dBlock,
+            rnn_layer=LSTM2d,
             mlp_layer=Mlp,
             norm_layer=partial(nn.LayerNorm, eps=1e-6),
             act_layer=nn.GELU,
             num_rnn_layers=1,
             bidirectional=True,
             union="cat",
             with_fc=True,
@@ -287,133 +336,205 @@
     ):
         super().__init__()
         assert global_pool in ('', 'avg')
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_features = embed_dims[-1]  # num_features for consistency with other models
         self.feature_dim = -1  # channel dim index for feature outputs (rank 4, NHWC)
-        self.embed_dims = embed_dims
+        self.output_fmt = 'NHWC'
+        self.feature_info = []
+
         self.stem = PatchEmbed(
-            img_size=img_size, patch_size=patch_sizes[0], in_chans=in_chans,
-            embed_dim=embed_dims[0], norm_layer=norm_layer if stem_norm else None,
-            flatten=False)
-
-        self.blocks = nn.Sequential(*[
-            get_stage(
-                i, layers, patch_sizes, embed_dims, hidden_sizes, mlp_ratios, block_layer=block_layer,
-                rnn_layer=rnn_layer, mlp_layer=mlp_layer, norm_layer=norm_layer, act_layer=act_layer,
-                num_layers=num_rnn_layers, bidirectional=bidirectional,
-                union=union, with_fc=with_fc, drop=drop_rate, drop_path_rate=drop_path_rate,
-            )
-            for i, _ in enumerate(embed_dims)])
+            img_size=None,
+            patch_size=patch_sizes[0],
+            in_chans=in_chans,
+            embed_dim=embed_dims[0],
+            norm_layer=norm_layer if stem_norm else None,
+            flatten=False,
+            output_fmt='NHWC',
+        )
+
+        assert len(layers) == len(patch_sizes) == len(embed_dims) == len(hidden_sizes) == len(mlp_ratios)
+        reductions = list(accumulate(patch_sizes, lambda x, y: x * y))
+        stages = []
+        prev_dim = embed_dims[0]
+        for i, _ in enumerate(embed_dims):
+            stages += [Sequencer2dStage(
+                prev_dim,
+                embed_dims[i],
+                depth=layers[i],
+                downsample=i > 0,
+                patch_size=patch_sizes[i],
+                hidden_size=hidden_sizes[i],
+                mlp_ratio=mlp_ratios[i],
+                block_layer=block_layer,
+                rnn_layer=rnn_layer,
+                mlp_layer=mlp_layer,
+                norm_layer=norm_layer,
+                act_layer=act_layer,
+                num_layers=num_rnn_layers,
+                bidirectional=bidirectional,
+                union=union,
+                with_fc=with_fc,
+                drop=drop_rate,
+                drop_path=drop_path_rate,
+            )]
+            prev_dim = embed_dims[i]
+            self.feature_info += [dict(num_chs=prev_dim, reduction=reductions[i], module=f'stages.{i}')]
 
+        self.stages = nn.Sequential(*stages)
         self.norm = norm_layer(embed_dims[-1])
-        self.head = nn.Linear(embed_dims[-1], self.num_classes) if num_classes > 0 else nn.Identity()
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+            input_fmt=self.output_fmt,
+        )
 
         self.init_weights(nlhb=nlhb)
 
     def init_weights(self, nlhb=False):
         head_bias = -math.log(self.num_classes) if nlhb else 0.
         named_apply(partial(_init_weights, head_bias=head_bias), module=self)  # depth-first
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
             blocks=[
-                (r'^blocks\.(\d+)\..*\.down', (99999,)),
-                (r'^blocks\.(\d+)', None) if coarse else (r'^blocks\.(\d+)\.(\d+)', None),
+                (r'^stages\.(\d+)', None),
+                (r'^norm', (99999,))
+            ] if coarse else [
+                (r'^stages\.(\d+)\.blocks\.(\d+)', None),
+                (r'^stages\.(\d+)\.downsample', (0,)),
                 (r'^norm', (99999,))
             ]
         )
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head
 
     def reset_classifier(self, num_classes, global_pool=None):
         self.num_classes = num_classes
-        if global_pool is not None:
-            assert global_pool in ('', 'avg')
-            self.global_pool = global_pool
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+        self.head.reset(num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
         x = self.stem(x)
-        x = self.blocks(x)
+        x = self.stages(x)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool == 'avg':
-            x = x.mean(dim=(1, 2))
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
+def checkpoint_filter_fn(state_dict, model):
+    """ Remap original checkpoints -> timm """
+    if 'stages.0.blocks.0.norm1.weight' in state_dict:
+        return state_dict  # already translated checkpoint
+    if 'model' in state_dict:
+        state_dict = state_dict['model']
+
+    import re
+    out_dict = {}
+    for k, v in state_dict.items():
+        k = re.sub(r'blocks.([0-9]+).([0-9]+).down', lambda x: f'stages.{int(x.group(1)) + 1}.downsample.down', k)
+        k = re.sub(r'blocks.([0-9]+).([0-9]+)', r'stages.\1.blocks.\2', k)
+        k = k.replace('head.', 'head.fc.')
+        out_dict[k] = v
+
+    return out_dict
+
+
 def _create_sequencer2d(variant, pretrained=False, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Sequencer2D models.')
+    default_out_indices = tuple(range(3))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
 
-    model = build_model_with_cfg(Sequencer2D, variant, pretrained, **kwargs)
+    model = build_model_with_cfg(
+        Sequencer2d,
+        variant,
+        pretrained,
+        pretrained_filter_fn=checkpoint_filter_fn,
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
+        **kwargs,
+    )
     return model
 
 
-# main
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': DEFAULT_CROP_PCT, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.proj', 'classifier': 'head.fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'sequencer2d_s.in1k': _cfg(hf_hub_id='timm/'),
+    'sequencer2d_m.in1k': _cfg(hf_hub_id='timm/'),
+    'sequencer2d_l.in1k': _cfg(hf_hub_id='timm/'),
+})
+
 
 @register_model
-def sequencer2d_s(pretrained=False, **kwargs):
+def sequencer2d_s(pretrained=False, **kwargs) -> Sequencer2d:
     model_args = dict(
         layers=[4, 3, 8, 3],
         patch_sizes=[7, 2, 1, 1],
         embed_dims=[192, 384, 384, 384],
         hidden_sizes=[48, 96, 96, 96],
         mlp_ratios=[3.0, 3.0, 3.0, 3.0],
-        rnn_layer=LSTM2D,
+        rnn_layer=LSTM2d,
         bidirectional=True,
         union="cat",
         with_fc=True,
-        **kwargs)
-    model = _create_sequencer2d('sequencer2d_s', pretrained=pretrained, **model_args)
+    )
+    model = _create_sequencer2d('sequencer2d_s', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def sequencer2d_m(pretrained=False, **kwargs):
+def sequencer2d_m(pretrained=False, **kwargs) -> Sequencer2d:
     model_args = dict(
         layers=[4, 3, 14, 3],
         patch_sizes=[7, 2, 1, 1],
         embed_dims=[192, 384, 384, 384],
         hidden_sizes=[48, 96, 96, 96],
         mlp_ratios=[3.0, 3.0, 3.0, 3.0],
-        rnn_layer=LSTM2D,
+        rnn_layer=LSTM2d,
         bidirectional=True,
         union="cat",
         with_fc=True,
         **kwargs)
-    model = _create_sequencer2d('sequencer2d_m', pretrained=pretrained, **model_args)
+    model = _create_sequencer2d('sequencer2d_m', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def sequencer2d_l(pretrained=False, **kwargs):
+def sequencer2d_l(pretrained=False, **kwargs) -> Sequencer2d:
     model_args = dict(
         layers=[8, 8, 16, 4],
         patch_sizes=[7, 2, 1, 1],
         embed_dims=[192, 384, 384, 384],
         hidden_sizes=[48, 96, 96, 96],
         mlp_ratios=[3.0, 3.0, 3.0, 3.0],
-        rnn_layer=LSTM2D,
+        rnn_layer=LSTM2d,
         bidirectional=True,
         union="cat",
         with_fc=True,
         **kwargs)
-    model = _create_sequencer2d('sequencer2d_l', pretrained=pretrained, **model_args)
+    model = _create_sequencer2d('sequencer2d_l', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/sknet.py` & `timm-0.9.0/timm/models/sknet.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,42 +11,18 @@
 import math
 
 from torch import nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectiveKernel, ConvNormAct, create_attn
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .resnet import ResNet
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'conv1', 'classifier': 'fc',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'skresnet18': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnet18_ra-4eec2804.pth'),
-    'skresnet34': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnet34_ra-bdc0ccde.pth'),
-    'skresnet50': _cfg(),
-    'skresnet50d': _cfg(
-        first_conv='conv1.0'),
-    'skresnext50_32x4d': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnext50_ra-f40e40bf.pth'),
-}
-
-
 class SelectiveKernelBasic(nn.Module):
     expansion = 1
 
     def __init__(
             self,
             inplanes,
             planes,
@@ -162,75 +138,101 @@
             shortcut = self.downsample(shortcut)
         x += shortcut
         x = self.act(x)
         return x
 
 
 def _create_skresnet(variant, pretrained=False, **kwargs):
-    return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)
+    return build_model_with_cfg(
+        ResNet,
+        variant,
+        pretrained,
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'conv1', 'classifier': 'fc',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'skresnet18.ra_in1k': _cfg(hf_hub_id='timm/'),
+    'skresnet34.ra_in1k': _cfg(hf_hub_id='timm/'),
+    'skresnet50.untrained': _cfg(),
+    'skresnet50d.untrained': _cfg(
+        first_conv='conv1.0'),
+    'skresnext50_32x4d.ra_in1k': _cfg(hf_hub_id='timm/'),
+})
 
 
 @register_model
-def skresnet18(pretrained=False, **kwargs):
+def skresnet18(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Selective Kernel ResNet-18 model.
 
     Different from configs in Select Kernel paper or "Compounding the Performance Improvements..." this
     variation splits the input channels to the selective convolutions to keep param count down.
     """
     sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)
     model_args = dict(
         block=SelectiveKernelBasic, layers=[2, 2, 2, 2], block_args=dict(sk_kwargs=sk_kwargs),
         zero_init_last=False, **kwargs)
     return _create_skresnet('skresnet18', pretrained, **model_args)
 
 
 @register_model
-def skresnet34(pretrained=False, **kwargs):
+def skresnet34(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Selective Kernel ResNet-34 model.
 
     Different from configs in Select Kernel paper or "Compounding the Performance Improvements..." this
     variation splits the input channels to the selective convolutions to keep param count down.
     """
     sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)
     model_args = dict(
         block=SelectiveKernelBasic, layers=[3, 4, 6, 3], block_args=dict(sk_kwargs=sk_kwargs),
         zero_init_last=False, **kwargs)
     return _create_skresnet('skresnet34', pretrained, **model_args)
 
 
 @register_model
-def skresnet50(pretrained=False, **kwargs):
+def skresnet50(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Select Kernel ResNet-50 model.
 
     Different from configs in Select Kernel paper or "Compounding the Performance Improvements..." this
     variation splits the input channels to the selective convolutions to keep param count down.
     """
     sk_kwargs = dict(split_input=True)
     model_args = dict(
         block=SelectiveKernelBottleneck, layers=[3, 4, 6, 3], block_args=dict(sk_kwargs=sk_kwargs),
         zero_init_last=False, **kwargs)
     return _create_skresnet('skresnet50', pretrained, **model_args)
 
 
 @register_model
-def skresnet50d(pretrained=False, **kwargs):
+def skresnet50d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Select Kernel ResNet-50-D model.
 
     Different from configs in Select Kernel paper or "Compounding the Performance Improvements..." this
     variation splits the input channels to the selective convolutions to keep param count down.
     """
     sk_kwargs = dict(split_input=True)
     model_args = dict(
         block=SelectiveKernelBottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True,
         block_args=dict(sk_kwargs=sk_kwargs), zero_init_last=False, **kwargs)
     return _create_skresnet('skresnet50d', pretrained, **model_args)
 
 
 @register_model
-def skresnext50_32x4d(pretrained=False, **kwargs):
+def skresnext50_32x4d(pretrained=False, **kwargs) -> ResNet:
     """Constructs a Select Kernel ResNeXt50-32x4d model. This should be equivalent to
     the SKNet-50 model in the Select Kernel Paper
     """
     sk_kwargs = dict(rd_ratio=1/16, rd_divisor=32, split_input=False)
     model_args = dict(
         block=SelectiveKernelBottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4,
         block_args=dict(sk_kwargs=sk_kwargs), zero_init_last=False, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/swin_transformer.py` & `timm-0.9.0/timm/models/swin_transformer.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,94 +13,33 @@
 # Swin Transformer
 # Copyright (c) 2021 Microsoft
 # Licensed under The MIT License [see LICENSE for details]
 # Written by Ze Liu
 # --------------------------------------------------------
 import logging
 import math
-from typing import Optional
+from typing import Callable, List, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, to_ntuple, trunc_normal_, _assert
+from timm.layers import PatchEmbed, Mlp, DropPath, ClassifierHead, to_2tuple, to_ntuple, trunc_normal_, \
+    _assert, use_fused_attn
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
 from ._manipulate import checkpoint_seq, named_apply
-from ._registry import register_model
-from .vision_transformer import checkpoint_filter_fn, get_init_weights_vit
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
+from .vision_transformer import get_init_weights_vit
 
 __all__ = ['SwinTransformer']  # model_registry will add each entrypoint fn to this
 
 _logger = logging.getLogger(__name__)
 
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'swin_base_patch4_window12_384': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-
-    'swin_base_patch4_window7_224': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth',
-    ),
-
-    'swin_large_patch4_window12_384': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0),
-
-    'swin_large_patch4_window7_224': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth',
-    ),
-
-    'swin_small_patch4_window7_224': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth',
-    ),
-
-    'swin_tiny_patch4_window7_224': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',
-    ),
-
-    'swin_base_patch4_window12_384_in22k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0, num_classes=21841),
-
-    'swin_base_patch4_window7_224_in22k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth',
-        num_classes=21841),
-
-    'swin_large_patch4_window12_384_in22k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',
-        input_size=(3, 384, 384), crop_pct=1.0, num_classes=21841),
-
-    'swin_large_patch4_window7_224_in22k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth',
-        num_classes=21841),
-
-    'swin_s3_tiny_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_t-1d53f6a8.pth'
-    ),
-    'swin_s3_small_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_s-3bb4c69d.pth'
-    ),
-    'swin_s3_base_224': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_b-a1e95db4.pth'
-    )
-}
+_int_or_tuple_2_t = Union[int, Tuple[int, int]]
 
 
 def window_partition(x, window_size: int):
     """
     Args:
         x: (B, H, W, C)
         window_size (int): window size
@@ -122,57 +61,68 @@
         window_size (int): Window size
         H (int): Height of image
         W (int): Width of image
 
     Returns:
         x: (B, H, W, C)
     """
-    B = int(windows.shape[0] / (H * W / window_size / window_size))
-    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
-    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
+    C = windows.shape[-1]
+    x = windows.view(-1, H // window_size, W // window_size, window_size, window_size, C)
+    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)
     return x
 
 
-def get_relative_position_index(win_h, win_w):
+def get_relative_position_index(win_h: int, win_w: int):
     # get pair-wise relative position index for each token inside the window
     coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww
     coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
     relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
     relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
     relative_coords[:, :, 0] += win_h - 1  # shift to start from 0
     relative_coords[:, :, 1] += win_w - 1
     relative_coords[:, :, 0] *= 2 * win_w - 1
     return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
 
 
 class WindowAttention(nn.Module):
-    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
-    It supports both of shifted and non-shifted window.
-
-    Args:
-        dim (int): Number of input channels.
-        num_heads (int): Number of attention heads.
-        head_dim (int): Number of channels per head (dim // num_heads if not set)
-        window_size (tuple[int]): The height and width of the window.
-        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
-        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
-        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
+    """ Window based multi-head self attention (W-MSA) module with relative position bias.
+    It supports shifted and non-shifted windows.
     """
+    fused_attn: torch.jit.Final[bool]
 
-    def __init__(self, dim, num_heads, head_dim=None, window_size=7, qkv_bias=True, attn_drop=0., proj_drop=0.):
-
+    def __init__(
+            self,
+            dim: int,
+            num_heads: int,
+            head_dim: Optional[int] = None,
+            window_size: _int_or_tuple_2_t = 7,
+            qkv_bias: bool = True,
+            attn_drop: float = 0.,
+            proj_drop: float = 0.,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            num_heads: Number of attention heads.
+            head_dim: Number of channels per head (dim // num_heads if not set)
+            window_size: The height and width of the window.
+            qkv_bias:  If True, add a learnable bias to query, key, value.
+            attn_drop: Dropout ratio of attention weight.
+            proj_drop: Dropout ratio of output.
+        """
         super().__init__()
         self.dim = dim
         self.window_size = to_2tuple(window_size)  # Wh, Ww
         win_h, win_w = self.window_size
         self.window_area = win_h * win_w
         self.num_heads = num_heads
         head_dim = head_dim or dim // num_heads
         attn_dim = head_dim * num_heads
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn(experimental=True)  # NOTE not tested for prime-time yet
 
         # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH
         self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * win_h - 1) * (2 * win_w - 1), num_heads))
 
         # get pair-wise relative position index for each token inside the window
         self.register_buffer("relative_position_index", get_relative_position_index(win_h, win_w))
 
@@ -194,79 +144,112 @@
         """
         Args:
             x: input features with shape of (num_windows*B, N, C)
             mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
         """
         B_, N, C = x.shape
         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
+        q, k, v = qkv.unbind(0)
 
-        q = q * self.scale
-        attn = (q @ k.transpose(-2, -1))
-        attn = attn + self._get_rel_pos_bias()
-
-        if mask is not None:
-            num_win = mask.shape[0]
-            attn = attn.view(B_ // num_win, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
-            attn = attn.view(-1, self.num_heads, N, N)
-            attn = self.softmax(attn)
+        if self.fused_attn:
+            attn_mask = self._get_rel_pos_bias()
+            if mask is not None:
+                num_win = mask.shape[0]
+                mask = mask.view(1, num_win, 1, N, N).expand(B_ // num_win, -1, self.num_heads, -1, -1)
+                attn_mask = attn_mask + mask.reshape(-1, self.num_heads, N, N)
+            x = torch.nn.functional.scaled_dot_product_attention(
+                q, k, v,
+                attn_mask=attn_mask,
+                dropout_p=self.attn_drop.p,
+            )
         else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn + self._get_rel_pos_bias()
+            if mask is not None:
+                num_win = mask.shape[0]
+                attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
+                attn = attn.view(-1, self.num_heads, N, N)
             attn = self.softmax(attn)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-        attn = self.attn_drop(attn)
-
-        x = (attn @ v).transpose(1, 2).reshape(B_, N, -1)
+        x = x.transpose(1, 2).reshape(B_, N, -1)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class SwinTransformerBlock(nn.Module):
-    r""" Swin Transformer Block.
-
-    Args:
-        dim (int): Number of input channels.
-        input_resolution (tuple[int]): Input resulotion.
-        window_size (int): Window size.
-        num_heads (int): Number of attention heads.
-        head_dim (int): Enforce the number of channels per head
-        shift_size (int): Shift size for SW-MSA.
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-        drop (float, optional): Dropout rate. Default: 0.0
-        attn_drop (float, optional): Attention dropout rate. Default: 0.0
-        drop_path (float, optional): Stochastic depth rate. Default: 0.0
-        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
+    """ Swin Transformer Block.
     """
 
     def __init__(
-            self, dim, input_resolution, num_heads=4, head_dim=None, window_size=7, shift_size=0,
-            mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
-            act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim: int,
+            input_resolution: _int_or_tuple_2_t,
+            num_heads: int = 4,
+            head_dim: Optional[int] = None,
+            window_size: _int_or_tuple_2_t = 7,
+            shift_size: int = 0,
+            mlp_ratio: float = 4.,
+            qkv_bias: bool = True,
+            proj_drop: float = 0.,
+            attn_drop: float = 0.,
+            drop_path: float = 0.,
+            act_layer: Callable = nn.GELU,
+            norm_layer: Callable = nn.LayerNorm,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            input_resolution: Input resolution.
+            window_size: Window size.
+            num_heads: Number of attention heads.
+            head_dim: Enforce the number of channels per head
+            shift_size: Shift size for SW-MSA.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: If True, add a learnable bias to query, key, value.
+            proj_drop: Dropout rate.
+            attn_drop: Attention dropout rate.
+            drop_path: Stochastic depth rate.
+            act_layer: Activation layer.
+            norm_layer: Normalization layer.
+        """
         super().__init__()
         self.dim = dim
         self.input_resolution = input_resolution
         self.window_size = window_size
         self.shift_size = shift_size
         self.mlp_ratio = mlp_ratio
         if min(self.input_resolution) <= self.window_size:
             # if window size is larger than input resolution, we don't partition windows
             self.shift_size = 0
             self.window_size = min(self.input_resolution)
         assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"
 
         self.norm1 = norm_layer(dim)
         self.attn = WindowAttention(
-            dim, num_heads=num_heads, head_dim=head_dim, window_size=to_2tuple(self.window_size),
-            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+            dim,
+            num_heads=num_heads,
+            head_dim=head_dim,
+            window_size=to_2tuple(self.window_size),
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
 
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
         if self.shift_size > 0:
             # calculate attention mask for SW-MSA
             H, W = self.input_resolution
             img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
             cnt = 0
             for h in (
@@ -281,25 +264,23 @@
                     cnt += 1
             mask_windows = window_partition(img_mask, self.window_size)  # num_win, window_size, window_size, 1
             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
             attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
         else:
             attn_mask = None
-
         self.register_buffer("attn_mask", attn_mask)
 
     def forward(self, x):
-        H, W = self.input_resolution
-        B, L, C = x.shape
-        _assert(L == H * W, "input feature has wrong size")
+        B, H, W, C = x.shape
+        _assert(H == self.input_resolution[0], "input feature has wrong size")
+        _assert(W == self.input_resolution[1], "input feature has wrong size")
 
         shortcut = x
         x = self.norm1(x)
-        x = x.view(B, H, W, C)
 
         # cyclic shift
         if self.shift_size > 0:
             shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
         else:
             shifted_x = x
 
@@ -315,387 +296,504 @@
         shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
 
         # reverse cyclic shift
         if self.shift_size > 0:
             x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
         else:
             x = shifted_x
-        x = x.view(B, H * W, C)
 
         # FFN
         x = shortcut + self.drop_path(x)
-        x = x + self.drop_path(self.mlp(self.norm2(x)))
 
+        x = x.reshape(B, -1, C)
+        x = x + self.drop_path(self.mlp(self.norm2(x)))
+        x = x.reshape(B, H, W, C)
         return x
 
 
 class PatchMerging(nn.Module):
-    r""" Patch Merging Layer.
-
-    Args:
-        input_resolution (tuple[int]): Resolution of input feature.
-        dim (int): Number of input channels.
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
+    """ Patch Merging Layer.
     """
 
-    def __init__(self, input_resolution, dim, out_dim=None, norm_layer=nn.LayerNorm):
+    def __init__(
+            self,
+            dim: int,
+            out_dim: Optional[int] = None,
+            norm_layer: Callable = nn.LayerNorm,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            out_dim: Number of output channels (or 2 * dim if None)
+            norm_layer: Normalization layer.
+        """
         super().__init__()
-        self.input_resolution = input_resolution
         self.dim = dim
         self.out_dim = out_dim or 2 * dim
         self.norm = norm_layer(4 * dim)
         self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)
 
     def forward(self, x):
-        """
-        x: B, H*W, C
-        """
-        H, W = self.input_resolution
-        B, L, C = x.shape
-        _assert(L == H * W, "input feature has wrong size")
-        _assert(H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even.")
-
-        x = x.view(B, H, W, C)
-
-        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
-        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
-        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
-        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
-        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
-        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C
-
+        B, H, W, C = x.shape
+        _assert(H % 2 == 0, f"x height ({H}) is not even.")
+        _assert(W % 2 == 0, f"x width ({W}) is not even.")
+        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)
         x = self.norm(x)
         x = self.reduction(x)
-
         return x
 
 
-class BasicLayer(nn.Module):
+class SwinTransformerStage(nn.Module):
     """ A basic Swin Transformer layer for one stage.
-
-    Args:
-        dim (int): Number of input channels.
-        input_resolution (tuple[int]): Input resolution.
-        depth (int): Number of blocks.
-        num_heads (int): Number of attention heads.
-        head_dim (int): Channels per head (dim // num_heads if not set)
-        window_size (int): Local window size.
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-        drop (float, optional): Dropout rate. Default: 0.0
-        attn_drop (float, optional): Attention dropout rate. Default: 0.0
-        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
-        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
-        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
     """
 
     def __init__(
-            self, dim, out_dim, input_resolution, depth, num_heads=4, head_dim=None,
-            window_size=7, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,
-            drop_path=0., norm_layer=nn.LayerNorm, downsample=None):
-
+            self,
+            dim: int,
+            out_dim: int,
+            input_resolution: Tuple[int, int],
+            depth: int,
+            downsample: bool = True,
+            num_heads: int = 4,
+            head_dim: Optional[int] = None,
+            window_size: _int_or_tuple_2_t = 7,
+            mlp_ratio: float = 4.,
+            qkv_bias: bool = True,
+            proj_drop: float = 0.,
+            attn_drop: float = 0.,
+            drop_path: Union[List[float], float] = 0.,
+            norm_layer: Callable = nn.LayerNorm,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            input_resolution: Input resolution.
+            depth: Number of blocks.
+            downsample: Downsample layer at the end of the layer.
+            num_heads: Number of attention heads.
+            head_dim: Channels per head (dim // num_heads if not set)
+            window_size: Local window size.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: If True, add a learnable bias to query, key, value.
+            proj_drop: Projection dropout rate.
+            attn_drop: Attention dropout rate.
+            drop_path: Stochastic depth rate.
+            norm_layer: Normalization layer.
+        """
         super().__init__()
         self.dim = dim
         self.input_resolution = input_resolution
+        self.output_resolution = tuple(i // 2 for i in input_resolution) if downsample else input_resolution
         self.depth = depth
         self.grad_checkpointing = False
 
+        # patch merging layer
+        if downsample:
+            self.downsample = PatchMerging(
+                dim=dim,
+                out_dim=out_dim,
+                norm_layer=norm_layer,
+            )
+        else:
+            assert dim == out_dim
+            self.downsample = nn.Identity()
+
         # build blocks
         self.blocks = nn.Sequential(*[
             SwinTransformerBlock(
-                dim=dim, input_resolution=input_resolution, num_heads=num_heads, head_dim=head_dim,
-                window_size=window_size, shift_size=0 if (i % 2 == 0) else window_size // 2,
-                mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop,
-                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer)
+                dim=out_dim,
+                input_resolution=self.output_resolution,
+                num_heads=num_heads,
+                head_dim=head_dim,
+                window_size=window_size,
+                shift_size=0 if (i % 2 == 0) else window_size // 2,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop,
+                attn_drop=attn_drop,
+                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
+                norm_layer=norm_layer,
+            )
             for i in range(depth)])
 
-        # patch merging layer
-        if downsample is not None:
-            self.downsample = downsample(input_resolution, dim=dim, out_dim=out_dim, norm_layer=norm_layer)
-        else:
-            self.downsample = None
-
     def forward(self, x):
+        x = self.downsample(x)
+
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
             x = self.blocks(x)
-        if self.downsample is not None:
-            x = self.downsample(x)
         return x
 
 
 class SwinTransformer(nn.Module):
-    r""" Swin Transformer
-        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
-          https://arxiv.org/pdf/2103.14030
+    """ Swin Transformer
 
-    Args:
-        img_size (int | tuple(int)): Input image size. Default 224
-        patch_size (int | tuple(int)): Patch size. Default: 4
-        in_chans (int): Number of input image channels. Default: 3
-        num_classes (int): Number of classes for classification head. Default: 1000
-        embed_dim (int): Patch embedding dimension. Default: 96
-        depths (tuple(int)): Depth of each Swin Transformer layer.
-        num_heads (tuple(int)): Number of attention heads in different layers.
-        head_dim (int, tuple(int)):
-        window_size (int): Window size. Default: 7
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
-        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
-        drop_rate (float): Dropout rate. Default: 0
-        attn_drop_rate (float): Attention dropout rate. Default: 0
-        drop_path_rate (float): Stochastic depth rate. Default: 0.1
-        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
-        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
-        patch_norm (bool): If True, add normalization after patch embedding. Default: True
+    A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
+          https://arxiv.org/pdf/2103.14030
     """
 
     def __init__(
-            self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, global_pool='avg',
-            embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), head_dim=None,
-            window_size=7, mlp_ratio=4., qkv_bias=True,
-            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
-            norm_layer=nn.LayerNorm, ape=False, patch_norm=True, weight_init='', **kwargs):
+            self,
+            img_size: _int_or_tuple_2_t = 224,
+            patch_size: int = 4,
+            in_chans: int = 3,
+            num_classes: int = 1000,
+            global_pool: str = 'avg',
+            embed_dim: int = 96,
+            depths: Tuple[int, ...] = (2, 2, 6, 2),
+            num_heads: Tuple[int, ...] = (3, 6, 12, 24),
+            head_dim: Optional[int] = None,
+            window_size: _int_or_tuple_2_t = 7,
+            mlp_ratio: float = 4.,
+            qkv_bias: bool = True,
+            drop_rate: float = 0.,
+            proj_drop_rate: float = 0.,
+            attn_drop_rate: float = 0.,
+            drop_path_rate: float = 0.1,
+            norm_layer: Union[str, Callable] = nn.LayerNorm,
+            weight_init: str = '',
+            **kwargs,
+    ):
+        """
+        Args:
+            img_size: Input image size.
+            patch_size: Patch size.
+            in_chans: Number of input image channels.
+            num_classes: Number of classes for classification head.
+            embed_dim: Patch embedding dimension.
+            depths: Depth of each Swin Transformer layer.
+            num_heads: Number of attention heads in different layers.
+            head_dim: Dimension of self-attention heads.
+            window_size: Window size.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: If True, add a learnable bias to query, key, value.
+            drop_rate: Dropout rate.
+            attn_drop_rate (float): Attention dropout rate.
+            drop_path_rate (float): Stochastic depth rate.
+            norm_layer (nn.Module): Normalization layer.
+        """
         super().__init__()
         assert global_pool in ('', 'avg')
         self.num_classes = num_classes
         self.global_pool = global_pool
+        self.output_fmt = 'NHWC'
+
         self.num_layers = len(depths)
         self.embed_dim = embed_dim
         self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
+        self.feature_info = []
+
+        if not isinstance(embed_dim, (tuple, list)):
+            embed_dim = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
 
         # split image into non-overlapping patches
         self.patch_embed = PatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
-            norm_layer=norm_layer if patch_norm else None)
-        num_patches = self.patch_embed.num_patches
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim[0],
+            norm_layer=norm_layer,
+            output_fmt='NHWC',
+        )
         self.patch_grid = self.patch_embed.grid_size
 
-        # absolute position embedding
-        self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim)) if ape else None
-        self.pos_drop = nn.Dropout(p=drop_rate)
-
         # build layers
-        if not isinstance(embed_dim, (tuple, list)):
-            embed_dim = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
-        embed_out_dim = embed_dim[1:] + [None]
         head_dim = to_ntuple(self.num_layers)(head_dim)
         window_size = to_ntuple(self.num_layers)(window_size)
         mlp_ratio = to_ntuple(self.num_layers)(mlp_ratio)
-        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule
+        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]
         layers = []
+        in_dim = embed_dim[0]
+        scale = 1
         for i in range(self.num_layers):
-            layers += [BasicLayer(
-                dim=embed_dim[i],
-                out_dim=embed_out_dim[i],
-                input_resolution=(self.patch_grid[0] // (2 ** i), self.patch_grid[1] // (2 ** i)),
+            out_dim = embed_dim[i]
+            layers += [SwinTransformerStage(
+                dim=in_dim,
+                out_dim=out_dim,
+                input_resolution=(
+                    self.patch_grid[0] // scale,
+                    self.patch_grid[1] // scale
+                ),
                 depth=depths[i],
+                downsample=i > 0,
                 num_heads=num_heads[i],
                 head_dim=head_dim[i],
                 window_size=window_size[i],
                 mlp_ratio=mlp_ratio[i],
                 qkv_bias=qkv_bias,
-                drop=drop_rate,
+                proj_drop=proj_drop_rate,
                 attn_drop=attn_drop_rate,
-                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],
+                drop_path=dpr[i],
                 norm_layer=norm_layer,
-                downsample=PatchMerging if (i < self.num_layers - 1) else None
             )]
+            in_dim = out_dim
+            if i > 0:
+                scale *= 2
+            self.feature_info += [dict(num_chs=out_dim, reduction=4 * scale, module=f'layers.{i}')]
         self.layers = nn.Sequential(*layers)
 
         self.norm = norm_layer(self.num_features)
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+            input_fmt=self.output_fmt,
+        )
         if weight_init != 'skip':
             self.init_weights(weight_init)
 
     @torch.jit.ignore
     def init_weights(self, mode=''):
         assert mode in ('jax', 'jax_nlhb', 'moco', '')
-        if self.absolute_pos_embed is not None:
-            trunc_normal_(self.absolute_pos_embed, std=.02)
         head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.
         named_apply(get_init_weights_vit(mode, head_bias=head_bias), self)
 
     @torch.jit.ignore
     def no_weight_decay(self):
-        nwd = {'absolute_pos_embed'}
+        nwd = set()
         for n, _ in self.named_parameters():
             if 'relative_position_bias_table' in n:
                 nwd.add(n)
         return nwd
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(
-            stem=r'^absolute_pos_embed|patch_embed',  # stem and embed
+            stem=r'^patch_embed',  # stem and embed
             blocks=r'^layers\.(\d+)' if coarse else [
                 (r'^layers\.(\d+).downsample', (0,)),
                 (r'^layers\.(\d+)\.\w+\.(\d+)', None),
                 (r'^norm', (99999,)),
             ]
         )
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         for l in self.layers:
             l.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
-        return self.head
+        return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool=None):
         self.num_classes = num_classes
-        if global_pool is not None:
-            assert global_pool in ('', 'avg')
-            self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head.reset(num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
         x = self.patch_embed(x)
-        if self.absolute_pos_embed is not None:
-            x = x + self.absolute_pos_embed
-        x = self.pos_drop(x)
         x = self.layers(x)
-        x = self.norm(x)  # B L C
+        x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool == 'avg':
-            x = x.mean(dim=1)
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
+def checkpoint_filter_fn(state_dict, model):
+    """ convert patch embedding weight from manual patchify + linear proj to conv"""
+    if 'head.fc.weight' in state_dict:
+        return state_dict
+    import re
+    out_dict = {}
+    state_dict = state_dict.get('model', state_dict)
+    state_dict = state_dict.get('state_dict', state_dict)
+    for k, v in state_dict.items():
+        k = re.sub(r'layers.(\d+).downsample', lambda x: f'layers.{int(x.group(1)) + 1}.downsample', k)
+        k = k.replace('head.', 'head.fc.')
+        out_dict[k] = v
+    return out_dict
+
+
 def _create_swin_transformer(variant, pretrained=False, **kwargs):
+    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 3, 1))))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
+
     model = build_model_with_cfg(
         SwinTransformer, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs)
 
     return model
 
 
-@register_model
-def swin_base_patch4_window12_384(pretrained=False, **kwargs):
-    """ Swin-B @ 384x384, pretrained ImageNet-22k, fine tune 1k
-    """
-    model_kwargs = dict(
-        patch_size=4, window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer('swin_base_patch4_window12_384', pretrained=pretrained, **model_kwargs)
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',
+        'license': 'mit', **kwargs
+    }
 
 
-@register_model
-def swin_base_patch4_window7_224(pretrained=False, **kwargs):
-    """ Swin-B @ 224x224, pretrained ImageNet-22k, fine tune 1k
-    """
-    model_kwargs = dict(
-        patch_size=4, window_size=7, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer('swin_base_patch4_window7_224', pretrained=pretrained, **model_kwargs)
+default_cfgs = generate_default_cfgs({
+    'swin_small_patch4_window7_224.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22kto1k_finetune.pth', ),
+    'swin_base_patch4_window7_224.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth',),
+    'swin_base_patch4_window12_384.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+    'swin_large_patch4_window7_224.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth',),
+    'swin_large_patch4_window12_384.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
 
+    'swin_tiny_patch4_window7_224.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',),
+    'swin_small_patch4_window7_224.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth',),
+    'swin_base_patch4_window7_224.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth',),
+    'swin_base_patch4_window12_384.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),
+
+    # tiny 22k pretrain is worse than 1k, so moved after (untagged priority is based on order)
+    'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22kto1k_finetune.pth',),
+
+    'swin_tiny_patch4_window7_224.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22k.pth',
+        num_classes=21841),
+    'swin_small_patch4_window7_224.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22k.pth',
+        num_classes=21841),
+    'swin_base_patch4_window7_224.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth',
+        num_classes=21841),
+    'swin_base_patch4_window12_384.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21841),
+    'swin_large_patch4_window7_224.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth',
+        num_classes=21841),
+    'swin_large_patch4_window12_384.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21841),
 
-@register_model
-def swin_large_patch4_window12_384(pretrained=False, **kwargs):
-    """ Swin-L @ 384x384, pretrained ImageNet-22k, fine tune 1k
-    """
-    model_kwargs = dict(
-        patch_size=4, window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48), **kwargs)
-    return _create_swin_transformer('swin_large_patch4_window12_384', pretrained=pretrained, **model_kwargs)
+    'swin_s3_tiny_224.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_t-1d53f6a8.pth'),
+    'swin_s3_small_224.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_s-3bb4c69d.pth'),
+    'swin_s3_base_224.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_b-a1e95db4.pth'),
+})
 
 
 @register_model
-def swin_large_patch4_window7_224(pretrained=False, **kwargs):
-    """ Swin-L @ 224x224, pretrained ImageNet-22k, fine tune 1k
+def swin_tiny_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-T @ 224x224, trained ImageNet-1k
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=7, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48), **kwargs)
-    return _create_swin_transformer('swin_large_patch4_window7_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=4, window_size=7, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer(
+        'swin_tiny_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_small_patch4_window7_224(pretrained=False, **kwargs):
-    """ Swin-S @ 224x224, trained ImageNet-1k
+def swin_small_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-S @ 224x224
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=7, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer('swin_small_patch4_window7_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=4, window_size=7, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer(
+        'swin_small_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_tiny_patch4_window7_224(pretrained=False, **kwargs):
-    """ Swin-T @ 224x224, trained ImageNet-1k
+def swin_base_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-B @ 224x224
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=7, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer('swin_tiny_patch4_window7_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=4, window_size=7, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))
+    return _create_swin_transformer(
+        'swin_base_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_base_patch4_window12_384_in22k(pretrained=False, **kwargs):
-    """ Swin-B @ 384x384, trained ImageNet-22k
+def swin_base_patch4_window12_384(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-B @ 384x384
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer('swin_base_patch4_window12_384_in22k', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=4, window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))
+    return _create_swin_transformer(
+        'swin_base_patch4_window12_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_base_patch4_window7_224_in22k(pretrained=False, **kwargs):
-    """ Swin-B @ 224x224, trained ImageNet-22k
+def swin_large_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-L @ 224x224
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=7, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer('swin_base_patch4_window7_224_in22k', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=4, window_size=7, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48))
+    return _create_swin_transformer(
+        'swin_large_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_large_patch4_window12_384_in22k(pretrained=False, **kwargs):
-    """ Swin-L @ 384x384, trained ImageNet-22k
+def swin_large_patch4_window12_384(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-L @ 384x384
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48), **kwargs)
-    return _create_swin_transformer('swin_large_patch4_window12_384_in22k', pretrained=pretrained, **model_kwargs)
+    model_args = dict(patch_size=4, window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48))
+    return _create_swin_transformer(
+        'swin_large_patch4_window12_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_large_patch4_window7_224_in22k(pretrained=False, **kwargs):
-    """ Swin-L @ 224x224, trained ImageNet-22k
+def swin_s3_tiny_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-S3-T @ 224x224, https://arxiv.org/abs/2111.14725
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=7, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48), **kwargs)
-    return _create_swin_transformer('swin_large_patch4_window7_224_in22k', pretrained=pretrained, **model_kwargs)
+    model_args = dict(
+        patch_size=4, window_size=(7, 7, 14, 7), embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer('swin_s3_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_s3_tiny_224(pretrained=False, **kwargs):
-    """ Swin-S3-T @ 224x224, ImageNet-1k. https://arxiv.org/abs/2111.14725
+def swin_s3_small_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-S3-S @ 224x224, https://arxiv.org/abs/2111.14725
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=(7, 7, 14, 7), embed_dim=96, depths=(2, 2, 6, 2),
-        num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer('swin_s3_tiny_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(
+        patch_size=4, window_size=(14, 14, 14, 7), embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer('swin_s3_small_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swin_s3_small_224(pretrained=False, **kwargs):
-    """ Swin-S3-S @ 224x224, trained ImageNet-1k. https://arxiv.org/abs/2111.14725
+def swin_s3_base_224(pretrained=False, **kwargs) -> SwinTransformer:
+    """ Swin-S3-B @ 224x224, https://arxiv.org/abs/2111.14725
     """
-    model_kwargs = dict(
-        patch_size=4, window_size=(14, 14, 14, 7), embed_dim=96, depths=(2, 2, 18, 2),
-        num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer('swin_s3_small_224', pretrained=pretrained, **model_kwargs)
+    model_args = dict(
+        patch_size=4, window_size=(7, 7, 14, 7), embed_dim=96, depths=(2, 2, 30, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer('swin_s3_base_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
-@register_model
-def swin_s3_base_224(pretrained=False, **kwargs):
-    """ Swin-S3-B @ 224x224, trained ImageNet-1k. https://arxiv.org/abs/2111.14725
-    """
-    model_kwargs = dict(
-        patch_size=4, window_size=(7, 7, 14, 7), embed_dim=96, depths=(2, 2, 30, 2),
-        num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer('swin_s3_base_224', pretrained=pretrained, **model_kwargs)
-
+register_model_deprecations(__name__, {
+    'swin_base_patch4_window7_224_in22k': 'swin_base_patch4_window7_224.ms_in22k',
+    'swin_base_patch4_window12_384_in22k': 'swin_base_patch4_window12_384.ms_in22k',
+    'swin_large_patch4_window7_224_in22k': 'swin_large_patch4_window7_224.ms_in22k',
+    'swin_large_patch4_window12_384_in22k': 'swin_large_patch4_window12_384.ms_in22k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/swin_transformer_v2.py` & `timm-0.9.0/timm/models/swin_transformer_v2.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,92 +9,30 @@
 # --------------------------------------------------------
 # Swin Transformer V2
 # Copyright (c) 2022 Microsoft
 # Licensed under The MIT License [see LICENSE for details]
 # Written by Ze Liu
 # --------------------------------------------------------
 import math
-from typing import Tuple, Optional
+from typing import Callable, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.utils.checkpoint as checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert
+from timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, ClassifierHead
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['SwinTransformerV2']  # model_registry will add each entrypoint fn to this
 
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'swinv2_tiny_window8_256': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_tiny_window16_256': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window16_256.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_small_window8_256': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window8_256.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_small_window16_256': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window16_256.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_base_window8_256': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window8_256.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_base_window16_256': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window16_256.pth',
-        input_size=(3, 256, 256)
-    ),
-
-    'swinv2_base_window12_192_22k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth',
-        num_classes=21841, input_size=(3, 192, 192)
-    ),
-    'swinv2_base_window12to16_192to256_22kft1k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_base_window12to24_192to384_22kft1k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.pth',
-        input_size=(3, 384, 384), crop_pct=1.0,
-    ),
-    'swinv2_large_window12_192_22k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth',
-        num_classes=21841, input_size=(3, 192, 192)
-    ),
-    'swinv2_large_window12to16_192to256_22kft1k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.pth',
-        input_size=(3, 256, 256)
-    ),
-    'swinv2_large_window12to24_192to384_22kft1k': _cfg(
-        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.pth',
-        input_size=(3, 384, 384), crop_pct=1.0,
-    ),
-}
+_int_or_tuple_2_t = Union[int, Tuple[int, int]]
 
 
 def window_partition(x, window_size: Tuple[int, int]):
     """
     Args:
         x: (B, H, W, C)
         window_size (int): window size
@@ -116,17 +54,17 @@
         window_size (Tuple[int, int]): Window size
         img_size (Tuple[int, int]): Image size
 
     Returns:
         x: (B, H, W, C)
     """
     H, W = img_size
-    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))
-    x = windows.view(B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1)
-    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
+    C = windows.shape[-1]
+    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)
+    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)
     return x
 
 
 class WindowAttention(nn.Module):
     r""" Window based multi-head self attention (W-MSA) module with relative position bias.
     It supports both of shifted and non-shifted window.
 
@@ -137,17 +75,23 @@
         qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
         attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
         proj_drop (float, optional): Dropout ratio of output. Default: 0.0
         pretrained_window_size (tuple[int]): The height and width of the window in pre-training.
     """
 
     def __init__(
-            self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,
-            pretrained_window_size=[0, 0]):
-
+            self,
+            dim,
+            window_size,
+            num_heads,
+            qkv_bias=True,
+            attn_drop=0.,
+            proj_drop=0.,
+            pretrained_window_size=[0, 0],
+    ):
         super().__init__()
         self.dim = dim
         self.window_size = window_size  # Wh, Ww
         self.pretrained_window_size = pretrained_window_size
         self.num_heads = num_heads
 
         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))
@@ -227,70 +171,93 @@
         relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(
             self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
         relative_position_bias = 16 * torch.sigmoid(relative_position_bias)
         attn = attn + relative_position_bias.unsqueeze(0)
 
         if mask is not None:
-            nW = mask.shape[0]
-            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
+            num_win = mask.shape[0]
+            attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
             attn = attn.view(-1, self.num_heads, N, N)
             attn = self.softmax(attn)
         else:
             attn = self.softmax(attn)
 
         attn = self.attn_drop(attn)
 
         x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
-class SwinTransformerBlock(nn.Module):
-    r""" Swin Transformer Block.
-
-    Args:
-        dim (int): Number of input channels.
-        input_resolution (tuple[int]): Input resolution.
-        num_heads (int): Number of attention heads.
-        window_size (int): Window size.
-        shift_size (int): Shift size for SW-MSA.
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-        drop (float, optional): Dropout rate. Default: 0.0
-        attn_drop (float, optional): Attention dropout rate. Default: 0.0
-        drop_path (float, optional): Stochastic depth rate. Default: 0.0
-        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
-        pretrained_window_size (int): Window size in pretraining.
+class SwinTransformerV2Block(nn.Module):
+    """ Swin Transformer Block.
     """
 
     def __init__(
-            self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
-            mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
-            act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):
+            self,
+            dim,
+            input_resolution,
+            num_heads,
+            window_size=7,
+            shift_size=0,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            pretrained_window_size=0,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            input_resolution: Input resolution.
+            num_heads: Number of attention heads.
+            window_size: Window size.
+            shift_size: Shift size for SW-MSA.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: If True, add a learnable bias to query, key, value.
+            proj_drop: Dropout rate.
+            attn_drop: Attention dropout rate.
+            drop_path: Stochastic depth rate.
+            act_layer: Activation layer.
+            norm_layer: Normalization layer.
+            pretrained_window_size: Window size in pretraining.
+        """
         super().__init__()
         self.dim = dim
         self.input_resolution = to_2tuple(input_resolution)
         self.num_heads = num_heads
         ws, ss = self._calc_window_shift(window_size, shift_size)
         self.window_size: Tuple[int, int] = ws
         self.shift_size: Tuple[int, int] = ss
         self.window_area = self.window_size[0] * self.window_size[1]
         self.mlp_ratio = mlp_ratio
 
         self.attn = WindowAttention(
-            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
-            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,
-            pretrained_window_size=to_2tuple(pretrained_window_size))
+            dim,
+            window_size=to_2tuple(self.window_size),
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            pretrained_window_size=to_2tuple(pretrained_window_size),
+        )
         self.norm1 = norm_layer(dim)
         self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
         self.norm2 = norm_layer(dim)
         self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         if any(self.shift_size):
             # calculate attention mask for SW-MSA
             H, W = self.input_resolution
             img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
@@ -308,28 +275,25 @@
             mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
             mask_windows = mask_windows.view(-1, self.window_area)
             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
             attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
         else:
             attn_mask = None
 
-        self.register_buffer("attn_mask", attn_mask)
+        self.register_buffer("attn_mask", attn_mask, persistent=False)
 
     def _calc_window_shift(self, target_window_size, target_shift_size) -> Tuple[Tuple[int, int], Tuple[int, int]]:
         target_window_size = to_2tuple(target_window_size)
         target_shift_size = to_2tuple(target_shift_size)
         window_size = [r if r <= w else w for r, w in zip(self.input_resolution, target_window_size)]
         shift_size = [0 if r <= w else s for r, w, s in zip(self.input_resolution, window_size, target_shift_size)]
         return tuple(window_size), tuple(shift_size)
 
     def _attn(self, x):
-        H, W = self.input_resolution
-        B, L, C = x.shape
-        _assert(L == H * W, "input feature has wrong size")
-        x = x.view(B, H, W, C)
+        B, H, W, C = x.shape
 
         # cyclic shift
         has_shift = any(self.shift_size)
         if has_shift:
             shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))
         else:
             shifted_x = x
@@ -346,228 +310,267 @@
         shifted_x = window_reverse(attn_windows, self.window_size, self.input_resolution)  # B H' W' C
 
         # reverse cyclic shift
         if has_shift:
             x = torch.roll(shifted_x, shifts=self.shift_size, dims=(1, 2))
         else:
             x = shifted_x
-        x = x.view(B, H * W, C)
         return x
 
     def forward(self, x):
+        B, H, W, C = x.shape
         x = x + self.drop_path1(self.norm1(self._attn(x)))
+        x = x.reshape(B, -1, C)
         x = x + self.drop_path2(self.norm2(self.mlp(x)))
+        x = x.reshape(B, H, W, C)
         return x
 
 
 class PatchMerging(nn.Module):
-    r""" Patch Merging Layer.
-
-    Args:
-        input_resolution (tuple[int]): Resolution of input feature.
-        dim (int): Number of input channels.
-        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
+    """ Patch Merging Layer.
     """
 
-    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
+    def __init__(self, dim, out_dim=None, norm_layer=nn.LayerNorm):
+        """
+        Args:
+            dim (int): Number of input channels.
+            out_dim (int): Number of output channels (or 2 * dim if None)
+            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
+        """
         super().__init__()
-        self.input_resolution = input_resolution
         self.dim = dim
-        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
-        self.norm = norm_layer(2 * dim)
+        self.out_dim = out_dim or 2 * dim
+        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)
+        self.norm = norm_layer(self.out_dim)
 
     def forward(self, x):
-        """
-        x: B, H*W, C
-        """
-        H, W = self.input_resolution
-        B, L, C = x.shape
-        _assert(L == H * W, "input feature has wrong size")
-        _assert(H % 2 == 0, f"x size ({H}*{W}) are not even.")
-        _assert(W % 2 == 0, f"x size ({H}*{W}) are not even.")
-
-        x = x.view(B, H, W, C)
-
-        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
-        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
-        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
-        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
-        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
-        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C
-
+        B, H, W, C = x.shape
+        _assert(H % 2 == 0, f"x height ({H}) is not even.")
+        _assert(W % 2 == 0, f"x width ({W}) is not even.")
+        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)
         x = self.reduction(x)
         x = self.norm(x)
-
         return x
 
 
-class BasicLayer(nn.Module):
-    """ A basic Swin Transformer layer for one stage.
-
-    Args:
-        dim (int): Number of input channels.
-        input_resolution (tuple[int]): Input resolution.
-        depth (int): Number of blocks.
-        num_heads (int): Number of attention heads.
-        window_size (int): Local window size.
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
-        drop (float, optional): Dropout rate. Default: 0.0
-        attn_drop (float, optional): Attention dropout rate. Default: 0.0
-        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
-        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
-        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
-        pretrained_window_size (int): Local window size in pre-training.
+class SwinTransformerV2Stage(nn.Module):
+    """ A Swin Transformer V2 Stage.
     """
 
     def __init__(
-            self, dim, input_resolution, depth, num_heads, window_size,
-            mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
-            norm_layer=nn.LayerNorm, downsample=None, pretrained_window_size=0):
-
+            self,
+            dim,
+            out_dim,
+            input_resolution,
+            depth,
+            num_heads,
+            window_size,
+            downsample=False,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            norm_layer=nn.LayerNorm,
+            pretrained_window_size=0,
+            output_nchw=False,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            input_resolution: Input resolution.
+            depth: Number of blocks.
+            num_heads: Number of attention heads.
+            window_size: Local window size.
+            downsample: Use downsample layer at start of the block.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: If True, add a learnable bias to query, key, value.
+            proj_drop: Projection dropout rate
+            attn_drop: Attention dropout rate.
+            drop_path: Stochastic depth rate.
+            norm_layer: Normalization layer.
+            pretrained_window_size: Local window size in pretraining.
+            output_nchw: Output tensors on NCHW format instead of NHWC.
+        """
         super().__init__()
         self.dim = dim
         self.input_resolution = input_resolution
+        self.output_resolution = tuple(i // 2 for i in input_resolution) if downsample else input_resolution
         self.depth = depth
+        self.output_nchw = output_nchw
         self.grad_checkpointing = False
 
+        # patch merging / downsample layer
+        if downsample:
+            self.downsample = PatchMerging(dim=dim, out_dim=out_dim, norm_layer=norm_layer)
+        else:
+            assert dim == out_dim
+            self.downsample = nn.Identity()
+
         # build blocks
         self.blocks = nn.ModuleList([
-            SwinTransformerBlock(
-                dim=dim, input_resolution=input_resolution,
-                num_heads=num_heads, window_size=window_size,
+            SwinTransformerV2Block(
+                dim=out_dim,
+                input_resolution=self.output_resolution,
+                num_heads=num_heads,
+                window_size=window_size,
                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                 mlp_ratio=mlp_ratio,
                 qkv_bias=qkv_bias,
-                drop=drop, attn_drop=attn_drop,
+                proj_drop=proj_drop,
+                attn_drop=attn_drop,
                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                 norm_layer=norm_layer,
-                pretrained_window_size=pretrained_window_size)
+                pretrained_window_size=pretrained_window_size,
+            )
             for i in range(depth)])
 
-        # patch merging layer
-        if downsample is not None:
-            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
-        else:
-            self.downsample = nn.Identity()
-
     def forward(self, x):
+        x = self.downsample(x)
+
         for blk in self.blocks:
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint.checkpoint(blk, x)
             else:
                 x = blk(x)
-        x = self.downsample(x)
         return x
 
     def _init_respostnorm(self):
         for blk in self.blocks:
             nn.init.constant_(blk.norm1.bias, 0)
             nn.init.constant_(blk.norm1.weight, 0)
             nn.init.constant_(blk.norm2.bias, 0)
             nn.init.constant_(blk.norm2.weight, 0)
 
 
 class SwinTransformerV2(nn.Module):
-    r""" Swin Transformer V2
-        A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`
-            - https://arxiv.org/abs/2111.09883
-    Args:
-        img_size (int | tuple(int)): Input image size. Default 224
-        patch_size (int | tuple(int)): Patch size. Default: 4
-        in_chans (int): Number of input image channels. Default: 3
-        num_classes (int): Number of classes for classification head. Default: 1000
-        embed_dim (int): Patch embedding dimension. Default: 96
-        depths (tuple(int)): Depth of each Swin Transformer layer.
-        num_heads (tuple(int)): Number of attention heads in different layers.
-        window_size (int): Window size. Default: 7
-        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
-        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
-        drop_rate (float): Dropout rate. Default: 0
-        attn_drop_rate (float): Attention dropout rate. Default: 0
-        drop_path_rate (float): Stochastic depth rate. Default: 0.1
-        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
-        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
-        patch_norm (bool): If True, add normalization after patch embedding. Default: True
-        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
-        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.
+    """ Swin Transformer V2
+
+    A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`
+        - https://arxiv.org/abs/2111.09883
     """
 
     def __init__(
-            self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, global_pool='avg',
-            embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24),
-            window_size=7, mlp_ratio=4., qkv_bias=True,
-            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
-            norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
-            pretrained_window_sizes=(0, 0, 0, 0), **kwargs):
+            self,
+            img_size: _int_or_tuple_2_t = 224,
+            patch_size: int = 4,
+            in_chans: int = 3,
+            num_classes: int = 1000,
+            global_pool: str = 'avg',
+            embed_dim: int = 96,
+            depths: Tuple[int, ...] = (2, 2, 6, 2),
+            num_heads: Tuple[int, ...] = (3, 6, 12, 24),
+            window_size: _int_or_tuple_2_t = 7,
+            mlp_ratio: float = 4.,
+            qkv_bias: bool = True,
+            drop_rate: float = 0.,
+            proj_drop_rate: float = 0.,
+            attn_drop_rate: float = 0.,
+            drop_path_rate: float = 0.1,
+            norm_layer: Callable = nn.LayerNorm,
+            pretrained_window_sizes: Tuple[int, ...] = (0, 0, 0, 0),
+            **kwargs,
+    ):
+        """
+        Args:
+            img_size: Input image size.
+            patch_size: Patch size.
+            in_chans: Number of input image channels.
+            num_classes: Number of classes for classification head.
+            embed_dim: Patch embedding dimension.
+            depths: Depth of each Swin Transformer stage (layer).
+            num_heads: Number of attention heads in different layers.
+            window_size: Window size.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: If True, add a learnable bias to query, key, value.
+            drop_rate: Head dropout rate.
+            proj_drop_rate: Projection dropout rate.
+            attn_drop_rate: Attention dropout rate.
+            drop_path_rate: Stochastic depth rate.
+            norm_layer: Normalization layer.
+            patch_norm: If True, add normalization after patch embedding.
+            pretrained_window_sizes: Pretrained window sizes of each layer.
+            output_fmt: Output tensor format if not None, otherwise output 'NHWC' by default.
+        """
         super().__init__()
 
         self.num_classes = num_classes
         assert global_pool in ('', 'avg')
         self.global_pool = global_pool
+        self.output_fmt = 'NHWC'
         self.num_layers = len(depths)
         self.embed_dim = embed_dim
-        self.patch_norm = patch_norm
         self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
+        self.feature_info = []
+
+        if not isinstance(embed_dim, (tuple, list)):
+            embed_dim = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
 
         # split image into non-overlapping patches
         self.patch_embed = PatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
-            norm_layer=norm_layer if self.patch_norm else None)
-        num_patches = self.patch_embed.num_patches
-
-        # absolute position embedding
-        if ape:
-            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
-            trunc_normal_(self.absolute_pos_embed, std=.02)
-        else:
-            self.absolute_pos_embed = None
-
-        self.pos_drop = nn.Dropout(p=drop_rate)
-
-        # stochastic depth
-        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim[0],
+            norm_layer=norm_layer,
+            output_fmt='NHWC',
+        )
 
-        # build layers
-        self.layers = nn.ModuleList()
-        for i_layer in range(self.num_layers):
-            layer = BasicLayer(
-                dim=int(embed_dim * 2 ** i_layer),
+        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]
+        layers = []
+        in_dim = embed_dim[0]
+        scale = 1
+        for i in range(self.num_layers):
+            out_dim = embed_dim[i]
+            layers += [SwinTransformerV2Stage(
+                dim=in_dim,
+                out_dim=out_dim,
                 input_resolution=(
-                    self.patch_embed.grid_size[0] // (2 ** i_layer),
-                    self.patch_embed.grid_size[1] // (2 ** i_layer)),
-                depth=depths[i_layer],
-                num_heads=num_heads[i_layer],
+                    self.patch_embed.grid_size[0] // scale,
+                    self.patch_embed.grid_size[1] // scale),
+                depth=depths[i],
+                downsample=i > 0,
+                num_heads=num_heads[i],
                 window_size=window_size,
                 mlp_ratio=mlp_ratio,
                 qkv_bias=qkv_bias,
-                drop=drop_rate, attn_drop=attn_drop_rate,
-                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
                 norm_layer=norm_layer,
-                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
-                pretrained_window_size=pretrained_window_sizes[i_layer]
-            )
-            self.layers.append(layer)
+                pretrained_window_size=pretrained_window_sizes[i],
+            )]
+            in_dim = out_dim
+            if i > 0:
+                scale *= 2
+            self.feature_info += [dict(num_chs=out_dim, reduction=4 * scale, module=f'layers.{i}')]
 
+        self.layers = nn.Sequential(*layers)
         self.norm = norm_layer(self.num_features)
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+            input_fmt=self.output_fmt,
+        )
 
         self.apply(self._init_weights)
         for bly in self.layers:
             bly._init_respostnorm()
 
     def _init_weights(self, m):
         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
                 nn.init.constant_(m.bias, 0)
 
     @torch.jit.ignore
     def no_weight_decay(self):
-        nod = {'absolute_pos_embed'}
+        nod = set()
         for n, m in self.named_modules():
             if any([kw in n for kw in ("cpb_mlp", "logit_scale", 'relative_position_bias_table')]):
                 nod.add(n)
         return nod
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -583,173 +586,251 @@
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         for l in self.layers:
             l.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
-        return self.head
+        return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool=None):
         self.num_classes = num_classes
-        if global_pool is not None:
-            assert global_pool in ('', 'avg')
-            self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x):
         x = self.patch_embed(x)
-        if self.absolute_pos_embed is not None:
-            x = x + self.absolute_pos_embed
-        x = self.pos_drop(x)
-
-        for layer in self.layers:
-            x = layer(x)
-
-        x = self.norm(x)  # B L C
+        x = self.layers(x)
+        x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool == 'avg':
-            x = x.mean(dim=1)
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def checkpoint_filter_fn(state_dict, model):
+    state_dict = state_dict.get('model', state_dict)
+    state_dict = state_dict.get('state_dict', state_dict)
+    native_checkpoint = 'head.fc.weight' in state_dict
     out_dict = {}
-    if 'model' in state_dict:
-        # For deit models
-        state_dict = state_dict['model']
+    import re
     for k, v in state_dict.items():
-        if any([n in k for n in ('relative_position_index', 'relative_coords_table')]):
+        if any([n in k for n in ('relative_position_index', 'relative_coords_table', 'attn_mask')]):
             continue  # skip buffers that should not be persistent
+        if not native_checkpoint:
+            # skip layer remapping for updated checkpoints
+            k = re.sub(r'layers.(\d+).downsample', lambda x: f'layers.{int(x.group(1)) + 1}.downsample', k)
+            k = k.replace('head.', 'head.fc.')
         out_dict[k] = v
+      
     return out_dict
 
 
 def _create_swin_transformer_v2(variant, pretrained=False, **kwargs):
+    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 1, 1))))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
+
     model = build_model_with_cfg(
         SwinTransformerV2, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs)
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',
+        'license': 'mit', **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.pth',
+    ),
+    'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0,
+    ),
+    'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.pth',
+    ),
+    'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.pth',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0,
+    ),
+
+    'swinv2_tiny_window8_256.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth',
+    ),
+    'swinv2_tiny_window16_256.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window16_256.pth',
+    ),
+    'swinv2_small_window8_256.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window8_256.pth',
+    ),
+    'swinv2_small_window16_256.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window16_256.pth',
+    ),
+    'swinv2_base_window8_256.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window8_256.pth',
+    ),
+    'swinv2_base_window16_256.ms_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window16_256.pth',
+    ),
+
+    'swinv2_base_window12_192.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth',
+        num_classes=21841, input_size=(3, 192, 192), pool_size=(6, 6)
+    ),
+    'swinv2_large_window12_192.ms_in22k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth',
+        num_classes=21841, input_size=(3, 192, 192), pool_size=(6, 6)
+    ),
+})
+
+
 @register_model
-def swinv2_tiny_window16_256(pretrained=False, **kwargs):
+def swinv2_tiny_window16_256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=16, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer_v2('swinv2_tiny_window16_256', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=16, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer_v2(
+        'swinv2_tiny_window16_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_tiny_window8_256(pretrained=False, **kwargs):
+def swinv2_tiny_window8_256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=8, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer_v2('swinv2_tiny_window8_256', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=8, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer_v2(
+        'swinv2_tiny_window8_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_small_window16_256(pretrained=False, **kwargs):
+def swinv2_small_window16_256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=16, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer_v2('swinv2_small_window16_256', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=16, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer_v2(
+        'swinv2_small_window16_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_small_window8_256(pretrained=False, **kwargs):
+def swinv2_small_window8_256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=8, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24), **kwargs)
-    return _create_swin_transformer_v2('swinv2_small_window8_256', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=8, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))
+    return _create_swin_transformer_v2(
+        'swinv2_small_window8_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_base_window16_256(pretrained=False, **kwargs):
+def swinv2_base_window16_256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=16, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer_v2('swinv2_base_window16_256', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=16, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))
+    return _create_swin_transformer_v2(
+        'swinv2_base_window16_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_base_window8_256(pretrained=False, **kwargs):
+def swinv2_base_window8_256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=8, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer_v2('swinv2_base_window8_256', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=8, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))
+    return _create_swin_transformer_v2(
+        'swinv2_base_window8_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_base_window12_192_22k(pretrained=False, **kwargs):
+def swinv2_base_window12_192(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32), **kwargs)
-    return _create_swin_transformer_v2('swinv2_base_window12_192_22k', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))
+    return _create_swin_transformer_v2(
+        'swinv2_base_window12_192', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_base_window12to16_192to256_22kft1k(pretrained=False, **kwargs):
+def swinv2_base_window12to16_192to256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
+    model_args = dict(
         window_size=16, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32),
-        pretrained_window_sizes=(12, 12, 12, 6), **kwargs)
+        pretrained_window_sizes=(12, 12, 12, 6))
     return _create_swin_transformer_v2(
-        'swinv2_base_window12to16_192to256_22kft1k', pretrained=pretrained, **model_kwargs)
+        'swinv2_base_window12to16_192to256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_base_window12to24_192to384_22kft1k(pretrained=False, **kwargs):
+def swinv2_base_window12to24_192to384(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
+    model_args = dict(
         window_size=24, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32),
-        pretrained_window_sizes=(12, 12, 12, 6), **kwargs)
+        pretrained_window_sizes=(12, 12, 12, 6))
     return _create_swin_transformer_v2(
-        'swinv2_base_window12to24_192to384_22kft1k', pretrained=pretrained, **model_kwargs)
+        'swinv2_base_window12to24_192to384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_large_window12_192_22k(pretrained=False, **kwargs):
+def swinv2_large_window12_192(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
-        window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48), **kwargs)
-    return _create_swin_transformer_v2('swinv2_large_window12_192_22k', pretrained=pretrained, **model_kwargs)
+    model_args = dict(window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48))
+    return _create_swin_transformer_v2(
+        'swinv2_large_window12_192', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_large_window12to16_192to256_22kft1k(pretrained=False, **kwargs):
+def swinv2_large_window12to16_192to256(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
+    model_args = dict(
         window_size=16, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48),
-        pretrained_window_sizes=(12, 12, 12, 6), **kwargs)
+        pretrained_window_sizes=(12, 12, 12, 6))
     return _create_swin_transformer_v2(
-        'swinv2_large_window12to16_192to256_22kft1k', pretrained=pretrained, **model_kwargs)
+        'swinv2_large_window12to16_192to256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_large_window12to24_192to384_22kft1k(pretrained=False, **kwargs):
+def swinv2_large_window12to24_192to384(pretrained=False, **kwargs) -> SwinTransformerV2:
     """
     """
-    model_kwargs = dict(
+    model_args = dict(
         window_size=24, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48),
-        pretrained_window_sizes=(12, 12, 12, 6), **kwargs)
+        pretrained_window_sizes=(12, 12, 12, 6))
     return _create_swin_transformer_v2(
-        'swinv2_large_window12to24_192to384_22kft1k', pretrained=pretrained, **model_kwargs)
+        'swinv2_large_window12to24_192to384', pretrained=pretrained, **dict(model_args, **kwargs))
+
+
+register_model_deprecations(__name__, {
+    'swinv2_base_window12_192_22k': 'swinv2_base_window12_192.ms_in22k',
+    'swinv2_base_window12to16_192to256_22kft1k': 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k',
+    'swinv2_base_window12to24_192to384_22kft1k': 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k',
+    'swinv2_large_window12_192_22k': 'swinv2_large_window12_192.ms_in22k',
+    'swinv2_large_window12to16_192to256_22kft1k': 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k',
+    'swinv2_large_window12to24_192to384_22kft1k': 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/swin_transformer_v2_cr.py` & `timm-0.9.0/timm/models/swin_transformer_v2_cr.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,79 +33,25 @@
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.utils.checkpoint as checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import DropPath, Mlp, to_2tuple, _assert
+from timm.layers import DropPath, Mlp, ClassifierHead, to_2tuple, _assert
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_function
 from ._manipulate import named_apply
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model
 
 __all__ = ['SwinTransformerV2Cr']  # model_registry will add each entrypoint fn to this
 
 _logger = logging.getLogger(__name__)
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000,
-        'input_size': (3, 224, 224),
-        'pool_size': (7, 7),
-        'crop_pct': 0.9,
-        'interpolation': 'bicubic',
-        'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN,
-        'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj',
-        'classifier': 'head',
-        **kwargs,
-    }
-
-
-default_cfgs = {
-    'swinv2_cr_tiny_384': _cfg(
-        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
-    'swinv2_cr_tiny_224': _cfg(
-        url="", input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_tiny_ns_224': _cfg(
-        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_tiny_ns_224-ba8166c6.pth",
-        input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_small_384': _cfg(
-        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
-    'swinv2_cr_small_224': _cfg(
-        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_small_224-0813c165.pth",
-        input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_small_ns_224': _cfg(
-        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_small_ns_224_iv-2ce90f8e.pth",
-        input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_base_384': _cfg(
-        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
-    'swinv2_cr_base_224': _cfg(
-        url="", input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_base_ns_224': _cfg(
-        url="", input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_large_384': _cfg(
-        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
-    'swinv2_cr_large_224': _cfg(
-        url="", input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_huge_384': _cfg(
-        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
-    'swinv2_cr_huge_224': _cfg(
-        url="", input_size=(3, 224, 224), crop_pct=0.9),
-    'swinv2_cr_giant_384': _cfg(
-        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
-    'swinv2_cr_giant_224': _cfg(
-        url="", input_size=(3, 224, 224), crop_pct=0.9),
-}
-
-
 def bchw_to_bhwc(x: torch.Tensor) -> torch.Tensor:
     """Permutes a tensor from the shape (B, C, H, W) to (B, H, W, C). """
     return x.permute(0, 2, 3, 1)
 
 
 def bhwc_to_bchw(x: torch.Tensor) -> torch.Tensor:
     """Permutes a tensor from the shape (B, H, W, C) to (B, C, H, W). """
@@ -135,17 +81,17 @@
         window_size (Tuple[int, int]): Window size
         img_size (Tuple[int, int]): Image size
 
     Returns:
         x: (B, H, W, C)
     """
     H, W = img_size
-    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))
-    x = windows.view(B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1)
-    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
+    C = windows.shape[-1]
+    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)
+    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)
     return x
 
 
 class WindowMultiHeadAttention(nn.Module):
     r"""This class implements window-based Multi-Head-Attention with log-spaced continuous position bias.
 
     Args:
@@ -226,30 +172,22 @@
         relative_position_bias = self.meta_mlp(self.relative_coordinates_log)
         relative_position_bias = relative_position_bias.transpose(1, 0).reshape(
             self.num_heads, window_area, window_area
         )
         relative_position_bias = relative_position_bias.unsqueeze(0)
         return relative_position_bias
 
-    def _forward_sequential(
-        self,
-        x: torch.Tensor,
-        mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-        """
-        """
-        # FIXME TODO figure out 'sequential' attention mentioned in paper (should reduce GPU memory)
-        assert False, "not implemented"
+    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+        """ Forward pass.
+        Args:
+            x (torch.Tensor): Input tensor of the shape (B * windows, N, C)
+            mask (Optional[torch.Tensor]): Attention mask for the shift case
 
-    def _forward_batch(
-        self,
-        x: torch.Tensor,
-        mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-        """This function performs standard (non-sequential) scaled cosine self-attention.
+        Returns:
+            Output tensor of the shape [B * windows, N, C]
         """
         Bw, L, C = x.shape
 
         qkv = self.qkv(x).view(Bw, L, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
         query, key, value = qkv.unbind(0)
 
         # compute attention map with scaled cosine attention
@@ -268,40 +206,26 @@
         attn = self.attn_drop(attn)
 
         x = (attn @ value).transpose(1, 2).reshape(Bw, L, -1)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
-    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-        """ Forward pass.
-        Args:
-            x (torch.Tensor): Input tensor of the shape (B * windows, N, C)
-            mask (Optional[torch.Tensor]): Attention mask for the shift case
 
-        Returns:
-            Output tensor of the shape [B * windows, N, C]
-        """
-        if self.sequential_attn:
-            return self._forward_sequential(x, mask)
-        else:
-            return self._forward_batch(x, mask)
-
-
-class SwinTransformerBlock(nn.Module):
+class SwinTransformerV2CrBlock(nn.Module):
     r"""This class implements the Swin transformer block.
 
     Args:
         dim (int): Number of input channels
         num_heads (int): Number of attention heads to be utilized
         feat_size (Tuple[int, int]): Input resolution
         window_size (Tuple[int, int]): Window size to be utilized
         shift_size (int): Shifting size to be used
         mlp_ratio (int): Ratio of the hidden dimension in the FFN to the input channels
-        drop (float): Dropout in input mapping
+        proj_drop (float): Dropout in input mapping
         drop_attn (float): Dropout rate of attention map
         drop_path (float): Dropout in main path
         extra_norm (bool): Insert extra norm on 'main' branch if True
         sequential_attn (bool): If true sequential self-attention is performed
         norm_layer (Type[nn.Module]): Type of normalization layer to be utilized
     """
 
@@ -310,46 +234,46 @@
         dim: int,
         num_heads: int,
         feat_size: Tuple[int, int],
         window_size: Tuple[int, int],
         shift_size: Tuple[int, int] = (0, 0),
         mlp_ratio: float = 4.0,
         init_values: Optional[float] = 0,
-        drop: float = 0.0,
+        proj_drop: float = 0.0,
         drop_attn: float = 0.0,
         drop_path: float = 0.0,
         extra_norm: bool = False,
         sequential_attn: bool = False,
         norm_layer: Type[nn.Module] = nn.LayerNorm,
     ) -> None:
-        super(SwinTransformerBlock, self).__init__()
+        super(SwinTransformerV2CrBlock, self).__init__()
         self.dim: int = dim
         self.feat_size: Tuple[int, int] = feat_size
         self.target_shift_size: Tuple[int, int] = to_2tuple(shift_size)
         self.window_size, self.shift_size = self._calc_window_shift(to_2tuple(window_size))
         self.window_area = self.window_size[0] * self.window_size[1]
         self.init_values: Optional[float] = init_values
 
         # attn branch
         self.attn = WindowMultiHeadAttention(
             dim=dim,
             num_heads=num_heads,
             window_size=self.window_size,
             drop_attn=drop_attn,
-            drop_proj=drop,
+            drop_proj=proj_drop,
             sequential_attn=sequential_attn,
         )
         self.norm1 = norm_layer(dim)
         self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()
 
         # mlp branch
         self.mlp = Mlp(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
-            drop=drop,
+            drop=proj_drop,
             out_features=dim,
         )
         self.norm2 = norm_layer(dim)
         self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()
 
         # Extra main branch norm layer mentioned for Huge/Giant models in V2 paper.
         # Also being used as final network norm and optional stage ending norm while still in a C-last format.
@@ -406,17 +330,15 @@
         self.feat_size: Tuple[int, int] = new_feat_size
         self.window_size, self.shift_size = self._calc_window_shift(to_2tuple(new_window_size))
         self.window_area = self.window_size[0] * self.window_size[1]
         self.attn.update_input_size(new_window_size=self.window_size)
         self._make_attention_mask()
 
     def _shifted_window_attn(self, x):
-        H, W = self.feat_size
-        B, L, C = x.shape
-        x = x.view(B, H, W, C)
+        B, H, W, C = x.shape
 
         # cyclic shift
         sh, sw = self.shift_size
         do_shift: bool = any(self.shift_size)
         if do_shift:
             # FIXME PyTorch XLA needs cat impl, roll not lowered
             # x = torch.cat([x[:, sh:], x[:, :sh]], dim=1)
@@ -437,30 +359,33 @@
         # reverse cyclic shift
         if do_shift:
             # FIXME PyTorch XLA needs cat impl, roll not lowered
             # x = torch.cat([x[:, -sh:], x[:, :-sh]], dim=1)
             # x = torch.cat([x[:, :, -sw:], x[:, :, :-sw]], dim=2)
             x = torch.roll(x, shifts=(sh, sw), dims=(1, 2))
 
-        x = x.view(B, L, C)
         return x
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         """Forward pass.
 
         Args:
             x (torch.Tensor): Input tensor of the shape [B, C, H, W]
 
         Returns:
             output (torch.Tensor): Output tensor of the shape [B, C, H, W]
         """
         # post-norm branches (op -> norm -> drop)
         x = x + self.drop_path1(self.norm1(self._shifted_window_attn(x)))
+
+        B, H, W, C = x.shape
+        x = x.reshape(B, -1, C)
         x = x + self.drop_path2(self.norm2(self.mlp(x)))
         x = self.norm3(x)  # main-branch norm enabled for some blocks / stages (every 6 for Huge/Giant)
+        x = x.reshape(B, H, W, C)
         return x
 
 
 class PatchMerging(nn.Module):
     """ This class implements the patch merging as a strided convolution with a normalization before.
     Args:
         dim (int): Number of input channels
@@ -475,20 +400,18 @@
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         """ Forward pass.
         Args:
             x (torch.Tensor): Input tensor of the shape [B, C, H, W]
         Returns:
             output (torch.Tensor): Output tensor of the shape [B, 2 * C, H // 2, W // 2]
         """
-        B, C, H, W = x.shape
-        # unfold + BCHW -> BHWC together
-        # ordering, 5, 3, 1 instead of 3, 5, 1 maintains compat with original swin v1 merge
-        x = x.reshape(B, C, H // 2, 2, W // 2, 2).permute(0, 2, 4, 5, 3, 1).flatten(3)
+        B, H, W, C = x.shape
+        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)
         x = self.norm(x)
-        x = bhwc_to_bchw(self.reduction(x))
+        x = self.reduction(x)
         return x
 
 
 class PatchEmbed(nn.Module):
     """ 2D Image to Patch Embedding """
     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None):
         super().__init__()
@@ -507,26 +430,26 @@
         _assert(H == self.img_size[0], f"Input image height ({H}) doesn't match model ({self.img_size[0]}).")
         _assert(W == self.img_size[1], f"Input image width ({W}) doesn't match model ({self.img_size[1]}).")
         x = self.proj(x)
         x = self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
         return x
 
 
-class SwinTransformerStage(nn.Module):
+class SwinTransformerV2CrStage(nn.Module):
     r"""This class implements a stage of the Swin transformer including multiple layers.
 
     Args:
         embed_dim (int): Number of input channels
         depth (int): Depth of the stage (number of layers)
         downscale (bool): If true input is downsampled (see Fig. 3 or V1 paper)
         feat_size (Tuple[int, int]): input feature map size (H, W)
         num_heads (int): Number of attention heads to be utilized
         window_size (int): Window size to be utilized
         mlp_ratio (int): Ratio of the hidden dimension in the FFN to the input channels
-        drop (float): Dropout in input mapping
+        proj_drop (float): Dropout in input mapping
         drop_attn (float): Dropout rate of attention map
         drop_path (float): Dropout in main path
         norm_layer (Type[nn.Module]): Type of normalization layer to be utilized. Default: nn.LayerNorm
         extra_norm_period (int): Insert extra norm layer on main branch every N (period) blocks
         extra_norm_stage (bool): End each stage with an extra norm layer in main branch
         sequential_attn (bool): If true sequential self-attention is performed
     """
@@ -537,46 +460,49 @@
         depth: int,
         downscale: bool,
         num_heads: int,
         feat_size: Tuple[int, int],
         window_size: Tuple[int, int],
         mlp_ratio: float = 4.0,
         init_values: Optional[float] = 0.0,
-        drop: float = 0.0,
+        proj_drop: float = 0.0,
         drop_attn: float = 0.0,
         drop_path: Union[List[float], float] = 0.0,
         norm_layer: Type[nn.Module] = nn.LayerNorm,
         extra_norm_period: int = 0,
         extra_norm_stage: bool = False,
         sequential_attn: bool = False,
     ) -> None:
-        super(SwinTransformerStage, self).__init__()
+        super(SwinTransformerV2CrStage, self).__init__()
         self.downscale: bool = downscale
         self.grad_checkpointing: bool = False
         self.feat_size: Tuple[int, int] = (feat_size[0] // 2, feat_size[1] // 2) if downscale else feat_size
 
-        self.downsample = PatchMerging(embed_dim, norm_layer=norm_layer) if downscale else nn.Identity()
+        if downscale:
+            self.downsample = PatchMerging(embed_dim, norm_layer=norm_layer)
+            embed_dim = embed_dim * 2
+        else:
+            self.downsample = nn.Identity()
 
         def _extra_norm(index):
             i = index + 1
             if extra_norm_period and i % extra_norm_period == 0:
                 return True
             return i == depth if extra_norm_stage else False
 
-        embed_dim = embed_dim * 2 if downscale else embed_dim
         self.blocks = nn.Sequential(*[
-            SwinTransformerBlock(
+            SwinTransformerV2CrBlock(
                 dim=embed_dim,
                 num_heads=num_heads,
                 feat_size=self.feat_size,
                 window_size=window_size,
                 shift_size=tuple([0 if ((index % 2) == 0) else w // 2 for w in window_size]),
                 mlp_ratio=mlp_ratio,
                 init_values=init_values,
-                drop=drop,
+                proj_drop=proj_drop,
                 drop_attn=drop_attn,
                 drop_path=drop_path[index] if isinstance(drop_path, list) else drop_path,
                 extra_norm=_extra_norm(index),
                 sequential_attn=sequential_attn,
                 norm_layer=norm_layer,
             )
             for index in range(depth)]
@@ -598,52 +524,50 @@
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         """Forward pass.
         Args:
             x (torch.Tensor): Input tensor of the shape [B, C, H, W] or [B, L, C]
         Returns:
             output (torch.Tensor): Output tensor of the shape [B, 2 * C, H // 2, W // 2]
         """
+        x = bchw_to_bhwc(x)
         x = self.downsample(x)
-        B, C, H, W = x.shape
-        L = H * W
-
-        x = bchw_to_bhwc(x).reshape(B, L, C)
         for block in self.blocks:
             # Perform checkpointing if utilized
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint.checkpoint(block, x)
             else:
                 x = block(x)
-        x = bhwc_to_bchw(x.reshape(B, H, W, -1))
+        x = bhwc_to_bchw(x)
         return x
 
 
 class SwinTransformerV2Cr(nn.Module):
     r""" Swin Transformer V2
         A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`  -
           https://arxiv.org/pdf/2111.09883
 
     Args:
-        img_size (Tuple[int, int]): Input resolution.
-        window_size (Optional[int]): Window size. If None, img_size // window_div. Default: None
-        img_window_ratio (int): Window size to image size ratio. Default: 32
-        patch_size (int | tuple(int)): Patch size. Default: 4
-        in_chans (int): Number of input channels.
-        depths (int): Depth of the stage (number of layers).
-        num_heads (int): Number of attention heads to be utilized.
-        embed_dim (int): Patch embedding dimension. Default: 96
-        num_classes (int): Number of output classes. Default: 1000
-        mlp_ratio (int):  Ratio of the hidden dimension in the FFN to the input channels. Default: 4
-        drop_rate (float): Dropout rate. Default: 0.0
-        attn_drop_rate (float): Dropout rate of attention map. Default: 0.0
-        drop_path_rate (float): Stochastic depth rate. Default: 0.0
-        norm_layer (Type[nn.Module]): Type of normalization layer to be utilized. Default: nn.LayerNorm
-        extra_norm_period (int): Insert extra norm layer on main branch every N (period) blocks in stage
-        extra_norm_stage (bool): End each stage with an extra norm layer in main branch
-        sequential_attn (bool): If true sequential self-attention is performed. Default: False
+        img_size: Input resolution.
+        window_size: Window size. If None, img_size // window_div
+        img_window_ratio: Window size to image size ratio.
+        patch_size: Patch size.
+        in_chans: Number of input channels.
+        depths: Depth of the stage (number of layers).
+        num_heads: Number of attention heads to be utilized.
+        embed_dim: Patch embedding dimension.
+        num_classes: Number of output classes.
+        mlp_ratio:  Ratio of the hidden dimension in the FFN to the input channels.
+        drop_rate: Dropout rate.
+        proj_drop_rate: Projection dropout rate.
+        attn_drop_rate: Dropout rate of attention map.
+        drop_path_rate: Stochastic depth rate.
+        norm_layer: Type of normalization layer to be utilized.
+        extra_norm_period: Insert extra norm layer on main branch every N (period) blocks in stage
+        extra_norm_stage: End each stage with an extra norm layer in main branch
+        sequential_attn: If true sequential self-attention is performed.
     """
 
     def __init__(
         self,
         img_size: Tuple[int, int] = (224, 224),
         patch_size: int = 4,
         window_size: Optional[int] = None,
@@ -652,14 +576,15 @@
         num_classes: int = 1000,
         embed_dim: int = 96,
         depths: Tuple[int, ...] = (2, 2, 6, 2),
         num_heads: Tuple[int, ...] = (3, 6, 12, 24),
         mlp_ratio: float = 4.0,
         init_values: Optional[float] = 0.,
         drop_rate: float = 0.0,
+        proj_drop_rate: float = 0.0,
         attn_drop_rate: float = 0.0,
         drop_path_rate: float = 0.0,
         norm_layer: Type[nn.Module] = nn.LayerNorm,
         extra_norm_period: int = 0,
         extra_norm_stage: bool = False,
         sequential_attn: bool = False,
         global_pool: str = 'avg',
@@ -672,47 +597,62 @@
             s // img_window_ratio for s in img_size]) if window_size is None else to_2tuple(window_size)
 
         self.num_classes: int = num_classes
         self.patch_size: int = patch_size
         self.img_size: Tuple[int, int] = img_size
         self.window_size: int = window_size
         self.num_features: int = int(embed_dim * 2 ** (len(depths) - 1))
+        self.feature_info = []
 
         self.patch_embed = PatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans,
-            embed_dim=embed_dim, norm_layer=norm_layer)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+            norm_layer=norm_layer,
+        )
         patch_grid_size: Tuple[int, int] = self.patch_embed.grid_size
 
-        drop_path_rate = torch.linspace(0.0, drop_path_rate, sum(depths)).tolist()
+        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]
         stages = []
-        for index, (depth, num_heads) in enumerate(zip(depths, num_heads)):
-            stage_scale = 2 ** max(index - 1, 0)
-            stages.append(
-                SwinTransformerStage(
-                    embed_dim=embed_dim * stage_scale,
-                    depth=depth,
-                    downscale=index != 0,
-                    feat_size=(patch_grid_size[0] // stage_scale, patch_grid_size[1] // stage_scale),
-                    num_heads=num_heads,
-                    window_size=window_size,
-                    mlp_ratio=mlp_ratio,
-                    init_values=init_values,
-                    drop=drop_rate,
-                    drop_attn=attn_drop_rate,
-                    drop_path=drop_path_rate[sum(depths[:index]):sum(depths[:index + 1])],
-                    extra_norm_period=extra_norm_period,
-                    extra_norm_stage=extra_norm_stage or (index + 1) == len(depths),  # last stage ends w/ norm
-                    sequential_attn=sequential_attn,
-                    norm_layer=norm_layer,
-                )
-            )
+        in_dim = embed_dim
+        in_scale = 1
+        for stage_idx, (depth, num_heads) in enumerate(zip(depths, num_heads)):
+            stages += [SwinTransformerV2CrStage(
+                embed_dim=in_dim,
+                depth=depth,
+                downscale=stage_idx != 0,
+                feat_size=(
+                    patch_grid_size[0] // in_scale,
+                    patch_grid_size[1] // in_scale
+                ),
+                num_heads=num_heads,
+                window_size=window_size,
+                mlp_ratio=mlp_ratio,
+                init_values=init_values,
+                proj_drop=proj_drop_rate,
+                drop_attn=attn_drop_rate,
+                drop_path=dpr[stage_idx],
+                extra_norm_period=extra_norm_period,
+                extra_norm_stage=extra_norm_stage or (stage_idx + 1) == len(depths),  # last stage ends w/ norm
+                sequential_attn=sequential_attn,
+                norm_layer=norm_layer,
+            )]
+            if stage_idx != 0:
+                in_dim *= 2
+                in_scale *= 2
+            self.feature_info += [dict(num_chs=in_dim, reduction=4 * in_scale, module=f'stages.{stage_idx}')]
         self.stages = nn.Sequential(*stages)
 
-        self.global_pool: str = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes else nn.Identity()
+        self.head = ClassifierHead(
+            self.num_features,
+            num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+        )
 
         # current weight init skips custom init and uses pytorch layer defaults, seems to work well
         # FIXME more experiments needed
         if weight_init != 'skip':
             named_apply(init_weights, self)
 
     def update_input_size(
@@ -761,37 +701,33 @@
 
     @torch.jit.ignore()
     def get_classifier(self) -> nn.Module:
         """Method returns the classification head of the model.
         Returns:
             head (nn.Module): Current classification head
         """
-        return self.head
+        return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None) -> None:
         """Method results the classification head
 
         Args:
             num_classes (int): Number of classes to be predicted
             global_pool (str): Unused
         """
-        self.num_classes: int = num_classes
-        if global_pool is not None:
-            self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.num_classes = num_classes
+        self.head.reset(num_classes, global_pool)
 
     def forward_features(self, x: torch.Tensor) -> torch.Tensor:
         x = self.patch_embed(x)
         x = self.stages(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool == 'avg':
-            x = x.mean(dim=(2, 3))
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
@@ -810,221 +746,278 @@
             nn.init.zeros_(module.bias)
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
 def checkpoint_filter_fn(state_dict, model):
     """ convert patch embedding weight from manual patchify + linear proj to conv"""
+    state_dict = state_dict.get('model', state_dict)
+    state_dict = state_dict.get('state_dict', state_dict)
+    if 'head.fc.weight' in state_dict:
+        return state_dict
     out_dict = {}
-    if 'model' in state_dict:
-        # For deit models
-        state_dict = state_dict['model']
     for k, v in state_dict.items():
         if 'tau' in k:
             # convert old tau based checkpoints -> logit_scale (inverse)
             v = torch.log(1 / v)
             k = k.replace('tau', 'logit_scale')
+        k = k.replace('head.', 'head.fc.')
         out_dict[k] = v
     return out_dict
 
 
 def _create_swin_transformer_v2_cr(variant, pretrained=False, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Vision Transformer models.')
+    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 1, 1))))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
+
     model = build_model_with_cfg(
         SwinTransformerV2Cr, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs
     )
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000,
+        'input_size': (3, 224, 224),
+        'pool_size': (7, 7),
+        'crop_pct': 0.9,
+        'interpolation': 'bicubic',
+        'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN,
+        'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj',
+        'classifier': 'head.fc',
+        **kwargs,
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'swinv2_cr_tiny_384.untrained': _cfg(
+        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
+    'swinv2_cr_tiny_224.untrained': _cfg(
+        url="", input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_tiny_ns_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
+        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_tiny_ns_224-ba8166c6.pth",
+        input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_small_384.untrained': _cfg(
+        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
+    'swinv2_cr_small_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
+        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_small_224-0813c165.pth",
+        input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_small_ns_224.sw_in1k': _cfg(
+        hf_hub_id='timm/',
+        url="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_small_ns_224_iv-2ce90f8e.pth",
+        input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_small_ns_256.untrained': _cfg(
+        url="", input_size=(3, 256, 256), crop_pct=1.0, pool_size=(8, 8)),
+    'swinv2_cr_base_384.untrained': _cfg(
+        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
+    'swinv2_cr_base_224.untrained': _cfg(
+        url="", input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_base_ns_224.untrained': _cfg(
+        url="", input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_large_384.untrained': _cfg(
+        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
+    'swinv2_cr_large_224.untrained': _cfg(
+        url="", input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_huge_384.untrained': _cfg(
+        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
+    'swinv2_cr_huge_224.untrained': _cfg(
+        url="", input_size=(3, 224, 224), crop_pct=0.9),
+    'swinv2_cr_giant_384.untrained': _cfg(
+        url="", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),
+    'swinv2_cr_giant_224.untrained': _cfg(
+        url="", input_size=(3, 224, 224), crop_pct=0.9),
+})
+
+
 @register_model
-def swinv2_cr_tiny_384(pretrained=False, **kwargs):
+def swinv2_cr_tiny_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-T V2 CR @ 384x384, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=96,
         depths=(2, 2, 6, 2),
         num_heads=(3, 6, 12, 24),
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_384', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_tiny_224(pretrained=False, **kwargs):
+def swinv2_cr_tiny_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-T V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=96,
         depths=(2, 2, 6, 2),
         num_heads=(3, 6, 12, 24),
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_tiny_ns_224(pretrained=False, **kwargs):
+def swinv2_cr_tiny_ns_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-T V2 CR @ 224x224, trained ImageNet-1k w/ extra stage norms.
     ** Experimental, may make default if results are improved. **
     """
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=96,
         depths=(2, 2, 6, 2),
         num_heads=(3, 6, 12, 24),
         extra_norm_stage=True,
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_ns_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_small_384(pretrained=False, **kwargs):
+def swinv2_cr_small_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-S V2 CR @ 384x384, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=96,
         depths=(2, 2, 18, 2),
         num_heads=(3, 6, 12, 24),
-        **kwargs
-    )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_384', pretrained=pretrained, **model_kwargs
     )
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_small_224(pretrained=False, **kwargs):
+def swinv2_cr_small_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-S V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=96,
         depths=(2, 2, 18, 2),
         num_heads=(3, 6, 12, 24),
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_small_ns_224(pretrained=False, **kwargs):
+def swinv2_cr_small_ns_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-S V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
+        embed_dim=96,
+        depths=(2, 2, 18, 2),
+        num_heads=(3, 6, 12, 24),
+        extra_norm_stage=True,
+    )
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))
+
+
+@register_model
+def swinv2_cr_small_ns_256(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
+    """Swin-S V2 CR @ 256x256, trained ImageNet-1k"""
+    model_args = dict(
         embed_dim=96,
         depths=(2, 2, 18, 2),
         num_heads=(3, 6, 12, 24),
         extra_norm_stage=True,
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_256', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_base_384(pretrained=False, **kwargs):
+def swinv2_cr_base_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-B V2 CR @ 384x384, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=128,
         depths=(2, 2, 18, 2),
         num_heads=(4, 8, 16, 32),
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_base_384', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_base_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_base_224(pretrained=False, **kwargs):
+def swinv2_cr_base_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-B V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=128,
         depths=(2, 2, 18, 2),
         num_heads=(4, 8, 16, 32),
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_base_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_base_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_base_ns_224(pretrained=False, **kwargs):
+def swinv2_cr_base_ns_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-B V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=128,
         depths=(2, 2, 18, 2),
         num_heads=(4, 8, 16, 32),
         extra_norm_stage=True,
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_base_ns_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_base_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_large_384(pretrained=False, **kwargs):
+def swinv2_cr_large_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-L V2 CR @ 384x384, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=192,
         depths=(2, 2, 18, 2),
         num_heads=(6, 12, 24, 48),
-        **kwargs
-    )
-    return _create_swin_transformer_v2_cr('swinv2_cr_large_384', pretrained=pretrained, **model_kwargs
     )
+    return _create_swin_transformer_v2_cr('swinv2_cr_large_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_large_224(pretrained=False, **kwargs):
+def swinv2_cr_large_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-L V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=192,
         depths=(2, 2, 18, 2),
         num_heads=(6, 12, 24, 48),
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_large_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_large_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_huge_384(pretrained=False, **kwargs):
+def swinv2_cr_huge_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-H V2 CR @ 384x384, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=352,
         depths=(2, 2, 18, 2),
         num_heads=(11, 22, 44, 88),  # head count not certain for Huge, 384 & 224 trying diff values
         extra_norm_period=6,
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_huge_384', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_huge_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_huge_224(pretrained=False, **kwargs):
+def swinv2_cr_huge_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-H V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=352,
         depths=(2, 2, 18, 2),
         num_heads=(8, 16, 32, 64),  # head count not certain for Huge, 384 & 224 trying diff values
         extra_norm_period=6,
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_huge_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_huge_224', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_giant_384(pretrained=False, **kwargs):
+def swinv2_cr_giant_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-G V2 CR @ 384x384, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=512,
         depths=(2, 2, 42, 2),
         num_heads=(16, 32, 64, 128),
         extra_norm_period=6,
-        **kwargs
-    )
-    return _create_swin_transformer_v2_cr('swinv2_cr_giant_384', pretrained=pretrained, **model_kwargs
     )
+    return _create_swin_transformer_v2_cr('swinv2_cr_giant_384', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def swinv2_cr_giant_224(pretrained=False, **kwargs):
+def swinv2_cr_giant_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:
     """Swin-G V2 CR @ 224x224, trained ImageNet-1k"""
-    model_kwargs = dict(
+    model_args = dict(
         embed_dim=512,
         depths=(2, 2, 42, 2),
         num_heads=(16, 32, 64, 128),
         extra_norm_period=6,
-        **kwargs
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_giant_224', pretrained=pretrained, **model_kwargs)
+    return _create_swin_transformer_v2_cr('swinv2_cr_giant_224', pretrained=pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/tnt.py` & `timm-0.9.0/timm/models/tnt.py`

 * *Files 20% similar despite different names*

```diff
@@ -76,39 +76,72 @@
         return x
 
 
 class Block(nn.Module):
     """ TNT Block
     """
     def __init__(
-            self, dim, in_dim, num_pixel, num_heads=12, in_num_head=4, mlp_ratio=4.,
-            qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim,
+            dim_out,
+            num_pixel,
+            num_heads_in=4,
+            num_heads_out=12,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
         # Inner transformer
-        self.norm_in = norm_layer(in_dim)
+        self.norm_in = norm_layer(dim)
         self.attn_in = Attention(
-            in_dim, in_dim, num_heads=in_num_head, qkv_bias=qkv_bias,
-            attn_drop=attn_drop, proj_drop=drop)
+            dim,
+            dim,
+            num_heads=num_heads_in,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
         
-        self.norm_mlp_in = norm_layer(in_dim)
-        self.mlp_in = Mlp(in_features=in_dim, hidden_features=int(in_dim * 4),
-            out_features=in_dim, act_layer=act_layer, drop=drop)
+        self.norm_mlp_in = norm_layer(dim)
+        self.mlp_in = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * 4),
+            out_features=dim,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
         
-        self.norm1_proj = norm_layer(in_dim)
-        self.proj = nn.Linear(in_dim * num_pixel, dim, bias=True)
+        self.norm1_proj = norm_layer(dim)
+        self.proj = nn.Linear(dim * num_pixel, dim_out, bias=True)
+
         # Outer transformer
-        self.norm_out = norm_layer(dim)
+        self.norm_out = norm_layer(dim_out)
         self.attn_out = Attention(
-            dim, dim, num_heads=num_heads, qkv_bias=qkv_bias,
-            attn_drop=attn_drop, proj_drop=drop)
+            dim_out,
+            dim_out,
+            num_heads=num_heads_out,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+        )
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         
-        self.norm_mlp = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio),
-            out_features=dim, act_layer=act_layer, drop=drop)
+        self.norm_mlp = norm_layer(dim_out)
+        self.mlp = Mlp(
+            in_features=dim_out,
+            hidden_features=int(dim_out * mlp_ratio),
+            out_features=dim_out,
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
 
     def forward(self, pixel_embed, patch_embed):
         # inner
         pixel_embed = pixel_embed + self.drop_path(self.attn_in(self.norm_in(pixel_embed)))
         pixel_embed = pixel_embed + self.drop_path(self.mlp_in(self.norm_mlp_in(pixel_embed)))
         # outer
         B, N, C = patch_embed.size()
@@ -153,50 +186,83 @@
         return x
 
 
 class TNT(nn.Module):
     """ Transformer in Transformer - https://arxiv.org/abs/2103.00112
     """
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, global_pool='token',
-            embed_dim=768, in_dim=48, depth=12, num_heads=12, in_num_head=4, mlp_ratio=4., qkv_bias=False,
-            drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, first_stride=4):
+            self,
+            img_size=224,
+            patch_size=16,
+            in_chans=3,
+            num_classes=1000,
+            global_pool='token',
+            embed_dim=768,
+            inner_dim=48,
+            depth=12,
+            num_heads_inner=4,
+            num_heads_outer=12,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            norm_layer=nn.LayerNorm,
+            first_stride=4,
+    ):
         super().__init__()
         assert global_pool in ('', 'token', 'avg')
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
         self.grad_checkpointing = False
 
         self.pixel_embed = PixelEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, in_dim=in_dim, stride=first_stride)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            in_dim=inner_dim,
+            stride=first_stride,
+        )
         num_patches = self.pixel_embed.num_patches
         self.num_patches = num_patches
         new_patch_size = self.pixel_embed.new_patch_size
         num_pixel = new_patch_size[0] * new_patch_size[1]
         
-        self.norm1_proj = norm_layer(num_pixel * in_dim)
-        self.proj = nn.Linear(num_pixel * in_dim, embed_dim)
+        self.norm1_proj = norm_layer(num_pixel * inner_dim)
+        self.proj = nn.Linear(num_pixel * inner_dim, embed_dim)
         self.norm2_proj = norm_layer(embed_dim)
 
         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
         self.patch_pos = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
-        self.pixel_pos = nn.Parameter(torch.zeros(1, in_dim, new_patch_size[0], new_patch_size[1]))
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pixel_pos = nn.Parameter(torch.zeros(1, inner_dim, new_patch_size[0], new_patch_size[1]))
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
         blocks = []
         for i in range(depth):
             blocks.append(Block(
-                dim=embed_dim, in_dim=in_dim, num_pixel=num_pixel, num_heads=num_heads, in_num_head=in_num_head,
-                mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,
-                drop_path=dpr[i], norm_layer=norm_layer))
+                dim=inner_dim,
+                dim_out=embed_dim,
+                num_pixel=num_pixel,
+                num_heads_in=num_heads_inner,
+                num_heads_out=num_heads_outer,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+            ))
         self.blocks = nn.ModuleList(blocks)
         self.norm = norm_layer(embed_dim)
 
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
         trunc_normal_(self.cls_token, std=.02)
         trunc_normal_(self.patch_pos, std=.02)
         trunc_normal_(self.pixel_pos, std=.02)
         self.apply(self._init_weights)
 
@@ -256,14 +322,15 @@
 
         patch_embed = self.norm(patch_embed)
         return patch_embed
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool:
             x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -284,22 +351,22 @@
         TNT, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
         **kwargs)
     return model
 
 
 @register_model
-def tnt_s_patch16_224(pretrained=False, **kwargs):
+def tnt_s_patch16_224(pretrained=False, **kwargs) -> TNT:
     model_cfg = dict(
-        patch_size=16, embed_dim=384, in_dim=24, depth=12, num_heads=6, in_num_head=4,
-        qkv_bias=False, **kwargs)
-    model = _create_tnt('tnt_s_patch16_224', pretrained=pretrained, **model_cfg)
+        patch_size=16, embed_dim=384, inner_dim=24, depth=12, num_heads_outer=6,
+        qkv_bias=False)
+    model = _create_tnt('tnt_s_patch16_224', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def tnt_b_patch16_224(pretrained=False, **kwargs):
+def tnt_b_patch16_224(pretrained=False, **kwargs) -> TNT:
     model_cfg = dict(
-        patch_size=16, embed_dim=640, in_dim=40, depth=12, num_heads=10, in_num_head=4,
-        qkv_bias=False, **kwargs)
-    model = _create_tnt('tnt_b_patch16_224', pretrained=pretrained, **model_cfg)
+        patch_size=16, embed_dim=640, inner_dim=40, depth=12, num_heads_outer=10,
+        qkv_bias=False)
+    model = _create_tnt('tnt_b_patch16_224', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
```

### Comparing `timm-0.8.6.dev0/timm/models/tresnet.py` & `timm-0.9.0/timm/models/xception_aligned.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,331 +1,417 @@
-"""
-TResNet: High Performance GPU-Dedicated Architecture
-https://arxiv.org/pdf/2003.13630.pdf
+"""Pytorch impl of Aligned Xception 41, 65, 71
 
-Original model: https://github.com/mrT23/TResNet
+This is a correct, from scratch impl of Aligned Xception (Deeplab) models compatible with TF weights at
+https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md
 
+Hacked together by / Copyright 2020 Ross Wightman
 """
-from collections import OrderedDict
+from functools import partial
 
 import torch
 import torch.nn as nn
 
-from timm.layers import SpaceToDepthModule, BlurPool2d, InplaceAbn, ClassifierHead, SEModule
+from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
+from timm.layers import ClassifierHead, ConvNormAct, create_conv2d, get_norm_act_layer
+from timm.layers.helpers import to_3tuple
 from ._builder import build_model_with_cfg
-from ._registry import register_model
-
-__all__ = ['TResNet']  # model_registry will add each entrypoint fn to this
+from ._manipulate import checkpoint_seq
+from ._registry import register_model, generate_default_cfgs
 
+__all__ = ['XceptionAligned']
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bilinear',
-        'mean': (0., 0., 0.), 'std': (1., 1., 1.),
-        'first_conv': 'body.conv1.0', 'classifier': 'head.fc',
-        **kwargs
-    }
 
+class SeparableConv2d(nn.Module):
+    def __init__(
+            self,
+            in_chs,
+            out_chs,
+            kernel_size=3,
+            stride=1,
+            dilation=1,
+            padding='',
+            act_layer=nn.ReLU,
+            norm_layer=nn.BatchNorm2d,
+    ):
+        super(SeparableConv2d, self).__init__()
+        self.kernel_size = kernel_size
+        self.dilation = dilation
+
+        # depthwise convolution
+        self.conv_dw = create_conv2d(
+            in_chs, in_chs, kernel_size, stride=stride,
+            padding=padding, dilation=dilation, depthwise=True)
+        self.bn_dw = norm_layer(in_chs)
+        self.act_dw = act_layer(inplace=True) if act_layer is not None else nn.Identity()
+
+        # pointwise convolution
+        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1)
+        self.bn_pw = norm_layer(out_chs)
+        self.act_pw = act_layer(inplace=True) if act_layer is not None else nn.Identity()
 
-default_cfgs = {
-    'tresnet_m': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_1k_miil_83_1-d236afcb.pth'),
-    'tresnet_m_miil_in21k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_miil_in21k-901b6ed4.pth', num_classes=11221),
-    'tresnet_l': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_81_5-235b486c.pth'),
-    'tresnet_xl': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_82_0-a2d51b00.pth'),
-    'tresnet_m_448': _cfg(
-        input_size=(3, 448, 448), pool_size=(14, 14),
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_448-bc359d10.pth'),
-    'tresnet_l_448': _cfg(
-        input_size=(3, 448, 448), pool_size=(14, 14),
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth'),
-    'tresnet_xl_448': _cfg(
-        input_size=(3, 448, 448), pool_size=(14, 14),
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_448-8c1815de.pth'),
-
-    'tresnet_v2_l': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_v2_83_9-f36e4445.pth'),
-}
-
-
-def IABN2Float(module: nn.Module) -> nn.Module:
-    """If `module` is IABN don't use half precision."""
-    if isinstance(module, InplaceAbn):
-        module.float()
-    for child in module.children():
-        IABN2Float(child)
-    return module
-
-
-def conv2d_iabn(ni, nf, stride, kernel_size=3, groups=1, act_layer="leaky_relu", act_param=1e-2):
-    return nn.Sequential(
-        nn.Conv2d(
-            ni, nf, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2, groups=groups, bias=False),
-        InplaceAbn(nf, act_layer=act_layer, act_param=act_param)
-    )
+    def forward(self, x):
+        x = self.conv_dw(x)
+        x = self.bn_dw(x)
+        x = self.act_dw(x)
+        x = self.conv_pw(x)
+        x = self.bn_pw(x)
+        x = self.act_pw(x)
+        return x
 
 
-class BasicBlock(nn.Module):
-    expansion = 1
+class PreSeparableConv2d(nn.Module):
+    def __init__(
+            self,
+            in_chs,
+            out_chs,
+            kernel_size=3,
+            stride=1,
+            dilation=1,
+            padding='',
+            act_layer=nn.ReLU,
+            norm_layer=nn.BatchNorm2d,
+            first_act=True,
+    ):
+        super(PreSeparableConv2d, self).__init__()
+        norm_act_layer = get_norm_act_layer(norm_layer, act_layer=act_layer)
+        self.kernel_size = kernel_size
+        self.dilation = dilation
+
+        self.norm = norm_act_layer(in_chs, inplace=True) if first_act else nn.Identity()
+        # depthwise convolution
+        self.conv_dw = create_conv2d(
+            in_chs, in_chs, kernel_size, stride=stride,
+            padding=padding, dilation=dilation, depthwise=True)
 
-    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True, aa_layer=None):
-        super(BasicBlock, self).__init__()
-        if stride == 1:
-            self.conv1 = conv2d_iabn(inplanes, planes, stride=1, act_param=1e-3)
-        else:
-            if aa_layer is None:
-                self.conv1 = conv2d_iabn(inplanes, planes, stride=2, act_param=1e-3)
-            else:
-                self.conv1 = nn.Sequential(
-                    conv2d_iabn(inplanes, planes, stride=1, act_param=1e-3),
-                    aa_layer(channels=planes, filt_size=3, stride=2))
-
-        self.conv2 = conv2d_iabn(planes, planes, stride=1, act_layer="identity")
-        self.relu = nn.ReLU(inplace=True)
-        self.downsample = downsample
-        self.stride = stride
-        rd_chs = max(planes * self.expansion // 4, 64)
-        self.se = SEModule(planes * self.expansion, rd_channels=rd_chs) if use_se else None
+        # pointwise convolution
+        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1)
 
     def forward(self, x):
-        if self.downsample is not None:
-            shortcut = self.downsample(x)
-        else:
-            shortcut = x
+        x = self.norm(x)
+        x = self.conv_dw(x)
+        x = self.conv_pw(x)
+        return x
 
-        out = self.conv1(x)
-        out = self.conv2(out)
 
-        if self.se is not None:
-            out = self.se(out)
+class XceptionModule(nn.Module):
+    def __init__(
+            self,
+            in_chs,
+            out_chs,
+            stride=1,
+            dilation=1,
+            pad_type='',
+            start_with_relu=True,
+            no_skip=False,
+            act_layer=nn.ReLU,
+            norm_layer=None,
+    ):
+        super(XceptionModule, self).__init__()
+        out_chs = to_3tuple(out_chs)
+        self.in_channels = in_chs
+        self.out_channels = out_chs[-1]
+        self.no_skip = no_skip
+        if not no_skip and (self.out_channels != self.in_channels or stride != 1):
+            self.shortcut = ConvNormAct(
+                in_chs, self.out_channels, 1, stride=stride, norm_layer=norm_layer, apply_act=False)
+        else:
+            self.shortcut = None
 
-        out = out + shortcut
-        out = self.relu(out)
-        return out
+        separable_act_layer = None if start_with_relu else act_layer
+        self.stack = nn.Sequential()
+        for i in range(3):
+            if start_with_relu:
+                self.stack.add_module(f'act{i + 1}', act_layer(inplace=i > 0))
+            self.stack.add_module(f'conv{i + 1}', SeparableConv2d(
+                in_chs, out_chs[i], 3, stride=stride if i == 2 else 1, dilation=dilation, padding=pad_type,
+                act_layer=separable_act_layer, norm_layer=norm_layer))
+            in_chs = out_chs[i]
 
+    def forward(self, x):
+        skip = x
+        x = self.stack(x)
+        if self.shortcut is not None:
+            skip = self.shortcut(skip)
+        if not self.no_skip:
+            x = x + skip
+        return x
 
-class Bottleneck(nn.Module):
-    expansion = 4
 
+class PreXceptionModule(nn.Module):
     def __init__(
-            self, inplanes, planes, stride=1, downsample=None, use_se=True,
-            act_layer="leaky_relu", aa_layer=None):
-        super(Bottleneck, self).__init__()
-        self.conv1 = conv2d_iabn(
-            inplanes, planes, kernel_size=1, stride=1, act_layer=act_layer, act_param=1e-3)
-        if stride == 1:
-            self.conv2 = conv2d_iabn(
-                planes, planes, kernel_size=3, stride=1, act_layer=act_layer, act_param=1e-3)
+            self,
+            in_chs,
+            out_chs,
+            stride=1,
+            dilation=1,
+            pad_type='',
+            no_skip=False,
+            act_layer=nn.ReLU,
+            norm_layer=None,
+    ):
+        super(PreXceptionModule, self).__init__()
+        out_chs = to_3tuple(out_chs)
+        self.in_channels = in_chs
+        self.out_channels = out_chs[-1]
+        self.no_skip = no_skip
+        if not no_skip and (self.out_channels != self.in_channels or stride != 1):
+            self.shortcut = create_conv2d(in_chs, self.out_channels, 1, stride=stride)
         else:
-            if aa_layer is None:
-                self.conv2 = conv2d_iabn(
-                    planes, planes, kernel_size=3, stride=2, act_layer=act_layer, act_param=1e-3)
-            else:
-                self.conv2 = nn.Sequential(
-                    conv2d_iabn(planes, planes, kernel_size=3, stride=1, act_layer=act_layer, act_param=1e-3),
-                    aa_layer(channels=planes, filt_size=3, stride=2))
-
-        reduction_chs = max(planes * self.expansion // 8, 64)
-        self.se = SEModule(planes, rd_channels=reduction_chs) if use_se else None
-
-        self.conv3 = conv2d_iabn(
-            planes, planes * self.expansion, kernel_size=1, stride=1, act_layer="identity")
-
-        self.act = nn.ReLU(inplace=True)
-        self.downsample = downsample
-        self.stride = stride
+            self.shortcut = nn.Identity()
 
-    def forward(self, x):
-        if self.downsample is not None:
-            shortcut = self.downsample(x)
-        else:
-            shortcut = x
+        self.norm = get_norm_act_layer(norm_layer, act_layer=act_layer)(in_chs, inplace=True)
+        self.stack = nn.Sequential()
+        for i in range(3):
+            self.stack.add_module(f'conv{i + 1}', PreSeparableConv2d(
+                in_chs,
+                out_chs[i],
+                3,
+                stride=stride if i == 2 else 1,
+                dilation=dilation,
+                padding=pad_type,
+                act_layer=act_layer,
+                norm_layer=norm_layer,
+                first_act=i > 0,
+            ))
+            in_chs = out_chs[i]
 
-        out = self.conv1(x)
-        out = self.conv2(out)
-        if self.se is not None:
-            out = self.se(out)
-        out = self.conv3(out)
-        out = out + shortcut  # no inplace
-        out = self.act(out)
+    def forward(self, x):
+        x = self.norm(x)
+        skip = x
+        x = self.stack(x)
+        if not self.no_skip:
+            x = x + self.shortcut(skip)
+        return x
 
-        return out
 
+class XceptionAligned(nn.Module):
+    """Modified Aligned Xception
+    """
 
-class TResNet(nn.Module):
     def __init__(
             self,
-            layers,
-            in_chans=3,
+            block_cfg,
             num_classes=1000,
-            width_factor=1.0,
-            v2=False,
-            global_pool='fast',
+            in_chans=3,
+            output_stride=32,
+            preact=False,
+            act_layer=nn.ReLU,
+            norm_layer=nn.BatchNorm2d,
             drop_rate=0.,
+            global_pool='avg',
     ):
+        super(XceptionAligned, self).__init__()
+        assert output_stride in (8, 16, 32)
         self.num_classes = num_classes
         self.drop_rate = drop_rate
-        super(TResNet, self).__init__()
-
-        aa_layer = BlurPool2d
+        self.grad_checkpointing = False
 
-        # TResnet stages
-        self.inplanes = int(64 * width_factor)
-        self.planes = int(64 * width_factor)
-        if v2:
-            self.inplanes = self.inplanes // 8 * 8
-            self.planes = self.planes // 8 * 8
-
-        conv1 = conv2d_iabn(in_chans * 16, self.planes, stride=1, kernel_size=3)
-        layer1 = self._make_layer(
-            Bottleneck if v2 else BasicBlock, self.planes, layers[0], stride=1, use_se=True, aa_layer=aa_layer)
-        layer2 = self._make_layer(
-            Bottleneck if v2 else BasicBlock, self.planes * 2, layers[1], stride=2, use_se=True, aa_layer=aa_layer)
-        layer3 = self._make_layer(
-            Bottleneck, self.planes * 4, layers[2], stride=2, use_se=True, aa_layer=aa_layer)
-        layer4 = self._make_layer(
-            Bottleneck, self.planes * 8, layers[3], stride=2, use_se=False, aa_layer=aa_layer)
-
-        # body
-        self.body = nn.Sequential(OrderedDict([
-            ('SpaceToDepth', SpaceToDepthModule()),
-            ('conv1', conv1),
-            ('layer1', layer1),
-            ('layer2', layer2),
-            ('layer3', layer3),
-            ('layer4', layer4)]))
-
-        self.feature_info = [
-            dict(num_chs=self.planes, reduction=2, module=''),  # Not with S2D?
-            dict(num_chs=self.planes * (Bottleneck.expansion if v2 else 1), reduction=4, module='body.layer1'),
-            dict(num_chs=self.planes * 2 * (Bottleneck.expansion if v2 else 1), reduction=8, module='body.layer2'),
-            dict(num_chs=self.planes * 4 * Bottleneck.expansion, reduction=16, module='body.layer3'),
-            dict(num_chs=self.planes * 8 * Bottleneck.expansion, reduction=32, module='body.layer4'),
-        ]
-
-        # head
-        self.num_features = (self.planes * 8) * Bottleneck.expansion
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate)
-
-        # model initialization
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
-            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, InplaceAbn):
-                nn.init.constant_(m.weight, 1)
-                nn.init.constant_(m.bias, 0)
-
-        # residual connections special initialization
-        for m in self.modules():
-            if isinstance(m, BasicBlock):
-                m.conv2[1].weight = nn.Parameter(torch.zeros_like(m.conv2[1].weight))  # BN to zero
-            if isinstance(m, Bottleneck):
-                m.conv3[1].weight = nn.Parameter(torch.zeros_like(m.conv3[1].weight))  # BN to zero
-            if isinstance(m, nn.Linear):
-                m.weight.data.normal_(0, 0.01)
-
-    def _make_layer(self, block, planes, blocks, stride=1, use_se=True, aa_layer=None):
-        downsample = None
-        if stride != 1 or self.inplanes != planes * block.expansion:
-            layers = []
-            if stride == 2:
-                # avg pooling before 1x1 conv
-                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True, count_include_pad=False))
-            layers += [conv2d_iabn(
-                self.inplanes, planes * block.expansion, kernel_size=1, stride=1, act_layer="identity")]
-            downsample = nn.Sequential(*layers)
-
-        layers = []
-        layers.append(block(
-            self.inplanes, planes, stride, downsample, use_se=use_se, aa_layer=aa_layer))
-        self.inplanes = planes * block.expansion
-        for i in range(1, blocks):
-            layers.append(
-                block(self.inplanes, planes, use_se=use_se, aa_layer=aa_layer))
-        return nn.Sequential(*layers)
+        layer_args = dict(act_layer=act_layer, norm_layer=norm_layer)
+        self.stem = nn.Sequential(*[
+            ConvNormAct(in_chans, 32, kernel_size=3, stride=2, **layer_args),
+            create_conv2d(32, 64, kernel_size=3, stride=1) if preact else
+            ConvNormAct(32, 64, kernel_size=3, stride=1, **layer_args)
+        ])
+
+        curr_dilation = 1
+        curr_stride = 2
+        self.feature_info = []
+        self.blocks = nn.Sequential()
+        module_fn = PreXceptionModule if preact else XceptionModule
+        for i, b in enumerate(block_cfg):
+            b['dilation'] = curr_dilation
+            if b['stride'] > 1:
+                name = f'blocks.{i}.stack.conv2' if preact else f'blocks.{i}.stack.act3'
+                self.feature_info += [dict(num_chs=to_3tuple(b['out_chs'])[-2], reduction=curr_stride, module=name)]
+                next_stride = curr_stride * b['stride']
+                if next_stride > output_stride:
+                    curr_dilation *= b['stride']
+                    b['stride'] = 1
+                else:
+                    curr_stride = next_stride
+            self.blocks.add_module(str(i), module_fn(**b, **layer_args))
+            self.num_features = self.blocks[-1].out_channels
+
+        self.feature_info += [dict(
+            num_chs=self.num_features, reduction=curr_stride, module='blocks.' + str(len(self.blocks) - 1))]
+        self.act = act_layer(inplace=True) if preact else nn.Identity()
+        self.head = ClassifierHead(
+            in_features=self.num_features,
+            num_classes=num_classes,
+            pool_type=global_pool,
+            drop_rate=drop_rate,
+        )
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
-        matcher = dict(stem=r'^body\.conv1', blocks=r'^body\.layer(\d+)' if coarse else r'^body\.layer(\d+)\.(\d+)')
-        return matcher
+        return dict(
+            stem=r'^stem',
+            blocks=r'^blocks\.(\d+)',
+        )
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
-        assert not enable, 'gradient checkpointing not supported'
+        self.grad_checkpointing = enable
 
     @torch.jit.ignore
     def get_classifier(self):
         return self.head.fc
 
-    def reset_classifier(self, num_classes, global_pool='fast'):
-        self.head = ClassifierHead(
-            self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)
+    def reset_classifier(self, num_classes, global_pool='avg'):
+        self.head.reset(num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
-        return self.body(x)
+        x = self.stem(x)
+        if self.grad_checkpointing and not torch.jit.is_scripting():
+            x = checkpoint_seq(self.blocks, x)
+        else:
+            x = self.blocks(x)
+        x = self.act(x)
+        return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=pre_logits)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _create_tresnet(variant, pretrained=False, **kwargs):
+def _xception(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        TResNet, variant, pretrained,
-        feature_cfg=dict(out_indices=(1, 2, 3, 4), flatten_sequential=True),
-        **kwargs)
-
-
-@register_model
-def tresnet_m(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[3, 4, 11, 3], **kwargs)
-    return _create_tresnet('tresnet_m', pretrained=pretrained, **model_kwargs)
+        XceptionAligned,
+        variant,
+        pretrained,
+        feature_cfg=dict(flatten_sequential=True, feature_cls='hook'),
+        **kwargs,
+    )
 
 
-@register_model
-def tresnet_m_miil_in21k(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[3, 4, 11, 3], **kwargs)
-    return _create_tresnet('tresnet_m_miil_in21k', pretrained=pretrained, **model_kwargs)
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (10, 10),
+        'crop_pct': 0.903, 'interpolation': 'bicubic',
+        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
+        'first_conv': 'stem.0.conv', 'classifier': 'head.fc',
+        **kwargs
+    }
 
 
-@register_model
-def tresnet_l(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[4, 5, 18, 3], width_factor=1.2, **kwargs)
-    return _create_tresnet('tresnet_l', pretrained=pretrained, **model_kwargs)
+default_cfgs = generate_default_cfgs({
+    'xception65.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.94,
+    ),
+
+    'xception41.tf_in1k': _cfg(hf_hub_id='timm/'),
+    'xception65.tf_in1k': _cfg(hf_hub_id='timm/'),
+    'xception71.tf_in1k': _cfg(hf_hub_id='timm/'),
+
+    'xception41p.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.94,
+    ),
+    'xception65p.ra3_in1k': _cfg(
+        hf_hub_id='timm/',
+        crop_pct=0.94,
+    ),
+})
 
 
 @register_model
-def tresnet_v2_l(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[3, 4, 23, 3], width_factor=1.0, v2=True, **kwargs)
-    return _create_tresnet('tresnet_v2_l', pretrained=pretrained, **model_kwargs)
+def xception41(pretrained=False, **kwargs) -> XceptionAligned:
+    """ Modified Aligned Xception-41
+    """
+    block_cfg = [
+        # entry flow
+        dict(in_chs=64, out_chs=128, stride=2),
+        dict(in_chs=128, out_chs=256, stride=2),
+        dict(in_chs=256, out_chs=728, stride=2),
+        # middle flow
+        *([dict(in_chs=728, out_chs=728, stride=1)] * 8),
+        # exit flow
+        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),
+        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),
+    ]
+    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)
+    return _xception('xception41', pretrained=pretrained, **model_args)
 
 
 @register_model
-def tresnet_xl(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[4, 5, 24, 3], width_factor=1.3, **kwargs)
-    return _create_tresnet('tresnet_xl', pretrained=pretrained, **model_kwargs)
+def xception65(pretrained=False, **kwargs) -> XceptionAligned:
+    """ Modified Aligned Xception-65
+    """
+    block_cfg = [
+        # entry flow
+        dict(in_chs=64, out_chs=128, stride=2),
+        dict(in_chs=128, out_chs=256, stride=2),
+        dict(in_chs=256, out_chs=728, stride=2),
+        # middle flow
+        *([dict(in_chs=728, out_chs=728, stride=1)] * 16),
+        # exit flow
+        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),
+        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),
+    ]
+    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)
+    return _xception('xception65', pretrained=pretrained, **model_args)
 
 
 @register_model
-def tresnet_m_448(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[3, 4, 11, 3], **kwargs)
-    return _create_tresnet('tresnet_m_448', pretrained=pretrained, **model_kwargs)
+def xception71(pretrained=False, **kwargs) -> XceptionAligned:
+    """ Modified Aligned Xception-71
+    """
+    block_cfg = [
+        # entry flow
+        dict(in_chs=64, out_chs=128, stride=2),
+        dict(in_chs=128, out_chs=256, stride=1),
+        dict(in_chs=256, out_chs=256, stride=2),
+        dict(in_chs=256, out_chs=728, stride=1),
+        dict(in_chs=728, out_chs=728, stride=2),
+        # middle flow
+        *([dict(in_chs=728, out_chs=728, stride=1)] * 16),
+        # exit flow
+        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),
+        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),
+    ]
+    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)
+    return _xception('xception71', pretrained=pretrained, **model_args)
 
 
 @register_model
-def tresnet_l_448(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[4, 5, 18, 3], width_factor=1.2, **kwargs)
-    return _create_tresnet('tresnet_l_448', pretrained=pretrained, **model_kwargs)
+def xception41p(pretrained=False, **kwargs) -> XceptionAligned:
+    """ Modified Aligned Xception-41 w/ Pre-Act
+    """
+    block_cfg = [
+        # entry flow
+        dict(in_chs=64, out_chs=128, stride=2),
+        dict(in_chs=128, out_chs=256, stride=2),
+        dict(in_chs=256, out_chs=728, stride=2),
+        # middle flow
+        *([dict(in_chs=728, out_chs=728, stride=1)] * 8),
+        # exit flow
+        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),
+        dict(in_chs=1024, out_chs=(1536, 1536, 2048), no_skip=True, stride=1),
+    ]
+    model_args = dict(block_cfg=block_cfg, preact=True, norm_layer=nn.BatchNorm2d, **kwargs)
+    return _xception('xception41p', pretrained=pretrained, **model_args)
 
 
 @register_model
-def tresnet_xl_448(pretrained=False, **kwargs):
-    model_kwargs = dict(layers=[4, 5, 24, 3], width_factor=1.3, **kwargs)
-    return _create_tresnet('tresnet_xl_448', pretrained=pretrained, **model_kwargs)
+def xception65p(pretrained=False, **kwargs) -> XceptionAligned:
+    """ Modified Aligned Xception-65 w/ Pre-Act
+    """
+    block_cfg = [
+        # entry flow
+        dict(in_chs=64, out_chs=128, stride=2),
+        dict(in_chs=128, out_chs=256, stride=2),
+        dict(in_chs=256, out_chs=728, stride=2),
+        # middle flow
+        *([dict(in_chs=728, out_chs=728, stride=1)] * 16),
+        # exit flow
+        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),
+        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True),
+    ]
+    model_args = dict(
+        block_cfg=block_cfg, preact=True, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)
+    return _xception('xception65p', pretrained=pretrained, **model_args)
```

### Comparing `timm-0.8.6.dev0/timm/models/twins.py` & `timm-0.9.0/timm/models/twins.py`

 * *Files 5% similar despite different names*

```diff
@@ -16,71 +16,41 @@
 from typing import Tuple
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import Mlp, DropPath, to_2tuple, trunc_normal_
+from timm.layers import Mlp, DropPath, to_2tuple, trunc_normal_, use_fused_attn
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_module
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 from .vision_transformer import Attention
 
 __all__ = ['Twins']  # model_registry will add each entrypoint fn to this
 
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embeds.0.proj', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'twins_pcpvt_small': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/twins_pcpvt_small-e70e7e7a.pth',
-        ),
-    'twins_pcpvt_base': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/twins_pcpvt_base-e5ecb09b.pth',
-        ),
-    'twins_pcpvt_large': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/twins_pcpvt_large-d273f802.pth',
-        ),
-    'twins_svt_small': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/twins_svt_small-42e5f78c.pth',
-        ),
-    'twins_svt_base': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/twins_svt_base-c2265010.pth',
-        ),
-    'twins_svt_large': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/twins_svt_large-90f6aaa9.pth',
-        ),
-}
-
 Size_ = Tuple[int, int]
 
 
 @register_notrace_module  # reason: FX can't symbolically trace control flow in forward method
 class LocallyGroupedAttn(nn.Module):
     """ LSA: self attention within a group
     """
+    fused_attn: torch.jit.Final[bool]
+
     def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., ws=1):
         assert ws != 1
         super(LocallyGroupedAttn, self).__init__()
         assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."
 
         self.dim = dim
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.qkv = nn.Linear(dim, dim * 3, bias=True)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
         self.ws = ws
 
@@ -96,20 +66,30 @@
         pad_b = (self.ws - H % self.ws) % self.ws
         x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
         _, Hp, Wp, _ = x.shape
         _h, _w = Hp // self.ws, Wp // self.ws
         x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3)
         qkv = self.qkv(x).reshape(
             B, _h * _w, self.ws * self.ws, 3, self.num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)
-        q, k, v = qkv[0], qkv[1], qkv[2]
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
-        attn = (attn @ v).transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C)
-        x = attn.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C)
+        q, k, v = qkv.unbind(0)
+
+        if self.fused_attn:
+            x = F.scaled_dot_product_attention(
+                q, k, v,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
+
+        x = x.transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C)
+        x = x.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C)
         if pad_r > 0 or pad_b > 0:
             x = x[:, :H, :W, :].contiguous()
         x = x.reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
@@ -148,22 +128,25 @@
     #     x = self.proj_drop(x)
     #     return x
 
 
 class GlobalSubSampleAttn(nn.Module):
     """ GSA: using a  key to summarize the information for a group to be efficient.
     """
+    fused_attn: torch.jit.Final[bool]
+
     def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., sr_ratio=1):
         super().__init__()
         assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."
 
         self.dim = dim
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.q = nn.Linear(dim, dim, bias=True)
         self.kv = nn.Linear(dim, dim * 2, bias=True)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
 
@@ -180,56 +163,82 @@
         q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
 
         if self.sr is not None:
             x = x.permute(0, 2, 1).reshape(B, C, *size)
             x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)
             x = self.norm(x)
         kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
-        k, v = kv[0], kv[1]
+        k, v = kv.unbind(0)
 
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        if self.fused_attn:
+            x = torch.nn.functional.scaled_dot_product_attention(
+                q, k, v,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
+        x = x.transpose(1, 2).reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
 
         return x
 
 
 class Block(nn.Module):
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0.,
-            act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, ws=None):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            sr_ratio=1,
+            ws=None,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         if ws is None:
-            self.attn = Attention(dim, num_heads, False, None, attn_drop, drop)
+            self.attn = Attention(dim, num_heads, False, None, attn_drop, proj_drop)
         elif ws == 1:
-            self.attn = GlobalSubSampleAttn(dim, num_heads, attn_drop, drop, sr_ratio)
+            self.attn = GlobalSubSampleAttn(dim, num_heads, attn_drop, proj_drop, sr_ratio)
         else:
-            self.attn = LocallyGroupedAttn(dim, num_heads, attn_drop, drop, ws)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+            self.attn = LocallyGroupedAttn(dim, num_heads, attn_drop, proj_drop, ws)
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+
         self.norm2 = norm_layer(dim)
-        mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
     def forward(self, x, size: Size_):
-        x = x + self.drop_path(self.attn(self.norm1(x), size))
-        x = x + self.drop_path(self.mlp(self.norm2(x)))
+        x = x + self.drop_path1(self.attn(self.norm1(x), size))
+        x = x + self.drop_path2(self.mlp(self.norm2(x)))
         return x
 
 
 class PosConv(nn.Module):
     # PEG  from https://arxiv.org/abs/2102.10882
     def __init__(self, in_chans, embed_dim=768, stride=1):
         super(PosConv, self).__init__()
-        self.proj = nn.Sequential(nn.Conv2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim), )
+        self.proj = nn.Sequential(
+            nn.Conv2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim),
+        )
         self.stride = stride
 
     def forward(self, x, size: Size_):
         B, N, C = x.shape
         cnn_feat_token = x.transpose(1, 2).view(B, C, *size)
         x = self.proj(cnn_feat_token)
         if self.stride == 1:
@@ -271,53 +280,77 @@
 
 class Twins(nn.Module):
     """ Twins Vision Transfomer (Revisiting Spatial Attention)
 
     Adapted from PVT (PyramidVisionTransformer) class at https://github.com/whai362/PVT.git
     """
     def __init__(
-            self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, global_pool='avg',
-            embed_dims=(64, 128, 256, 512), num_heads=(1, 2, 4, 8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3),
-            sr_ratios=(8, 4, 2, 1), wss=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,
-            norm_layer=partial(nn.LayerNorm, eps=1e-6), block_cls=Block):
+            self,
+            img_size=224,
+            patch_size=4,
+            in_chans=3,
+            num_classes=1000,
+            global_pool='avg',
+            embed_dims=(64, 128, 256, 512),
+            num_heads=(1, 2, 4, 8),
+            mlp_ratios=(4, 4, 4, 4),
+            depths=(3, 4, 6, 3),
+            sr_ratios=(8, 4, 2, 1),
+            wss=None,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            norm_layer=partial(nn.LayerNorm, eps=1e-6),
+            block_cls=Block,
+    ):
         super().__init__()
         self.num_classes = num_classes
         self.global_pool = global_pool
         self.depths = depths
         self.embed_dims = embed_dims
         self.num_features = embed_dims[-1]
         self.grad_checkpointing = False
 
         img_size = to_2tuple(img_size)
         prev_chs = in_chans
         self.patch_embeds = nn.ModuleList()
         self.pos_drops = nn.ModuleList()
         for i in range(len(depths)):
             self.patch_embeds.append(PatchEmbed(img_size, patch_size, prev_chs, embed_dims[i]))
-            self.pos_drops.append(nn.Dropout(p=drop_rate))
+            self.pos_drops.append(nn.Dropout(p=pos_drop_rate))
             prev_chs = embed_dims[i]
             img_size = tuple(t // patch_size for t in img_size)
             patch_size = 2
 
         self.blocks = nn.ModuleList()
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule
         cur = 0
         for k in range(len(depths)):
             _block = nn.ModuleList([block_cls(
-                dim=embed_dims[k], num_heads=num_heads[k], mlp_ratio=mlp_ratios[k], drop=drop_rate,
-                attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[k],
-                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])])
+                dim=embed_dims[k],
+                num_heads=num_heads[k],
+                mlp_ratio=mlp_ratios[k],
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[k],
+                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])],
+            )
             self.blocks.append(_block)
             cur += depths[k]
 
         self.pos_block = nn.ModuleList([PosConv(embed_dim, embed_dim) for embed_dim in embed_dims])
 
         self.norm = norm_layer(self.num_features)
 
         # classification head
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
         # init weights
         self.apply(self._init_weights)
 
     @torch.jit.ignore
     def no_weight_decay(self):
@@ -382,14 +415,15 @@
                 x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool == 'avg':
             x = x.mean(dim=1)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -398,53 +432,74 @@
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
     model = build_model_with_cfg(Twins, variant, pretrained, **kwargs)
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embeds.0.proj', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'twins_pcpvt_small.in1k': _cfg(hf_hub_id='timm/'),
+    'twins_pcpvt_base.in1k': _cfg(hf_hub_id='timm/'),
+    'twins_pcpvt_large.in1k': _cfg(hf_hub_id='timm/'),
+    'twins_svt_small.in1k': _cfg(hf_hub_id='timm/'),
+    'twins_svt_base.in1k': _cfg(hf_hub_id='timm/'),
+    'twins_svt_large.in1k': _cfg(hf_hub_id='timm/'),
+})
+
+
 @register_model
-def twins_pcpvt_small(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def twins_pcpvt_small(pretrained=False, **kwargs) -> Twins:
+    model_args = dict(
         patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],
-        depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], **kwargs)
-    return _create_twins('twins_pcpvt_small', pretrained=pretrained, **model_kwargs)
+        depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1])
+    return _create_twins('twins_pcpvt_small', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def twins_pcpvt_base(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def twins_pcpvt_base(pretrained=False, **kwargs) -> Twins:
+    model_args = dict(
         patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],
-        depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1], **kwargs)
-    return _create_twins('twins_pcpvt_base', pretrained=pretrained, **model_kwargs)
+        depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1])
+    return _create_twins('twins_pcpvt_base', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def twins_pcpvt_large(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def twins_pcpvt_large(pretrained=False, **kwargs) -> Twins:
+    model_args = dict(
         patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],
-        depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1], **kwargs)
-    return _create_twins('twins_pcpvt_large', pretrained=pretrained, **model_kwargs)
+        depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1])
+    return _create_twins('twins_pcpvt_large', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def twins_svt_small(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def twins_svt_small(pretrained=False, **kwargs) -> Twins:
+    model_args = dict(
         patch_size=4, embed_dims=[64, 128, 256, 512], num_heads=[2, 4, 8, 16], mlp_ratios=[4, 4, 4, 4],
-        depths=[2, 2, 10, 4], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1], **kwargs)
-    return _create_twins('twins_svt_small', pretrained=pretrained, **model_kwargs)
+        depths=[2, 2, 10, 4], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1])
+    return _create_twins('twins_svt_small', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def twins_svt_base(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def twins_svt_base(pretrained=False, **kwargs) -> Twins:
+    model_args = dict(
         patch_size=4, embed_dims=[96, 192, 384, 768], num_heads=[3, 6, 12, 24], mlp_ratios=[4, 4, 4, 4],
-        depths=[2, 2, 18, 2], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1], **kwargs)
-    return _create_twins('twins_svt_base', pretrained=pretrained, **model_kwargs)
+        depths=[2, 2, 18, 2], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1])
+    return _create_twins('twins_svt_base', pretrained=pretrained, **dict(model_args, **kwargs))
 
 
 @register_model
-def twins_svt_large(pretrained=False, **kwargs):
-    model_kwargs = dict(
+def twins_svt_large(pretrained=False, **kwargs) -> Twins:
+    model_args = dict(
         patch_size=4, embed_dims=[128, 256, 512, 1024], num_heads=[4, 8, 16, 32], mlp_ratios=[4, 4, 4, 4],
-        depths=[2, 2, 18, 2], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1], **kwargs)
-    return _create_twins('twins_svt_large', pretrained=pretrained, **model_kwargs)
+        depths=[2, 2, 18, 2], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1])
+    return _create_twins('twins_svt_large', pretrained=pretrained, **dict(model_args, **kwargs))
```

### Comparing `timm-0.8.6.dev0/timm/models/visformer.py` & `timm-0.9.0/timm/models/visformer.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,45 +7,33 @@
 Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman
 """
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
-from timm.layers import to_2tuple, trunc_normal_, DropPath, PatchEmbed, LayerNorm2d, create_classifier
+from timm.layers import to_2tuple, trunc_normal_, DropPath, PatchEmbed, LayerNorm2d, create_classifier, use_fused_attn
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['Visformer']
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.0', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = dict(
-    visformer_tiny=_cfg(),
-    visformer_small=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vt3p-weights/visformer_small-839e1f5b.pth'
-    ),
-)
-
-
 class SpatialMlp(nn.Module):
     def __init__(
-            self, in_features, hidden_features=None, out_features=None,
-            act_layer=nn.GELU, drop=0., group=8, spatial_conv=False):
+            self,
+            in_features,
+            hidden_features=None,
+            out_features=None,
+            act_layer=nn.GELU,
+            drop=0.,
+            group=8,
+            spatial_conv=False,
+    ):
         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         drop_probs = to_2tuple(drop)
 
         self.in_features = in_features
         self.out_features = out_features
@@ -79,176 +67,292 @@
             x = self.act2(x)
         x = self.conv3(x)
         x = self.drop3(x)
         return x
 
 
 class Attention(nn.Module):
+    fused_attn: torch.jit.Final[bool]
+
     def __init__(self, dim, num_heads=8, head_dim_ratio=1., attn_drop=0., proj_drop=0.):
         super().__init__()
         self.dim = dim
         self.num_heads = num_heads
         head_dim = round(dim // num_heads * head_dim_ratio)
         self.head_dim = head_dim
         self.scale = head_dim ** -0.5
+        self.fused_attn = use_fused_attn(experimental=True)
+
         self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, stride=1, padding=0, bias=False)
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False)
         self.proj_drop = nn.Dropout(proj_drop)
 
     def forward(self, x):
         B, C, H, W = x.shape
         x = self.qkv(x).reshape(B, 3, self.num_heads, self.head_dim, -1).permute(1, 0, 2, 4, 3)
         q, k, v = x.unbind(0)
 
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
-        x = attn @ v
+        if self.fused_attn:
+            x = torch.nn.functional.scaled_dot_product_attention(
+                q.contiguous(), k.contiguous(), v.contiguous(),
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            attn = (q @ k.transpose(-2, -1)) * self.scale
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
         x = x.permute(0, 1, 3, 2).reshape(B, -1, H, W)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class Block(nn.Module):
     def __init__(
-            self, dim, num_heads, head_dim_ratio=1., mlp_ratio=4.,
-            drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=LayerNorm2d,
-            group=8, attn_disabled=False, spatial_conv=False):
+            self,
+            dim,
+            num_heads,
+            head_dim_ratio=1.,
+            mlp_ratio=4.,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=LayerNorm2d,
+            group=8,
+            attn_disabled=False,
+            spatial_conv=False,
+    ):
         super().__init__()
         self.spatial_conv = spatial_conv
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         if attn_disabled:
             self.norm1 = None
             self.attn = None
         else:
             self.norm1 = norm_layer(dim)
             self.attn = Attention(
-                dim, num_heads=num_heads, head_dim_ratio=head_dim_ratio, attn_drop=attn_drop, proj_drop=drop)
+                dim,
+                num_heads=num_heads,
+                head_dim_ratio=head_dim_ratio,
+                attn_drop=attn_drop,
+                proj_drop=proj_drop,
+            )
 
         self.norm2 = norm_layer(dim)
         self.mlp = SpatialMlp(
-            in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop,
-            group=group, spatial_conv=spatial_conv)  # new setting
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+            group=group,
+            spatial_conv=spatial_conv,
+        )
 
     def forward(self, x):
         if self.attn is not None:
             x = x + self.drop_path(self.attn(self.norm1(x)))
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
 class Visformer(nn.Module):
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, init_channels=32, embed_dim=384,
-            depth=12, num_heads=6, mlp_ratio=4., drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,
-            norm_layer=LayerNorm2d, attn_stage='111', pos_embed=True, spatial_conv='111',
-            vit_stem=False, group=8, global_pool='avg', conv_init=False, embed_norm=None):
+            self,
+            img_size=224,
+            patch_size=16,
+            in_chans=3,
+            num_classes=1000,
+            init_channels=32,
+            embed_dim=384,
+            depth=12,
+            num_heads=6,
+            mlp_ratio=4.,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            norm_layer=LayerNorm2d,
+            attn_stage='111',
+            use_pos_embed=True,
+            spatial_conv='111',
+            vit_stem=False,
+            group=8,
+            global_pool='avg',
+            conv_init=False,
+            embed_norm=None,
+    ):
         super().__init__()
         img_size = to_2tuple(img_size)
         self.num_classes = num_classes
         self.embed_dim = embed_dim
         self.init_channels = init_channels
         self.img_size = img_size
         self.vit_stem = vit_stem
         self.conv_init = conv_init
         if isinstance(depth, (list, tuple)):
             self.stage_num1, self.stage_num2, self.stage_num3 = depth
             depth = sum(depth)
         else:
             self.stage_num1 = self.stage_num3 = depth // 3
             self.stage_num2 = depth - self.stage_num1 - self.stage_num3
-        self.pos_embed = pos_embed
+        self.use_pos_embed = use_pos_embed
         self.grad_checkpointing = False
 
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
         # stage 1
         if self.vit_stem:
             self.stem = None
             self.patch_embed1 = PatchEmbed(
-                img_size=img_size, patch_size=patch_size, in_chans=in_chans,
-                embed_dim=embed_dim, norm_layer=embed_norm, flatten=False)
+                img_size=img_size,
+                patch_size=patch_size,
+                in_chans=in_chans,
+                embed_dim=embed_dim,
+                norm_layer=embed_norm,
+                flatten=False,
+            )
             img_size = [x // patch_size for x in img_size]
         else:
             if self.init_channels is None:
                 self.stem = None
                 self.patch_embed1 = PatchEmbed(
-                    img_size=img_size, patch_size=patch_size // 2, in_chans=in_chans,
-                    embed_dim=embed_dim // 2, norm_layer=embed_norm, flatten=False)
+                    img_size=img_size,
+                    patch_size=patch_size // 2,
+                    in_chans=in_chans,
+                    embed_dim=embed_dim // 2,
+                    norm_layer=embed_norm,
+                    flatten=False,
+                )
                 img_size = [x // (patch_size // 2) for x in img_size]
             else:
                 self.stem = nn.Sequential(
                     nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),
                     nn.BatchNorm2d(self.init_channels),
                     nn.ReLU(inplace=True)
                 )
                 img_size = [x // 2 for x in img_size]
                 self.patch_embed1 = PatchEmbed(
-                    img_size=img_size, patch_size=patch_size // 4, in_chans=self.init_channels,
-                    embed_dim=embed_dim // 2, norm_layer=embed_norm, flatten=False)
+                    img_size=img_size,
+                    patch_size=patch_size // 4,
+                    in_chans=self.init_channels,
+                    embed_dim=embed_dim // 2,
+                    norm_layer=embed_norm,
+                    flatten=False,
+                )
                 img_size = [x // (patch_size // 4) for x in img_size]
 
-        if self.pos_embed:
+        if self.use_pos_embed:
             if self.vit_stem:
                 self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim, *img_size))
             else:
                 self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim//2, *img_size))
-            self.pos_drop = nn.Dropout(p=drop_rate)
+            self.pos_drop = nn.Dropout(p=pos_drop_rate)
+        else:
+            self.pos_embed1 = None
+
         self.stage1 = nn.Sequential(*[
             Block(
-                dim=embed_dim//2, num_heads=num_heads, head_dim_ratio=0.5, mlp_ratio=mlp_ratio,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                group=group, attn_disabled=(attn_stage[0] == '0'), spatial_conv=(spatial_conv[0] == '1')
+                dim=embed_dim//2,
+                num_heads=num_heads,
+                head_dim_ratio=0.5,
+                mlp_ratio=mlp_ratio,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+                group=group,
+                attn_disabled=(attn_stage[0] == '0'),
+                spatial_conv=(spatial_conv[0] == '1'),
             )
             for i in range(self.stage_num1)
         ])
 
         # stage2
         if not self.vit_stem:
             self.patch_embed2 = PatchEmbed(
-                img_size=img_size, patch_size=patch_size // 8, in_chans=embed_dim // 2,
-                embed_dim=embed_dim, norm_layer=embed_norm, flatten=False)
+                img_size=img_size,
+                patch_size=patch_size // 8,
+                in_chans=embed_dim // 2,
+                embed_dim=embed_dim,
+                norm_layer=embed_norm,
+                flatten=False,
+            )
             img_size = [x // (patch_size // 8) for x in img_size]
-            if self.pos_embed:
+            if self.use_pos_embed:
                 self.pos_embed2 = nn.Parameter(torch.zeros(1, embed_dim, *img_size))
+            else:
+                self.pos_embed2 = None
+        else:
+            self.patch_embed2 = None
         self.stage2 = nn.Sequential(*[
             Block(
-                dim=embed_dim, num_heads=num_heads, head_dim_ratio=1.0, mlp_ratio=mlp_ratio,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                group=group, attn_disabled=(attn_stage[1] == '0'), spatial_conv=(spatial_conv[1] == '1')
+                dim=embed_dim,
+                num_heads=num_heads,
+                head_dim_ratio=1.0,
+                mlp_ratio=mlp_ratio,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+                group=group,
+                attn_disabled=(attn_stage[1] == '0'),
+                spatial_conv=(spatial_conv[1] == '1'),
             )
             for i in range(self.stage_num1, self.stage_num1+self.stage_num2)
         ])
 
         # stage 3
         if not self.vit_stem:
             self.patch_embed3 = PatchEmbed(
-                img_size=img_size, patch_size=patch_size // 8, in_chans=embed_dim,
-                embed_dim=embed_dim * 2, norm_layer=embed_norm, flatten=False)
+                img_size=img_size,
+                patch_size=patch_size // 8,
+                in_chans=embed_dim,
+                embed_dim=embed_dim * 2,
+                norm_layer=embed_norm,
+                flatten=False,
+            )
             img_size = [x // (patch_size // 8) for x in img_size]
-            if self.pos_embed:
+            if self.use_pos_embed:
                 self.pos_embed3 = nn.Parameter(torch.zeros(1, embed_dim*2, *img_size))
+            else:
+                self.pos_embed3 = None
+        else:
+            self.patch_embed3 = None
         self.stage3 = nn.Sequential(*[
             Block(
-                dim=embed_dim*2, num_heads=num_heads, head_dim_ratio=1.0, mlp_ratio=mlp_ratio,
-                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
-                group=group, attn_disabled=(attn_stage[2] == '0'), spatial_conv=(spatial_conv[2] == '1')
+                dim=embed_dim * 2,
+                num_heads=num_heads,
+                head_dim_ratio=1.0,
+                mlp_ratio=mlp_ratio,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[i],
+                norm_layer=norm_layer,
+                group=group,
+                attn_disabled=(attn_stage[2] == '0'),
+                spatial_conv=(spatial_conv[2] == '1'),
             )
             for i in range(self.stage_num1+self.stage_num2, depth)
         ])
 
-        # head
         self.num_features = embed_dim if self.vit_stem else embed_dim * 2
         self.norm = norm_layer(self.num_features)
-        self.global_pool, self.head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+
+        # head
+        global_pool, head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)
+        self.global_pool = global_pool
+        self.head_drop = nn.Dropout(drop_rate)
+        self.head = head
 
         # weights init
-        if self.pos_embed:
+        if self.use_pos_embed:
             trunc_normal_(self.pos_embed1, std=0.02)
             if not self.vit_stem:
                 trunc_normal_(self.pos_embed2, std=0.02)
                 trunc_normal_(self.pos_embed3, std=0.02)
         self.apply(self._init_weights)
 
     def _init_weights(self, m):
@@ -289,46 +393,47 @@
 
     def forward_features(self, x):
         if self.stem is not None:
             x = self.stem(x)
 
         # stage 1
         x = self.patch_embed1(x)
-        if self.pos_embed:
+        if self.pos_embed1 is not None:
             x = self.pos_drop(x + self.pos_embed1)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stage1, x)
         else:
             x = self.stage1(x)
 
         # stage 2
-        if not self.vit_stem:
+        if self.patch_embed2 is not None:
             x = self.patch_embed2(x)
-            if self.pos_embed:
+            if self.pos_embed2 is not None:
                 x = self.pos_drop(x + self.pos_embed2)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stage2, x)
         else:
             x = self.stage2(x)
 
         # stage3
-        if not self.vit_stem:
+        if self.patch_embed3 is not None:
             x = self.patch_embed3(x)
-            if self.pos_embed:
+            if self.pos_embed3 is not None:
                 x = self.pos_drop(x + self.pos_embed3)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stage3, x)
         else:
             x = self.stage3(x)
 
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -336,31 +441,48 @@
 def _create_visformer(variant, pretrained=False, default_cfg=None, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
     model = build_model_with_cfg(Visformer, variant, pretrained, **kwargs)
     return model
 
 
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.0', 'classifier': 'head',
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'visformer_tiny.in1k': _cfg(hf_hub_id='timm/'),
+    'visformer_small.in1k': _cfg(hf_hub_id='timm/'),
+})
+
+
 @register_model
-def visformer_tiny(pretrained=False, **kwargs):
+def visformer_tiny(pretrained=False, **kwargs) -> Visformer:
     model_cfg = dict(
         init_channels=16, embed_dim=192, depth=(7, 4, 4), num_heads=3, mlp_ratio=4., group=8,
         attn_stage='011', spatial_conv='100', norm_layer=nn.BatchNorm2d, conv_init=True,
-        embed_norm=nn.BatchNorm2d, **kwargs)
-    model = _create_visformer('visformer_tiny', pretrained=pretrained, **model_cfg)
+        embed_norm=nn.BatchNorm2d)
+    model = _create_visformer('visformer_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 @register_model
-def visformer_small(pretrained=False, **kwargs):
+def visformer_small(pretrained=False, **kwargs) -> Visformer:
     model_cfg = dict(
         init_channels=32, embed_dim=384, depth=(7, 4, 4), num_heads=6, mlp_ratio=4., group=8,
         attn_stage='011', spatial_conv='100', norm_layer=nn.BatchNorm2d, conv_init=True,
-        embed_norm=nn.BatchNorm2d, **kwargs)
-    model = _create_visformer('visformer_small', pretrained=pretrained, **model_cfg)
+        embed_norm=nn.BatchNorm2d)
+    model = _create_visformer('visformer_small', pretrained=pretrained, **dict(model_cfg, **kwargs))
     return model
 
 
 # @register_model
 # def visformer_net1(pretrained=False, **kwargs):
 #     model = Visformer(
 #         init_channels=None, embed_dim=384, depth=(0, 12, 0), num_heads=6, mlp_ratio=4., attn_stage='111',
```

### Comparing `timm-0.8.6.dev0/timm/models/vision_transformer.py` & `timm-0.9.0/timm/models/vision_transformer.py`

 * *Files 20% similar despite different names*

```diff
@@ -23,60 +23,82 @@
 
 Hacked together by / Copyright 2020, Ross Wightman
 """
 import logging
 import math
 from collections import OrderedDict
 from functools import partial
-from typing import Optional, List
+from typing import Callable, List, Optional, Sequence, Tuple, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.utils.checkpoint
+from torch.jit import Final
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, \
     OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
 from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_, resample_patch_embed, \
-    resample_abs_pos_embed
+    resample_abs_pos_embed, RmsNorm, PatchDropout, use_fused_attn, SwiGLUPacked
 from ._builder import build_model_with_cfg
 from ._manipulate import named_apply, checkpoint_seq, adapt_input_conv
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
-
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 
 __all__ = ['VisionTransformer']  # model_registry will add each entrypoint fn to this
 
 
 _logger = logging.getLogger(__name__)
 
 
 class Attention(nn.Module):
-    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
+    fused_attn: Final[bool]
+
+    def __init__(
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            qk_norm=False,
+            attn_drop=0.,
+            proj_drop=0.,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
         assert dim % num_heads == 0, 'dim should be divisible by num_heads'
         self.num_heads = num_heads
-        head_dim = dim // num_heads
-        self.scale = head_dim ** -0.5
+        self.head_dim = dim // num_heads
+        self.scale = self.head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
 
         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
+        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
+        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
         self.attn_drop = nn.Dropout(attn_drop)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(proj_drop)
 
     def forward(self, x):
         B, N, C = x.shape
-        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)
-
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
+        q, k, v = qkv.unbind(0)
+        q, k = self.q_norm(q), self.k_norm(k)
+
+        if self.fused_attn:
+            x = F.scaled_dot_product_attention(
+                q, k, v,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x = attn @ v
 
-        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
+        x = x.transpose(1, 2).reshape(B, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
         return x
 
 
 class LayerScale(nn.Module):
     def __init__(self, dim, init_values=1e-5, inplace=False):
@@ -92,30 +114,44 @@
 
     def __init__(
             self,
             dim,
             num_heads,
             mlp_ratio=4.,
             qkv_bias=False,
-            drop=0.,
+            qk_norm=False,
+            proj_drop=0.,
             attn_drop=0.,
             init_values=None,
             drop_path=0.,
             act_layer=nn.GELU,
-            norm_layer=nn.LayerNorm
+            norm_layer=nn.LayerNorm,
+            mlp_layer=Mlp,
     ):
         super().__init__()
         self.norm1 = norm_layer(dim)
-        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+        self.attn = Attention(
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            qk_norm=qk_norm,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            norm_layer=norm_layer,
+        )
         self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
-        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
         self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = mlp_layer(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
         self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
         self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
     def forward(self, x):
         x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
@@ -125,29 +161,44 @@
 
     def __init__(
             self,
             dim,
             num_heads,
             mlp_ratio=4.,
             qkv_bias=False,
-            drop=0.,
+            qk_norm=False,
+            proj_drop=0.,
             attn_drop=0.,
             init_values=None,
             drop_path=0.,
             act_layer=nn.GELU,
-            norm_layer=nn.LayerNorm
+            norm_layer=nn.LayerNorm,
+            mlp_layer=Mlp,
     ):
         super().__init__()
         self.init_values = init_values
 
-        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+        self.attn = Attention(
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            qk_norm=qk_norm,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop,
+            norm_layer=norm_layer,
+        )
         self.norm1 = norm_layer(dim)
         self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = mlp_layer(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+        )
         self.norm2 = norm_layer(dim)
         self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         self.init_weights()
 
     def init_weights(self):
         # NOTE this init overrides that base model init with specific changes for the block type
@@ -157,44 +208,157 @@
 
     def forward(self, x):
         x = x + self.drop_path1(self.norm1(self.attn(x)))
         x = x + self.drop_path2(self.norm2(self.mlp(x)))
         return x
 
 
-class ParallelBlock(nn.Module):
+class ParallelScalingBlock(nn.Module):
+    """ Parallel ViT block (MLP & Attention in parallel)
+    Based on:
+      'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442
+    """
+    fused_attn: Final[bool]
+
+    def __init__(
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            qk_norm=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            init_values=None,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            mlp_layer=None,  # NOTE: not used
+    ):
+        super().__init__()
+        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
+        self.num_heads = num_heads
+        self.head_dim = dim // num_heads
+        self.scale = self.head_dim ** -0.5
+        self.fused_attn = use_fused_attn()
+        mlp_hidden_dim = int(mlp_ratio * dim)
+        in_proj_out_dim = mlp_hidden_dim + 3 * dim
+
+        self.in_norm = norm_layer(dim)
+        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias)
+        self.in_split = [mlp_hidden_dim] + [dim] * 3
+        if qkv_bias:
+            self.register_buffer('qkv_bias', None)
+            self.register_parameter('mlp_bias', None)
+        else:
+            self.register_buffer('qkv_bias', torch.zeros(3 * dim), persistent=False)
+            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim))
+
+        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
+        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
+        self.attn_drop = nn.Dropout(attn_drop)
+        self.attn_out_proj = nn.Linear(dim, dim)
+
+        self.mlp_drop = nn.Dropout(proj_drop)
+        self.mlp_act = act_layer()
+        self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim)
+
+        self.ls = LayerScale(dim, init_values=init_values) if init_values is not None else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
+    def forward(self, x):
+        B, N, C = x.shape
+
+        # Combined MLP fc1 & qkv projections
+        y = self.in_norm(x)
+        if self.mlp_bias is not None:
+            # Concat constant zero-bias for qkv w/ trainable mlp_bias.
+            # Appears faster than adding to x_mlp separately
+            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))
+        else:
+            y = self.in_proj(y)
+        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)
+
+        # Dot product attention w/ qk norm
+        q = self.q_norm(q.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)
+        k = self.k_norm(k.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)
+        v = v.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
+        if self.fused_attn:
+            x_attn = F.scaled_dot_product_attention(
+                q, k, v,
+                dropout_p=self.attn_drop.p,
+            )
+        else:
+            q = q * self.scale
+            attn = q @ k.transpose(-2, -1)
+            attn = attn.softmax(dim=-1)
+            attn = self.attn_drop(attn)
+            x_attn = attn @ v
+        x_attn = x_attn.transpose(1, 2).reshape(B, N, C)
+        x_attn = self.attn_out_proj(x_attn)
+
+        # MLP activation, dropout, fc2
+        x_mlp = self.mlp_act(x_mlp)
+        x_mlp = self.mlp_drop(x_mlp)
+        x_mlp = self.mlp_out_proj(x_mlp)
+
+        # Add residual w/ drop path & layer scale applied
+        y = self.drop_path(self.ls(x_attn + x_mlp))
+        x = x + y
+        return x
+
+
+class ParallelThingsBlock(nn.Module):
+    """ Parallel ViT block (N parallel attention followed by N parallel MLP)
+    Based on:
+      `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795
+    """
     def __init__(
             self,
             dim,
             num_heads,
             num_parallel=2,
             mlp_ratio=4.,
             qkv_bias=False,
+            qk_norm=False,
             init_values=None,
-            drop=0.,
+            proj_drop=0.,
             attn_drop=0.,
             drop_path=0.,
             act_layer=nn.GELU,
-            norm_layer=nn.LayerNorm
+            norm_layer=nn.LayerNorm,
+            mlp_layer=Mlp,
     ):
         super().__init__()
         self.num_parallel = num_parallel
         self.attns = nn.ModuleList()
         self.ffns = nn.ModuleList()
         for _ in range(num_parallel):
             self.attns.append(nn.Sequential(OrderedDict([
                 ('norm', norm_layer(dim)),
-                ('attn', Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)),
+                ('attn', Attention(
+                    dim,
+                    num_heads=num_heads,
+                    qkv_bias=qkv_bias,
+                    qk_norm=qk_norm,
+                    attn_drop=attn_drop,
+                    proj_drop=proj_drop,
+                    norm_layer=norm_layer,
+                )),
                 ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),
                 ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())
             ])))
             self.ffns.append(nn.Sequential(OrderedDict([
                 ('norm', norm_layer(dim)),
-                ('mlp', Mlp(dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)),
+                ('mlp', mlp_layer(
+                    dim,
+                    hidden_features=int(dim * mlp_ratio),
+                    act_layer=act_layer,
+                    drop=proj_drop,
+                )),
                 ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),
                 ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())
             ])))
 
     def _forward_jit(self, x):
         x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)
         x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)
@@ -218,60 +382,67 @@
 
     A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
         - https://arxiv.org/abs/2010.11929
     """
 
     def __init__(
             self,
-            img_size=224,
-            patch_size=16,
-            in_chans=3,
-            num_classes=1000,
-            global_pool='token',
-            embed_dim=768,
-            depth=12,
-            num_heads=12,
-            mlp_ratio=4.,
-            qkv_bias=True,
-            init_values=None,
-            class_token=True,
-            no_embed_class=False,
-            pre_norm=False,
-            fc_norm=None,
-            drop_rate=0.,
-            attn_drop_rate=0.,
-            drop_path_rate=0.,
-            weight_init='',
-            embed_layer=PatchEmbed,
-            norm_layer=None,
-            act_layer=None,
-            block_fn=Block,
+            img_size: Union[int, Tuple[int, int]] = 224,
+            patch_size: Union[int, Tuple[int, int]] = 16,
+            in_chans: int = 3,
+            num_classes: int = 1000,
+            global_pool: str = 'token',
+            embed_dim: int = 768,
+            depth: int = 12,
+            num_heads: int = 12,
+            mlp_ratio: float = 4.,
+            qkv_bias: bool = True,
+            qk_norm: bool = False,
+            init_values: Optional[float] = None,
+            class_token: bool = True,
+            no_embed_class: bool = False,
+            pre_norm: bool = False,
+            fc_norm: Optional[bool] = None,
+            drop_rate: float = 0.,
+            pos_drop_rate: float = 0.,
+            patch_drop_rate: float = 0.,
+            proj_drop_rate: float = 0.,
+            attn_drop_rate: float = 0.,
+            drop_path_rate: float = 0.,
+            weight_init: str = '',
+            embed_layer: Callable = PatchEmbed,
+            norm_layer: Optional[Callable] = None,
+            act_layer: Optional[Callable] = None,
+            block_fn: Callable = Block,
+            mlp_layer: Callable = Mlp,
     ):
         """
         Args:
-            img_size (int, tuple): input image size
-            patch_size (int, tuple): patch size
-            in_chans (int): number of input channels
-            num_classes (int): number of classes for classification head
-            global_pool (str): type of global pooling for final sequence (default: 'token')
-            embed_dim (int): embedding dimension
-            depth (int): depth of transformer
-            num_heads (int): number of attention heads
-            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
-            qkv_bias (bool): enable bias for qkv if True
-            init_values: (float): layer-scale init values
-            class_token (bool): use class token
-            fc_norm (Optional[bool]): pre-fc norm after pool, set if global_pool == 'avg' if None (default: None)
-            drop_rate (float): dropout rate
-            attn_drop_rate (float): attention dropout rate
-            drop_path_rate (float): stochastic depth rate
-            weight_init (str): weight init scheme
-            embed_layer (nn.Module): patch embedding layer
-            norm_layer: (nn.Module): normalization layer
-            act_layer: (nn.Module): MLP activation layer
+            img_size: Input image size.
+            patch_size: Patch size.
+            in_chans: Number of image input channels.
+            num_classes: Mumber of classes for classification head.
+            global_pool: Type of global pooling for final sequence (default: 'token').
+            embed_dim: Transformer embedding dimension.
+            depth: Depth of transformer.
+            num_heads: Number of attention heads.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            qkv_bias: Enable bias for qkv projections if True.
+            init_values: Layer-scale init values (layer-scale enabled if not None).
+            class_token: Use class token.
+            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.
+            drop_rate: Head dropout rate.
+            pos_drop_rate: Position embedding dropout rate.
+            attn_drop_rate: Attention dropout rate.
+            drop_path_rate: Stochastic depth rate.
+            weight_init: Weight initialization scheme.
+            embed_layer: Patch embedding layer.
+            norm_layer: Normalization layer.
+            act_layer: MLP activation layer.
+            block_fn: Transformer block layer.
         """
         super().__init__()
         assert global_pool in ('', 'avg', 'token')
         assert class_token or global_pool != 'token'
         use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm
         norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
         act_layer = act_layer or nn.GELU
@@ -291,36 +462,46 @@
             bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)
         )
         num_patches = self.patch_embed.num_patches
 
         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None
         embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
         self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+        if patch_drop_rate > 0:
+            self.patch_drop = PatchDropout(
+                patch_drop_rate,
+                num_prefix_tokens=self.num_prefix_tokens,
+            )
+        else:
+            self.patch_drop = nn.Identity()
         self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()
 
         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
         self.blocks = nn.Sequential(*[
             block_fn(
                 dim=embed_dim,
                 num_heads=num_heads,
                 mlp_ratio=mlp_ratio,
                 qkv_bias=qkv_bias,
+                qk_norm=qk_norm,
                 init_values=init_values,
-                drop=drop_rate,
+                proj_drop=proj_drop_rate,
                 attn_drop=attn_drop_rate,
                 drop_path=dpr[i],
                 norm_layer=norm_layer,
-                act_layer=act_layer
+                act_layer=act_layer,
+                mlp_layer=mlp_layer,
             )
             for i in range(depth)])
         self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()
 
         # Classifier Head
         self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
 
         if weight_init != 'skip':
             self.init_weights(weight_init)
 
     def init_weights(self, mode=''):
         assert mode in ('jax', 'jax_nlhb', 'moco', '')
@@ -375,29 +556,80 @@
             # original timm, JAX, and deit vit impl
             # pos_embed has entry for class token, concat then add
             if self.cls_token is not None:
                 x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
             x = x + self.pos_embed
         return self.pos_drop(x)
 
+    def _intermediate_layers(
+            self,
+            x: torch.Tensor,
+            n: Union[int, Sequence] = 1,
+    ):
+        outputs, num_blocks = [], len(self.blocks)
+        take_indices = set(range(num_blocks - n, num_blocks) if isinstance(n, int) else n)
+
+        # forward pass
+        x = self.patch_embed(x)
+        x = self._pos_embed(x)
+        x = self.patch_drop(x)
+        x = self.norm_pre(x)
+        for i, blk in enumerate(self.blocks):
+            x = blk(x)
+            if i in take_indices:
+                outputs.append(x)
+
+        return outputs
+
+    def get_intermediate_layers(
+            self,
+            x: torch.Tensor,
+            n: Union[int, Sequence] = 1,
+            reshape: bool = False,
+            return_class_token: bool = False,
+            norm: bool = False,
+    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:
+        """ Intermediate layer accessor (NOTE: This is a WIP experiment).
+        Inspired by DINO / DINOv2 interface
+        """
+        # take last n blocks if n is an int, if in is a sequence, select by matching indices
+        outputs = self._intermediate_layers(x, n)
+        if norm:
+            outputs = [self.norm(out) for out in outputs]
+        class_tokens = [out[:, 0:self.num_prefix_tokens] for out in outputs]
+        outputs = [out[:, self.num_prefix_tokens:] for out in outputs]
+
+        if reshape:
+            grid_size = self.patch_embed.grid_size
+            outputs = [
+                out.reshape(x.shape[0], grid_size[0], grid_size[1], -1).permute(0, 3, 1, 2).contiguous()
+                for out in outputs
+            ]
+
+        if return_class_token:
+            return tuple(zip(outputs, class_tokens))
+        return tuple(outputs)
+
     def forward_features(self, x):
         x = self.patch_embed(x)
         x = self._pos_embed(x)
+        x = self.patch_drop(x)
         x = self.norm_pre(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
             x = self.blocks(x)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool:
             x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
         x = self.fc_norm(x)
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -627,31 +859,49 @@
                     0 if getattr(model, 'no_embed_class') else getattr(model, 'num_prefix_tokens', 1),
                     model.patch_embed.grid_size
                 )
         out_dict[k] = v
     return out_dict
 
 
+def _convert_dinov2(state_dict, model):
+    import re
+    out_dict = {}
+    for k, v in state_dict.items():
+        if k == "mask_token":
+            continue
+        elif re.match(r"blocks\.(\d+)\.mlp\.w12\.(?:weight|bias)", k):
+            out_dict[k.replace("w12", "fc1")] = v
+            continue
+        elif re.match(r"blocks\.(\d+)\.mlp\.w3\.(?:weight|bias)", k):
+            out_dict[k.replace("w3", "fc2")] = v
+            continue
+        out_dict[k] = v
+    return out_dict
+
+
 def checkpoint_filter_fn(
         state_dict,
         model,
         adapt_layer_scale=False,
         interpolation='bicubic',
         antialias=True,
 ):
     """ convert patch embedding weight from manual patchify + linear proj to conv"""
     import re
     out_dict = {}
-    if 'model' in state_dict:
-        # For deit models
-        state_dict = state_dict['model']
+    state_dict = state_dict.get('model', state_dict)
+    state_dict = state_dict.get('state_dict', state_dict)
 
     if 'visual.class_embedding' in state_dict:
         return _convert_openai_clip(state_dict, model)
 
+    if "mask_token" in state_dict:
+        return _convert_dinov2(state_dict, model)
+
     for k, v in state_dict.items():
         if 'patch_embed.proj.weight' in k:
             O, I, H, W = model.patch_embed.proj.weight.shape
             if len(v.shape) < 4:
                 # For old models that I trained prior to conv based patchification
                 O, I, H, W = model.patch_embed.proj.weight.shape
                 v = v.reshape(O, -1, H, W)
@@ -839,18 +1089,18 @@
         custom_load=True, num_classes=21843),
     'vit_large_patch16_224.augreg_in21k': _cfg(
         url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz',
         hf_hub_id='timm/',
         custom_load=True, num_classes=21843),
 
     # SAM trained models (https://arxiv.org/abs/2106.01548)
-    'vit_base_patch32_224.sam': _cfg(
+    'vit_base_patch32_224.sam_in1k': _cfg(
         url='https://storage.googleapis.com/vit_models/sam/ViT-B_32.npz', custom_load=True,
         hf_hub_id='timm/'),
-    'vit_base_patch16_224.sam': _cfg(
+    'vit_base_patch16_224.sam_in1k': _cfg(
         url='https://storage.googleapis.com/vit_models/sam/ViT-B_16.npz', custom_load=True,
         hf_hub_id='timm/'),
 
     # DINO pretrained - https://arxiv.org/abs/2104.14294 (no classifier head, for fine-tune only)
     'vit_small_patch16_224.dino': _cfg(
         url='https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth',
         hf_hub_id='timm/',
@@ -863,36 +1113,62 @@
         url='https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth',
         hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),
     'vit_base_patch8_224.dino': _cfg(
         url='https://dl.fbaipublicfiles.com/dino/dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth',
         hf_hub_id='timm/',
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),
+    
+    # DINOv2 pretrained - https://arxiv.org/abs/2304.07193 (no classifier head, for fine-tune/features only)
+    'vit_small_patch14_dinov2.lvd142m': _cfg(
+        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,
+        input_size=(3, 518, 518), crop_pct=1.0),
+    'vit_base_patch14_dinov2.lvd142m': _cfg(
+        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,
+        input_size=(3, 518, 518), crop_pct=1.0),
+    'vit_large_patch14_dinov2.lvd142m': _cfg(
+        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,
+        input_size=(3, 518, 518), crop_pct=1.0),
+    'vit_giant_patch14_dinov2.lvd142m': _cfg(
+        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,
+        input_size=(3, 518, 518), crop_pct=1.0),
 
     # ViT ImageNet-21K-P pretraining by MILL
     'vit_base_patch16_224_miil.in21k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/vit_base_patch16_224_in21k_miil-887286df.pth',
         hf_hub_id='timm/',
         mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear', num_classes=11221),
     'vit_base_patch16_224_miil.in21k_ft_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/vit_base_patch16_224_1k_miil_84_4-2deb18e3.pth',
         hf_hub_id='timm/',
         mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear'),
 
     # Custom timm variants
-    'vit_base_patch16_rpn_224.in1k': _cfg(
+    'vit_base_patch16_rpn_224.sw_in1k': _cfg(
         url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_base_patch16_rpn_224-sw-3b07e89d.pth',
         hf_hub_id='timm/'),
-    'vit_medium_patch16_gap_240.in12k': _cfg(
+    'vit_medium_patch16_gap_240.sw_in12k': _cfg(
         hf_hub_id='timm/',
         input_size=(3, 240, 240), crop_pct=0.95, num_classes=11821),
-    'vit_medium_patch16_gap_256.in12k_ft_in1k': _cfg(
+    'vit_medium_patch16_gap_256.sw_in12k_ft_in1k': _cfg(
         hf_hub_id='timm/',
         input_size=(3, 256, 256), crop_pct=0.95),
-    'vit_medium_patch16_gap_384.in12k_ft_in1k': _cfg(
+    'vit_medium_patch16_gap_384.sw_in12k_ft_in1k': _cfg(
         hf_hub_id='timm/',
         input_size=(3, 384, 384), crop_pct=0.95, crop_mode='squash'),
     'vit_base_patch16_gap_224': _cfg(),
 
     # CLIP pretrained image tower and related fine-tuned weights
     'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k': _cfg(
         hf_hub_id='timm/',
@@ -922,15 +1198,15 @@
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),
     'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         crop_pct=1.0, input_size=(3, 336, 336), crop_mode='squash'),
 
     'vit_base_patch32_clip_224.openai_ft_in12k_in1k': _cfg(
-        # hf_hub_id='timm/vit_base_patch32_clip_224.openai_ft_in12k_in1k',
+        # hf_hub_id='timm/vit_base_patch32_clip_224.openai_ft_in12k_in1k',  # FIXME weight exists, need to push
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD),
     'vit_base_patch32_clip_384.openai_ft_in12k_in1k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         crop_pct=0.95, input_size=(3, 384, 384), crop_mode='squash'),
     'vit_base_patch16_clip_224.openai_ft_in12k_in1k': _cfg(
         hf_hub_id='timm/',
@@ -983,94 +1259,106 @@
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         crop_pct=1.0, input_size=(3, 384, 384), crop_mode='squash'),
     'vit_large_patch14_clip_224.openai_ft_in1k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),
 
     'vit_base_patch32_clip_224.laion2b_ft_in12k': _cfg(
-        #hf_hub_id='timm/vit_base_patch32_clip_224.laion2b_ft_in12k',
+        #hf_hub_id='timm/vit_base_patch32_clip_224.laion2b_ft_in12k',  # FIXME weight exists, need to push
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),
     'vit_base_patch16_clip_224.laion2b_ft_in12k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),
     'vit_large_patch14_clip_224.laion2b_ft_in12k': _cfg(
         hf_hub_id='timm/',
         mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, crop_pct=1.0, num_classes=11821),
     'vit_huge_patch14_clip_224.laion2b_ft_in12k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=11821),
 
     'vit_base_patch32_clip_224.openai_ft_in12k': _cfg(
-        # hf_hub_id='timm/vit_base_patch32_clip_224.openai_ft_in12k',
+        # hf_hub_id='timm/vit_base_patch32_clip_224.openai_ft_in12k',  # FIXME weight exists, need to push
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),
     'vit_base_patch16_clip_224.openai_ft_in12k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),
     'vit_large_patch14_clip_224.openai_ft_in12k': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=11821),
 
     'vit_base_patch32_clip_224.laion2b': _cfg(
         hf_hub_id='laion/CLIP-ViT-B-32-laion2B-s34B-b79K',
         hf_hub_filename='open_clip_pytorch_model.bin',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=512),
     'vit_base_patch16_clip_224.laion2b': _cfg(
-        # hf_hub_id='laion/CLIP-ViT-B-16-laion2B-s34B-b88K',
+        hf_hub_id='laion/CLIP-ViT-B-16-laion2B-s34B-b88K',
         hf_hub_filename='open_clip_pytorch_model.bin',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=512),
     'vit_large_patch14_clip_224.laion2b': _cfg(
         hf_hub_id='laion/CLIP-ViT-L-14-laion2B-s32B-b82K',
         hf_hub_filename='open_clip_pytorch_model.bin',
         mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, crop_pct=1.0, num_classes=768),
+    'vit_large_patch14_clip_224.datacompxl': _cfg(
+        hf_hub_id='laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=768),
     'vit_huge_patch14_clip_224.laion2b': _cfg(
         hf_hub_id='laion/CLIP-ViT-H-14-laion2B-s32B-b79K',
         hf_hub_filename='open_clip_pytorch_model.bin',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=1024),
     'vit_giant_patch14_clip_224.laion2b': _cfg(
         hf_hub_id='laion/CLIP-ViT-g-14-laion2B-s12B-b42K',
         hf_hub_filename='open_clip_pytorch_model.bin',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=1024),
+    'vit_gigantic_patch14_clip_224.laion2b': _cfg(
+        hf_hub_id='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k',
+        hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=1280),
 
     'vit_base_patch32_clip_224.openai': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=512),
     'vit_base_patch16_clip_224.openai': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=512),
     'vit_large_patch14_clip_224.openai': _cfg(
         hf_hub_id='timm/',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=768),
+    'vit_large_patch14_clip_336.openai': _cfg(
+        hf_hub_id='timm/', hf_hub_filename='open_clip_pytorch_model.bin',
+        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
+        crop_pct=1.0, input_size=(3, 336, 336), num_classes=768),
 
     # experimental (may be removed)
-    'vit_base_patch32_plus_256': _cfg(url='', input_size=(3, 256, 256), crop_pct=0.95),
-    'vit_base_patch16_plus_240': _cfg(url='', input_size=(3, 240, 240), crop_pct=0.95),
-    'vit_small_patch16_36x1_224': _cfg(url=''),
-    'vit_small_patch16_18x2_224': _cfg(url=''),
-    'vit_base_patch16_18x2_224': _cfg(url=''),
+    'vit_base_patch32_plus_256.untrained': _cfg(url='', input_size=(3, 256, 256), crop_pct=0.95),
+    'vit_base_patch16_plus_240.untrained': _cfg(url='', input_size=(3, 240, 240), crop_pct=0.95),
+    'vit_small_patch16_36x1_224.untrained': _cfg(url=''),
+    'vit_small_patch16_18x2_224.untrained': _cfg(url=''),
+    'vit_base_patch16_18x2_224.untrained': _cfg(url=''),
 
     # EVA fine-tuned weights from MAE style MIM - EVA-CLIP target pretrain
     # https://github.com/baaivision/EVA/blob/7ecf2c0a370d97967e86d047d7af9188f78d2df3/eva/README.md#eva-l-learning-better-mim-representations-from-eva-clip
     'eva_large_patch14_196.in22k_ft_in22k_in1k': _cfg(
         # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_196px_21k_to_1k_ft_88p6.pt',
-        hf_hub_id='timm/',
+        hf_hub_id='timm/', license='mit',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         input_size=(3, 196, 196), crop_pct=1.0),
     'eva_large_patch14_336.in22k_ft_in22k_in1k': _cfg(
         # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_336px_21k_to_1k_ft_89p2.pt',
-        hf_hub_id='timm/',
+        hf_hub_id='timm/', license='mit',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),
     'eva_large_patch14_196.in22k_ft_in1k': _cfg(
         # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_196px_1k_ft_88p0.pt',
-        hf_hub_id='timm/',
+        hf_hub_id='timm/', license='mit',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         input_size=(3, 196, 196), crop_pct=1.0),
     'eva_large_patch14_336.in22k_ft_in1k': _cfg(
         # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_336px_1k_ft_88p65.pt',
-        hf_hub_id='timm/',
+        hf_hub_id='timm/', license='mit',
         mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,
         input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),
 
     'flexivit_small.1200ep_in1k': _cfg(
         url='https://storage.googleapis.com/big_vision/flexivit/flexivit_s_i1k.npz', custom_load=True,
         hf_hub_id='timm/',
         input_size=(3, 240, 240), crop_pct=0.95),
@@ -1121,14 +1409,34 @@
         url='https://storage.googleapis.com/big_vision/flexivit/vit_b16_i21k_300ep.npz', custom_load=True,
         hf_hub_id='timm/',
         input_size=(3, 240, 240), crop_pct=0.95, num_classes=21843),
     'flexivit_base.patch30_in21k': _cfg(
         url='https://storage.googleapis.com/big_vision/flexivit/vit_b30_i21k_300ep.npz', custom_load=True,
         hf_hub_id='timm/',
         input_size=(3, 240, 240), crop_pct=0.95, num_classes=21843),
+
+    'vit_base_patch16_xp_224.untrained': _cfg(url=''),
+    'vit_large_patch14_xp_224.untrained': _cfg(url=''),
+    'vit_huge_patch14_xp_224.untrained': _cfg(url=''),
+
+    'vit_base_patch16_224.mae': _cfg(
+        url='https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),
+    'vit_large_patch16_224.mae': _cfg(
+        url='https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_large.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),
+    'vit_huge_patch14_224.mae': _cfg(
+        url='https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_huge.pth',
+        hf_hub_id='timm/',
+        license='cc-by-nc-4.0',
+        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),
 })
 
 
 def _create_vision_transformer(variant, pretrained=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
 
@@ -1136,477 +1444,609 @@
         # FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed
         # interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.
         _filter_fn = partial(checkpoint_filter_fn, interpolation='bilinear', antialias=False)
     else:
         _filter_fn = checkpoint_filter_fn
 
     return build_model_with_cfg(
-        VisionTransformer, variant, pretrained,
+        VisionTransformer,
+        variant,
+        pretrained,
         pretrained_filter_fn=_filter_fn,
         **kwargs,
     )
 
 
 @register_model
-def vit_tiny_patch16_224(pretrained=False, **kwargs):
+def vit_tiny_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Tiny (Vit-Ti/16)
     """
-    model_kwargs = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
-    model = _create_vision_transformer('vit_tiny_patch16_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
+    model = _create_vision_transformer('vit_tiny_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_tiny_patch16_384(pretrained=False, **kwargs):
+def vit_tiny_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Tiny (Vit-Ti/16) @ 384x384.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
-    model = _create_vision_transformer('vit_tiny_patch16_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
+    model = _create_vision_transformer('vit_tiny_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch32_224(pretrained=False, **kwargs):
+def vit_small_patch32_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Small (ViT-S/32)
     """
-    model_kwargs = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch32_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)
+    model = _create_vision_transformer('vit_small_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch32_384(pretrained=False, **kwargs):
+def vit_small_patch32_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Small (ViT-S/32) at 384x384.
     """
-    model_kwargs = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch32_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)
+    model = _create_vision_transformer('vit_small_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch16_224(pretrained=False, **kwargs):
+def vit_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Small (ViT-S/16)
     """
-    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch16_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
+    model = _create_vision_transformer('vit_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch16_384(pretrained=False, **kwargs):
+def vit_small_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Small (ViT-S/16)
     """
-    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch16_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
+    model = _create_vision_transformer('vit_small_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch8_224(pretrained=False, **kwargs):
+def vit_small_patch8_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Small (ViT-S/8)
     """
-    model_kwargs = dict(patch_size=8, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch8_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=8, embed_dim=384, depth=12, num_heads=6)
+    model = _create_vision_transformer('vit_small_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch32_224(pretrained=False, **kwargs):
+def vit_base_patch32_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch32_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)
+    model = _create_vision_transformer('vit_base_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch32_384(pretrained=False, **kwargs):
+def vit_base_patch32_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch32_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)
+    model = _create_vision_transformer('vit_base_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_224(pretrained=False, **kwargs):
+def vit_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
+    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_384(pretrained=False, **kwargs):
+def vit_base_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch16_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
+    model = _create_vision_transformer('vit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch8_224(pretrained=False, **kwargs):
+def vit_base_patch8_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/8) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch8_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12)
+    model = _create_vision_transformer('vit_base_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch32_224(pretrained=False, **kwargs):
+def vit_large_patch32_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929). No pretrained weights.
     """
-    model_kwargs = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch32_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)
+    model = _create_vision_transformer('vit_large_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch32_384(pretrained=False, **kwargs):
+def vit_large_patch32_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch32_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)
+    model = _create_vision_transformer('vit_large_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch16_224(pretrained=False, **kwargs):
+def vit_large_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch16_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)
+    model = _create_vision_transformer('vit_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch16_384(pretrained=False, **kwargs):
+def vit_large_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch16_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)
+    model = _create_vision_transformer('vit_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch14_224(pretrained=False, **kwargs):
+def vit_large_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/14)
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch14_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16)
+    model = _create_vision_transformer('vit_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_huge_patch14_224(pretrained=False, **kwargs):
+def vit_huge_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16)
-    model = _create_vision_transformer('vit_huge_patch14_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16)
+    model = _create_vision_transformer('vit_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_giant_patch14_224(pretrained=False, **kwargs):
+def vit_giant_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Giant (little-g) model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16)
-    model = _create_vision_transformer('vit_giant_patch14_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16)
+    model = _create_vision_transformer('vit_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_gigantic_patch14_224(pretrained=False, **kwargs):
+def vit_gigantic_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Gigantic (big-G) model (ViT-G/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16)
+    model_args = dict(patch_size=14, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16)
     model = _create_vision_transformer(
-        'vit_gigantic_patch14_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_gigantic_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_224_miil(pretrained=False, **kwargs):
+def vit_base_patch16_224_miil(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).
     Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False)
     model = _create_vision_transformer(
-        'vit_base_patch16_224_miil', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_224_miil', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
+def vit_medium_patch16_gap_240(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Medium (ViT-M/16) w/o class token, w/ avg-pool @ 240x240
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_medium_patch16_gap_240', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_medium_patch16_gap_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
+def vit_medium_patch16_gap_256(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Medium (ViT-M/16) w/o class token, w/ avg-pool @ 256x256
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_medium_patch16_gap_256', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_medium_patch16_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
+def vit_medium_patch16_gap_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Medium (ViT-M/16) w/o class token, w/ avg-pool @ 384x384
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_medium_patch16_gap_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_medium_patch16_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_gap_224(pretrained=False, **kwargs):
+def vit_base_patch16_gap_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/16) w/o class token, w/ avg-pool @ 256x256
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=16, class_token=False, global_pool='avg', fc_norm=False)
     model = _create_vision_transformer(
-        'vit_base_patch16_gap_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch32_clip_224(pretrained=False, **kwargs):
+def vit_base_patch32_clip_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-B/32 CLIP image tower @ 224x224
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch32_clip_384(pretrained=False, **kwargs):
+def vit_base_patch32_clip_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-B/32 CLIP image tower @ 384x384
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch32_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch32_clip_448(pretrained=False, **kwargs):
+def vit_base_patch32_clip_448(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-B/32 CLIP image tower @ 448x448
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_448', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch32_clip_448', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_clip_224(pretrained=False, **kwargs):
+def vit_base_patch16_clip_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-B/16 CLIP image tower
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_base_patch16_clip_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_clip_384(pretrained=False, **kwargs):
+def vit_base_patch16_clip_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-B/16 CLIP image tower @ 384x384
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_base_patch16_clip_384', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch14_clip_224(pretrained=False, **kwargs):
+def vit_large_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/14) CLIP image tower
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
+    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_large_patch14_clip_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_large_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_patch14_clip_336(pretrained=False, **kwargs):
+def vit_large_patch14_clip_336(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Large model (ViT-L/14) CLIP image tower @ 336x336
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
+    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_large_patch14_clip_336', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_large_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
+def vit_huge_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Huge model (ViT-H/14) CLIP image tower.
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
+    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_huge_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
+def vit_huge_patch14_clip_336(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Huge model (ViT-H/14) CLIP image tower @ 336x336
     """
-    model_kwargs = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
+    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_336', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_huge_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
+def vit_giant_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Giant (little-g) model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560
     Pretrained weights from CLIP image tower.
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
     model = _create_vision_transformer(
-        'vit_giant_patch14_clip_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_giant_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
+@register_model
+def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-bigG model (ViT-G/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560
+    Pretrained weights from CLIP image tower.
+    """
+    model_args = dict(
+        patch_size=14, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)
+    model = _create_vision_transformer(
+        'vit_gigantic_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
 # Experimental models below
 
 @register_model
-def vit_base_patch32_plus_256(pretrained=False, **kwargs):
+def vit_base_patch32_plus_256(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/32+)
     """
-    model_kwargs = dict(patch_size=32, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)
+    model_args = dict(patch_size=32, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_base_patch32_plus_256', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch32_plus_256', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_plus_240(pretrained=False, **kwargs):
+def vit_base_patch16_plus_240(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/16+)
     """
-    model_kwargs = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)
+    model_args = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_base_patch16_plus_240', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_plus_240', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
+def vit_base_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base (ViT-B/16) w/ residual post-norm
     """
-    model_kwargs = dict(
+    model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, init_values=1e-5,
         class_token=False, block_fn=ResPostBlock, global_pool='avg')
     model = _create_vision_transformer(
-        'vit_base_patch16_rpn_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
+def vit_small_patch16_36x1_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base w/ LayerScale + 36 x 1 (36 block serial) config. Experimental, may remove.
     Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795
     Paper focuses on 24x2 + 48x1 for 'Small' width but those are extremely slow.
     """
-    model_kwargs = dict(patch_size=16, embed_dim=384, depth=36, num_heads=6, init_values=1e-5)
+    model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=6, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_small_patch16_36x1_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_small_patch16_36x1_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
+def vit_small_patch16_18x2_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Small w/ LayerScale + 18 x 2 (36 block parallel) config. Experimental, may remove.
     Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795
     Paper focuses on 24x2 + 48x1 for 'Small' width but those are extremely slow.
     """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=18, num_heads=6, init_values=1e-5, block_fn=ParallelBlock)
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=18, num_heads=6, init_values=1e-5, block_fn=ParallelThingsBlock)
     model = _create_vision_transformer(
-        'vit_small_patch16_18x2_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_small_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
+def vit_base_patch16_18x2_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ ViT-Base w/ LayerScale + 18 x 2 (36 block parallel) config. Experimental, may remove.
     Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=18, num_heads=12, init_values=1e-5, block_fn=ParallelBlock)
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=18, num_heads=12, init_values=1e-5, block_fn=ParallelThingsBlock)
     model = _create_vision_transformer(
-        'vit_base_patch16_18x2_224', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'vit_base_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def eva_large_patch14_196(pretrained=False, **kwargs):
+def eva_large_patch14_196(pretrained=False, **kwargs) -> VisionTransformer:
     """ EVA-large model https://arxiv.org/abs/2211.07636 /via MAE MIM pretrain"""
-    model_kwargs = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')
+    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')
     model = _create_vision_transformer(
-        'eva_large_patch14_196', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+        'eva_large_patch14_196', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def eva_large_patch14_336(pretrained=False, **kwargs):
+def eva_large_patch14_336(pretrained=False, **kwargs) -> VisionTransformer:
     """ EVA-large model https://arxiv.org/abs/2211.07636 via MAE MIM pretrain"""
-    model_kwargs = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')
-    model = _create_vision_transformer('eva_large_patch14_336', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')
+    model = _create_vision_transformer('eva_large_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def flexivit_small(pretrained=False, **kwargs):
+def flexivit_small(pretrained=False, **kwargs) -> VisionTransformer:
     """ FlexiViT-Small
     """
-    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True)
-    model = _create_vision_transformer('flexivit_small', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True)
+    model = _create_vision_transformer('flexivit_small', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def flexivit_base(pretrained=False, **kwargs):
+def flexivit_base(pretrained=False, **kwargs) -> VisionTransformer:
     """ FlexiViT-Base
     """
-    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True)
-    model = _create_vision_transformer('flexivit_base', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True)
+    model = _create_vision_transformer('flexivit_base', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def flexivit_large(pretrained=False, **kwargs):
+def flexivit_large(pretrained=False, **kwargs) -> VisionTransformer:
     """ FlexiViT-Large
     """
-    model_kwargs = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True)
-    model = _create_vision_transformer('flexivit_large', pretrained=pretrained, **dict(model_kwargs, **kwargs))
+    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True)
+    model = _create_vision_transformer('flexivit_large', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_base_patch16_xp_224(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-Large model (ViT-L/14) w/ parallel blocks and qk norm enabled.
+    """
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, no_embed_class=True,
+        norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,
+    )
+    model = _create_vision_transformer(
+        'vit_base_patch16_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
+
+
+@register_model
+def vit_large_patch14_xp_224(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-Large model (ViT-L/14) w/ parallel blocks and qk norm enabled.
+    """
+    model_args = dict(
+        patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, no_embed_class=True,
+        norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,
+    )
+    model = _create_vision_transformer(
+        'vit_large_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_huge_patch14_xp_224(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-Huge model (ViT-H/14) w/ parallel blocks and qk norm enabled.
+    """
+    model_args = dict(
+        patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, no_embed_class=True,
+        norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,
+    )
+    model = _create_vision_transformer(
+        'vit_huge_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_small_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-S/14 for DINOv2
+    """
+    model_args = dict(
+        patch_size=14, embed_dim=384, depth=12, num_heads=6, init_values=1.0, img_size=518,
+    )
+    model = _create_vision_transformer(
+        'vit_small_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_base_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-B/14 for DINOv2
+    """
+    model_args = dict(
+        patch_size=14, embed_dim=768, depth=12, num_heads=12, init_values=1.0, img_size=518,
+    )
+    model = _create_vision_transformer(
+        'vit_base_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_large_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-L/14 for DINOv2
+    """
+    model_args = dict(
+        patch_size=14, embed_dim=1024, depth=24, num_heads=16, init_values=1.0, img_size=518,
+    )
+    model = _create_vision_transformer(
+        'vit_large_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+@register_model
+def vit_giant_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:
+    """ ViT-G/14 for DINOv2
+    """
+
+    # The hidden_features of SwiGLU is calculated by:
+    # hidden_features = (int(hidden_features * 2 / 3) + 7) // 8 * 8
+    # When embed_dim=1536, hidden_features=4096
+    # With SwiGLUPacked, we need to set hidden_features = 2 * 4096 = 8192
+
+    model_args = dict(
+        patch_size=14, embed_dim=1536, depth=40, num_heads=24, init_values=1.0, 
+        mlp_ratio=2.66667 * 2, mlp_layer=SwiGLUPacked, img_size=518, act_layer=nn.SiLU
+    )
+    model = _create_vision_transformer(
+        'vit_giant_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return model
+
+
+register_model_deprecations(__name__, {
+    'vit_tiny_patch16_224_in21k': 'vit_tiny_patch16_224.augreg_in21k',
+    'vit_small_patch32_224_in21k': 'vit_small_patch32_224.augreg_in21k',
+    'vit_small_patch16_224_in21k': 'vit_small_patch16_224.augreg_in21k',
+    'vit_base_patch32_224_in21k': 'vit_base_patch32_224.augreg_in21k',
+    'vit_base_patch16_224_in21k': 'vit_base_patch16_224.augreg_in21k',
+    'vit_base_patch8_224_in21k': 'vit_base_patch8_224.augreg_in21k',
+    'vit_large_patch32_224_in21k': 'vit_large_patch32_224.orig_in21k',
+    'vit_large_patch16_224_in21k': 'vit_large_patch16_224.augreg_in21k',
+    'vit_huge_patch14_224_in21k': 'vit_huge_patch14_224.orig_in21k',
+    'vit_base_patch32_224_sam': 'vit_base_patch32_224.sam',
+    'vit_base_patch16_224_sam': 'vit_base_patch16_224.sam',
+    'vit_small_patch16_224_dino': 'vit_small_patch16_224.dino',
+    'vit_small_patch8_224_dino': 'vit_small_patch8_224.dino',
+    'vit_base_patch16_224_dino': 'vit_base_patch16_224.dino',
+    'vit_base_patch8_224_dino': 'vit_base_patch8_224.dino',
+    'vit_base_patch16_224_miil_in21k': 'vit_base_patch16_224_miil.in21k',
+    'vit_base_patch32_224_clip_laion2b': 'vit_base_patch32_clip_224.laion2b',
+    'vit_large_patch14_224_clip_laion2b': 'vit_large_patch14_clip_224.laion2b',
+    'vit_huge_patch14_224_clip_laion2b': 'vit_huge_patch14_clip_224.laion2b',
+    'vit_giant_patch14_224_clip_laion2b': 'vit_giant_patch14_clip_224.laion2b',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/vision_transformer_hybrid.py` & `timm-0.9.0/timm/models/vision_transformer_hybrid.py`

 * *Files 26% similar despite different names*

```diff
@@ -10,25 +10,25 @@
 
 NOTE These hybrid model definitions depend on code in vision_transformer.py.
 They were moved here to keep file sizes sane.
 
 Hacked together by / Copyright 2020, Ross Wightman
 """
 from functools import partial
+from typing import List, Tuple
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import StdConv2dSame, StdConv2d, to_2tuple
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
+from ._registry import generate_default_cfgs, register_model, register_model_deprecations
 from .resnet import resnet26d, resnet50d
 from .resnetv2 import ResNetV2, create_resnetv2_stem
-from .vision_transformer import _create_vision_transformer
+from .vision_transformer import _create_vision_transformer, VisionTransformer
 
 
 class HybridEmbed(nn.Module):
     """ CNN Feature Map Embedding
     Extract feature map from CNN, flatten, project to embedding dim.
     """
     def __init__(
@@ -71,18 +71,51 @@
         self.num_patches = self.grid_size[0] * self.grid_size[1]
         self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)
 
     def forward(self, x):
         x = self.backbone(x)
         if isinstance(x, (list, tuple)):
             x = x[-1]  # last feature if backbone outputs list/tuple of features
-        x = self.proj(x).flatten(2).transpose(1, 2)
+        x = self.proj(x)
+        x = x.flatten(2).transpose(1, 2)
         return x
 
 
+class HybridEmbedWithSize(nn.Module):
+    """ CNN Feature Map Embedding
+    Extract feature map from CNN, flatten, project to embedding dim.
+    """
+    def __init__(
+            self,
+            backbone,
+            img_size=224,
+            patch_size=1,
+            feature_size=None,
+            in_chans=3,
+            embed_dim=768,
+            bias=True,
+    ):
+        super().__init__(
+            backbone=backbone,
+            img_size=img_size,
+            patch_size=patch_size,
+            feature_size=feature_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+            bias=bias,
+        )
+
+    def forward(self, x) -> Tuple[torch.Tensor, List[int]]:
+        x = self.backbone(x)
+        if isinstance(x, (list, tuple)):
+            x = x[-1]  # last feature if backbone outputs list/tuple of features
+        x = self.proj(x)
+        return x.flatten(2).transpose(1, 2), x.shape[-2:]
+
+
 def _create_vision_transformer_hybrid(variant, backbone, pretrained=False, **kwargs):
     embed_layer = partial(HybridEmbed, backbone=backbone)
     kwargs.setdefault('patch_size', 1)  # default patch size for hybrid models if not set
     return _create_vision_transformer(variant, pretrained=pretrained, embed_layer=embed_layer, **kwargs)
 
 
 def _resnetv2(layers=(3, 4, 9), **kwargs):
@@ -174,148 +207,158 @@
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, first_conv='patch_embed.backbone.conv1.0'),
     'vit_base_resnet50d_224.untrained': _cfg(
         mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, first_conv='patch_embed.backbone.conv1.0'),
 })
 
 
 @register_model
-def vit_tiny_r_s16_p8_224(pretrained=False, **kwargs):
+def vit_tiny_r_s16_p8_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ R+ViT-Ti/S16 w/ 8x8 patch hybrid @ 224 x 224.
     """
     backbone = _resnetv2(layers=(), **kwargs)
-    model_kwargs = dict(patch_size=8, embed_dim=192, depth=12, num_heads=3, **kwargs)
+    model_args = dict(patch_size=8, embed_dim=192, depth=12, num_heads=3)
     model = _create_vision_transformer_hybrid(
-        'vit_tiny_r_s16_p8_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_tiny_r_s16_p8_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_tiny_r_s16_p8_384(pretrained=False, **kwargs):
+def vit_tiny_r_s16_p8_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ R+ViT-Ti/S16 w/ 8x8 patch hybrid @ 384 x 384.
     """
     backbone = _resnetv2(layers=(), **kwargs)
-    model_kwargs = dict(patch_size=8, embed_dim=192, depth=12, num_heads=3, **kwargs)
+    model_args = dict(patch_size=8, embed_dim=192, depth=12, num_heads=3)
     model = _create_vision_transformer_hybrid(
-        'vit_tiny_r_s16_p8_384', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_tiny_r_s16_p8_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_r26_s32_224(pretrained=False, **kwargs):
+def vit_small_r26_s32_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ R26+ViT-S/S32 hybrid.
     """
     backbone = _resnetv2((2, 2, 2, 2), **kwargs)
-    model_kwargs = dict(embed_dim=384, depth=12, num_heads=6, **kwargs)
+    model_args = dict(embed_dim=384, depth=12, num_heads=6)
     model = _create_vision_transformer_hybrid(
-        'vit_small_r26_s32_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_small_r26_s32_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_r26_s32_384(pretrained=False, **kwargs):
+def vit_small_r26_s32_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ R26+ViT-S/S32 hybrid.
     """
     backbone = _resnetv2((2, 2, 2, 2), **kwargs)
-    model_kwargs = dict(embed_dim=384, depth=12, num_heads=6, **kwargs)
+    model_args = dict(embed_dim=384, depth=12, num_heads=6)
     model = _create_vision_transformer_hybrid(
-        'vit_small_r26_s32_384', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_small_r26_s32_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_r26_s32_224(pretrained=False, **kwargs):
+def vit_base_r26_s32_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ R26+ViT-B/S32 hybrid.
     """
     backbone = _resnetv2((2, 2, 2, 2), **kwargs)
-    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(embed_dim=768, depth=12, num_heads=12)
     model = _create_vision_transformer_hybrid(
-        'vit_base_r26_s32_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_base_r26_s32_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_r50_s16_224(pretrained=False, **kwargs):
+def vit_base_r50_s16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ R50+ViT-B/S16 hybrid from original paper (https://arxiv.org/abs/2010.11929).
     """
     backbone = _resnetv2((3, 4, 9), **kwargs)
-    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(embed_dim=768, depth=12, num_heads=12)
     model = _create_vision_transformer_hybrid(
-        'vit_base_r50_s16_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_base_r50_s16_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_r50_s16_384(pretrained=False, **kwargs):
+def vit_base_r50_s16_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ R50+ViT-B/16 hybrid from original paper (https://arxiv.org/abs/2010.11929).
     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
     backbone = _resnetv2((3, 4, 9), **kwargs)
-    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(embed_dim=768, depth=12, num_heads=12)
     model = _create_vision_transformer_hybrid(
-        'vit_base_r50_s16_384', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_base_r50_s16_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_r50_s32_224(pretrained=False, **kwargs):
+def vit_large_r50_s32_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ R50+ViT-L/S32 hybrid.
     """
     backbone = _resnetv2((3, 4, 6, 3), **kwargs)
-    model_kwargs = dict(embed_dim=1024, depth=24, num_heads=16, **kwargs)
+    model_args = dict(embed_dim=1024, depth=24, num_heads=16)
     model = _create_vision_transformer_hybrid(
-        'vit_large_r50_s32_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_large_r50_s32_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_large_r50_s32_384(pretrained=False, **kwargs):
+def vit_large_r50_s32_384(pretrained=False, **kwargs) -> VisionTransformer:
     """ R50+ViT-L/S32 hybrid.
     """
     backbone = _resnetv2((3, 4, 6, 3), **kwargs)
-    model_kwargs = dict(embed_dim=1024, depth=24, num_heads=16, **kwargs)
+    model_args = dict(embed_dim=1024, depth=24, num_heads=16)
     model = _create_vision_transformer_hybrid(
-        'vit_large_r50_s32_384', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_large_r50_s32_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_resnet26d_224(pretrained=False, **kwargs):
+def vit_small_resnet26d_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ Custom ViT small hybrid w/ ResNet26D stride 32. No pretrained weights.
     """
     backbone = resnet26d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[4])
-    model_kwargs = dict(embed_dim=768, depth=8, num_heads=8, mlp_ratio=3, **kwargs)
+    model_args = dict(embed_dim=768, depth=8, num_heads=8, mlp_ratio=3)
     model = _create_vision_transformer_hybrid(
-        'vit_small_resnet26d_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_small_resnet26d_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_small_resnet50d_s16_224(pretrained=False, **kwargs):
+def vit_small_resnet50d_s16_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ Custom ViT small hybrid w/ ResNet50D 3-stages, stride 16. No pretrained weights.
     """
     backbone = resnet50d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[3])
-    model_kwargs = dict(embed_dim=768, depth=8, num_heads=8, mlp_ratio=3, **kwargs)
+    model_args = dict(embed_dim=768, depth=8, num_heads=8, mlp_ratio=3)
     model = _create_vision_transformer_hybrid(
-        'vit_small_resnet50d_s16_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_small_resnet50d_s16_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_resnet26d_224(pretrained=False, **kwargs):
+def vit_base_resnet26d_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ Custom ViT base hybrid w/ ResNet26D stride 32. No pretrained weights.
     """
     backbone = resnet26d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[4])
-    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(embed_dim=768, depth=12, num_heads=12)
     model = _create_vision_transformer_hybrid(
-        'vit_base_resnet26d_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_base_resnet26d_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def vit_base_resnet50d_224(pretrained=False, **kwargs):
+def vit_base_resnet50d_224(pretrained=False, **kwargs) -> VisionTransformer:
     """ Custom ViT base hybrid w/ ResNet50D stride 32. No pretrained weights.
     """
     backbone = resnet50d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[4])
-    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
+    model_args = dict(embed_dim=768, depth=12, num_heads=12)
     model = _create_vision_transformer_hybrid(
-        'vit_base_resnet50d_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
+        'vit_base_resnet50d_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))
     return model
+
+
+register_model_deprecations(__name__, {
+    'vit_tiny_r_s16_p8_224_in21k': 'vit_tiny_r_s16_p8_224.augreg_in21k',
+    'vit_small_r26_s32_224_in21k': 'vit_small_r26_s32_224.augreg_in21k',
+    'vit_base_r50_s16_224_in21k': 'vit_base_r50_s16_224.orig_in21k',
+    'vit_base_resnet50_224_in21k': 'vit_base_r50_s16_224.orig_in21k',
+    'vit_large_r50_s32_224_in21k': 'vit_large_r50_s32_224.augreg_in21k',
+    'vit_base_resnet50_384': 'vit_base_r50_s16_384.orig_in21k_ft_in1k'
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/vision_transformer_relpos.py` & `timm-0.9.0/timm/models/focalnet.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,495 +1,651 @@
-""" Relative Position Vision Transformer (ViT) in PyTorch
+""" FocalNet
 
-NOTE: these models are experimental / WIP, expect changes
+As described in `Focal Modulation Networks` - https://arxiv.org/abs/2203.11926
 
-Hacked together by / Copyright 2022, Ross Wightman
+Significant modifications and refactoring from the original impl at https://github.com/microsoft/FocalNet
+
+This impl is/has:
+* fully convolutional, NCHW tensor layout throughout, seemed to have minimal performance impact but more flexible
+* re-ordered downsample / layer so that striding always at beginning of layer (stage)
+* no input size constraints or input resolution/H/W tracking through the model
+* torchscript fixed and a number of quirks cleaned up
+* feature extraction support via `features_only=True`
 """
-import logging
-import math
+# --------------------------------------------------------
+# FocalNets -- Focal Modulation Networks
+# Copyright (c) 2022 Microsoft
+# Licensed under The MIT License [see LICENSE for details]
+# Written by Jianwei Yang (jianwyan@microsoft.com)
+# --------------------------------------------------------
 from functools import partial
-from typing import Optional, Tuple
+from typing import Callable, Optional, Tuple
 
 import torch
 import torch.nn as nn
-from torch.utils.checkpoint import checkpoint
+import torch.utils.checkpoint as checkpoint
 
-from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
-from timm.layers import PatchEmbed, Mlp, DropPath, RelPosMlp, RelPosBias
+from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
+from timm.layers import Mlp, DropPath, LayerNorm2d, trunc_normal_, ClassifierHead, NormMlpClassifierHead
 from ._builder import build_model_with_cfg
-from ._pretrained import generate_default_cfgs
-from ._registry import register_model
-
-__all__ = ['VisionTransformerRelPos']  # model_registry will add each entrypoint fn to this
+from ._manipulate import named_apply
+from ._registry import generate_default_cfgs, register_model
 
-_logger = logging.getLogger(__name__)
+__all__ = ['FocalNet']
 
 
-class RelPosAttention(nn.Module):
-    def __init__(self, dim, num_heads=8, qkv_bias=False, rel_pos_cls=None, attn_drop=0., proj_drop=0.):
+class FocalModulation(nn.Module):
+    def __init__(
+            self,
+            dim: int,
+            focal_window,
+            focal_level: int,
+            focal_factor: int = 2,
+            bias: bool = True,
+            use_post_norm: bool = False,
+            normalize_modulator: bool = False,
+            proj_drop: float = 0.,
+            norm_layer: Callable = LayerNorm2d,
+    ):
         super().__init__()
-        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
-        self.num_heads = num_heads
-        head_dim = dim // num_heads
-        self.scale = head_dim ** -0.5
-
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
-        self.rel_pos = rel_pos_cls(num_heads=num_heads) if rel_pos_cls else None
-        self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Linear(dim, dim)
+
+        self.dim = dim
+        self.focal_window = focal_window
+        self.focal_level = focal_level
+        self.focal_factor = focal_factor
+        self.use_post_norm = use_post_norm
+        self.normalize_modulator = normalize_modulator
+        self.input_split = [dim, dim, self.focal_level + 1]
+
+        self.f = nn.Conv2d(dim, 2 * dim + (self.focal_level + 1), kernel_size=1, bias=bias)
+        self.h = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
+
+        self.act = nn.GELU()
+        self.proj = nn.Conv2d(dim, dim, kernel_size=1)
         self.proj_drop = nn.Dropout(proj_drop)
+        self.focal_layers = nn.ModuleList()
 
-    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):
-        B, N, C = x.shape
-        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
-        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)
-
-        attn = (q @ k.transpose(-2, -1)) * self.scale
-        if self.rel_pos is not None:
-            attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)
-        elif shared_rel_pos is not None:
-            attn = attn + shared_rel_pos
-        attn = attn.softmax(dim=-1)
-        attn = self.attn_drop(attn)
+        self.kernel_sizes = []
+        for k in range(self.focal_level):
+            kernel_size = self.focal_factor * k + self.focal_window
+            self.focal_layers.append(nn.Sequential(
+                nn.Conv2d(dim, dim, kernel_size=kernel_size, groups=dim, padding=kernel_size // 2, bias=False),
+                nn.GELU(),
+            ))
+            self.kernel_sizes.append(kernel_size)
+        self.norm = norm_layer(dim) if self.use_post_norm else nn.Identity()
 
-        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
-        x = self.proj(x)
-        x = self.proj_drop(x)
-        return x
+    def forward(self, x):
+        # pre linear projection
+        x = self.f(x)
+        q, ctx, gates = torch.split(x, self.input_split, 1)
+
+        # context aggreation
+        ctx_all = 0
+        for l, focal_layer in enumerate(self.focal_layers):
+            ctx = focal_layer(ctx)
+            ctx_all = ctx_all + ctx * gates[:, l:l + 1]
+        ctx_global = self.act(ctx.mean((2, 3), keepdim=True))
+        ctx_all = ctx_all + ctx_global * gates[:, self.focal_level:]
+
+        # normalize context
+        if self.normalize_modulator:
+            ctx_all = ctx_all / (self.focal_level + 1)
+
+        # focal modulation
+        x_out = q * self.h(ctx_all)
+        x_out = self.norm(x_out)
+
+        # post linear projection
+        x_out = self.proj(x_out)
+        x_out = self.proj_drop(x_out)
+        return x_out
 
 
-class LayerScale(nn.Module):
+class LayerScale2d(nn.Module):
     def __init__(self, dim, init_values=1e-5, inplace=False):
         super().__init__()
         self.inplace = inplace
         self.gamma = nn.Parameter(init_values * torch.ones(dim))
 
     def forward(self, x):
-        return x.mul_(self.gamma) if self.inplace else x * self.gamma
+        gamma = self.gamma.view(1, -1, 1, 1)
+        return x.mul_(gamma) if self.inplace else x * gamma
 
 
-class RelPosBlock(nn.Module):
+class FocalNetBlock(nn.Module):
+    """ Focal Modulation Network Block.
+    """
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, rel_pos_cls=None, init_values=None,
-            drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim: int,
+            mlp_ratio: float = 4.,
+            focal_level: int = 1,
+            focal_window: int = 3,
+            use_post_norm: bool = False,
+            use_post_norm_in_modulation: bool = False,
+            normalize_modulator: bool = False,
+            layerscale_value: float = 1e-4,
+            proj_drop: float = 0.,
+            drop_path: float = 0.,
+            act_layer: Callable = nn.GELU,
+            norm_layer: Callable = LayerNorm2d,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            focal_level: Number of focal levels.
+            focal_window: Focal window size at first focal level.
+            use_post_norm: Whether to use layer norm after modulation.
+            use_post_norm_in_modulation: Whether to use layer norm in modulation.
+            layerscale_value: Initial layerscale value.
+            proj_drop: Dropout rate.
+            drop_path: Stochastic depth rate.
+            act_layer: Activation layer.
+            norm_layer: Normalization layer.
+        """
         super().__init__()
-        self.norm1 = norm_layer(dim)
-        self.attn = RelPosAttention(
-            dim, num_heads, qkv_bias=qkv_bias, rel_pos_cls=rel_pos_cls, attn_drop=attn_drop, proj_drop=drop)
-        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
-        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
+        self.dim = dim
+        self.mlp_ratio = mlp_ratio
+
+        self.focal_window = focal_window
+        self.focal_level = focal_level
+        self.use_post_norm = use_post_norm
+
+        self.norm1 = norm_layer(dim) if not use_post_norm else nn.Identity()
+        self.modulation = FocalModulation(
+            dim,
+            focal_window=focal_window,
+            focal_level=self.focal_level,
+            use_post_norm=use_post_norm_in_modulation,
+            normalize_modulator=normalize_modulator,
+            proj_drop=proj_drop,
+            norm_layer=norm_layer,
+        )
+        self.norm1_post = norm_layer(dim) if use_post_norm else nn.Identity()
+        self.ls1 = LayerScale2d(dim, layerscale_value) if layerscale_value is not None else nn.Identity()
         self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-        self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
-        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
+        self.norm2 = norm_layer(dim) if not use_post_norm else nn.Identity()
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=int(dim * mlp_ratio),
+            act_layer=act_layer,
+            drop=proj_drop,
+            use_conv=True,
+        )
+        self.norm2_post = norm_layer(dim) if use_post_norm else nn.Identity()
+        self.ls2 = LayerScale2d(dim, layerscale_value) if layerscale_value is not None else nn.Identity()
         self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
-    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):
-        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))
-        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
+    def forward(self, x):
+        shortcut = x
+
+        # Focal Modulation
+        x = self.norm1(x)
+        x = self.modulation(x)
+        x = self.norm1_post(x)
+        x = shortcut + self.drop_path1(self.ls1(x))
+
+        # FFN
+        x = x + self.drop_path2(self.ls2(self.norm2_post(self.mlp(self.norm2(x)))))
+
         return x
 
 
-class ResPostRelPosBlock(nn.Module):
+class FocalNetStage(nn.Module):
+    """ A basic Focal Transformer layer for one stage.
+    """
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, rel_pos_cls=None, init_values=None,
-            drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim: int,
+            out_dim: int,
+            depth: int,
+            mlp_ratio: float = 4.,
+            downsample: bool = True,
+            focal_level: int = 1,
+            focal_window: int = 1,
+            use_overlap_down: bool = False,
+            use_post_norm: bool = False,
+            use_post_norm_in_modulation: bool = False,
+            normalize_modulator: bool = False,
+            layerscale_value: float = 1e-4,
+            proj_drop: float = 0.,
+            drop_path: float = 0.,
+            norm_layer: Callable = LayerNorm2d,
+    ):
+        """
+        Args:
+            dim: Number of input channels.
+            out_dim: Number of output channels.
+            depth: Number of blocks.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            downsample: Downsample layer at start of the layer.
+            focal_level: Number of focal levels
+            focal_window: Focal window size at first focal level
+            use_overlap_down: User overlapped convolution in downsample layer.
+            use_post_norm: Whether to use layer norm after modulation.
+            use_post_norm_in_modulation: Whether to use layer norm in modulation.
+            layerscale_value: Initial layerscale value
+            proj_drop: Dropout rate for projections.
+            drop_path: Stochastic depth rate.
+            norm_layer: Normalization layer.
+        """
         super().__init__()
-        self.init_values = init_values
+        self.dim = dim
+        self.depth = depth
+        self.grad_checkpointing = False
 
-        self.attn = RelPosAttention(
-            dim, num_heads, qkv_bias=qkv_bias, rel_pos_cls=rel_pos_cls, attn_drop=attn_drop, proj_drop=drop)
-        self.norm1 = norm_layer(dim)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        if downsample:
+            self.downsample = Downsample(
+                in_chs=dim,
+                out_chs=out_dim,
+                stride=2,
+                overlap=use_overlap_down,
+                norm_layer=norm_layer,
+            )
+        else:
+            self.downsample = nn.Identity()
 
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
-        self.norm2 = norm_layer(dim)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        # build blocks
+        self.blocks = nn.ModuleList([
+            FocalNetBlock(
+                dim=out_dim,
+                mlp_ratio=mlp_ratio,
+                focal_level=focal_level,
+                focal_window=focal_window,
+                use_post_norm=use_post_norm,
+                use_post_norm_in_modulation=use_post_norm_in_modulation,
+                normalize_modulator=normalize_modulator,
+                layerscale_value=layerscale_value,
+                proj_drop=proj_drop,
+                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
+                norm_layer=norm_layer,
+            )
+            for i in range(depth)])
 
-        self.init_weights()
+    @torch.jit.ignore
+    def set_grad_checkpointing(self, enable=True):
+        self.grad_checkpointing = enable
 
-    def init_weights(self):
-        # NOTE this init overrides that base model init with specific changes for the block type
-        if self.init_values is not None:
-            nn.init.constant_(self.norm1.weight, self.init_values)
-            nn.init.constant_(self.norm2.weight, self.init_values)
-
-    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):
-        x = x + self.drop_path1(self.norm1(self.attn(x, shared_rel_pos=shared_rel_pos)))
-        x = x + self.drop_path2(self.norm2(self.mlp(x)))
+    def forward(self, x):
+        x = self.downsample(x)
+        for blk in self.blocks:
+            if self.grad_checkpointing and not torch.jit.is_scripting():
+                x = checkpoint.checkpoint(blk, x)
+            else:
+                x = blk(x)
         return x
 
 
-class VisionTransformerRelPos(nn.Module):
-    """ Vision Transformer w/ Relative Position Bias
+class Downsample(nn.Module):
+
+    def __init__(
+            self,
+            in_chs: int,
+            out_chs: int,
+            stride: int = 4,
+            overlap: bool = False,
+            norm_layer: Optional[Callable] = None,
+    ):
+        """
+
+        Args:
+            in_chs: Number of input image channels.
+            out_chs: Number of linear projection output channels.
+            stride: Downsample stride.
+            overlap: Use overlapping convolutions if True.
+            norm_layer: Normalization layer.
+        """
+        super().__init__()
+        self.stride = stride
+        padding = 0
+        kernel_size = stride
+        if overlap:
+            assert stride in (2, 4)
+            if stride == 4:
+                kernel_size, padding = 7, 2
+            elif stride == 2:
+                kernel_size, padding = 3, 1
+        self.proj = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding)
+        self.norm = norm_layer(out_chs) if norm_layer is not None else nn.Identity()
 
-    Differing from classic vit, this impl
-      * uses relative position index (swin v1 / beit) or relative log coord + mlp (swin v2) pos embed
-      * defaults to no class token (can be enabled)
-      * defaults to global avg pool for head (can be changed)
-      * layer-scale (residual branch gain) enabled
+    def forward(self, x):
+        x = self.proj(x)
+        x = self.norm(x)
+        return x
+
+
+class FocalNet(nn.Module):
+    """" Focal Modulation Networks (FocalNets)
     """
 
     def __init__(
             self,
-            img_size=224,
-            patch_size=16,
-            in_chans=3,
-            num_classes=1000,
-            global_pool='avg',
-            embed_dim=768,
-            depth=12,
-            num_heads=12,
-            mlp_ratio=4.,
-            qkv_bias=True,
-            init_values=1e-6,
-            class_token=False,
-            fc_norm=False,
-            rel_pos_type='mlp',
-            rel_pos_dim=None,
-            shared_rel_pos=False,
-            drop_rate=0.,
-            attn_drop_rate=0.,
-            drop_path_rate=0.,
-            weight_init='skip',
-            embed_layer=PatchEmbed,
-            norm_layer=None,
-            act_layer=None,
-            block_fn=RelPosBlock
+            in_chans: int = 3,
+            num_classes: int = 1000,
+            global_pool: str = 'avg',
+            embed_dim: int = 96,
+            depths: Tuple[int, ...] = (2, 2, 6, 2),
+            mlp_ratio: float = 4.,
+            focal_levels: Tuple[int, ...] = (2, 2, 2, 2),
+            focal_windows: Tuple[int, ...] = (3, 3, 3, 3),
+            use_overlap_down: bool = False,
+            use_post_norm: bool = False,
+            use_post_norm_in_modulation: bool = False,
+            normalize_modulator: bool = False,
+            head_hidden_size: Optional[int] = None,
+            head_init_scale: float = 1.0,
+            layerscale_value: Optional[float] = None,
+            drop_rate: bool = 0.,
+            proj_drop_rate: bool = 0.,
+            drop_path_rate: bool = 0.1,
+            norm_layer: Callable = partial(LayerNorm2d, eps=1e-5),
     ):
         """
         Args:
-            img_size (int, tuple): input image size
-            patch_size (int, tuple): patch size
-            in_chans (int): number of input channels
-            num_classes (int): number of classes for classification head
-            global_pool (str): type of global pooling for final sequence (default: 'avg')
-            embed_dim (int): embedding dimension
-            depth (int): depth of transformer
-            num_heads (int): number of attention heads
-            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
-            qkv_bias (bool): enable bias for qkv if True
-            init_values: (float): layer-scale init values
-            class_token (bool): use class token (default: False)
-            fc_norm (bool): use pre classifier norm instead of pre-pool
-            rel_pos_ty pe (str): type of relative position
-            shared_rel_pos (bool): share relative pos across all blocks
-            drop_rate (float): dropout rate
-            attn_drop_rate (float): attention dropout rate
-            drop_path_rate (float): stochastic depth rate
-            weight_init (str): weight init scheme
-            embed_layer (nn.Module): patch embedding layer
-            norm_layer: (nn.Module): normalization layer
-            act_layer: (nn.Module): MLP activation layer
+            in_chans: Number of input image channels.
+            num_classes: Number of classes for classification head.
+            embed_dim: Patch embedding dimension.
+            depths: Depth of each Focal Transformer layer.
+            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
+            focal_levels: How many focal levels at all stages. Note that this excludes the finest-grain level.
+            focal_windows: The focal window size at all stages.
+            use_overlap_down: Whether to use convolutional embedding.
+            use_post_norm: Whether to use layernorm after modulation (it helps stablize training of large models)
+            layerscale_value: Value for layer scale.
+            drop_rate: Dropout rate.
+            drop_path_rate: Stochastic depth rate.
+            norm_layer: Normalization layer.
         """
         super().__init__()
-        assert global_pool in ('', 'avg', 'token')
-        assert class_token or global_pool != 'token'
-        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
-        act_layer = act_layer or nn.GELU
+
+        self.num_layers = len(depths)
+        embed_dim = [embed_dim * (2 ** i) for i in range(self.num_layers)]
 
         self.num_classes = num_classes
-        self.global_pool = global_pool
-        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
-        self.num_prefix_tokens = 1 if class_token else 0
-        self.grad_checkpointing = False
+        self.embed_dim = embed_dim
+        self.num_features = embed_dim[-1]
+        self.feature_info = []
+
+        self.stem = Downsample(
+            in_chs=in_chans,
+            out_chs=embed_dim[0],
+            overlap=use_overlap_down,
+            norm_layer=norm_layer,
+        )
+        in_dim = embed_dim[0]
 
-        self.patch_embed = embed_layer(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
-        feat_size = self.patch_embed.grid_size
-
-        rel_pos_args = dict(window_size=feat_size, prefix_tokens=self.num_prefix_tokens)
-        if rel_pos_type.startswith('mlp'):
-            if rel_pos_dim:
-                rel_pos_args['hidden_dim'] = rel_pos_dim
-            # FIXME experimenting with different relpos log coord configs
-            if 'swin' in rel_pos_type:
-                rel_pos_args['mode'] = 'swin'
-            elif 'rw' in rel_pos_type:
-                rel_pos_args['mode'] = 'rw'
-            rel_pos_cls = partial(RelPosMlp, **rel_pos_args)
+        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule
+        layers = []
+        for i_layer in range(self.num_layers):
+            out_dim = embed_dim[i_layer]
+            layer = FocalNetStage(
+                dim=in_dim,
+                out_dim=out_dim,
+                depth=depths[i_layer],
+                mlp_ratio=mlp_ratio,
+                downsample=i_layer > 0,
+                focal_level=focal_levels[i_layer],
+                focal_window=focal_windows[i_layer],
+                use_overlap_down=use_overlap_down,
+                use_post_norm=use_post_norm,
+                use_post_norm_in_modulation=use_post_norm_in_modulation,
+                normalize_modulator=normalize_modulator,
+                layerscale_value=layerscale_value,
+                proj_drop=proj_drop_rate,
+                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
+                norm_layer=norm_layer,
+            )
+            in_dim = out_dim
+            layers += [layer]
+            self.feature_info += [dict(num_chs=out_dim, reduction=4 * 2 ** i_layer, module=f'layers.{i_layer}')]
+
+        self.layers = nn.Sequential(*layers)
+
+        if head_hidden_size:
+            self.norm = nn.Identity()
+            self.head = NormMlpClassifierHead(
+                self.num_features,
+                num_classes,
+                hidden_size=head_hidden_size,
+                pool_type=global_pool,
+                drop_rate=drop_rate,
+                norm_layer=norm_layer,
+            )
         else:
-            rel_pos_cls = partial(RelPosBias, **rel_pos_args)
-        self.shared_rel_pos = None
-        if shared_rel_pos:
-            self.shared_rel_pos = rel_pos_cls(num_heads=num_heads)
-            # NOTE shared rel pos currently mutually exclusive w/ per-block, but could support both...
-            rel_pos_cls = None
-
-        self.cls_token = nn.Parameter(torch.zeros(1, self.num_prefix_tokens, embed_dim)) if class_token else None
-
-        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
-        self.blocks = nn.ModuleList([
-            block_fn(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, rel_pos_cls=rel_pos_cls,
-                init_values=init_values, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],
-                norm_layer=norm_layer, act_layer=act_layer)
-            for i in range(depth)])
-        self.norm = norm_layer(embed_dim) if not fc_norm else nn.Identity()
+            self.norm = norm_layer(self.num_features)
+            self.head = ClassifierHead(
+                self.num_features,
+                num_classes,
+                pool_type=global_pool,
+                drop_rate=drop_rate
+            )
 
-        # Classifier Head
-        self.fc_norm = norm_layer(embed_dim) if fc_norm else nn.Identity()
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
-
-        if weight_init != 'skip':
-            self.init_weights(weight_init)
-
-    def init_weights(self, mode=''):
-        assert mode in ('jax', 'moco', '')
-        if self.cls_token is not None:
-            nn.init.normal_(self.cls_token, std=1e-6)
-        # FIXME weight init scheme using PyTorch defaults curently
-        #named_apply(get_init_weights_vit(mode, head_bias), self)
+        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)
 
     @torch.jit.ignore
     def no_weight_decay(self):
-        return {'cls_token'}
+        return {''}
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
         return dict(
-            stem=r'^cls_token|patch_embed',  # stem and embed
-            blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))]
+            stem=r'^stem',
+            blocks=[
+                (r'^layers\.(\d+)', None),
+                (r'^norm', (99999,))
+            ] if coarse else [
+                (r'^layers\.(\d+).downsample', (0,)),
+                (r'^layers\.(\d+)\.\w+\.(\d+)', None),
+                (r'^norm', (99999,)),
+            ]
         )
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
+        for l in self.layers:
+            l.set_grad_checkpointing(enable=enable)
 
     @torch.jit.ignore
     def get_classifier(self):
-        return self.head
+        return self.head.fc
 
-    def reset_classifier(self, num_classes: int, global_pool=None):
-        self.num_classes = num_classes
-        if global_pool is not None:
-            assert global_pool in ('', 'avg', 'token')
-            self.global_pool = global_pool
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+    def reset_classifier(self, num_classes, global_pool=None):
+        self.head.reset(num_classes, pool_type=global_pool)
 
     def forward_features(self, x):
-        x = self.patch_embed(x)
-        if self.cls_token is not None:
-            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
-
-        shared_rel_pos = self.shared_rel_pos.get_bias() if self.shared_rel_pos is not None else None
-        for blk in self.blocks:
-            if self.grad_checkpointing and not torch.jit.is_scripting():
-                x = checkpoint(blk, x, shared_rel_pos=shared_rel_pos)
-            else:
-                x = blk(x, shared_rel_pos=shared_rel_pos)
+        x = self.stem(x)
+        x = self.layers(x)
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
-        if self.global_pool:
-            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
-        x = self.fc_norm(x)
-        return x if pre_logits else self.head(x)
+        return self.head(x, pre_logits=pre_logits)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _create_vision_transformer_relpos(variant, pretrained=False, **kwargs):
-    if kwargs.get('features_only', None):
-        raise RuntimeError('features_only not implemented for Vision Transformer models.')
-
-    model = build_model_with_cfg(VisionTransformerRelPos, variant, pretrained, **kwargs)
-    return model
+def _init_weights(module, name=None, head_init_scale=1.0):
+    if isinstance(module, nn.Conv2d):
+        trunc_normal_(module.weight, std=.02)
+        if module.bias is not None:
+            nn.init.zeros_(module.bias)
+    elif isinstance(module, nn.Linear):
+        trunc_normal_(module.weight, std=.02)
+        if module.bias is not None:
+            nn.init.zeros_(module.bias)
+        if name and 'head.fc' in name:
+            module.weight.data.mul_(head_init_scale)
+            module.bias.data.mul_(head_init_scale)
 
 
 def _cfg(url='', **kwargs):
     return {
         'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,
-        'first_conv': 'patch_embed.proj', 'classifier': 'head',
-        **kwargs
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': .9, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.proj', 'classifier': 'head.fc',
+        'license': 'mit', **kwargs
     }
 
 
 default_cfgs = generate_default_cfgs({
-    'vit_relpos_base_patch32_plus_rpn_256.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_replos_base_patch32_plus_rpn_256-sw-dd486f51.pth',
-        hf_hub_id='timm/',
-        input_size=(3, 256, 256)),
-    'vit_relpos_base_patch16_plus_240.untrained': _cfg(url='', input_size=(3, 240, 240)),
-
-    'vit_relpos_small_patch16_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_small_patch16_224-sw-ec2778b4.pth',
-        hf_hub_id='timm/'),
-    'vit_relpos_medium_patch16_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_224-sw-11c174af.pth',
+    "focalnet_tiny_srf.ms_in1k": _cfg(
         hf_hub_id='timm/'),
-    'vit_relpos_base_patch16_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_base_patch16_224-sw-49049aed.pth',
+    "focalnet_small_srf.ms_in1k": _cfg(
         hf_hub_id='timm/'),
-
-    'vit_srelpos_small_patch16_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_srelpos_small_patch16_224-sw-6cdb8849.pth',
+    "focalnet_base_srf.ms_in1k": _cfg(
         hf_hub_id='timm/'),
-    'vit_srelpos_medium_patch16_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_srelpos_medium_patch16_224-sw-ad702b8c.pth',
+    "focalnet_tiny_lrf.ms_in1k": _cfg(
         hf_hub_id='timm/'),
-
-    'vit_relpos_medium_patch16_cls_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_cls_224-sw-cfe8e259.pth',
+    "focalnet_small_lrf.ms_in1k": _cfg(
         hf_hub_id='timm/'),
-    'vit_relpos_base_patch16_cls_224.untrained': _cfg(),
-    'vit_relpos_base_patch16_clsgap_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_base_patch16_gapcls_224-sw-1a341d6c.pth',
+    "focalnet_base_lrf.ms_in1k": _cfg(
         hf_hub_id='timm/'),
 
-    'vit_relpos_small_patch16_rpn_224.untrained': _cfg(),
-    'vit_relpos_medium_patch16_rpn_224.sw_in1k': _cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_rpn_224-sw-5d2befd8.pth',
-        hf_hub_id='timm/'),
-    'vit_relpos_base_patch16_rpn_224.untrained': _cfg(),
+    "focalnet_large_fl3.ms_in22k": _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),
+    "focalnet_large_fl4.ms_in22k": _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),
+    "focalnet_xlarge_fl3.ms_in22k": _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),
+    "focalnet_xlarge_fl4.ms_in22k": _cfg(
+        hf_hub_id='timm/',
+        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),
+    "focalnet_huge_fl3.ms_in22k": _cfg(
+        hf_hub_id='timm/',
+        num_classes=21842),
+    "focalnet_huge_fl4.ms_in22k": _cfg(
+        hf_hub_id='timm/',
+        num_classes=0),
 })
 
 
-@register_model
-def vit_relpos_base_patch32_plus_rpn_256(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/32+) w/ relative log-coord position and residual post-norm, no class token
-    """
-    model_kwargs = dict(
-        patch_size=32, embed_dim=896, depth=12, num_heads=14, block_fn=ResPostRelPosBlock, **kwargs)
-    model = _create_vision_transformer_relpos(
-        'vit_relpos_base_patch32_plus_rpn_256', pretrained=pretrained, **model_kwargs)
+def checkpoint_filter_fn(state_dict, model: FocalNet):
+    state_dict = state_dict.get('model', state_dict)
+    if 'stem.proj.weight' in state_dict:
+        return state_dict
+    import re
+    out_dict = {}
+    dest_dict = model.state_dict()
+    for k, v in state_dict.items():
+        k = re.sub(r'gamma_([0-9])', r'ls\1.gamma', k)
+        k = k.replace('patch_embed', 'stem')
+        k = re.sub(r'layers.(\d+).downsample', lambda x: f'layers.{int(x.group(1)) + 1}.downsample', k)
+        if 'norm' in k and k not in dest_dict:
+            k = re.sub(r'norm([0-9])', r'norm\1_post', k)
+        k = k.replace('ln.', 'norm.')
+        k = k.replace('head', 'head.fc')
+        if k in dest_dict and dest_dict[k].numel() == v.numel() and dest_dict[k].shape != v.shape:
+            v = v.reshape(dest_dict[k].shape)
+        out_dict[k] = v
+    return out_dict
+
+
+def _create_focalnet(variant, pretrained=False, **kwargs):
+    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 3, 1))))
+    out_indices = kwargs.pop('out_indices', default_out_indices)
+
+    model = build_model_with_cfg(
+        FocalNet, variant, pretrained,
+        pretrained_filter_fn=checkpoint_filter_fn,
+        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
+        **kwargs)
     return model
 
 
 @register_model
-def vit_relpos_base_patch16_plus_240(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16+) w/ relative log-coord position, no class token
-    """
-    model_kwargs = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14, **kwargs)
-    model = _create_vision_transformer_relpos('vit_relpos_base_patch16_plus_240', pretrained=pretrained, **model_kwargs)
-    return model
+def focalnet_tiny_srf(pretrained=False, **kwargs) -> FocalNet:
+    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, **kwargs)
+    return _create_focalnet('focalnet_tiny_srf', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_small_patch16_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position, no class token
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, fc_norm=True, **kwargs)
-    model = _create_vision_transformer_relpos('vit_relpos_small_patch16_224', pretrained=pretrained, **model_kwargs)
-    return model
+def focalnet_small_srf(pretrained=False, **kwargs) -> FocalNet:
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, **kwargs)
+    return _create_focalnet('focalnet_small_srf', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_medium_patch16_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position, no class token
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=True, **kwargs)
-    model = _create_vision_transformer_relpos('vit_relpos_medium_patch16_224', pretrained=pretrained, **model_kwargs)
-    return model
+def focalnet_base_srf(pretrained=False, **kwargs) -> FocalNet:
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, **kwargs)
+    return _create_focalnet('focalnet_base_srf', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_base_patch16_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position, no class token
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, fc_norm=True, **kwargs)
-    model = _create_vision_transformer_relpos('vit_relpos_base_patch16_224', pretrained=pretrained, **model_kwargs)
-    return model
+def focalnet_tiny_lrf(pretrained=False, **kwargs) -> FocalNet:
+    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)
+    return _create_focalnet('focalnet_tiny_lrf', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_srelpos_small_patch16_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ shared relative log-coord position, no class token
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, fc_norm=False,
-        rel_pos_dim=384, shared_rel_pos=True, **kwargs)
-    model = _create_vision_transformer_relpos('vit_srelpos_small_patch16_224', pretrained=pretrained, **model_kwargs)
-    return model
+def focalnet_small_lrf(pretrained=False, **kwargs) -> FocalNet:
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)
+    return _create_focalnet('focalnet_small_lrf', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_srelpos_medium_patch16_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ shared relative log-coord position, no class token
-    """
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=False,
-        rel_pos_dim=512, shared_rel_pos=True, **kwargs)
-    model = _create_vision_transformer_relpos(
-        'vit_srelpos_medium_patch16_224', pretrained=pretrained, **model_kwargs)
-    return model
+def focalnet_base_lrf(pretrained=False, **kwargs) -> FocalNet:
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, focal_levels=[3, 3, 3, 3], **kwargs)
+    return _create_focalnet('focalnet_base_lrf', pretrained=pretrained, **model_kwargs)
 
 
+# FocalNet large+ models
 @register_model
-def vit_relpos_medium_patch16_cls_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-M/16) w/ relative log-coord position, class token present
-    """
+def focalnet_large_fl3(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=False,
-        rel_pos_dim=256, class_token=True, global_pool='token', **kwargs)
-    model = _create_vision_transformer_relpos(
-        'vit_relpos_medium_patch16_cls_224', pretrained=pretrained, **model_kwargs)
-    return model
+        depths=[2, 2, 18, 2], embed_dim=192, focal_levels=[3, 3, 3, 3], focal_windows=[5] * 4,
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
+    return _create_focalnet('focalnet_large_fl3', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_base_patch16_cls_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position, class token present
-    """
+def focalnet_large_fl4(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False,
-        class_token=True, global_pool='token', **kwargs)
-    model = _create_vision_transformer_relpos('vit_relpos_base_patch16_cls_224', pretrained=pretrained, **model_kwargs)
-    return model
+        depths=[2, 2, 18, 2], embed_dim=192, focal_levels=[4, 4, 4, 4],
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
+    return _create_focalnet('focalnet_large_fl4', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_base_patch16_clsgap_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position, class token present
-    NOTE this config is a bit of a mistake, class token was enabled but global avg-pool w/ fc-norm was not disabled
-    Leaving here for comparisons w/ a future re-train as it performs quite well.
-    """
+def focalnet_xlarge_fl3(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, fc_norm=True, class_token=True, **kwargs)
-    model = _create_vision_transformer_relpos('vit_relpos_base_patch16_clsgap_224', pretrained=pretrained, **model_kwargs)
-    return model
+        depths=[2, 2, 18, 2], embed_dim=256, focal_levels=[3, 3, 3, 3], focal_windows=[5] * 4,
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
+    return _create_focalnet('focalnet_xlarge_fl3', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_small_patch16_rpn_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token
-    """
+def focalnet_xlarge_fl4(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, block_fn=ResPostRelPosBlock, **kwargs)
-    model = _create_vision_transformer_relpos(
-        'vit_relpos_small_patch16_rpn_224', pretrained=pretrained, **model_kwargs)
-    return model
+        depths=[2, 2, 18, 2], embed_dim=256, focal_levels=[4, 4, 4, 4],
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
+    return _create_focalnet('focalnet_xlarge_fl4', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_medium_patch16_rpn_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token
-    """
+def focalnet_huge_fl3(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, block_fn=ResPostRelPosBlock, **kwargs)
-    model = _create_vision_transformer_relpos(
-        'vit_relpos_medium_patch16_rpn_224', pretrained=pretrained, **model_kwargs)
-    return model
+        depths=[2, 2, 18, 2], embed_dim=352, focal_levels=[3, 3, 3, 3], focal_windows=[3] * 4,
+        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
+    return _create_focalnet('focalnet_huge_fl3', pretrained=pretrained, **model_kwargs)
 
 
 @register_model
-def vit_relpos_base_patch16_rpn_224(pretrained=False, **kwargs):
-    """ ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token
-    """
+def focalnet_huge_fl4(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, block_fn=ResPostRelPosBlock, **kwargs)
-    model = _create_vision_transformer_relpos(
-        'vit_relpos_base_patch16_rpn_224', pretrained=pretrained, **model_kwargs)
-    return model
+        depths=[2, 2, 18, 2], embed_dim=352, focal_levels=[4, 4, 4, 4],
+        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
+    return _create_focalnet('focalnet_huge_fl4', pretrained=pretrained, **model_kwargs)
+
```

### Comparing `timm-0.8.6.dev0/timm/models/volo.py` & `timm-0.9.0/timm/models/volo.py`

 * *Files 6% similar despite different names*

```diff
@@ -26,70 +26,32 @@
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.utils.checkpoint import checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, Mlp, to_2tuple, to_ntuple, trunc_normal_
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['VOLO']  # model_registry will add each entrypoint fn to this
 
 
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': .96, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.conv.0', 'classifier': ('head', 'aux_head'),
-        **kwargs
-    }
-
-
-default_cfgs = {
-    'volo_d1_224': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d1_224_84.2.pth.tar',
-        crop_pct=0.96),
-    'volo_d1_384': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d1_384_85.2.pth.tar',
-        crop_pct=1.0, input_size=(3, 384, 384)),
-    'volo_d2_224': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d2_224_85.2.pth.tar',
-        crop_pct=0.96),
-    'volo_d2_384': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d2_384_86.0.pth.tar',
-        crop_pct=1.0, input_size=(3, 384, 384)),
-    'volo_d3_224': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d3_224_85.4.pth.tar',
-        crop_pct=0.96),
-    'volo_d3_448': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d3_448_86.3.pth.tar',
-        crop_pct=1.0, input_size=(3, 448, 448)),
-    'volo_d4_224': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d4_224_85.7.pth.tar',
-        crop_pct=0.96),
-    'volo_d4_448': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d4_448_86.79.pth.tar',
-        crop_pct=1.15, input_size=(3, 448, 448)),
-    'volo_d5_224': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_224_86.10.pth.tar',
-        crop_pct=0.96),
-    'volo_d5_448': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_448_87.0.pth.tar',
-        crop_pct=1.15, input_size=(3, 448, 448)),
-    'volo_d5_512': _cfg(
-        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_512_87.07.pth.tar',
-        crop_pct=1.15, input_size=(3, 512, 512)),
-}
-
-
 class OutlookAttention(nn.Module):
 
-    def __init__(self, dim, num_heads, kernel_size=3, padding=1, stride=1, qkv_bias=False, attn_drop=0., proj_drop=0.):
+    def __init__(
+            self,
+            dim,
+            num_heads,
+            kernel_size=3,
+            padding=1,
+            stride=1,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+    ):
         super().__init__()
         head_dim = dim // num_heads
         self.num_heads = num_heads
         self.kernel_size = kernel_size
         self.padding = padding
         self.stride = stride
         self.scale = head_dim ** -0.5
@@ -129,40 +91,65 @@
         x = self.proj_drop(x)
 
         return x
 
 
 class Outlooker(nn.Module):
     def __init__(
-            self, dim, kernel_size, padding, stride=1, num_heads=1, mlp_ratio=3., attn_drop=0.,
-            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, qkv_bias=False
+            self,
+            dim,
+            kernel_size,
+            padding,
+            stride=1,
+            num_heads=1,
+            mlp_ratio=3.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            qkv_bias=False,
     ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = OutlookAttention(
-            dim, num_heads, kernel_size=kernel_size,
-            padding=padding, stride=stride,
-            qkv_bias=qkv_bias, attn_drop=attn_drop)
+            dim,
+            num_heads,
+            kernel_size=kernel_size,
+            padding=padding,
+            stride=stride,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+        )
 
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+        )
 
     def forward(self, x):
         x = x + self.drop_path(self.attn(self.norm1(x)))
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
 class Attention(nn.Module):
 
     def __init__(
-            self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
+            self,
+            dim,
+            num_heads=8,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+    ):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
 
         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
         self.attn_drop = nn.Dropout(attn_drop)
@@ -185,16 +172,24 @@
 
         return x
 
 
 class Transformer(nn.Module):
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False,
-            attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop)
 
         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
@@ -207,15 +202,22 @@
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
 class ClassAttention(nn.Module):
 
     def __init__(
-            self, dim, num_heads=8, head_dim=None, qkv_bias=False, attn_drop=0., proj_drop=0.):
+            self,
+            dim,
+            num_heads=8,
+            head_dim=None,
+            qkv_bias=False,
+            attn_drop=0.,
+            proj_drop=0.,
+    ):
         super().__init__()
         self.num_heads = num_heads
         if head_dim is not None:
             self.head_dim = head_dim
         else:
             head_dim = dim // num_heads
             self.head_dim = head_dim
@@ -242,25 +244,46 @@
         cls_embed = self.proj_drop(cls_embed)
         return cls_embed
 
 
 class ClassBlock(nn.Module):
 
     def __init__(
-            self, dim, num_heads, head_dim=None, mlp_ratio=4., qkv_bias=False,
-            drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
+            self,
+            dim,
+            num_heads,
+            head_dim=None,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
         self.attn = ClassAttention(
-            dim, num_heads=num_heads, head_dim=head_dim, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+            dim,
+            num_heads=num_heads,
+            head_dim=head_dim,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=drop,
+        )
         # NOTE: drop path for stochastic depth
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
         mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=drop,
+        )
 
     def forward(self, x):
         cls_embed = x[:, :1]
         cls_embed = cls_embed + self.drop_path(self.attn(self.norm1(x)))
         cls_embed = cls_embed + self.drop_path(self.mlp(self.norm2(cls_embed)))
         return torch.cat([cls_embed, x[:, 1:]], dim=1)
 
@@ -295,16 +318,23 @@
 
 class PatchEmbed(nn.Module):
     """ Image to Patch Embedding.
     Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding
     """
 
     def __init__(
-            self, img_size=224, stem_conv=False, stem_stride=1,
-            patch_size=8, in_chans=3, hidden_dim=64, embed_dim=384):
+            self,
+            img_size=224,
+            stem_conv=False,
+            stem_stride=1,
+            patch_size=8,
+            in_chans=3,
+            hidden_dim=64,
+            embed_dim=384,
+    ):
         super().__init__()
         assert patch_size in [4, 8, 16]
         if stem_conv:
             self.conv = nn.Sequential(
                 nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride, padding=3, bias=False),  # 112x112
                 nn.BatchNorm2d(hidden_dim),
                 nn.ReLU(inplace=True),
@@ -341,49 +371,77 @@
         x = x.permute(0, 3, 1, 2)
         x = self.proj(x)  # B, C, H, W
         x = x.permute(0, 2, 3, 1)
         return x
 
 
 def outlooker_blocks(
-        block_fn, index, dim, layers, num_heads=1, kernel_size=3, padding=1, stride=2,
-        mlp_ratio=3., qkv_bias=False, attn_drop=0, drop_path_rate=0., **kwargs):
+        block_fn,
+        index,
+        dim,
+        layers,
+        num_heads=1,
+        kernel_size=3,
+        padding=1,
+        stride=2,
+        mlp_ratio=3.,
+        qkv_bias=False,
+        attn_drop=0,
+        drop_path_rate=0.,
+        **kwargs,
+):
     """
     generate outlooker layer in stage1
     return: outlooker layers
     """
     blocks = []
     for block_idx in range(layers[index]):
         block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)
-        blocks.append(
-            block_fn(
-                dim, kernel_size=kernel_size, padding=padding,
-                stride=stride, num_heads=num_heads, mlp_ratio=mlp_ratio,
-                qkv_bias=qkv_bias, attn_drop=attn_drop, drop_path=block_dpr))
+        blocks.append(block_fn(
+            dim,
+            kernel_size=kernel_size,
+            padding=padding,
+            stride=stride,
+            num_heads=num_heads,
+            mlp_ratio=mlp_ratio,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            drop_path=block_dpr,
+        ))
     blocks = nn.Sequential(*blocks)
     return blocks
 
 
 def transformer_blocks(
-        block_fn, index, dim, layers, num_heads, mlp_ratio=3.,
-        qkv_bias=False, attn_drop=0, drop_path_rate=0., **kwargs):
+        block_fn,
+        index,
+        dim,
+        layers,
+        num_heads,
+        mlp_ratio=3.,
+        qkv_bias=False,
+        attn_drop=0,
+        drop_path_rate=0.,
+        **kwargs,
+):
     """
     generate transformer layers in stage2
     return: transformer layers
     """
     blocks = []
     for block_idx in range(layers[index]):
         block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)
-        blocks.append(
-            block_fn(
-                dim, num_heads,
-                mlp_ratio=mlp_ratio,
-                qkv_bias=qkv_bias,
-                attn_drop=attn_drop,
-                drop_path=block_dpr))
+        blocks.append(block_fn(
+            dim,
+            num_heads,
+            mlp_ratio=mlp_ratio,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            drop_path=block_dpr,
+        ))
     blocks = nn.Sequential(*blocks)
     return blocks
 
 
 class VOLO(nn.Module):
     """
     Vision Outlooker, the main class of our model
@@ -401,14 +459,15 @@
             embed_dims=None,
             num_heads=None,
             downsamples=(True, False, False, False),
             outlook_attention=(True, False, False, False),
             mlp_ratio=3.0,
             qkv_bias=False,
             drop_rate=0.,
+            pos_drop_rate=0.,
             attn_drop_rate=0.,
             drop_path_rate=0.,
             norm_layer=nn.LayerNorm,
             post_layers=('ca', 'ca'),
             use_aux_head=True,
             use_mix_token=False,
             pooling_scale=2,
@@ -425,72 +484,93 @@
         self.num_features = embed_dims[-1]
         if use_mix_token:  # enable token mixing, see token labeling for details.
             self.beta = 1.0
             assert global_pool == 'token', "return all tokens if mix_token is enabled"
         self.grad_checkpointing = False
 
         self.patch_embed = PatchEmbed(
-            stem_conv=True, stem_stride=2, patch_size=patch_size,
-            in_chans=in_chans, hidden_dim=stem_hidden_dim,
-            embed_dim=embed_dims[0])
+            stem_conv=True,
+            stem_stride=2,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            hidden_dim=stem_hidden_dim,
+            embed_dim=embed_dims[0],
+        )
 
         # inital positional encoding, we add positional encoding after outlooker blocks
         patch_grid = (img_size[0] // patch_size // pooling_scale, img_size[1] // patch_size // pooling_scale)
         self.pos_embed = nn.Parameter(torch.zeros(1, patch_grid[0], patch_grid[1], embed_dims[-1]))
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         # set the main block in network
         network = []
         for i in range(len(layers)):
             if outlook_attention[i]:
                 # stage 1
                 stage = outlooker_blocks(
-                    Outlooker, i, embed_dims[i], layers, num_heads[i], mlp_ratio=mlp_ratio[i],
-                    qkv_bias=qkv_bias, attn_drop=attn_drop_rate, norm_layer=norm_layer)
+                    Outlooker,
+                    i,
+                    embed_dims[i],
+                    layers,
+                    num_heads[i],
+                    mlp_ratio=mlp_ratio[i],
+                    qkv_bias=qkv_bias,
+                    attn_drop=attn_drop_rate,
+                    norm_layer=norm_layer,
+                )
                 network.append(stage)
             else:
                 # stage 2
                 stage = transformer_blocks(
-                    Transformer, i, embed_dims[i], layers, num_heads[i], mlp_ratio=mlp_ratio[i], qkv_bias=qkv_bias,
-                    drop_path_rate=drop_path_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer)
+                    Transformer,
+                    i,
+                    embed_dims[i],
+                    layers,
+                    num_heads[i],
+                    mlp_ratio=mlp_ratio[i],
+                    qkv_bias=qkv_bias,
+                    drop_path_rate=drop_path_rate,
+                    attn_drop=attn_drop_rate,
+                    norm_layer=norm_layer,
+                )
                 network.append(stage)
 
             if downsamples[i]:
                 # downsampling between two stages
                 network.append(Downsample(embed_dims[i], embed_dims[i + 1], 2))
 
         self.network = nn.ModuleList(network)
 
         # set post block, for example, class attention layers
         self.post_network = None
         if post_layers is not None:
-            self.post_network = nn.ModuleList(
-                [
-                    get_block(
-                        post_layers[i],
-                        dim=embed_dims[-1],
-                        num_heads=num_heads[-1],
-                        mlp_ratio=mlp_ratio[-1],
-                        qkv_bias=qkv_bias,
-                        attn_drop=attn_drop_rate,
-                        drop_path=0.,
-                        norm_layer=norm_layer)
-                    for i in range(len(post_layers))
-                ])
+            self.post_network = nn.ModuleList([
+                get_block(
+                    post_layers[i],
+                    dim=embed_dims[-1],
+                    num_heads=num_heads[-1],
+                    mlp_ratio=mlp_ratio[-1],
+                    qkv_bias=qkv_bias,
+                    attn_drop=attn_drop_rate,
+                    drop_path=0.,
+                    norm_layer=norm_layer)
+                for i in range(len(post_layers))
+            ])
             self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1]))
             trunc_normal_(self.cls_token, std=.02)
 
         # set output type
         if use_aux_head:
             self.aux_head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
         else:
             self.aux_head = None
         self.norm = norm_layer(self.num_features)
 
         # Classifier head
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
         trunc_normal_(self.pos_embed, std=.02)
         self.apply(self._init_weights)
 
     def _init_weights(self, m):
         if isinstance(m, nn.Linear):
@@ -626,14 +706,15 @@
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool == 'avg':
             out = x.mean(dim=1)
         elif self.global_pool == 'token':
             out = x[:, 0]
         else:
             out = x
+        x = self.head_drop(x)
         if pre_logits:
             return out
         out = self.head(out)
         if self.aux_head is not None:
             # generate classes in all feature tokens, see token labeling
             aux = self.aux_head(x[:, 1:])
             out = out + 0.5 * aux.max(1)[0]
@@ -645,107 +726,171 @@
         x = self.forward_head(x)
         return x
 
 
 def _create_volo(variant, pretrained=False, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
-    return build_model_with_cfg(VOLO, variant, pretrained, **kwargs)
+    return build_model_with_cfg(
+        VOLO,
+        variant,
+        pretrained,
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': .96, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.conv.0', 'classifier': ('head', 'aux_head'),
+        **kwargs
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'volo_d1_224.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d1_224_84.2.pth.tar',
+        crop_pct=0.96),
+    'volo_d1_384.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d1_384_85.2.pth.tar',
+        crop_pct=1.0, input_size=(3, 384, 384)),
+    'volo_d2_224.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d2_224_85.2.pth.tar',
+        crop_pct=0.96),
+    'volo_d2_384.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d2_384_86.0.pth.tar',
+        crop_pct=1.0, input_size=(3, 384, 384)),
+    'volo_d3_224.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d3_224_85.4.pth.tar',
+        crop_pct=0.96),
+    'volo_d3_448.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d3_448_86.3.pth.tar',
+        crop_pct=1.0, input_size=(3, 448, 448)),
+    'volo_d4_224.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d4_224_85.7.pth.tar',
+        crop_pct=0.96),
+    'volo_d4_448.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d4_448_86.79.pth.tar',
+        crop_pct=1.15, input_size=(3, 448, 448)),
+    'volo_d5_224.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_224_86.10.pth.tar',
+        crop_pct=0.96),
+    'volo_d5_448.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_448_87.0.pth.tar',
+        crop_pct=1.15, input_size=(3, 448, 448)),
+    'volo_d5_512.sail_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_512_87.07.pth.tar',
+        crop_pct=1.15, input_size=(3, 512, 512)),
+})
 
 
 @register_model
-def volo_d1_224(pretrained=False, **kwargs):
+def volo_d1_224(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D1 model, Params: 27M """
     model_args = dict(layers=(4, 4, 8, 2), embed_dims=(192, 384, 384, 384), num_heads=(6, 12, 12, 12), **kwargs)
     model = _create_volo('volo_d1_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d1_384(pretrained=False, **kwargs):
+def volo_d1_384(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D1 model, Params: 27M """
     model_args = dict(layers=(4, 4, 8, 2), embed_dims=(192, 384, 384, 384), num_heads=(6, 12, 12, 12), **kwargs)
     model = _create_volo('volo_d1_384', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d2_224(pretrained=False, **kwargs):
+def volo_d2_224(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D2 model, Params: 59M """
     model_args = dict(layers=(6, 4, 10, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)
     model = _create_volo('volo_d2_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d2_384(pretrained=False, **kwargs):
+def volo_d2_384(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D2 model, Params: 59M """
     model_args = dict(layers=(6, 4, 10, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)
     model = _create_volo('volo_d2_384', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d3_224(pretrained=False, **kwargs):
+def volo_d3_224(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D3 model, Params: 86M """
     model_args = dict(layers=(8, 8, 16, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)
     model = _create_volo('volo_d3_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d3_448(pretrained=False, **kwargs):
+def volo_d3_448(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D3 model, Params: 86M """
     model_args = dict(layers=(8, 8, 16, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)
     model = _create_volo('volo_d3_448', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d4_224(pretrained=False, **kwargs):
+def volo_d4_224(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D4 model, Params: 193M """
     model_args = dict(layers=(8, 8, 16, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16), **kwargs)
     model = _create_volo('volo_d4_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d4_448(pretrained=False, **kwargs):
+def volo_d4_448(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D4 model, Params: 193M """
     model_args = dict(layers=(8, 8, 16, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16), **kwargs)
     model = _create_volo('volo_d4_448', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d5_224(pretrained=False, **kwargs):
+def volo_d5_224(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D5 model, Params: 296M
     stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5
     """
     model_args = dict(
         layers=(12, 12, 20, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16),
         mlp_ratio=4, stem_hidden_dim=128, **kwargs)
     model = _create_volo('volo_d5_224', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d5_448(pretrained=False, **kwargs):
+def volo_d5_448(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D5 model, Params: 296M
     stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5
     """
     model_args = dict(
         layers=(12, 12, 20, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16),
         mlp_ratio=4, stem_hidden_dim=128, **kwargs)
     model = _create_volo('volo_d5_448', pretrained=pretrained, **model_args)
     return model
 
 
 @register_model
-def volo_d5_512(pretrained=False, **kwargs):
+def volo_d5_512(pretrained=False, **kwargs) -> VOLO:
     """ VOLO-D5 model, Params: 296M
     stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5
     """
     model_args = dict(
         layers=(12, 12, 20, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16),
         mlp_ratio=4, stem_hidden_dim=128, **kwargs)
     model = _create_volo('volo_d5_512', pretrained=pretrained, **model_args)
```

### Comparing `timm-0.8.6.dev0/timm/models/vovnet.py` & `timm-0.9.0/timm/models/vovnet.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,160 +14,22 @@
 from typing import List
 
 import torch
 import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ConvNormAct, SeparableConvNormAct, BatchNormAct2d, ClassifierHead, DropPath, \
-    create_attn, create_norm_act_layer, get_norm_act_layer
+    create_attn, create_norm_act_layer
 from ._builder import build_model_with_cfg
 from ._manipulate import checkpoint_seq
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs
 
 __all__ = ['VovNet']  # model_registry will add each entrypoint fn to this
 
 
-# model cfgs adapted from https://github.com/youngwanLEE/vovnet-detectron2 &
-# https://github.com/stigma0617/VoVNet.pytorch/blob/master/models_vovnet/vovnet.py
-model_cfgs = dict(
-    vovnet39a=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=5,
-        block_per_stage=[1, 1, 2, 2],
-        residual=False,
-        depthwise=False,
-        attn='',
-    ),
-    vovnet57a=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=5,
-        block_per_stage=[1, 1, 4, 3],
-        residual=False,
-        depthwise=False,
-        attn='',
-
-    ),
-    ese_vovnet19b_slim_dw=dict(
-        stem_chs=[64, 64, 64],
-        stage_conv_chs=[64, 80, 96, 112],
-        stage_out_chs=[112, 256, 384, 512],
-        layer_per_block=3,
-        block_per_stage=[1, 1, 1, 1],
-        residual=True,
-        depthwise=True,
-        attn='ese',
-
-    ),
-    ese_vovnet19b_dw=dict(
-        stem_chs=[64, 64, 64],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=3,
-        block_per_stage=[1, 1, 1, 1],
-        residual=True,
-        depthwise=True,
-        attn='ese',
-    ),
-    ese_vovnet19b_slim=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[64, 80, 96, 112],
-        stage_out_chs=[112, 256, 384, 512],
-        layer_per_block=3,
-        block_per_stage=[1, 1, 1, 1],
-        residual=True,
-        depthwise=False,
-        attn='ese',
-    ),
-    ese_vovnet19b=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=3,
-        block_per_stage=[1, 1, 1, 1],
-        residual=True,
-        depthwise=False,
-        attn='ese',
-
-    ),
-    ese_vovnet39b=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=5,
-        block_per_stage=[1, 1, 2, 2],
-        residual=True,
-        depthwise=False,
-        attn='ese',
-    ),
-    ese_vovnet57b=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=5,
-        block_per_stage=[1, 1, 4, 3],
-        residual=True,
-        depthwise=False,
-        attn='ese',
-
-    ),
-    ese_vovnet99b=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=5,
-        block_per_stage=[1, 3, 9, 3],
-        residual=True,
-        depthwise=False,
-        attn='ese',
-    ),
-    eca_vovnet39b=dict(
-        stem_chs=[64, 64, 128],
-        stage_conv_chs=[128, 160, 192, 224],
-        stage_out_chs=[256, 512, 768, 1024],
-        layer_per_block=5,
-        block_per_stage=[1, 1, 2, 2],
-        residual=True,
-        depthwise=False,
-        attn='eca',
-    ),
-)
-model_cfgs['ese_vovnet39b_evos'] = model_cfgs['ese_vovnet39b']
-model_cfgs['ese_vovnet99b_iabn'] = model_cfgs['ese_vovnet99b']
-
-
-def _cfg(url=''):
-    return {
-        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
-        'crop_pct': 0.875, 'interpolation': 'bicubic',
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'stem.0.conv', 'classifier': 'head.fc',
-    }
-
-
-default_cfgs = dict(
-    vovnet39a=_cfg(url=''),
-    vovnet57a=_cfg(url=''),
-    ese_vovnet19b_slim_dw=_cfg(url=''),
-    ese_vovnet19b_dw=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ese_vovnet19b_dw-a8741004.pth'),
-    ese_vovnet19b_slim=_cfg(url=''),
-    ese_vovnet39b=_cfg(
-        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ese_vovnet39b-f912fe73.pth'),
-    ese_vovnet57b=_cfg(url=''),
-    ese_vovnet99b=_cfg(url=''),
-    eca_vovnet39b=_cfg(url=''),
-    ese_vovnet39b_evos=_cfg(url=''),
-    ese_vovnet99b_iabn=_cfg(url=''),
-)
-
-
 class SequentialAppendList(nn.Sequential):
     def __init__(self, *args):
         super(SequentialAppendList, self).__init__(*args)
 
     def forward(self, x: torch.Tensor, concat_list: List[torch.Tensor]) -> torch.Tensor:
         for i, module in enumerate(self):
             if i == 0:
@@ -401,74 +263,208 @@
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
+# model cfgs adapted from https://github.com/youngwanLEE/vovnet-detectron2 &
+# https://github.com/stigma0617/VoVNet.pytorch/blob/master/models_vovnet/vovnet.py
+model_cfgs = dict(
+    vovnet39a=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=5,
+        block_per_stage=[1, 1, 2, 2],
+        residual=False,
+        depthwise=False,
+        attn='',
+    ),
+    vovnet57a=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=5,
+        block_per_stage=[1, 1, 4, 3],
+        residual=False,
+        depthwise=False,
+        attn='',
+
+    ),
+    ese_vovnet19b_slim_dw=dict(
+        stem_chs=[64, 64, 64],
+        stage_conv_chs=[64, 80, 96, 112],
+        stage_out_chs=[112, 256, 384, 512],
+        layer_per_block=3,
+        block_per_stage=[1, 1, 1, 1],
+        residual=True,
+        depthwise=True,
+        attn='ese',
+
+    ),
+    ese_vovnet19b_dw=dict(
+        stem_chs=[64, 64, 64],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=3,
+        block_per_stage=[1, 1, 1, 1],
+        residual=True,
+        depthwise=True,
+        attn='ese',
+    ),
+    ese_vovnet19b_slim=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[64, 80, 96, 112],
+        stage_out_chs=[112, 256, 384, 512],
+        layer_per_block=3,
+        block_per_stage=[1, 1, 1, 1],
+        residual=True,
+        depthwise=False,
+        attn='ese',
+    ),
+    ese_vovnet19b=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=3,
+        block_per_stage=[1, 1, 1, 1],
+        residual=True,
+        depthwise=False,
+        attn='ese',
+
+    ),
+    ese_vovnet39b=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=5,
+        block_per_stage=[1, 1, 2, 2],
+        residual=True,
+        depthwise=False,
+        attn='ese',
+    ),
+    ese_vovnet57b=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=5,
+        block_per_stage=[1, 1, 4, 3],
+        residual=True,
+        depthwise=False,
+        attn='ese',
+
+    ),
+    ese_vovnet99b=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=5,
+        block_per_stage=[1, 3, 9, 3],
+        residual=True,
+        depthwise=False,
+        attn='ese',
+    ),
+    eca_vovnet39b=dict(
+        stem_chs=[64, 64, 128],
+        stage_conv_chs=[128, 160, 192, 224],
+        stage_out_chs=[256, 512, 768, 1024],
+        layer_per_block=5,
+        block_per_stage=[1, 1, 2, 2],
+        residual=True,
+        depthwise=False,
+        attn='eca',
+    ),
+)
+model_cfgs['ese_vovnet39b_evos'] = model_cfgs['ese_vovnet39b']
+
+
 def _create_vovnet(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
-        VovNet, variant, pretrained,
+        VovNet,
+        variant,
+        pretrained,
         model_cfg=model_cfgs[variant],
         feature_cfg=dict(flatten_sequential=True),
-        **kwargs)
+        **kwargs,
+    )
+
+
+def _cfg(url='', **kwargs):
+    return {
+        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),
+        'crop_pct': 0.875, 'interpolation': 'bicubic',
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'stem.0.conv', 'classifier': 'head.fc', **kwargs,
+    }
+
+
+default_cfgs = generate_default_cfgs({
+    'vovnet39a.untrained': _cfg(url=''),
+    'vovnet57a.untrained': _cfg(url=''),
+    'ese_vovnet19b_slim_dw.untrained': _cfg(url=''),
+    'ese_vovnet19b_dw.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'ese_vovnet19b_slim.untrained': _cfg(url=''),
+    'ese_vovnet39b.ra_in1k': _cfg(
+        hf_hub_id='timm/',
+        test_input_size=(3, 288, 288), test_crop_pct=0.95),
+    'ese_vovnet57b.untrained': _cfg(url=''),
+    'ese_vovnet99b.untrained': _cfg(url=''),
+    'eca_vovnet39b.untrained': _cfg(url=''),
+    'ese_vovnet39b_evos.untrained': _cfg(url=''),
+})
 
 
 @register_model
-def vovnet39a(pretrained=False, **kwargs):
+def vovnet39a(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('vovnet39a', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def vovnet57a(pretrained=False, **kwargs):
+def vovnet57a(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('vovnet57a', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def ese_vovnet19b_slim_dw(pretrained=False, **kwargs):
+def ese_vovnet19b_slim_dw(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('ese_vovnet19b_slim_dw', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def ese_vovnet19b_dw(pretrained=False, **kwargs):
+def ese_vovnet19b_dw(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('ese_vovnet19b_dw', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def ese_vovnet19b_slim(pretrained=False, **kwargs):
+def ese_vovnet19b_slim(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('ese_vovnet19b_slim', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def ese_vovnet39b(pretrained=False, **kwargs):
+def ese_vovnet39b(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('ese_vovnet39b', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def ese_vovnet57b(pretrained=False, **kwargs):
+def ese_vovnet57b(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('ese_vovnet57b', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def ese_vovnet99b(pretrained=False, **kwargs):
+def ese_vovnet99b(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('ese_vovnet99b', pretrained=pretrained, **kwargs)
 
 
 @register_model
-def eca_vovnet39b(pretrained=False, **kwargs):
+def eca_vovnet39b(pretrained=False, **kwargs) -> VovNet:
     return _create_vovnet('eca_vovnet39b', pretrained=pretrained, **kwargs)
 
 
 # Experimental Models
 
 @register_model
-def ese_vovnet39b_evos(pretrained=False, **kwargs):
+def ese_vovnet39b_evos(pretrained=False, **kwargs) -> VovNet:
     def norm_act_fn(num_features, **nkwargs):
         return create_norm_act_layer('evonorms0', num_features, jit=False, **nkwargs)
     return _create_vovnet('ese_vovnet39b_evos', pretrained=pretrained, norm_layer=norm_act_fn, **kwargs)
-
-
-@register_model
-def ese_vovnet99b_iabn(pretrained=False, **kwargs):
-    norm_layer = get_norm_act_layer('iabn', act_layer='leaky_relu')
-    return _create_vovnet(
-        'ese_vovnet99b_iabn', pretrained=pretrained, norm_layer=norm_layer, act_layer=nn.LeakyReLU, **kwargs)
```

### Comparing `timm-0.8.6.dev0/timm/models/xception.py` & `timm-0.9.0/timm/models/xception.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,34 +23,18 @@
 """
 import torch.jit
 import torch.nn as nn
 import torch.nn.functional as F
 
 from timm.layers import create_classifier
 from ._builder import build_model_with_cfg
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 
 __all__ = ['Xception']
 
-default_cfgs = {
-    'xception': {
-        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth',
-        'input_size': (3, 299, 299),
-        'pool_size': (10, 10),
-        'crop_pct': 0.8975,
-        'interpolation': 'bicubic',
-        'mean': (0.5, 0.5, 0.5),
-        'std': (0.5, 0.5, 0.5),
-        'num_classes': 1000,
-        'first_conv': 'conv1',
-        'classifier': 'fc'
-        # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299
-    }
-}
-
 
 class SeparableConv2d(nn.Module):
     def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1):
         super(SeparableConv2d, self).__init__()
 
         self.conv1 = nn.Conv2d(
             in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=False)
@@ -240,10 +224,32 @@
 def _xception(variant, pretrained=False, **kwargs):
     return build_model_with_cfg(
         Xception, variant, pretrained,
         feature_cfg=dict(feature_cls='hook'),
         **kwargs)
 
 
+default_cfgs = generate_default_cfgs({
+    'legacy_xception.tf_in1k': {
+        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth',
+        'input_size': (3, 299, 299),
+        'pool_size': (10, 10),
+        'crop_pct': 0.8975,
+        'interpolation': 'bicubic',
+        'mean': (0.5, 0.5, 0.5),
+        'std': (0.5, 0.5, 0.5),
+        'num_classes': 1000,
+        'first_conv': 'conv1',
+        'classifier': 'fc'
+        # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299
+    }
+})
+
+
 @register_model
-def xception(pretrained=False, **kwargs):
-    return _xception('xception', pretrained=pretrained, **kwargs)
+def legacy_xception(pretrained=False, **kwargs) -> Xception:
+    return _xception('legacy_xception', pretrained=pretrained, **kwargs)
+
+
+register_model_deprecations(__name__, {
+    'xception': 'legacy_xception',
+})
```

### Comparing `timm-0.8.6.dev0/timm/models/xcit.py` & `timm-0.9.0/timm/models/xcit.py`

 * *Files 13% similar despite different names*

```diff
@@ -18,93 +18,19 @@
 import torch.nn as nn
 from torch.utils.checkpoint import checkpoint
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, trunc_normal_, to_2tuple
 from ._builder import build_model_with_cfg
 from ._features_fx import register_notrace_module
-from ._registry import register_model
+from ._registry import register_model, generate_default_cfgs, register_model_deprecations
 from .cait import ClassAttn
 from .vision_transformer import Mlp
 
-__all__ = ['XCiT']  # model_registry will add each entrypoint fn to this
-
-
-def _cfg(url='', **kwargs):
-    return {
-        'url': url,
-        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
-        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,
-        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
-        'first_conv': 'patch_embed.proj.0.0', 'classifier': 'head',
-        **kwargs
-    }
-
-
-default_cfgs = {
-    # Patch size 16
-    'xcit_nano_12_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_224.pth'),  
-    'xcit_nano_12_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_224_dist.pth'),
-    'xcit_nano_12_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_tiny_12_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224.pth'),
-    'xcit_tiny_12_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224_dist.pth'),
-    'xcit_tiny_12_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_tiny_24_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224.pth'),
-    'xcit_tiny_24_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224_dist.pth'),
-    'xcit_tiny_24_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_small_12_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_224.pth'),
-    'xcit_small_12_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_224_dist.pth'),
-    'xcit_small_12_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_small_24_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_224.pth'),
-    'xcit_small_24_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_224_dist.pth'),
-    'xcit_small_24_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_medium_24_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_224.pth'),
-    'xcit_medium_24_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_224_dist.pth'),
-    'xcit_medium_24_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_large_24_p16_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_224.pth'),
-    'xcit_large_24_p16_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_224_dist.pth'),
-    'xcit_large_24_p16_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_384_dist.pth', input_size=(3, 384, 384)),
-
-    # Patch size 8
-    'xcit_nano_12_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_224.pth'),  
-    'xcit_nano_12_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_224_dist.pth'),
-    'xcit_nano_12_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_tiny_12_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224.pth'),
-    'xcit_tiny_12_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224_dist.pth'),
-    'xcit_tiny_12_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_tiny_24_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224.pth'),
-    'xcit_tiny_24_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224_dist.pth'),
-    'xcit_tiny_24_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_small_12_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_224.pth'),
-    'xcit_small_12_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_224_dist.pth'),
-    'xcit_small_12_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_small_24_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_224.pth'),
-    'xcit_small_24_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_224_dist.pth'),
-    'xcit_small_24_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_medium_24_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_224.pth'),
-    'xcit_medium_24_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_224_dist.pth'),
-    'xcit_medium_24_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_384_dist.pth', input_size=(3, 384, 384)),
-    'xcit_large_24_p8_224': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_224.pth'),
-    'xcit_large_24_p8_224_dist': _cfg(url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_224_dist.pth'),
-    'xcit_large_24_p8_384_dist': _cfg(
-        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_384_dist.pth', input_size=(3, 384, 384)),
-}
+__all__ = ['Xcit']  # model_registry will add each entrypoint fn to this
 
 
 @register_notrace_module  # reason: FX can't symbolically trace torch.arange in forward method
 class PositionalEncodingFourier(nn.Module):
     """
     Positional encoding relying on a fourier kernel matching the one used in the "Attention is all you Need" paper.
     Based on the official XCiT code
@@ -215,25 +141,36 @@
         return x
 
 
 class ClassAttentionBlock(nn.Module):
     """Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239"""
 
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0.,
-            act_layer=nn.GELU, norm_layer=nn.LayerNorm, eta=1., tokens_norm=False):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            eta=1.,
+            tokens_norm=False,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
 
         self.attn = ClassAttn(
-            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)
 
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
         self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=proj_drop)
 
         if eta is not None:  # LayerScale Initialization (no layerscale when None)
             self.gamma1 = nn.Parameter(eta * torch.ones(dim))
             self.gamma2 = nn.Parameter(eta * torch.ones(dim))
         else:
             self.gamma1, self.gamma2 = 1.0, 1.0
 
@@ -293,63 +230,95 @@
     @torch.jit.ignore
     def no_weight_decay(self):
         return {'temperature'}
 
 
 class XCABlock(nn.Module):
     def __init__(
-            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
-            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, eta=1.):
+            self,
+            dim,
+            num_heads,
+            mlp_ratio=4.,
+            qkv_bias=False,
+            proj_drop=0.,
+            attn_drop=0.,
+            drop_path=0.,
+            act_layer=nn.GELU,
+            norm_layer=nn.LayerNorm,
+            eta=1.,
+    ):
         super().__init__()
         self.norm1 = norm_layer(dim)
-        self.attn = XCA(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
+        self.attn = XCA(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)
         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
 
         self.norm3 = norm_layer(dim)
         self.local_mp = LPI(in_features=dim, act_layer=act_layer)
 
         self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
+        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=proj_drop)
 
         self.gamma1 = nn.Parameter(eta * torch.ones(dim))
         self.gamma3 = nn.Parameter(eta * torch.ones(dim))
         self.gamma2 = nn.Parameter(eta * torch.ones(dim))
 
     def forward(self, x, H: int, W: int):
         x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x)))
         # NOTE official code has 3 then 2, so keeping it the same to be consistent with loaded weights
         # See https://github.com/rwightman/pytorch-image-models/pull/747#issuecomment-877795721
         x = x + self.drop_path(self.gamma3 * self.local_mp(self.norm3(x), H, W))
         x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
         return x
 
 
-class XCiT(nn.Module):
+class Xcit(nn.Module):
     """
     Based on timm and DeiT code bases
     https://github.com/rwightman/pytorch-image-models/tree/master/timm
     https://github.com/facebookresearch/deit/
     """
 
     def __init__(
-            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, global_pool='token', embed_dim=768,
-            depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,
-            act_layer=None, norm_layer=None, cls_attn_layers=2, use_pos_embed=True, eta=1., tokens_norm=False):
+            self,
+            img_size=224,
+            patch_size=16,
+            in_chans=3,
+            num_classes=1000,
+            global_pool='token',
+            embed_dim=768,
+            depth=12,
+            num_heads=12,
+            mlp_ratio=4.,
+            qkv_bias=True,
+            drop_rate=0.,
+            pos_drop_rate=0.,
+            proj_drop_rate=0.,
+            attn_drop_rate=0.,
+            drop_path_rate=0.,
+            act_layer=None,
+            norm_layer=None,
+            cls_attn_layers=2,
+            use_pos_embed=True,
+            eta=1.,
+            tokens_norm=False,
+    ):
         """
         Args:
             img_size (int, tuple): input image size
             patch_size (int): patch size
             in_chans (int): number of input channels
             num_classes (int): number of classes for classification head
             embed_dim (int): embedding dimension
             depth (int): depth of transformer
             num_heads (int): number of attention heads
             mlp_ratio (int): ratio of mlp hidden dim to embedding dim
             qkv_bias (bool): enable bias for qkv if True
             drop_rate (float): dropout rate after positional embedding, and in XCA/CA projection + MLP
+            pos_drop_rate: position embedding dropout rate
+            proj_drop_rate (float): projection dropout rate
             attn_drop_rate (float): attention dropout rate
             drop_path_rate (float): stochastic depth rate (constant across all layers)
             norm_layer: (nn.Module): normalization layer
             cls_attn_layers: (int) Depth of Class attention layers
             use_pos_embed: (bool) whether to use positional encoding
             eta: (float) layerscale initialization value
             tokens_norm: (bool) Whether to normalize all tokens or just the cls_token in the CA
@@ -368,36 +337,61 @@
 
         self.num_classes = num_classes
         self.num_features = self.embed_dim = embed_dim
         self.global_pool = global_pool
         self.grad_checkpointing = False
 
         self.patch_embed = ConvPatchEmbed(
-            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, act_layer=act_layer)
+            img_size=img_size,
+            patch_size=patch_size,
+            in_chans=in_chans,
+            embed_dim=embed_dim,
+            act_layer=act_layer,
+        )
 
         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
-        self.use_pos_embed = use_pos_embed
         if use_pos_embed:
             self.pos_embed = PositionalEncodingFourier(dim=embed_dim)
-        self.pos_drop = nn.Dropout(p=drop_rate)
+        else:
+            self.pos_embed = None
+        self.pos_drop = nn.Dropout(p=pos_drop_rate)
 
         self.blocks = nn.ModuleList([
             XCABlock(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,
-                attn_drop=attn_drop_rate, drop_path=drop_path_rate, act_layer=act_layer, norm_layer=norm_layer, eta=eta)
+                dim=embed_dim,
+                num_heads=num_heads,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=proj_drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=drop_path_rate,
+                act_layer=act_layer,
+                norm_layer=norm_layer,
+                eta=eta,
+            )
             for _ in range(depth)])
 
         self.cls_attn_blocks = nn.ModuleList([
             ClassAttentionBlock(
-                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,
-                attn_drop=attn_drop_rate, act_layer=act_layer, norm_layer=norm_layer, eta=eta, tokens_norm=tokens_norm)
+                dim=embed_dim,
+                num_heads=num_heads,
+                mlp_ratio=mlp_ratio,
+                qkv_bias=qkv_bias,
+                proj_drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                act_layer=act_layer,
+                norm_layer=norm_layer,
+                eta=eta,
+                tokens_norm=tokens_norm,
+            )
             for _ in range(cls_attn_layers)])
 
         # Classifier head
         self.norm = norm_layer(embed_dim)
+        self.head_drop = nn.Dropout(drop_rate)
         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
         # Init weights
         trunc_normal_(self.cls_token, std=.02)
         self.apply(self._init_weights)
 
     def _init_weights(self, m):
@@ -434,15 +428,15 @@
         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
     def forward_features(self, x):
         B = x.shape[0]
         # x is (B, N, C). (Hp, Hw) is (height in units of patches, width in units of patches)
         x, (Hp, Wp) = self.patch_embed(x)
 
-        if self.use_pos_embed:
+        if self.pos_embed is not None:
             # `pos_embed` (B, C, Hp, Wp), reshape -> (B, C, N), permute -> (B, N, C)
             pos_encoding = self.pos_embed(B, Hp, Wp).reshape(B, -1, x.shape[1]).permute(0, 2, 1)
             x = x + pos_encoding
         x = self.pos_drop(x)
 
         for blk in self.blocks:
             if self.grad_checkpointing and not torch.jit.is_scripting():
@@ -460,14 +454,15 @@
 
         x = self.norm(x)
         return x
 
     def forward_head(self, x, pre_logits: bool = False):
         if self.global_pool:
             x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
+        x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
     def forward(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
@@ -499,346 +494,417 @@
                 for j, subscript in enumerate('qkv'):
                     state_dict[f'cls_attn_blocks.{i}.attn.{subscript}.bias'] = qkv_bias[j]
     return state_dict
 
 
 def _create_xcit(variant, pretrained=False, default_cfg=None, **kwargs):
     model = build_model_with_cfg(
-        XCiT, variant, pretrained, pretrained_filter_fn=checkpoint_filter_fn, **kwargs)
-    return model
-
-
-@register_model
-def xcit_nano_12_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, **kwargs)
-    model = _create_xcit('xcit_nano_12_p16_224', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_nano_12_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, **kwargs)
-    model = _create_xcit('xcit_nano_12_p16_224_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_nano_12_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, img_size=384, **kwargs)
-    model = _create_xcit('xcit_nano_12_p16_384_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_tiny_12_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_12_p16_224', pretrained=pretrained, **model_kwargs)
+        Xcit,
+        variant,
+        pretrained,
+        pretrained_filter_fn=checkpoint_filter_fn,
+        **kwargs,
+    )
     return model
 
 
-@register_model
-def xcit_tiny_12_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_12_p16_224_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
+def _cfg(url='', **kwargs):
+    return {
+        'url': url,
+        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
+        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,
+        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
+        'first_conv': 'patch_embed.proj.0.0', 'classifier': 'head',
+        **kwargs
+    }
 
-@register_model
-def xcit_tiny_12_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_12_p16_384_dist', pretrained=pretrained, **model_kwargs)
-    return model
 
+default_cfgs = generate_default_cfgs({
+    # Patch size 16
+    'xcit_nano_12_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_224.pth'),
+    'xcit_nano_12_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_224_dist.pth'),
+    'xcit_nano_12_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_tiny_12_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224.pth'),
+    'xcit_tiny_12_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224_dist.pth'),
+    'xcit_tiny_12_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_tiny_24_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224.pth'),
+    'xcit_tiny_24_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224_dist.pth'),
+    'xcit_tiny_24_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_small_12_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_224.pth'),
+    'xcit_small_12_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_224_dist.pth'),
+    'xcit_small_12_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_small_24_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_224.pth'),
+    'xcit_small_24_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_224_dist.pth'),
+    'xcit_small_24_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_medium_24_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_224.pth'),
+    'xcit_medium_24_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_224_dist.pth'),
+    'xcit_medium_24_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_large_24_p16_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_224.pth'),
+    'xcit_large_24_p16_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_224_dist.pth'),
+    'xcit_large_24_p16_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_384_dist.pth', input_size=(3, 384, 384)),
 
-@register_model
-def xcit_small_12_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_12_p16_224', pretrained=pretrained, **model_kwargs)
-    return model
+    # Patch size 8
+    'xcit_nano_12_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_224.pth'),
+    'xcit_nano_12_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_224_dist.pth'),
+    'xcit_nano_12_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_tiny_12_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224.pth'),
+    'xcit_tiny_12_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224_dist.pth'),
+    'xcit_tiny_12_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_tiny_24_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224.pth'),
+    'xcit_tiny_24_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224_dist.pth'),
+    'xcit_tiny_24_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_small_12_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_224.pth'),
+    'xcit_small_12_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_224_dist.pth'),
+    'xcit_small_12_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_small_24_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_224.pth'),
+    'xcit_small_24_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_224_dist.pth'),
+    'xcit_small_24_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_medium_24_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_224.pth'),
+    'xcit_medium_24_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_224_dist.pth'),
+    'xcit_medium_24_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_384_dist.pth', input_size=(3, 384, 384)),
+    'xcit_large_24_p8_224.fb_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_224.pth'),
+    'xcit_large_24_p8_224.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_224_dist.pth'),
+    'xcit_large_24_p8_384.fb_dist_in1k': _cfg(
+        hf_hub_id='timm/',
+        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_384_dist.pth', input_size=(3, 384, 384)),
+})
 
 
 @register_model
-def xcit_small_12_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_12_p16_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_nano_12_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False)
+    model = _create_xcit('xcit_nano_12_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_12_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_12_p16_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_nano_12_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, img_size=384)
+    model = _create_xcit('xcit_nano_12_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_24_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_24_p16_224', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_12_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_12_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_24_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_24_p16_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_12_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_12_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_24_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_24_p16_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_small_12_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_small_12_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_24_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_24_p16_224', pretrained=pretrained, **model_kwargs)
+def xcit_small_12_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_small_12_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_24_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_24_p16_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_24_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_24_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_24_p16_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_24_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_medium_24_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_medium_24_p16_224', pretrained=pretrained, **model_kwargs)
+def xcit_small_24_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_small_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_medium_24_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_medium_24_p16_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_small_24_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_small_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_medium_24_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_medium_24_p16_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_medium_24_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_medium_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_large_24_p16_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_large_24_p16_224', pretrained=pretrained, **model_kwargs)
+def xcit_medium_24_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_medium_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_large_24_p16_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_large_24_p16_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_large_24_p16_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_large_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_large_24_p16_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_large_24_p16_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_large_24_p16_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_large_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 # Patch size 8x8 models
 @register_model
-def xcit_nano_12_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, **kwargs)
-    model = _create_xcit('xcit_nano_12_p8_224', pretrained=pretrained, **model_kwargs)
+def xcit_nano_12_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False)
+    model = _create_xcit('xcit_nano_12_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_nano_12_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, **kwargs)
-    model = _create_xcit('xcit_nano_12_p8_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_nano_12_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False)
+    model = _create_xcit('xcit_nano_12_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_nano_12_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, **kwargs)
-    model = _create_xcit('xcit_nano_12_p8_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_12_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_12_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_12_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_12_p8_224', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_12_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_12_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_12_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_12_p8_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_small_12_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_small_12_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_12_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_12_p8_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_small_12_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)
+    model = _create_xcit('xcit_small_12_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_12_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_12_p8_224', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_24_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_12_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_12_p8_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_tiny_24_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_tiny_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_12_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_12_p8_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_small_24_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_small_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_24_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_24_p8_224', pretrained=pretrained, **model_kwargs)
+def xcit_small_24_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_small_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_24_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_24_p8_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_medium_24_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_medium_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_tiny_24_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_tiny_24_p8_384_dist', pretrained=pretrained, **model_kwargs)
+def xcit_medium_24_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_medium_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_24_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_24_p8_224', pretrained=pretrained, **model_kwargs)
+def xcit_large_24_p8_224(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_large_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
 @register_model
-def xcit_small_24_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_24_p8_224_dist', pretrained=pretrained, **model_kwargs)
+def xcit_large_24_p8_384(pretrained=False, **kwargs) -> Xcit:
+    model_args = dict(
+        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)
+    model = _create_xcit('xcit_large_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))
     return model
 
 
-@register_model
-def xcit_small_24_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_small_24_p8_384_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_medium_24_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_medium_24_p8_224', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_medium_24_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_medium_24_p8_224_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_medium_24_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_medium_24_p8_384_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_large_24_p8_224(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_large_24_p8_224', pretrained=pretrained, **model_kwargs)
-    return model
-
-
-@register_model
-def xcit_large_24_p8_224_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_large_24_p8_224_dist', pretrained=pretrained, **model_kwargs)
-    return model
-
+register_model_deprecations(__name__, {
+    # Patch size 16
+    'xcit_nano_12_p16_224_dist': 'xcit_nano_12_p16_224.fb_dist_in1k',
+    'xcit_nano_12_p16_384_dist': 'xcit_nano_12_p16_384.fb_dist_in1k',
+    'xcit_tiny_12_p16_224_dist': 'xcit_tiny_12_p16_224.fb_dist_in1k',
+    'xcit_tiny_12_p16_384_dist': 'xcit_tiny_12_p16_384.fb_dist_in1k',
+    'xcit_tiny_24_p16_224_dist': 'xcit_tiny_24_p16_224.fb_dist_in1k',
+    'xcit_tiny_24_p16_384_dist': 'xcit_tiny_24_p16_384.fb_dist_in1k',
+    'xcit_small_12_p16_224_dist': 'xcit_small_12_p16_224.fb_dist_in1k',
+    'xcit_small_12_p16_384_dist': 'xcit_small_12_p16_384.fb_dist_in1k',
+    'xcit_small_24_p16_224_dist': 'xcit_small_24_p16_224.fb_dist_in1k',
+    'xcit_medium_24_p16_224_dist': 'xcit_medium_24_p16_224.fb_dist_in1k',
+    'xcit_medium_24_p16_384_dist': 'xcit_medium_24_p16_384.fb_dist_in1k',
+    'xcit_large_24_p16_224_dist': 'xcit_large_24_p16_224.fb_dist_in1k',
+    'xcit_large_24_p16_384_dist': 'xcit_large_24_p16_384.fb_dist_in1k',
 
-@register_model
-def xcit_large_24_p8_384_dist(pretrained=False, **kwargs):
-    model_kwargs = dict(
-        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True, **kwargs)
-    model = _create_xcit('xcit_large_24_p8_384_dist', pretrained=pretrained, **model_kwargs)
-    return model
+    # Patch size 8
+    'xcit_nano_12_p8_224_dist': 'xcit_nano_12_p8_224.fb_dist_in1k',
+    'xcit_nano_12_p8_384_dist': 'xcit_nano_12_p8_384.fb_dist_in1k',
+    'xcit_tiny_12_p8_224_dist': 'xcit_tiny_12_p8_224.fb_dist_in1k',
+    'xcit_tiny_12_p8_384_dist': 'xcit_tiny_12_p8_384.fb_dist_in1k',
+    'xcit_tiny_24_p8_224_dist': 'xcit_tiny_24_p8_224.fb_dist_in1k',
+    'xcit_tiny_24_p8_384_dist': 'xcit_tiny_24_p8_384.fb_dist_in1k',
+    'xcit_small_12_p8_224_dist': 'xcit_small_12_p8_224.fb_dist_in1k',
+    'xcit_small_12_p8_384_dist': 'xcit_small_12_p8_384.fb_dist_in1k',
+    'xcit_small_24_p8_224_dist': 'xcit_small_24_p8_224.fb_dist_in1k',
+    'xcit_small_24_p8_384_dist': 'xcit_small_24_p8_384.fb_dist_in1k',
+    'xcit_medium_24_p8_224_dist': 'xcit_medium_24_p8_224.fb_dist_in1k',
+    'xcit_medium_24_p8_384_dist': 'xcit_medium_24_p8_384.fb_dist_in1k',
+    'xcit_large_24_p8_224_dist': 'xcit_large_24_p8_224.fb_dist_in1k',
+    'xcit_large_24_p8_384_dist': 'xcit_large_24_p8_384.fb_dist_in1k',
+})
```

### Comparing `timm-0.8.6.dev0/timm/optim/adabelief.py` & `timm-0.9.0/timm/optim/adabelief.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/adafactor.py` & `timm-0.9.0/timm/optim/adafactor.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/adahessian.py` & `timm-0.9.0/timm/optim/adahessian.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/adamp.py` & `timm-0.9.0/timm/optim/adamp.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/adamw.py` & `timm-0.9.0/timm/optim/adamw.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/adan.py` & `timm-0.9.0/timm/optim/adan.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/lamb.py` & `timm-0.9.0/timm/optim/lamb.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/lars.py` & `timm-0.9.0/timm/optim/lars.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/lookahead.py` & `timm-0.9.0/timm/optim/lookahead.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/madgrad.py` & `timm-0.9.0/timm/optim/madgrad.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/nadam.py` & `timm-0.9.0/timm/optim/nadam.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/nvnovograd.py` & `timm-0.9.0/timm/optim/nvnovograd.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/optim_factory.py` & `timm-0.9.0/timm/optim/optim_factory.py`

 * *Files 19% similar despite different names*

```diff
@@ -14,31 +14,33 @@
 from .adabelief import AdaBelief
 from .adafactor import Adafactor
 from .adahessian import Adahessian
 from .adamp import AdamP
 from .adan import Adan
 from .lamb import Lamb
 from .lars import Lars
+from .lion import Lion
 from .lookahead import Lookahead
 from .madgrad import MADGRAD
 from .nadam import Nadam
 from .nvnovograd import NvNovoGrad
 from .radam import RAdam
 from .rmsprop_tf import RMSpropTF
 from .sgdp import SGDP
 
-try:
-    from apex.optimizers import FusedNovoGrad, FusedAdam, FusedLAMB, FusedSGD
-    has_apex = True
-except ImportError:
-    has_apex = False
 
 _logger = logging.getLogger(__name__)
 
 
+# optimizers to default to multi-tensor
+_DEFAULT_FOREACH = {
+    'lion',
+}
+
+
 def param_groups_weight_decay(
         model: nn.Module,
         weight_decay=1e-5,
         no_weight_decay_list=()
 ):
     no_weight_decay_list = set(no_weight_decay_list)
     decay = []
@@ -157,23 +159,26 @@
     """ cfg/argparse to kwargs helper
     Convert optimizer args in argparse args or cfg like object to keyword args for updated create fn.
     """
     kwargs = dict(
         opt=cfg.opt,
         lr=cfg.lr,
         weight_decay=cfg.weight_decay,
-        momentum=cfg.momentum)
+        momentum=cfg.momentum,
+    )
     if getattr(cfg, 'opt_eps', None) is not None:
         kwargs['eps'] = cfg.opt_eps
     if getattr(cfg, 'opt_betas', None) is not None:
         kwargs['betas'] = cfg.opt_betas
     if getattr(cfg, 'layer_decay', None) is not None:
         kwargs['layer_decay'] = cfg.layer_decay
     if getattr(cfg, 'opt_args', None) is not None:
         kwargs.update(cfg.opt_args)
+    if getattr(cfg, 'opt_foreach', None) is not None:
+        kwargs['foreach'] = cfg.opt_foreach
     return kwargs
 
 
 def create_optimizer(args, model, filter_bias_and_bn=True):
     """ Legacy optimizer factory for backwards compatibility.
     NOTE: Use create_optimizer_v2 for new code.
     """
@@ -186,14 +191,15 @@
 
 def create_optimizer_v2(
         model_or_params,
         opt: str = 'sgd',
         lr: Optional[float] = None,
         weight_decay: float = 0.,
         momentum: float = 0.9,
+        foreach: Optional[bool] = None,
         filter_bias_and_bn: bool = True,
         layer_decay: Optional[float] = None,
         param_group_fn: Optional[Callable] = None,
         **kwargs,
 ):
     """ Create an optimizer.
 
@@ -204,14 +210,15 @@
 
     Args:
         model_or_params (nn.Module): model containing parameters to optimize
         opt: name of optimizer to create
         lr: initial learning rate
         weight_decay: weight decay to apply in optimizer
         momentum:  momentum for momentum based optimizers (others may use betas via kwargs)
+        foreach: Enable / disable foreach (multi-tensor) operation if True / False. Choose safe default if None
         filter_bias_and_bn:  filter out bias, bn and other 1d params from weight decay
         **kwargs: extra optimizer specific kwargs to pass through
 
     Returns:
         Optimizer
     """
     if isinstance(model_or_params, nn.Module):
@@ -223,35 +230,57 @@
         if param_group_fn:
             parameters = param_group_fn(model_or_params)
         elif layer_decay is not None:
             parameters = param_groups_layer_decay(
                 model_or_params,
                 weight_decay=weight_decay,
                 layer_decay=layer_decay,
-                no_weight_decay_list=no_weight_decay)
+                no_weight_decay_list=no_weight_decay,
+            )
             weight_decay = 0.
         elif weight_decay and filter_bias_and_bn:
             parameters = param_groups_weight_decay(model_or_params, weight_decay, no_weight_decay)
             weight_decay = 0.
         else:
             parameters = model_or_params.parameters()
     else:
         # iterable of parameters or param groups passed in
         parameters = model_or_params
 
     opt_lower = opt.lower()
     opt_split = opt_lower.split('_')
     opt_lower = opt_split[-1]
-    if 'fused' in opt_lower:
+
+    if opt_lower.startswith('fused'):
+        try:
+            from apex.optimizers import FusedNovoGrad, FusedAdam, FusedLAMB, FusedSGD
+            has_apex = True
+        except ImportError:
+            has_apex = False
         assert has_apex and torch.cuda.is_available(), 'APEX and CUDA required for fused optimizers'
 
+    if opt_lower.startswith('bnb'):
+        try:
+            import bitsandbytes as bnb
+            has_bnb = True
+        except ImportError:
+            has_bnb = False
+        assert has_bnb and torch.cuda.is_available(), 'bitsandbytes and CUDA required for bnb optimizers'
+
     opt_args = dict(weight_decay=weight_decay, **kwargs)
+
     if lr is not None:
         opt_args.setdefault('lr', lr)
 
+    if foreach is None:
+        if opt in _DEFAULT_FOREACH:
+            opt_args.setdefault('foreach', True)
+    else:
+        opt_args['foreach'] = foreach
+
     # basic SGD & related
     if opt_lower == 'sgd' or opt_lower == 'nesterov':
         # NOTE 'sgd' refers to SGD + nesterov momentum for legacy / backwards compat reasons
         opt_args.pop('eps', None)
         optimizer = optim.SGD(parameters, momentum=momentum, nesterov=True, **opt_args)
     elif opt_lower == 'momentum':
         opt_args.pop('eps', None)
@@ -309,14 +338,16 @@
         optimizer = MADGRAD(parameters, momentum=momentum, decoupled_decay=True, **opt_args)
     elif opt_lower == 'novograd' or opt_lower == 'nvnovograd':
         optimizer = NvNovoGrad(parameters, **opt_args)
     elif opt_lower == 'rmsprop':
         optimizer = optim.RMSprop(parameters, alpha=0.9, momentum=momentum, **opt_args)
     elif opt_lower == 'rmsproptf':
         optimizer = RMSpropTF(parameters, alpha=0.9, momentum=momentum, **opt_args)
+    elif opt_lower == 'lion':
+        optimizer = Lion(parameters, **opt_args)
 
     # second order
     elif opt_lower == 'adahessian':
         optimizer = Adahessian(parameters, **opt_args)
 
     # NVIDIA fused optimizers, require APEX to be installed
     elif opt_lower == 'fusedsgd':
@@ -331,14 +362,48 @@
         optimizer = FusedAdam(parameters, adam_w_mode=True, **opt_args)
     elif opt_lower == 'fusedlamb':
         optimizer = FusedLAMB(parameters, **opt_args)
     elif opt_lower == 'fusednovograd':
         opt_args.setdefault('betas', (0.95, 0.98))
         optimizer = FusedNovoGrad(parameters, **opt_args)
 
+    # bitsandbytes optimizers, require bitsandbytes to be installed
+    elif opt_lower == 'bnbsgd':
+        opt_args.pop('eps', None)
+        optimizer = bnb.optim.SGD(parameters, momentum=momentum, nesterov=True, **opt_args)
+    elif opt_lower == 'bnbsgd8bit':
+        opt_args.pop('eps', None)
+        optimizer = bnb.optim.SGD8bit(parameters, momentum=momentum, nesterov=True, **opt_args)
+    elif opt_lower == 'bnbmomentum':
+        opt_args.pop('eps', None)
+        optimizer = bnb.optim.SGD(parameters, momentum=momentum, **opt_args)
+    elif opt_lower == 'bnbmomentum8bit':
+        opt_args.pop('eps', None)
+        optimizer = bnb.optim.SGD8bit(parameters, momentum=momentum, **opt_args)
+    elif opt_lower == 'bnbadam':
+        optimizer = bnb.optim.Adam(parameters, **opt_args)
+    elif opt_lower == 'bnbadam8bit':
+        optimizer = bnb.optim.Adam8bit(parameters, **opt_args)
+    elif opt_lower == 'bnbadamw':
+        optimizer = bnb.optim.AdamW(parameters, **opt_args)
+    elif opt_lower == 'bnbadamw8bit':
+        optimizer = bnb.optim.AdamW8bit(parameters, **opt_args)
+    elif opt_lower == 'bnblamb':
+        optimizer = bnb.optim.LAMB(parameters, **opt_args)
+    elif opt_lower == 'bnblamb8bit':
+        optimizer = bnb.optim.LAMB8bit(parameters, **opt_args)
+    elif opt_lower == 'bnblars':
+        optimizer = bnb.optim.LARS(parameters, **opt_args)
+    elif opt_lower == 'bnblarsb8bit':
+        optimizer = bnb.optim.LAMB8bit(parameters, **opt_args)
+    elif opt_lower == 'bnblion':
+        optimizer = bnb.optim.Lion(parameters, **opt_args)
+    elif opt_lower == 'bnblion8bit':
+        optimizer = bnb.optim.Lion8bit(parameters, **opt_args)
+
     else:
         assert False and "Invalid optimizer"
         raise ValueError
 
     if len(opt_split) > 1:
         if opt_split[0] == 'lookahead':
             optimizer = Lookahead(optimizer)
```

### Comparing `timm-0.8.6.dev0/timm/optim/radam.py` & `timm-0.9.0/timm/optim/radam.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/rmsprop_tf.py` & `timm-0.9.0/timm/optim/rmsprop_tf.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/optim/sgdp.py` & `timm-0.9.0/timm/optim/sgdp.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/cosine_lr.py` & `timm-0.9.0/timm/scheduler/cosine_lr.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/multistep_lr.py` & `timm-0.9.0/timm/scheduler/multistep_lr.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/plateau_lr.py` & `timm-0.9.0/timm/scheduler/plateau_lr.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/poly_lr.py` & `timm-0.9.0/timm/scheduler/poly_lr.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/scheduler.py` & `timm-0.9.0/timm/scheduler/scheduler.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/scheduler_factory.py` & `timm-0.9.0/timm/scheduler/scheduler_factory.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/step_lr.py` & `timm-0.9.0/timm/scheduler/step_lr.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/scheduler/tanh_lr.py` & `timm-0.9.0/timm/scheduler/tanh_lr.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/__init__.py` & `timm-0.9.0/timm/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/agc.py` & `timm-0.9.0/timm/utils/agc.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/checkpoint_saver.py` & `timm-0.9.0/timm/utils/checkpoint_saver.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/clip_grad.py` & `timm-0.9.0/timm/utils/clip_grad.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/cuda.py` & `timm-0.9.0/timm/utils/cuda.py`

 * *Files 20% similar despite different names*

```diff
@@ -13,20 +13,30 @@
 
 from .clip_grad import dispatch_clip_grad
 
 
 class ApexScaler:
     state_dict_key = "amp"
 
-    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None, create_graph=False):
+    def __call__(
+            self,
+            loss,
+            optimizer,
+            clip_grad=None,
+            clip_mode='norm',
+            parameters=None,
+            create_graph=False,
+            need_update=True,
+    ):
         with amp.scale_loss(loss, optimizer) as scaled_loss:
             scaled_loss.backward(create_graph=create_graph)
-        if clip_grad is not None:
-            dispatch_clip_grad(amp.master_params(optimizer), clip_grad, mode=clip_mode)
-        optimizer.step()
+        if need_update:
+            if clip_grad is not None:
+                dispatch_clip_grad(amp.master_params(optimizer), clip_grad, mode=clip_mode)
+            optimizer.step()
 
     def state_dict(self):
         if 'state_dict' in amp.__dict__:
             return amp.state_dict()
 
     def load_state_dict(self, state_dict):
         if 'load_state_dict' in amp.__dict__:
@@ -35,21 +45,31 @@
 
 class NativeScaler:
     state_dict_key = "amp_scaler"
 
     def __init__(self):
         self._scaler = torch.cuda.amp.GradScaler()
 
-    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None, create_graph=False):
+    def __call__(
+            self,
+            loss,
+            optimizer,
+            clip_grad=None,
+            clip_mode='norm',
+            parameters=None,
+            create_graph=False,
+            need_update=True,
+    ):
         self._scaler.scale(loss).backward(create_graph=create_graph)
-        if clip_grad is not None:
-            assert parameters is not None
-            self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
-            dispatch_clip_grad(parameters, clip_grad, mode=clip_mode)
-        self._scaler.step(optimizer)
-        self._scaler.update()
+        if need_update:
+            if clip_grad is not None:
+                assert parameters is not None
+                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
+                dispatch_clip_grad(parameters, clip_grad, mode=clip_mode)
+            self._scaler.step(optimizer)
+            self._scaler.update()
 
     def state_dict(self):
         return self._scaler.state_dict()
 
     def load_state_dict(self, state_dict):
         self._scaler.load_state_dict(state_dict)
```

### Comparing `timm-0.8.6.dev0/timm/utils/decay_batch.py` & `timm-0.9.0/timm/utils/decay_batch.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/distributed.py` & `timm-0.9.0/timm/utils/distributed.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/jit.py` & `timm-0.9.0/timm/utils/jit.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/log.py` & `timm-0.9.0/timm/utils/log.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/metrics.py` & `timm-0.9.0/timm/utils/metrics.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/misc.py` & `timm-0.9.0/timm/utils/misc.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/model.py` & `timm-0.9.0/timm/utils/model.py`

 * *Files 22% similar despite different names*

```diff
@@ -3,14 +3,16 @@
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import fnmatch
 
 import torch
 from torchvision.ops.misc import FrozenBatchNorm2d
 
+from timm.layers import BatchNormAct2d, SyncBatchNormAct, FrozenBatchNormAct2d,\
+    freeze_batch_norm_2d, unfreeze_batch_norm_2d
 from .model_ema import ModelEma
 
 
 def unwrap_model(model):
     if isinstance(model, ModelEma):
         return unwrap_model(model.ema)
     else:
@@ -96,78 +98,14 @@
     """
     x = torch.normal(0., 1., input_shape)
     hook = ActivationStatsHook(model, hook_fn_locs=hook_fn_locs, hook_fns=hook_fns)
     _ = model(x)
     return hook.stats
 
 
-def freeze_batch_norm_2d(module):
-    """
-    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is
-    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and
-    returned. Otherwise, the module is walked recursively and submodules are converted in place.
-
-    Args:
-        module (torch.nn.Module): Any PyTorch module.
-
-    Returns:
-        torch.nn.Module: Resulting module
-
-    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
-    """
-    res = module
-    if isinstance(module, (torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.SyncBatchNorm)):
-        res = FrozenBatchNorm2d(module.num_features)
-        res.num_features = module.num_features
-        res.affine = module.affine
-        if module.affine:
-            res.weight.data = module.weight.data.clone().detach()
-            res.bias.data = module.bias.data.clone().detach()
-        res.running_mean.data = module.running_mean.data
-        res.running_var.data = module.running_var.data
-        res.eps = module.eps
-    else:
-        for name, child in module.named_children():
-            new_child = freeze_batch_norm_2d(child)
-            if new_child is not child:
-                res.add_module(name, new_child)
-    return res
-
-
-def unfreeze_batch_norm_2d(module):
-    """
-    Converts all `FrozenBatchNorm2d` layers of provided module into `BatchNorm2d`. If `module` is itself and instance
-    of `FrozenBatchNorm2d`, it is converted into `BatchNorm2d` and returned. Otherwise, the module is walked
-    recursively and submodules are converted in place.
-
-    Args:
-        module (torch.nn.Module): Any PyTorch module.
-
-    Returns:
-        torch.nn.Module: Resulting module
-
-    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
-    """
-    res = module
-    if isinstance(module, FrozenBatchNorm2d):
-        res = torch.nn.BatchNorm2d(module.num_features)
-        if module.affine:
-            res.weight.data = module.weight.data.clone().detach()
-            res.bias.data = module.bias.data.clone().detach()
-        res.running_mean.data = module.running_mean.data
-        res.running_var.data = module.running_var.data
-        res.eps = module.eps
-    else:
-        for name, child in module.named_children():
-            new_child = unfreeze_batch_norm_2d(child)
-            if new_child is not child:
-                res.add_module(name, new_child)
-    return res
-
-
 def _freeze_unfreeze(root_module, submodules=[], include_bn_running_stats=True, mode='freeze'):
     """
     Freeze or unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is
     done in place.
     Args:
         root_module (nn.Module, optional): Root module relative to which the `submodules` are referenced.
         submodules (list[str]): List of modules for which the parameters will be (un)frozen. They are to be provided as
@@ -175,15 +113,20 @@
             means that the whole root module will be (un)frozen. Defaults to []
         include_bn_running_stats (bool): Whether to also (un)freeze the running statistics of batch norm 2d layers.
             Defaults to `True`.
         mode (bool): Whether to freeze ("freeze") or unfreeze ("unfreeze"). Defaults to `"freeze"`.
     """
     assert mode in ["freeze", "unfreeze"], '`mode` must be one of "freeze" or "unfreeze"'
 
-    if isinstance(root_module, (torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.SyncBatchNorm)):
+    if isinstance(root_module, (
+            torch.nn.modules.batchnorm.BatchNorm2d,
+            torch.nn.modules.batchnorm.SyncBatchNorm,
+            BatchNormAct2d,
+            SyncBatchNormAct,
+    )):
         # Raise assertion here because we can't convert it in place
         raise AssertionError(
             "You have provided a batch norm layer as the `root module`. Please use "
             "`timm.utils.model.freeze_batch_norm_2d` or `timm.utils.model.unfreeze_batch_norm_2d` instead.")
 
     if isinstance(submodules, str):
         submodules = [submodules]
@@ -209,21 +152,26 @@
 
             # Freeze batch norm
             if mode == 'freeze':
                 res = freeze_batch_norm_2d(m)
                 # It's possible that `m` is a type of BatchNorm in itself, in which case `unfreeze_batch_norm_2d` won't
                 # convert it in place, but will return the converted result. In this case `res` holds the converted
                 # result and we may try to re-assign the named module
-                if isinstance(m, (torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.SyncBatchNorm)):
+                if isinstance(m, (
+                        torch.nn.modules.batchnorm.BatchNorm2d,
+                        torch.nn.modules.batchnorm.SyncBatchNorm,
+                        BatchNormAct2d,
+                        SyncBatchNormAct,
+                )):
                     _add_submodule(root_module, n, res)
             # Unfreeze batch norm
             else:
                 res = unfreeze_batch_norm_2d(m)
                 # Ditto. See note above in mode == 'freeze' branch
-                if isinstance(m, FrozenBatchNorm2d):
+                if isinstance(m, (FrozenBatchNorm2d, FrozenBatchNormAct2d)):
                     _add_submodule(root_module, n, res)
 
 
 def freeze(root_module, submodules=[], include_bn_running_stats=True):
     """
     Freeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.
     Args:
```

### Comparing `timm-0.8.6.dev0/timm/utils/model_ema.py` & `timm-0.9.0/timm/utils/model_ema.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm/utils/summary.py` & `timm-0.9.0/timm/utils/summary.py`

 * *Files identical despite different names*

### Comparing `timm-0.8.6.dev0/timm.egg-info/SOURCES.txt` & `timm-0.9.0/timm.egg-info/SOURCES.txt`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 LICENSE
 MANIFEST.in
 README.md
+pyproject.toml
 setup.cfg
 setup.py
 timm/__init__.py
 timm/version.py
 timm.egg-info/PKG-INFO
 timm.egg-info/SOURCES.txt
 timm.egg-info/dependency_links.txt
@@ -12,22 +13,43 @@
 timm.egg-info/top_level.txt
 timm/data/__init__.py
 timm/data/auto_augment.py
 timm/data/config.py
 timm/data/constants.py
 timm/data/dataset.py
 timm/data/dataset_factory.py
+timm/data/dataset_info.py
 timm/data/distributed_sampler.py
+timm/data/imagenet_info.py
 timm/data/loader.py
 timm/data/mixup.py
 timm/data/random_erasing.py
 timm/data/real_labels.py
 timm/data/tf_preprocessing.py
 timm/data/transforms.py
 timm/data/transforms_factory.py
+timm/data/_info/imagenet12k_synsets.txt
+timm/data/_info/imagenet21k_goog_synsets.txt
+timm/data/_info/imagenet21k_goog_to_12k_indices.txt
+timm/data/_info/imagenet21k_goog_to_22k_indices.txt
+timm/data/_info/imagenet21k_miil_synsets.txt
+timm/data/_info/imagenet21k_miil_w21_synsets.txt
+timm/data/_info/imagenet22k_ms_synsets.txt
+timm/data/_info/imagenet22k_ms_to_12k_indices.txt
+timm/data/_info/imagenet22k_ms_to_22k_indices.txt
+timm/data/_info/imagenet22k_synsets.txt
+timm/data/_info/imagenet22k_to_12k_indices.txt
+timm/data/_info/imagenet_a_indices.txt
+timm/data/_info/imagenet_a_synsets.txt
+timm/data/_info/imagenet_r_indices.txt
+timm/data/_info/imagenet_r_synsets.txt
+timm/data/_info/imagenet_real_labels.json
+timm/data/_info/imagenet_synset_to_definition.txt
+timm/data/_info/imagenet_synset_to_lemma.txt
+timm/data/_info/imagenet_synsets.txt
 timm/data/readers/__init__.py
 timm/data/readers/class_map.py
 timm/data/readers/img_extensions.py
 timm/data/readers/reader.py
 timm/data/readers/reader_factory.py
 timm/data/readers/reader_hfds.py
 timm/data/readers/reader_image_folder.py
@@ -56,14 +78,15 @@
 timm/layers/create_norm.py
 timm/layers/create_norm_act.py
 timm/layers/drop.py
 timm/layers/eca.py
 timm/layers/evo_norm.py
 timm/layers/fast_norm.py
 timm/layers/filter_response_norm.py
+timm/layers/format.py
 timm/layers/gather_excite.py
 timm/layers/global_context.py
 timm/layers/grn.py
 timm/layers/halo_attn.py
 timm/layers/helpers.py
 timm/layers/inplace_abn.py
 timm/layers/lambda_layer.py
@@ -72,14 +95,15 @@
 timm/layers/mixed_conv2d.py
 timm/layers/ml_decoder.py
 timm/layers/mlp.py
 timm/layers/non_local_attn.py
 timm/layers/norm.py
 timm/layers/norm_act.py
 timm/layers/padding.py
+timm/layers/patch_dropout.py
 timm/layers/patch_embed.py
 timm/layers/pool2d_same.py
 timm/layers/pos_embed.py
 timm/layers/pos_embed_rel.py
 timm/layers/pos_embed_sincos.py
 timm/layers/selective_kernel.py
 timm/layers/separable_conv.py
@@ -115,47 +139,49 @@
 timm/models/cait.py
 timm/models/coat.py
 timm/models/convit.py
 timm/models/convmixer.py
 timm/models/convnext.py
 timm/models/crossvit.py
 timm/models/cspnet.py
+timm/models/davit.py
 timm/models/deit.py
 timm/models/densenet.py
 timm/models/dla.py
 timm/models/dpn.py
 timm/models/edgenext.py
 timm/models/efficientformer.py
+timm/models/efficientformer_v2.py
 timm/models/efficientnet.py
+timm/models/eva.py
 timm/models/factory.py
 timm/models/features.py
+timm/models/focalnet.py
 timm/models/fx_features.py
 timm/models/gcvit.py
 timm/models/ghostnet.py
-timm/models/gluon_resnet.py
-timm/models/gluon_xception.py
 timm/models/hardcorenas.py
 timm/models/helpers.py
 timm/models/hrnet.py
 timm/models/hub.py
 timm/models/inception_resnet_v2.py
 timm/models/inception_v3.py
 timm/models/inception_v4.py
 timm/models/levit.py
 timm/models/maxxvit.py
+timm/models/metaformer.py
 timm/models/mlp_mixer.py
 timm/models/mobilenetv3.py
 timm/models/mobilevit.py
 timm/models/mvitv2.py
 timm/models/nasnet.py
 timm/models/nest.py
 timm/models/nfnet.py
 timm/models/pit.py
 timm/models/pnasnet.py
-timm/models/poolformer.py
 timm/models/pvt_v2.py
 timm/models/registry.py
 timm/models/regnet.py
 timm/models/res2net.py
 timm/models/resnest.py
 timm/models/resnet.py
 timm/models/resnetv2.py
@@ -176,24 +202,30 @@
 timm/models/vision_transformer_hybrid.py
 timm/models/vision_transformer_relpos.py
 timm/models/volo.py
 timm/models/vovnet.py
 timm/models/xception.py
 timm/models/xception_aligned.py
 timm/models/xcit.py
+timm/models/_pruned/ecaresnet101d_pruned.txt
+timm/models/_pruned/ecaresnet50d_pruned.txt
+timm/models/_pruned/efficientnet_b1_pruned.txt
+timm/models/_pruned/efficientnet_b2_pruned.txt
+timm/models/_pruned/efficientnet_b3_pruned.txt
 timm/models/layers/__init__.py
 timm/optim/__init__.py
 timm/optim/adabelief.py
 timm/optim/adafactor.py
 timm/optim/adahessian.py
 timm/optim/adamp.py
 timm/optim/adamw.py
 timm/optim/adan.py
 timm/optim/lamb.py
 timm/optim/lars.py
+timm/optim/lion.py
 timm/optim/lookahead.py
 timm/optim/madgrad.py
 timm/optim/nadam.py
 timm/optim/nvnovograd.py
 timm/optim/optim_factory.py
 timm/optim/radam.py
 timm/optim/rmsprop_tf.py
@@ -216,9 +248,10 @@
 timm/utils/distributed.py
 timm/utils/jit.py
 timm/utils/log.py
 timm/utils/metrics.py
 timm/utils/misc.py
 timm/utils/model.py
 timm/utils/model_ema.py
+timm/utils/onnx.py
 timm/utils/random.py
 timm/utils/summary.py
```

