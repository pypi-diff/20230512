# Comparing `tmp/gpt4all-0.2.0-py3-none-win_amd64.whl.zip` & `tmp/gpt4all-0.2.1-py3-none-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,35 @@
-Zip file size: 2560595 bytes, number of entries: 30
--rw-rw-rw-  2.0 fat       76 b- defN 23-May-11 15:03 gpt4all/__init__.py
--rw-rw-rw-  2.0 fat    12007 b- defN 23-May-11 15:03 gpt4all/gpt4all.py
--rw-rw-rw-  2.0 fat     7686 b- defN 23-May-11 15:03 gpt4all/pyllmodel.py
--rw-rw-rw-  2.0 fat     3476 b- defN 23-May-11 15:08 gpt4all/llmodel_DO_NOT_MODIFY/common.h
--rw-rw-rw-  2.0 fat     2378 b- defN 23-May-11 15:08 gpt4all/llmodel_DO_NOT_MODIFY/ggml-cuda.h
--rw-rw-rw-  2.0 fat      663 b- defN 23-May-11 15:08 gpt4all/llmodel_DO_NOT_MODIFY/ggml-opencl.h
--rw-rw-rw-  2.0 fat    30753 b- defN 23-May-11 15:08 gpt4all/llmodel_DO_NOT_MODIFY/ggml.h
--rw-rw-rw-  2.0 fat     1006 b- defN 23-May-11 15:03 gpt4all/llmodel_DO_NOT_MODIFY/gptj.h
--rw-rw-rw-  2.0 fat       10 b- defN 23-May-11 15:08 gpt4all/llmodel_DO_NOT_MODIFY/llama.h
--rw-rw-rw-  2.0 fat    12387 b- defN 23-May-11 15:08 gpt4all/llmodel_DO_NOT_MODIFY/llama_util.h
--rw-rw-rw-  2.0 fat     1042 b- defN 23-May-11 15:03 gpt4all/llmodel_DO_NOT_MODIFY/llamamodel.h
--rw-rw-rw-  2.0 fat     1800 b- defN 23-May-11 15:03 gpt4all/llmodel_DO_NOT_MODIFY/llmodel.h
--rw-rw-rw-  2.0 fat     6149 b- defN 23-May-11 15:03 gpt4all/llmodel_DO_NOT_MODIFY/llmodel_c.h
--rw-rw-rw-  2.0 fat      998 b- defN 23-May-11 15:03 gpt4all/llmodel_DO_NOT_MODIFY/mpt.h
--rw-rw-rw-  2.0 fat     2410 b- defN 23-May-11 15:03 gpt4all/llmodel_DO_NOT_MODIFY/utils.h
+Zip file size: 2562200 bytes, number of entries: 33
+-rw-rw-rw-  2.0 fat       76 b- defN 23-May-12 16:59 gpt4all/__init__.py
+-rw-rw-rw-  2.0 fat    12502 b- defN 23-May-12 16:59 gpt4all/gpt4all.py
+-rw-rw-rw-  2.0 fat     8104 b- defN 23-May-12 16:59 gpt4all/pyllmodel.py
+-rw-rw-rw-  2.0 fat     3476 b- defN 23-May-12 17:03 gpt4all/llmodel_DO_NOT_MODIFY/common.h
+-rw-rw-rw-  2.0 fat     2378 b- defN 23-May-12 17:03 gpt4all/llmodel_DO_NOT_MODIFY/ggml-cuda.h
+-rw-rw-rw-  2.0 fat      663 b- defN 23-May-12 17:03 gpt4all/llmodel_DO_NOT_MODIFY/ggml-opencl.h
+-rw-rw-rw-  2.0 fat    30753 b- defN 23-May-12 17:03 gpt4all/llmodel_DO_NOT_MODIFY/ggml.h
+-rw-rw-rw-  2.0 fat     1006 b- defN 23-May-12 16:59 gpt4all/llmodel_DO_NOT_MODIFY/gptj.h
+-rw-rw-rw-  2.0 fat       10 b- defN 23-May-12 17:03 gpt4all/llmodel_DO_NOT_MODIFY/llama.h
+-rw-rw-rw-  2.0 fat    12387 b- defN 23-May-12 17:03 gpt4all/llmodel_DO_NOT_MODIFY/llama_util.h
+-rw-rw-rw-  2.0 fat     1042 b- defN 23-May-12 16:59 gpt4all/llmodel_DO_NOT_MODIFY/llamamodel.h
+-rw-rw-rw-  2.0 fat     1800 b- defN 23-May-12 16:59 gpt4all/llmodel_DO_NOT_MODIFY/llmodel.h
+-rw-rw-rw-  2.0 fat     6149 b- defN 23-May-12 16:59 gpt4all/llmodel_DO_NOT_MODIFY/llmodel_c.h
+-rw-rw-rw-  2.0 fat      998 b- defN 23-May-12 16:59 gpt4all/llmodel_DO_NOT_MODIFY/mpt.h
+-rw-rw-rw-  2.0 fat     2410 b- defN 23-May-12 16:59 gpt4all/llmodel_DO_NOT_MODIFY/utils.h
 -rw-rw-rw-  2.0 fat    32768 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libatomic-1.dll
 -rw-rw-rw-  2.0 fat   101376 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libgcc_s_seh-1.dll
 -rw-rw-rw-  2.0 fat  3094016 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libgfortran-5.dll
 -rw-rw-rw-  2.0 fat   258048 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libgomp-1.dll
--rw-rw-rw-  2.0 fat   459057 b- defN 23-May-11 15:09 gpt4all/llmodel_DO_NOT_MODIFY/build/libllama.dll
--rw-rw-rw-  2.0 fat   582869 b- defN 23-May-11 15:09 gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.dll
+-rw-rw-rw-  2.0 fat   459057 b- defN 23-May-12 17:04 gpt4all/llmodel_DO_NOT_MODIFY/build/libllama.dll
+-rw-rw-rw-  2.0 fat   582869 b- defN 23-May-12 17:04 gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.dll
 -rw-rw-rw-  2.0 fat   365568 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libquadmath-0.dll
 -rw-rw-rw-  2.0 fat    16896 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libssp-0.dll
 -rw-rw-rw-  2.0 fat  2020352 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libstdc++-6.dll
 -rw-rw-rw-  2.0 fat    54784 b- defN 22-Dec-08 23:38 gpt4all/llmodel_DO_NOT_MODIFY/build/libwinpthread-1.dll
--rw-rw-rw-  2.0 fat     1072 b- defN 23-May-11 15:09 gpt4all-0.2.0.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat      733 b- defN 23-May-11 15:09 gpt4all-0.2.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       98 b- defN 23-May-11 15:09 gpt4all-0.2.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-May-11 15:09 gpt4all-0.2.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2865 b- defN 23-May-11 15:09 gpt4all-0.2.0.dist-info/RECORD
-30 files, 7073351 bytes uncompressed, 2555895 bytes compressed:  63.9%
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-12 16:59 tests/__init__.py
+-rw-rw-rw-  2.0 fat     1863 b- defN 23-May-12 16:59 tests/test_gpt4all.py
+-rw-rw-rw-  2.0 fat     1646 b- defN 23-May-12 16:59 tests/test_pyllmodel.py
+-rw-rw-rw-  2.0 fat     1072 b- defN 23-May-12 17:05 gpt4all-0.2.1.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat      733 b- defN 23-May-12 17:05 gpt4all-0.2.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       98 b- defN 23-May-12 17:05 gpt4all-0.2.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-12 17:05 gpt4all-0.2.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3095 b- defN 23-May-12 17:05 gpt4all-0.2.1.dist-info/RECORD
+33 files, 7078009 bytes uncompressed, 2557150 bytes compressed:  63.9%
```

## zipnote {}

```diff
@@ -69,23 +69,32 @@
 
 Filename: gpt4all/llmodel_DO_NOT_MODIFY/build/libstdc++-6.dll
 Comment: 
 
 Filename: gpt4all/llmodel_DO_NOT_MODIFY/build/libwinpthread-1.dll
 Comment: 
 
-Filename: gpt4all-0.2.0.dist-info/LICENSE.txt
+Filename: tests/__init__.py
 Comment: 
 
-Filename: gpt4all-0.2.0.dist-info/METADATA
+Filename: tests/test_gpt4all.py
 Comment: 
 
-Filename: gpt4all-0.2.0.dist-info/WHEEL
+Filename: tests/test_pyllmodel.py
 Comment: 
 
-Filename: gpt4all-0.2.0.dist-info/top_level.txt
+Filename: gpt4all-0.2.1.dist-info/LICENSE.txt
 Comment: 
 
-Filename: gpt4all-0.2.0.dist-info/RECORD
+Filename: gpt4all-0.2.1.dist-info/METADATA
+Comment: 
+
+Filename: gpt4all-0.2.1.dist-info/WHEEL
+Comment: 
+
+Filename: gpt4all-0.2.1.dist-info/top_level.txt
+Comment: 
+
+Filename: gpt4all-0.2.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gpt4all/gpt4all.py

```diff
@@ -25,15 +25,15 @@
         """
         Constructor
 
         Args:
             model_name: Name of GPT4All or custom model. Including ".bin" file extension is optional but encouraged.
             model_path: Path to directory containing model file or, if file does not exist, where to download model.
                 Default is None, in which case models will be stored in `~/.cache/gpt4all/`.
-            model_type: Model architecture to use - currently, only options are 'llama' or 'gptj'. Only required if model
+            model_type: Model architecture to use - currently, options are 'llama', 'gptj', or 'mpt'. Only required if model
                 is custom. Note that these models still must be built from llama.cpp or GPTJ ggml architecture.
                 Default is None.
             allow_download: Allow API to download models from gpt4all.io. Default is True. 
         """
         self.model = None
 
         # Model type provided for when model is custom
@@ -165,15 +165,16 @@
         """
         return self.model.generate(prompt, **generate_kwargs)
     
     def chat_completion(self, 
                         messages: List[Dict], 
                         default_prompt_header: bool = True, 
                         default_prompt_footer: bool = True, 
-                        verbose: bool = True) -> str:
+                        verbose: bool = True,
+                        **generate_kwargs) -> str:
         """
         Format list of message dictionaries into a prompt and call model
         generate on prompt. Returns a response dictionary with metadata and
         generated content.
 
         Args:
             messages: List of dictionaries. Each dictionary should have a "role" key
@@ -181,14 +182,15 @@
                 string value. Messages are organized such that "system" messages are at top of prompt,
                 and "user" and "assistant" messages are displayed in order. Assistant messages get formatted as
                 "Reponse: {content}". 
             default_prompt_header: If True (default), add default prompt header after any system role messages and
                 before user/assistant role messages.
             default_prompt_footer: If True (default), add default footer at end of prompt.
             verbose: If True (default), print full prompt and generated response.
+            **generate_kwargs: Optional kwargs to pass to prompt context.
 
         Returns:
             Response dictionary with:  
                 "model": name of model.  
                 "usage": a dictionary with number of full prompt tokens, number of 
                     generated tokens in response, and total tokens.  
                 "choices": List of message dictionary where "content" is generated response and "role" is set
@@ -197,15 +199,15 @@
        
         full_prompt = self._build_prompt(messages, 
                                         default_prompt_header=default_prompt_header, 
                                         default_prompt_footer=default_prompt_footer)
         if verbose:
             print(full_prompt)
 
-        response = self.model.generate(full_prompt)
+        response = self.model.generate(full_prompt, **generate_kwargs)
 
         if verbose:
             print(response)
 
         response_dict = {
             "model": self.model.model_name,
             "usage": {"prompt_tokens": len(full_prompt), 
@@ -259,14 +261,16 @@
         # This needs to be updated for each new model type
         # TODO: Might be worth converting model_type to enum
 
         if model_type == "gptj":
             return pyllmodel.GPTJModel()
         elif model_type == "llama":
             return pyllmodel.LlamaModel()
+        elif model_type == "mpt":
+            return pyllmodel.MPTModel()
         else:
             raise ValueError(f"No corresponding model for model_type: {model_type}")
         
     @staticmethod
     def get_model_from_name(model_name: str) -> pyllmodel.LLModel:
         # This needs to be updated for each new model
 
@@ -282,19 +286,28 @@
         ]
 
         LLAMA_MODELS = [
             "ggml-gpt4all-l13b-snoozy.bin",
             "ggml-vicuna-7b-1.1-q4_2.bin",
             "ggml-vicuna-13b-1.1-q4_2.bin",
             "ggml-wizardLM-7B.q4_2.bin",
-            "ggml-stable-vicuna-13B.q4_2.bin"
+            "ggml-stable-vicuna-13B.q4_2.bin",
+            "ggml-nous-gpt4-vicuna-13b.bin"
+        ]
+
+        MPT_MODELS = [
+            "ggml-mpt-7b-base.bin",
+            "ggml-mpt-7b-chat.bin",
+            "ggml-mpt-7b-instruct.bin"
         ]
 
         if model_name in GPTJ_MODELS:
             return pyllmodel.GPTJModel()
         elif model_name in LLAMA_MODELS:
             return pyllmodel.LlamaModel()
+        elif model_name in MPT_MODELS:
+            return pyllmodel.MPTModel()
         else:
             err_msg = f"""No corresponding model for provided filename {model_name}.
             If this is a custom model, make sure to specify a valid model_type.
             """
             raise ValueError(err_msg)
```

## gpt4all/pyllmodel.py

```diff
@@ -42,14 +42,17 @@
 llmodel, llama = load_llmodel_library()
 
 # Define C function signatures using ctypes
 llmodel.llmodel_gptj_create.restype = ctypes.c_void_p
 llmodel.llmodel_gptj_destroy.argtypes = [ctypes.c_void_p]
 llmodel.llmodel_llama_create.restype = ctypes.c_void_p
 llmodel.llmodel_llama_destroy.argtypes = [ctypes.c_void_p]
+llmodel.llmodel_mpt_create.restype = ctypes.c_void_p
+llmodel.llmodel_mpt_destroy.argtypes = [ctypes.c_void_p]
+
 
 llmodel.llmodel_loadModel.argtypes = [ctypes.c_void_p, ctypes.c_char_p]
 llmodel.llmodel_loadModel.restype = ctypes.c_bool
 llmodel.llmodel_isModelLoaded.argtypes = [ctypes.c_void_p]
 llmodel.llmodel_isModelLoaded.restype = ctypes.c_bool
 
 class LLModelPromptContext(ctypes.Structure):
@@ -232,7 +235,21 @@
         super().__init__()
         self.model = llmodel.llmodel_llama_create()
 
     def __del__(self):
         if self.model is not None:
             llmodel.llmodel_llama_destroy(self.model)
         super().__del__()
+
+
+class MPTModel(LLModel):
+
+    model_type = "mpt"
+
+    def __init__(self):
+        super().__init__()
+        self.model = llmodel.llmodel_mpt_create()
+
+    def __del__(self):
+        if self.model is not None:
+            llmodel.llmodel_mpt_destroy(self.model)
+        super().__del__()
```

## gpt4all/llmodel_DO_NOT_MODIFY/build/libllama.dll

### objdump

```diff
@@ -5,15 +5,15 @@
 
 Characteristics 0x2026
 	executable
 	line numbers stripped
 	large address aware
 	DLL
 
-Time/Date		Thu May 11 15:09:04 2023
+Time/Date		Fri May 12 17:04:31 2023
 Magic			020b	(PE32+)
 MajorLinkerVersion	2
 MinorLinkerVersion	39
 SizeOfCode		0000000000043e00
 SizeOfInitializedData	0000000000052000
 SizeOfUninitializedData	00000000000a2600
 AddressOfEntryPoint	0000000000001320
@@ -26,15 +26,15 @@
 MajorImageVersion	0
 MinorImageVersion	0
 MajorSubsystemVersion	5
 MinorSubsystemVersion	2
 Win32Version		00000000
 SizeOfImage		00103000
 SizeOfHeaders		00000600
-CheckSum		00079424
+CheckSum		00076d46
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00000160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 SizeOfStackReserve	0000000000200000
 SizeOfStackCommit	0000000000001000
@@ -307,15 +307,15 @@
  000f712c	00000000 00000000 00000000 00000000 00000000
 
 There is an export table in .edata at 0x33bbd3000
 
 The Export Tables (interpreted .edata section contents)
 
 Export Flags 			0
-Time/Date stamp 		645d050f
+Time/Date stamp 		645e719f
 Major/Minor 			0/0
 Name 				00000000000f39d8 libllama.dll
 Ordinal Base 			1
 Number in:
 	Export Address Table 		000000f8
 	[Name Pointer/Ordinal] Table	000000f8
 Table Addresses
@@ -88333,16 +88333,16 @@
 	...
 
 Disassembly of section .edata:
 
 000000033bbd3000 <.edata>:
    33bbd3000:	add    %al,(%rax)
    33bbd3002:	add    %al,(%rax)
-   33bbd3004:	syscall
-   33bbd3006:	pop    %rbp
+   33bbd3004:	lahf
+   33bbd3005:	jno    33bbd3065 <.edata+0x65>
    33bbd3007:	add    %al,%fs:(%rax)
    33bbd300a:	add    %al,(%rax)
    33bbd300c:	fdivrs (%rcx)
    33bbd300e:	sldt   (%rcx)
    33bbd3011:	add    %al,(%rax)
    33bbd3013:	add    %bh,%al
    33bbd3015:	add    %al,(%rax)
```

## gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.dll

### objdump

```diff
@@ -5,15 +5,15 @@
 
 Characteristics 0x2026
 	executable
 	line numbers stripped
 	large address aware
 	DLL
 
-Time/Date		Thu May 11 15:09:26 2023
+Time/Date		Fri May 12 17:04:55 2023
 Magic			020b	(PE32+)
 MajorLinkerVersion	2
 MinorLinkerVersion	39
 SizeOfCode		0000000000040e00
 SizeOfInitializedData	0000000000057000
 SizeOfUninitializedData	0000000000000e00
 AddressOfEntryPoint	0000000000001320
@@ -26,15 +26,15 @@
 MajorImageVersion	0
 MinorImageVersion	1
 MajorSubsystemVersion	5
 MinorSubsystemVersion	2
 Win32Version		00000000
 SizeOfImage		00065000
 SizeOfHeaders		00000600
-CheckSum		00092e54
+CheckSum		00090779
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00000160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 SizeOfStackReserve	0000000000200000
 SizeOfStackCommit	0000000000001000
@@ -361,15 +361,15 @@
  0005912c	00000000 00000000 00000000 00000000 00000000
 
 There is an export table in .edata at 0x27a830000
 
 The Export Tables (interpreted .edata section contents)
 
 Export Flags 			0
-Time/Date stamp 		645d0526
+Time/Date stamp 		645e71b7
 Major/Minor 			0/0
 Name 				0000000000050e88 libllmodel.dll
 Ordinal Base 			1
 Number in:
 	Export Address Table 		00000170
 	[Name Pointer/Ordinal] Table	00000170
 Table Addresses
@@ -92472,15 +92472,17 @@
 	...
 
 Disassembly of section .edata:
 
 000000027a830000 <.edata>:
    27a830000:	add    %al,(%rax)
    27a830002:	add    %al,(%rax)
-   27a830004:	es add $0x645d,%eax
+   27a830004:	mov    $0x71,%bh
+   27a830006:	pop    %rsi
+   27a830007:	add    %al,%fs:(%rax)
    27a83000a:	add    %al,(%rax)
    27a83000c:	mov    %cl,(%rsi)
    27a83000e:	add    $0x100,%eax
    27a830013:	add    %dh,0x1(%rax)
    27a830016:	add    %al,(%rax)
    27a830018:	jo     27a83001b <.edata+0x1b>
    27a83001a:	add    %al,(%rax)
```

## Comparing `gpt4all-0.2.0.dist-info/LICENSE.txt` & `gpt4all-0.2.1.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `gpt4all-0.2.0.dist-info/METADATA` & `gpt4all-0.2.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gpt4all
-Version: 0.2.0
+Version: 0.2.1
 Summary: Python bindings for GPT4All
 Home-page: https://pypi.org/project/gpt4all/
 Author: Richard Guo
 Author-email: richard@nomic.ai
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
```

## Comparing `gpt4all-0.2.0.dist-info/RECORD` & `gpt4all-0.2.1.dist-info/RECORD`

 * *Files 27% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 gpt4all/__init__.py,sha256=IDrxvYTkNSgtIqLWMXngQFo6_cDseiYtJsTBeFvyObE,76
-gpt4all/gpt4all.py,sha256=xf_NZMBdnnjSg1NC-GJMkDB2uX-2NENZXnLo8249rMA,12007
-gpt4all/pyllmodel.py,sha256=fcJPlDoYWBSfJz68mrF503Sammtp-lTBCjwB6SwWfRI,7686
+gpt4all/gpt4all.py,sha256=Bp-EJc_AzwBMGgO-BnGifXLPDsizIj41gaEm7KGJg9E,12502
+gpt4all/pyllmodel.py,sha256=WkqnYCTeLdBPzsnxf-Oybb0NEh8J2ZdCHEIL7f-S5vY,8104
 gpt4all/llmodel_DO_NOT_MODIFY/common.h,sha256=xteYRmx4XOBqGE2N9GWAPbh1yDUOnCUcuzp-pQRbwtc,3476
 gpt4all/llmodel_DO_NOT_MODIFY/ggml-cuda.h,sha256=Iq-RXqLTxSHN8zoER_5jq6z_QU1BDr-nXS0C7zR3SuM,2378
 gpt4all/llmodel_DO_NOT_MODIFY/ggml-opencl.h,sha256=swYxebde1ubN1ypPHEz3Mylg2fBx1-z1-O5sxRPDywg,663
 gpt4all/llmodel_DO_NOT_MODIFY/ggml.h,sha256=0W3BxABlPfN6gbJsE-y17VbOM0S9fcYDGoscSbh3xXQ,30753
 gpt4all/llmodel_DO_NOT_MODIFY/gptj.h,sha256=HUDLf-trpUBEI4uyUgcvDxgHBAc04I01Xd_cLPAzw0I,1006
 gpt4all/llmodel_DO_NOT_MODIFY/llama.h,sha256=Og0Uewryq816DPuax4d2yl-AY3GxLd8u8NiGH0RT1Kc,10
 gpt4all/llmodel_DO_NOT_MODIFY/llama_util.h,sha256=xXXQ9UPSAnYi-RdrfZNVMLgqDwj2Pm-n1TJECNHGCRA,12387
@@ -13,18 +13,21 @@
 gpt4all/llmodel_DO_NOT_MODIFY/llmodel_c.h,sha256=r75qwTh0iwlohhtfVgv0NBcab7bM-m_rNE9kSt4dnYs,6149
 gpt4all/llmodel_DO_NOT_MODIFY/mpt.h,sha256=WJfRnHo-_Q7XTZMCepxuzsbV6q3F7ptp32tfggiFlXg,998
 gpt4all/llmodel_DO_NOT_MODIFY/utils.h,sha256=PTciHcu-d1DsxQD4GCDiQh3JHugAKe0z-6BTt6iyLo0,2410
 gpt4all/llmodel_DO_NOT_MODIFY/build/libatomic-1.dll,sha256=kG4efzc0PcVV1ReTxXPiezODs1eCmYfQo6wEQXYE7Pw,32768
 gpt4all/llmodel_DO_NOT_MODIFY/build/libgcc_s_seh-1.dll,sha256=1tpM-hmVu1L0DawZCrNtsdD1017zD3iDIb4NnEIWYBg,101376
 gpt4all/llmodel_DO_NOT_MODIFY/build/libgfortran-5.dll,sha256=GsInws9hv37x9nWWosrNIBCD1ZnNFxdwfRGsI276Z-k,3094016
 gpt4all/llmodel_DO_NOT_MODIFY/build/libgomp-1.dll,sha256=tmxzZP1_KK79N2oKVtnp2bzXiB50EAxJhmr1yEh6Izw,258048
-gpt4all/llmodel_DO_NOT_MODIFY/build/libllama.dll,sha256=4dNXiVHeGbo25qqm95NENbm6RWaXu4UsCPHGl997Gaw,459057
-gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.dll,sha256=nixKyjHkJCM2aX4fCs89L8zPM6gKUrWA-aDQ0qvFWXg,582869
+gpt4all/llmodel_DO_NOT_MODIFY/build/libllama.dll,sha256=7MCEaeboWA6oHEYW-imx5TxR6A2CjFTGKRIvPepHp1Q,459057
+gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.dll,sha256=89ejKnocYJ3ef4IOLqQI8xHcKrhBdGzvil0IptWclKw,582869
 gpt4all/llmodel_DO_NOT_MODIFY/build/libquadmath-0.dll,sha256=TT8L_8y-hYDCGhTm-ZKqKaWpKoq_lSSYg3H2e245yg8,365568
 gpt4all/llmodel_DO_NOT_MODIFY/build/libssp-0.dll,sha256=aACD5i4hyuzm5OisC6oLapaNPcuvzc9Obv_s-H6ShpQ,16896
 gpt4all/llmodel_DO_NOT_MODIFY/build/libstdc++-6.dll,sha256=EqU3yFBFTMN7GESspSYuaXq0r_tZsiNvXVoxfzXfsqI,2020352
 gpt4all/llmodel_DO_NOT_MODIFY/build/libwinpthread-1.dll,sha256=4BuOhf1nwrhh9k1MzH32B1m1iapBAACij6K4SiCJF1s,54784
-gpt4all-0.2.0.dist-info/LICENSE.txt,sha256=h3C3VURkUfyxCO-FBZgvCN9Hdf58UQq3PbeplPwEh8M,1072
-gpt4all-0.2.0.dist-info/METADATA,sha256=PZdaeZakXAg1AFsfNdfbEWKzy2m8p9M5uF0qzU80efw,733
-gpt4all-0.2.0.dist-info/WHEEL,sha256=bC8mYJUOJCh5KnyEeT6W_BCQYi3v39D3z64Vy_sFvVg,98
-gpt4all-0.2.0.dist-info/top_level.txt,sha256=Y67V_Sz17wbSZvGD6vn6RvzHVJydnRMiorOh-Dqr9rs,8
-gpt4all-0.2.0.dist-info/RECORD,,
+tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+tests/test_gpt4all.py,sha256=dVHraSO0cgplIfnc2Vbj0c1FxgjrEyjhs61A3BNvRYA,1863
+tests/test_pyllmodel.py,sha256=A4Kfi4U4n0YzPTsxqyyOTjeHFnxQcchNqMDWUJCxdvE,1646
+gpt4all-0.2.1.dist-info/LICENSE.txt,sha256=h3C3VURkUfyxCO-FBZgvCN9Hdf58UQq3PbeplPwEh8M,1072
+gpt4all-0.2.1.dist-info/METADATA,sha256=nF_yNOp5iBiONPk_0MAf6kTHwIX-JKf26RYnMLfUIdE,733
+gpt4all-0.2.1.dist-info/WHEEL,sha256=bC8mYJUOJCh5KnyEeT6W_BCQYi3v39D3z64Vy_sFvVg,98
+gpt4all-0.2.1.dist-info/top_level.txt,sha256=triCUv7cdvSMda7P6l7vXZqwelK-Z1KqA8LJ7qdqing,14
+gpt4all-0.2.1.dist-info/RECORD,,
```

